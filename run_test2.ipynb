{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.genfromtxt(\"data/x_train.csv\", delimiter=\",\", skip_header=1)\n",
    "features = np.genfromtxt(\"data/x_train.csv\", delimiter=\",\", dtype=str, max_rows=1)\n",
    "y_train = np.genfromtxt(\"data/y_train.csv\", delimiter=\",\", skip_header=1)\n",
    "y_features = np.genfromtxt(\"data/y_train.csv\", delimiter=\",\", dtype=str, max_rows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.genfromtxt(\"data/x_test.csv\",delimiter=\",\", skip_header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(array, range_min, range_max, n_bins):\n",
    "    # Filter array to include only values within the specified range\n",
    "    filtered_values = array[(array >= range_min) & (array <= range_max)]\n",
    "    \n",
    "    # Calculate the bin edges using quantiles\n",
    "    bin_edges = np.quantile(filtered_values, np.linspace(0, 1, n_bins + 1))\n",
    "    \n",
    "    def assign_bin(value):\n",
    "        # Check if the value is NaN\n",
    "        if np.isnan(value):\n",
    "            return -1\n",
    "        \n",
    "        # If the value is outside the range, return it as is\n",
    "        if value < range_min or value > range_max:\n",
    "            return value\n",
    "        \n",
    "        # Assign bin based on which range the value falls into\n",
    "        # We use right=True to ensure that values exactly equal to range_max are included in the last bin\n",
    "        return np.digitize(value, bin_edges, right=True)\n",
    "    \n",
    "    return assign_bin\n",
    "\n",
    "mapping_dict = {\n",
    "    \"GENHLTH\": lambda value: value if value <= 9 else -1,\n",
    "    \"PHYSHLTH\": to_categorical(array=x_train[:, features==\"PHYSHLTH\"].flatten(), range_min=0, range_max=30, n_bins=4),\n",
    "    \"MENTHLTH\": to_categorical(array=x_train[:, features==\"MENTHLTH\"].flatten(), range_min=0, range_max=30, n_bins=4),\n",
    "    \"POORHLTH\": to_categorical(array=x_train[:, features==\"POORHLTH\"].flatten(), range_min=0, range_max=30, n_bins=4),\n",
    "    \"HLTHPLN1\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"MEDCOST\": lambda value: value if value <= 7 else -1,\n",
    "    \"CHECKUP1\": lambda value: value if value <= 8 else -1,\n",
    "    \"BPHIGH4\": lambda value: value if value <= 7 else -1,\n",
    "    \"BPMEDS\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"BLOODCHO\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"CHOLCHK\": lambda value: value if not np.isnan(value) else -1,\n",
    "    # \"CVDINFR4\": lambda value: 1 if value == 1 else 0,\n",
    "    # \"CVDCRHD4\": lambda value: 1 if value == 1 else 0,\n",
    "    \"TOLDHI2\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"CVDSTRK3\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"ASTHMA3\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"ASTHNOW\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"CHCSCNCR\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"CHCOCNCR\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"CHCCOPD1\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"HAVARTH3\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"ADDEPEV2\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"CHCKIDNY\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"DIABETE3\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"SEX\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"MARITAL\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"EDUCA\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"VETERAN3\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"INCOME2\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"INTERNET\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"WTKG3\": to_categorical(array=x_train[:, features==\"WTKG3\"].flatten(), range_min=23, range_max=295, n_bins=6),\n",
    "    \"QLACTLM2\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"USEEQUIP\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"BLIND\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"DECIDE\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"DIFFWALK\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"DIFFDRES\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"DIFFALON\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"SMOKE100\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"SMOKDAY2\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"LASTSMK2\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"USENOW3\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"AVEDRNK2\": to_categorical(array=x_train[:, features==\"AVEDRNK2\"].flatten(), range_min=1, range_max=76, n_bins=5),\n",
    "    \"DRNK3GE5\": to_categorical(array=x_train[:, features==\"DRNK3GE5\"].flatten(), range_min=1, range_max=76, n_bins=5),\n",
    "    \"EXERANY2\": lambda value : value if not np.isnan(value) else -1,\n",
    "    # \"EXERHMM1\": lambda value: str(value//200) if value <= 959 and value not in [777,999] else -1,\n",
    "    \"LMTJOIN3\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"FLUSHOT6\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"PDIABTST\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"PREDIAB1\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"INSULIN\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"CIMEMLOS\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_RFHLTH\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_HCVU651\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_RFHYPE5\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_CHOLCHK\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_RFCHOL\": lambda value : value if not np.isnan(value) else -1,\n",
    "    # \"_MICHD\": lambda value: value if value <= 2 else -1,\n",
    "    \"_LTASTH1\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_CASTHM1\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_DRDXAR1\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_AGEG5YR\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_AGE_G\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"HTM4\": to_categorical(array=x_train[:, features==\"HTM4\"].flatten(), range_min=0.91, range_max=2.44, n_bins=6),\n",
    "    \"_RFBMI5\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_EDUCAG\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_SMOKER3\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_RFBING5\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_BMI5CAT\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_RFDRHV5\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"FTJUDA1_\": to_categorical(array=x_train[:, features==\"FTJUDA1_\"].flatten(), range_min=0, range_max=99.99, n_bins=4),\n",
    "    \"MAXVO2_\": to_categorical(array=x_train[:, features==\"MAXVO2_\"].flatten(), range_min=0, range_max=50.1, n_bins=6),\n",
    "    \"ACTIN11_\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"ACTIN21_\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_PACAT1\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_PA150R2\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_PA300R2\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_PASTRNG\":  lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_PASTAE1\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_LMTACT1\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_LMTWRK1\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_LMTSCL1\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_INCOMG\": lambda value : value if not np.isnan(value) else -1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def select_features_with_low_nan_ratio(x_train, features_to_check, threshold=0.1):\n",
    "    nan_ratios = {}\n",
    "    for feature in features_to_check:\n",
    "        nan_ratios[feature] = np.sum(np.isnan(x_train[:, features == feature])) / len(x_train)\n",
    "\n",
    "    selected_features = [feature for feature in nan_ratios if nan_ratios[feature] < threshold]\n",
    "\n",
    "    # print(f\"Selected {len(selected_features)} features over {len(features_to_check)}\")\n",
    "    # print(nan_ratios)\n",
    "    return selected_features\n",
    "\n",
    "def apply_mapping(x_train, selected_features, mapping_dict):\n",
    "    x_train_filtered = np.zeros((x_train.shape[0], len(selected_features)))\n",
    "    for feature in selected_features:\n",
    "        feature_values = x_train[:, features == feature].flatten()\n",
    "        if feature_values.size > 0:\n",
    "            x_train_filtered[:, selected_features.index(feature)] = np.array([mapping_dict[feature](value) for value in feature_values])\n",
    "    return x_train_filtered\n",
    "\n",
    "def fix_class_imbalance(X, y, target_value=1, dont_balance=False):\n",
    "    \"\"\"\n",
    "    Fix class imbalance by oversampling the minority class or undersampling the majority class.\n",
    "    \n",
    "    Parameters:\n",
    "    X (numpy.ndarray): Feature matrix of shape (n_samples, n_features)\n",
    "    y (numpy.ndarray): Target vector of shape (n_samples,), containing values -1 and 1\n",
    "    target_value (int): Class value to balance to (default is 1)\n",
    "    \n",
    "    Returns:\n",
    "    X_balanced (numpy.ndarray): Feature matrix with balanced classes\n",
    "    y_balanced (numpy.ndarray): Balanced target vector\n",
    "    \"\"\"\n",
    "    if dont_balance:\n",
    "        return X, y\n",
    "\n",
    "    # Separate samples by class\n",
    "    class_1_indices = np.where(y == target_value)[0]\n",
    "    class_minus_1_indices = np.where(y != target_value)[0]\n",
    "    \n",
    "    # Find class counts\n",
    "    class_1_count = len(class_1_indices)\n",
    "    class_minus_1_count = len(class_minus_1_indices)\n",
    "    \n",
    "    if class_1_count == class_minus_1_count:\n",
    "        # If classes are already balanced, return the original data\n",
    "        return X, y\n",
    "    \n",
    "    elif class_1_count < class_minus_1_count:\n",
    "        # If class 1 is the minority, oversample class 1\n",
    "        oversample_size = class_minus_1_count - class_1_count\n",
    "        oversampled_indices = np.random.choice(class_1_indices, oversample_size, replace=True)\n",
    "        new_indices = np.concatenate([np.arange(len(y)), oversampled_indices])\n",
    "    else:\n",
    "        # If class -1 is the minority, oversample class -1\n",
    "        oversample_size = class_1_count - class_minus_1_count\n",
    "        oversampled_indices = np.random.choice(class_minus_1_indices, oversample_size, replace=True)\n",
    "        new_indices = np.concatenate([np.arange(len(y)), oversampled_indices])\n",
    "    \n",
    "    # Create the balanced dataset\n",
    "    X_balanced = X[new_indices]\n",
    "    y_balanced = y[new_indices]\n",
    "    \n",
    "    return X_balanced, y_balanced\n",
    "\n",
    "# calculate the accuracy, precision, recall and F1 score\n",
    "def accuracy_precision_recall_f1(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    accuracy = (tp + tn) / len(y_true)\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def split_data(x, y, ratio=0.8):\n",
    "    indices = np.random.permutation(x.shape[0])\n",
    "    train_indices = indices[:int(ratio * x.shape[0])]\n",
    "    test_indices = indices[int(ratio * x.shape[0]):]\n",
    "    return x[train_indices], y[train_indices], x[test_indices], y[test_indices]\n",
    "\n",
    "def split_data_k_folds(x, y, n_folds=5):\n",
    "    # Shuffle the data\n",
    "    indices = np.random.permutation(x.shape[0])\n",
    "    \n",
    "    # Split indices into n equal-sized parts\n",
    "    fold_sizes = np.full(n_folds, x.shape[0] // n_folds, dtype=int)  # Base size of each fold\n",
    "    fold_sizes[:x.shape[0] % n_folds] += 1  # Distribute the remainder\n",
    "\n",
    "    current = 0\n",
    "    folds = []\n",
    "    for fold_size in fold_sizes:\n",
    "        start, stop = current, current + fold_size\n",
    "        test_indices = indices[start:stop]  # Select current fold as test set\n",
    "        train_indices = np.concatenate([indices[:start], indices[stop:]])  # Rest are training\n",
    "        \n",
    "        x_train, y_train = x[train_indices], y[train_indices]\n",
    "        x_test, y_test = x[test_indices], y[test_indices]\n",
    "        folds.append((x_train, y_train, x_test, y_test))\n",
    "        \n",
    "        current = stop\n",
    "\n",
    "    return folds\n",
    "\n",
    "def cleaning_x_pipeline(x_train, y_train, x_test, features, n_folds=5, dont_balance=False):\n",
    "    np.random.seed(41)\n",
    "    # keep only features with less than 10% nan values\n",
    "    # selected_features = select_features_with_low_nan_ratio(x_train, features, threshold=0.1)\n",
    "    # keep all features\n",
    "    selected_features = select_features_with_low_nan_ratio(x_train, features, threshold=1)\n",
    "    # cleaning\n",
    "    x_train_filtered_mapped = apply_mapping(x_train, selected_features, mapping_dict)\n",
    "    x_test_filtered_mapped = apply_mapping(x_test, selected_features, mapping_dict)\n",
    "\n",
    "    # Label encoding\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    combined = np.vstack((x_train_filtered_mapped, x_test_filtered_mapped))\n",
    "    combined_encoded = np.apply_along_axis(le.fit_transform, 0, combined)\n",
    "    x_train_encoded = combined_encoded[:x_train_filtered_mapped.shape[0], :]\n",
    "    x_test_encoded = combined_encoded[x_train_filtered_mapped.shape[0]:, :]\n",
    "\n",
    "    if n_folds==0:\n",
    "        # fix class imbalance in the training set\n",
    "        x_train_encoded_fixed, y_train_fixed = fix_class_imbalance(x_train_encoded, y_train, target_value=1, dont_balance=dont_balance)\n",
    "\n",
    "        return x_train_encoded, x_train_encoded_fixed, y_train_fixed, x_test_encoded\n",
    "    else:\n",
    "        # split the data into k folds\n",
    "        folds = split_data_k_folds(x_train_encoded, y_train, n_folds=n_folds)\n",
    "        balanced_folds = []\n",
    "\n",
    "        for x_train_fold, y_train_fold, x_test_fold, y_test_fold in folds:\n",
    "            # fix class imbalance in the training set\n",
    "            x_train_fold_fixed, y_train_fold_fixed = fix_class_imbalance(x_train_fold, y_train_fold, target_value=1, dont_balance=dont_balance)\n",
    "            balanced_folds.append((x_train_fold, x_train_fold_fixed, y_train_fold, y_train_fold_fixed, x_test_fold, y_test_fold))\n",
    "\n",
    "        return balanced_folds, x_test_encoded\n",
    "    \n",
    "def evaluate_model(x_train, y_train, x_test, final_features, dont_balance=False, n_folds=5, model=CategoricalNB()):\n",
    "    y_train_mapped = (1 + y_train[:, 1]) / 2\n",
    "    balanced_folds, x_test_encoded = cleaning_x_pipeline(x_train, y_train_mapped, x_test, final_features, n_folds=n_folds, dont_balance=dont_balance)\n",
    "\n",
    "    # Initialize array metrics of size n_folds*4\n",
    "    metrics_train = np.zeros((n_folds, 4))\n",
    "    metrics_train_fixed = np.zeros((n_folds, 4))\n",
    "    metrics_test = np.zeros((n_folds, 4))\n",
    "\n",
    "    for i in range(len(balanced_folds)):\n",
    "        x_train_fold, x_train_fold_fixed, y_train_fold, y_train_fold_fixed, x_test_fold, y_test_fold = balanced_folds[i]\n",
    "\n",
    "        model.fit(x_train_fold_fixed, y_train_fold_fixed)\n",
    "\n",
    "        # Predict on the train, train_fixed and test set\n",
    "        y_train_pred = model.predict(x_train_fold)\n",
    "        y_train_fixed_pred = model.predict(x_train_fold_fixed)\n",
    "        y_test_pred = model.predict(x_test_fold)\n",
    "\n",
    "        # Calculate the accuracy, precision, recall and F1 score\n",
    "        metrics_train[i] = accuracy_precision_recall_f1(y_train_fold, y_train_pred)\n",
    "        metrics_train_fixed[i] = accuracy_precision_recall_f1(y_train_fold_fixed, y_train_fixed_pred)\n",
    "        metrics_test[i] = accuracy_precision_recall_f1(y_test_fold, y_test_pred)\n",
    "\n",
    "    # take the average\n",
    "    metrics_train = np.mean(metrics_train, axis=0)\n",
    "    metrics_train_fixed = np.mean(metrics_train_fixed, axis=0)\n",
    "    metrics_test = np.mean(metrics_test, axis=0)\n",
    "\n",
    "    return metrics_train, metrics_train_fixed, metrics_test\n",
    "\n",
    "def fit_predict_model(x_train, y_train, x_test, final_features, dont_balance=False, model=CategoricalNB()):\n",
    "    y_train_mapped = (1 + y_train[:, 1]) / 2\n",
    "    x_train_encoded, x_train_encoded_fixed, y_train_fixed, x_test_encoded = cleaning_x_pipeline(x_train, y_train_mapped, x_test, final_features, dont_balance=dont_balance, n_folds=0)\n",
    "\n",
    "    model.fit(x_train_encoded_fixed, y_train_fixed)\n",
    "    y_pred = model.predict(x_test_encoded)\n",
    "    y_train_pred = model.predict(x_train_encoded)\n",
    "\n",
    "    # print metrics for the training set\n",
    "    accuracy, precision, recall, f1 = accuracy_precision_recall_f1(y_train_mapped, y_train_pred)\n",
    "    print(f\"Training set: accuracy={accuracy:.2f}, precision={precision:.2f}, recall={recall:.2f}, F1={f1:.5f}\")\n",
    "\n",
    "    return y_pred, y_train_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing by splitting the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Accuracy=0.73, Precision=0.22, Recall=0.76, F1=0.33561\n",
      "Train set fixed: Accuracy=0.75, Precision=0.74, Recall=0.76, F1=0.75126\n",
      "Test set: Accuracy=0.73, Precision=0.21, Recall=0.76, F1=0.33545\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "n_folds = 5\n",
    "final_features = ['_RFHLTH', 'MAXVO2_', 'GENHLTH', 'CVDSTRK3', 'BLOODCHO', 'SEX', 'HLTHPLN1']\n",
    "metrics_train, metrics_train_fixed, metrics_test = evaluate_model(x_train, y_train, x_test, final_features, dont_balance=False, n_folds=n_folds, model=CategoricalNB())\n",
    "\n",
    "print(f\"Train set: Accuracy={metrics_train[0]:.2f}, Precision={metrics_train[1]:.2f}, Recall={metrics_train[2]:.2f}, F1={metrics_train[3]:.5f}\")\n",
    "print(f\"Train set fixed: Accuracy={metrics_train_fixed[0]:.2f}, Precision={metrics_train_fixed[1]:.2f}, Recall={metrics_train_fixed[2]:.2f}, F1={metrics_train_fixed[3]:.5f}\")\n",
    "print(f\"Test set: Accuracy={metrics_test[0]:.2f}, Precision={metrics_test[1]:.2f}, Recall={metrics_test[2]:.2f}, F1={metrics_test[3]:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # greedy algorithm to find the best set of features that maximizes f1_test\n",
    "# # add features greedily one by one until the f1_test stops increasing\n",
    "# from tqdm import tqdm  # Import tqdm for the progress bar\n",
    "\n",
    "# def greedy_feature_selection(x_train, y_train, x_test, features, dont_balance=False):\n",
    "#     features = select_features_with_low_nan_ratio(x_train, features, threshold=0.1)\n",
    "#     n_features = len(features)\n",
    "#     selected_features = []\n",
    "#     remaining_features = features.copy()\n",
    "\n",
    "#     best_f1 = 0\n",
    "#     progress_bar = tqdm(total=n_features, desc=\"Selecting Features\")\n",
    "\n",
    "#     while remaining_features:\n",
    "#         # Track the best feature and F1 score in the current iteration\n",
    "#         best_feature = None\n",
    "#         best_f1_iteration = 0\n",
    "#         # print(remaining_features)\n",
    "#         # Try adding each remaining feature and evaluate F1 score\n",
    "#         for feature in remaining_features:\n",
    "#             current_features = selected_features + [feature]  # Add feature to the selected set\n",
    "#             # print(current_features)\n",
    "\n",
    "#             metrics_train, metrics_train_fixed, metrics_test = evaluate_model(x_train, y_train, x_test, current_features, dont_balance=dont_balance, n_folds=5, model=CategoricalNB())\n",
    "#             f1_test = metrics_test[3]\n",
    "\n",
    "#             # Check if the current F1 score is the best so far\n",
    "#             if f1_test > best_f1_iteration:\n",
    "#                 best_f1_iteration = f1_test\n",
    "#                 best_feature = feature\n",
    "#         # Stop if no improvement is made\n",
    "#         if best_f1_iteration <= best_f1:\n",
    "#             break\n",
    "        \n",
    "#         # Update selected features and remaining features\n",
    "#         selected_features.append(best_feature)\n",
    "#         remaining_features.remove(best_feature)\n",
    "#         best_f1 = best_f1_iteration\n",
    "#         print(f\"Best F1 score: {best_f1}\")\n",
    "#         print(f\"Selected features: {selected_features}\")\n",
    "        \n",
    "#         progress_bar.update(1)  # Update the progress bar\n",
    "        \n",
    "#     progress_bar.close()  # Close the progress bar when done\n",
    "#     return selected_features, best_f1\n",
    "\n",
    "# selected_features, best_f1 = greedy_feature_selection(x_train, y_train, x_test, list(mapping_dict.keys()), dont_balance=False)\n",
    "# print(f\"Selected features: {selected_features}\")\n",
    "\n",
    "# # found ['_RFHLTH', 'MAXVO2_', 'GENHLTH', 'CVDSTRK3', 'BLOODCHO', 'SEX', 'HLTHPLN1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction for the real test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: accuracy=0.78, precision=0.24, recall=0.68, F1=0.35548\n"
     ]
    }
   ],
   "source": [
    "final_features = ['_RFHLTH', 'MAXVO2_', 'GENHLTH', 'CVDSTRK3', 'BLOODCHO', 'SEX', 'HLTHPLN1']\n",
    "y_pred_test, y_train_pred = fit_predict_model(x_train, y_train, x_test, final_features, dont_balance=False, model=CategoricalNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ids = x_test[:,0]\n",
    "y_pred_test_final = 2*y_pred_test-1\n",
    "\n",
    "np.savetxt(\"data/submission_CategoricalNB_4.csv\", np.array([Ids, y_pred_test_final]).T, delimiter=\",\", fmt=\"%d\", header=\"Id,Prediction\", comments=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
