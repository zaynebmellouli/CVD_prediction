{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.genfromtxt(\"data/x_train.csv\", delimiter=\",\", skip_header=1)\n",
    "best_features = np.genfromtxt(\"data/x_train.csv\", delimiter=\",\", dtype=str, max_rows=1)\n",
    "y_train = np.genfromtxt(\"data/y_train.csv\", delimiter=\",\", skip_header=1)\n",
    "y_features = np.genfromtxt(\"data/y_train.csv\", delimiter=\",\", dtype=str, max_rows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.genfromtxt(\"data/x_test.csv\",delimiter=\",\", skip_header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(array, range_min, range_max, n_bins):\n",
    "    # Filter array to include only values within the specified range\n",
    "    filtered_values = array[(array >= range_min) & (array <= range_max)]\n",
    "    \n",
    "    # Calculate the bin edges using quantiles\n",
    "    bin_edges = np.quantile(filtered_values, np.linspace(0, 1, n_bins + 1))\n",
    "    \n",
    "    def assign_bin(value):\n",
    "        # Check if the value is NaN\n",
    "        if np.isnan(value):\n",
    "            return -1\n",
    "        \n",
    "        # If the value is outside the range, return it as is\n",
    "        if value < range_min or value > range_max:\n",
    "            return value\n",
    "        \n",
    "        # Assign bin based on which range the value falls into\n",
    "        # We use right=True to ensure that values exactly equal to range_max are included in the last bin\n",
    "        return np.digitize(value, bin_edges, right=True)\n",
    "    \n",
    "    return assign_bin\n",
    "\n",
    "mapping_dict = {\n",
    "    \"GENHLTH\": lambda value: value if value <= 9 else -1,\n",
    "    \"PHYSHLTH\": to_categorical(array=x_train[:, best_features==\"PHYSHLTH\"].flatten(), range_min=0, range_max=30, n_bins=4),\n",
    "    \"MENTHLTH\": to_categorical(array=x_train[:, best_features==\"MENTHLTH\"].flatten(), range_min=0, range_max=30, n_bins=4),\n",
    "    \"POORHLTH\": to_categorical(array=x_train[:, best_features==\"POORHLTH\"].flatten(), range_min=0, range_max=30, n_bins=4),\n",
    "    \"HLTHPLN1\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"MEDCOST\": lambda value: value if value <= 7 else -1,\n",
    "    \"CHECKUP1\": lambda value: value if value <= 8 else -1,\n",
    "    \"BPHIGH4\": lambda value: value if value <= 7 else -1,\n",
    "    \"BPMEDS\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"BLOODCHO\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"CHOLCHK\": lambda value: value if not np.isnan(value) else -1,\n",
    "    # \"CVDINFR4\": lambda value: 1 if value == 1 else 0,\n",
    "    # \"CVDCRHD4\": lambda value: 1 if value == 1 else 0,\n",
    "    \"TOLDHI2\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"CVDSTRK3\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"ASTHMA3\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"ASTHNOW\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"CHCSCNCR\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"CHCOCNCR\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"CHCCOPD1\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"HAVARTH3\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"ADDEPEV2\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"CHCKIDNY\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"DIABETE3\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"SEX\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"MARITAL\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"EDUCA\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"VETERAN3\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"INCOME2\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"INTERNET\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"WTKG3\": to_categorical(array=x_train[:, best_features==\"WTKG3\"].flatten(), range_min=23, range_max=295, n_bins=6),\n",
    "    \"QLACTLM2\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"USEEQUIP\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"BLIND\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"DECIDE\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"DIFFWALK\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"DIFFDRES\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"DIFFALON\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"SMOKE100\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"SMOKDAY2\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"LASTSMK2\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"USENOW3\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"AVEDRNK2\": to_categorical(array=x_train[:, best_features==\"AVEDRNK2\"].flatten(), range_min=1, range_max=76, n_bins=5),\n",
    "    \"DRNK3GE5\": to_categorical(array=x_train[:, best_features==\"DRNK3GE5\"].flatten(), range_min=1, range_max=76, n_bins=5),\n",
    "    \"EXERANY2\": lambda value : value if not np.isnan(value) else -1,\n",
    "    # \"EXERHMM1\": lambda value: str(value//200) if value <= 959 and value not in [777,999] else -1,\n",
    "    \"LMTJOIN3\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"FLUSHOT6\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"PDIABTST\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"PREDIAB1\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"INSULIN\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"CIMEMLOS\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_RFHLTH\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_HCVU651\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_RFHYPE5\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_CHOLCHK\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_RFCHOL\": lambda value : value if not np.isnan(value) else -1,\n",
    "    # \"_MICHD\": lambda value: value if value <= 2 else -1,\n",
    "    \"_LTASTH1\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_CASTHM1\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_DRDXAR1\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_AGEG5YR\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_AGE_G\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"HTM4\": to_categorical(array=x_train[:, best_features==\"HTM4\"].flatten(), range_min=0.91, range_max=2.44, n_bins=6),\n",
    "    \"_RFBMI5\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_EDUCAG\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_SMOKER3\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_RFBING5\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_BMI5CAT\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_RFDRHV5\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"FTJUDA1_\": to_categorical(array=x_train[:, best_features==\"FTJUDA1_\"].flatten(), range_min=0, range_max=99.99, n_bins=4),\n",
    "    \"MAXVO2_\": to_categorical(array=x_train[:, best_features==\"MAXVO2_\"].flatten(), range_min=0, range_max=50.1, n_bins=6),\n",
    "    \"ACTIN11_\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"ACTIN21_\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_PACAT1\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_PA150R2\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_PA300R2\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_PASTRNG\":  lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_PASTAE1\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_LMTACT1\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_LMTWRK1\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_LMTSCL1\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_INCOMG\": lambda value : value if not np.isnan(value) else -1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def select_features_with_low_nan_ratio(x_train, features_to_check, threshold=0.1):\n",
    "    nan_ratios = {}\n",
    "    for feature in features_to_check:\n",
    "        nan_ratios[feature] = np.sum(np.isnan(x_train[:, best_features == feature])) / len(x_train)\n",
    "\n",
    "    selected_features = [feature for feature in nan_ratios if nan_ratios[feature] < threshold]\n",
    "\n",
    "    # print(f\"Selected {len(selected_features)} features over {len(features_to_check)}\")\n",
    "    # print(nan_ratios)\n",
    "    return selected_features\n",
    "\n",
    "def apply_mapping(x_train, selected_features, mapping_dict):\n",
    "    x_train_filtered = np.zeros((x_train.shape[0], len(selected_features)))\n",
    "    for feature in selected_features:\n",
    "        feature_values = x_train[:, best_features == feature].flatten()\n",
    "        if feature_values.size > 0:\n",
    "            x_train_filtered[:, selected_features.index(feature)] = np.array([mapping_dict[feature](value) for value in feature_values])\n",
    "    return x_train_filtered\n",
    "\n",
    "def fix_class_imbalance(X, y, target_value=1, dont_balance=False):\n",
    "    \"\"\"\n",
    "    Fix class imbalance by oversampling the minority class or undersampling the majority class.\n",
    "    \n",
    "    Parameters:\n",
    "    X (numpy.ndarray): Feature matrix of shape (n_samples, n_features)\n",
    "    y (numpy.ndarray): Target vector of shape (n_samples,), containing values -1 and 1\n",
    "    target_value (int): Class value to balance to (default is 1)\n",
    "    \n",
    "    Returns:\n",
    "    X_balanced (numpy.ndarray): Feature matrix with balanced classes\n",
    "    y_balanced (numpy.ndarray): Balanced target vector\n",
    "    \"\"\"\n",
    "    if dont_balance:\n",
    "        return X, y\n",
    "\n",
    "    # Separate samples by class\n",
    "    class_1_indices = np.where(y == target_value)[0]\n",
    "    class_minus_1_indices = np.where(y != target_value)[0]\n",
    "    \n",
    "    # Find class counts\n",
    "    class_1_count = len(class_1_indices)\n",
    "    class_minus_1_count = len(class_minus_1_indices)\n",
    "    \n",
    "    if class_1_count == class_minus_1_count:\n",
    "        # If classes are already balanced, return the original data\n",
    "        return X, y\n",
    "    \n",
    "    elif class_1_count < class_minus_1_count:\n",
    "        # If class 1 is the minority, oversample class 1\n",
    "        oversample_size = class_minus_1_count - class_1_count\n",
    "        oversampled_indices = np.random.choice(class_1_indices, oversample_size, replace=True)\n",
    "        new_indices = np.concatenate([np.arange(len(y)), oversampled_indices])\n",
    "    else:\n",
    "        # If class -1 is the minority, oversample class -1\n",
    "        oversample_size = class_1_count - class_minus_1_count\n",
    "        oversampled_indices = np.random.choice(class_minus_1_indices, oversample_size, replace=True)\n",
    "        new_indices = np.concatenate([np.arange(len(y)), oversampled_indices])\n",
    "    \n",
    "    # Create the balanced dataset\n",
    "    X_balanced = X[new_indices]\n",
    "    y_balanced = y[new_indices]\n",
    "    \n",
    "    return X_balanced, y_balanced\n",
    "\n",
    "# calculate the accuracy, precision, recall and F1 score\n",
    "def accuracy_precision_recall_f1(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    accuracy = (tp + tn) / len(y_true)\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def split_data(x, y, ratio=0.8):\n",
    "    indices = np.random.permutation(x.shape[0])\n",
    "    train_indices = indices[:int(ratio * x.shape[0])]\n",
    "    test_indices = indices[int(ratio * x.shape[0]):]\n",
    "    return x[train_indices], y[train_indices], x[test_indices], y[test_indices]\n",
    "\n",
    "def split_data_k_folds(x, y, n_folds=5):\n",
    "    # Shuffle the data\n",
    "    indices = np.random.permutation(x.shape[0])\n",
    "    \n",
    "    # Split indices into n equal-sized parts\n",
    "    fold_sizes = np.full(n_folds, x.shape[0] // n_folds, dtype=int)  # Base size of each fold\n",
    "    fold_sizes[:x.shape[0] % n_folds] += 1  # Distribute the remainder\n",
    "\n",
    "    current = 0\n",
    "    folds = []\n",
    "    for fold_size in fold_sizes:\n",
    "        start, stop = current, current + fold_size\n",
    "        test_indices = indices[start:stop]  # Select current fold as test set\n",
    "        train_indices = np.concatenate([indices[:start], indices[stop:]])  # Rest are training\n",
    "        \n",
    "        x_train, y_train = x[train_indices], y[train_indices]\n",
    "        x_test, y_test = x[test_indices], y[test_indices]\n",
    "        folds.append((x_train, y_train, x_test, y_test))\n",
    "        \n",
    "        current = stop\n",
    "\n",
    "    return folds\n",
    "\n",
    "def cleaning_x_pipeline(x_train, y_train, x_test, features, n_folds=5, dont_balance=False):\n",
    "    np.random.seed(41)\n",
    "    # keep only features with less than 10% nan values\n",
    "    # selected_features = select_features_with_low_nan_ratio(x_train, features, threshold=0.1)\n",
    "    # keep all features\n",
    "    selected_features = select_features_with_low_nan_ratio(x_train, features, threshold=1)\n",
    "    # cleaning\n",
    "    x_train_filtered_mapped = apply_mapping(x_train, selected_features, mapping_dict)\n",
    "    x_test_filtered_mapped = apply_mapping(x_test, selected_features, mapping_dict)\n",
    "\n",
    "    # Label encoding\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    combined = np.vstack((x_train_filtered_mapped, x_test_filtered_mapped))\n",
    "    combined_encoded = np.apply_along_axis(le.fit_transform, 0, combined)\n",
    "    x_train_encoded = combined_encoded[:x_train_filtered_mapped.shape[0], :]\n",
    "    x_test_encoded = combined_encoded[x_train_filtered_mapped.shape[0]:, :]\n",
    "\n",
    "    if n_folds==0:\n",
    "        # fix class imbalance in the training set\n",
    "        x_train_encoded_fixed, y_train_fixed = fix_class_imbalance(x_train_encoded, y_train, target_value=1, dont_balance=dont_balance)\n",
    "\n",
    "        return x_train_encoded, x_train_encoded_fixed, y_train_fixed, x_test_encoded\n",
    "    else:\n",
    "        # split the data into k folds\n",
    "        folds = split_data_k_folds(x_train_encoded, y_train, n_folds=n_folds)\n",
    "        balanced_folds = []\n",
    "\n",
    "        for x_train_fold, y_train_fold, x_test_fold, y_test_fold in folds:\n",
    "            # fix class imbalance in the training set\n",
    "            x_train_fold_fixed, y_train_fold_fixed = fix_class_imbalance(x_train_fold, y_train_fold, target_value=1, dont_balance=dont_balance)\n",
    "            balanced_folds.append((x_train_fold, x_train_fold_fixed, y_train_fold, y_train_fold_fixed, x_test_fold, y_test_fold))\n",
    "\n",
    "        return balanced_folds, x_test_encoded\n",
    "    \n",
    "def evaluate_model(x_train, y_train, x_test, final_features, dont_balance=False, n_folds=5, model=CategoricalNB()):\n",
    "    y_train_mapped = (1 + y_train[:, 1]) / 2\n",
    "    balanced_folds, x_test_encoded = cleaning_x_pipeline(x_train, y_train_mapped, x_test, final_features, n_folds=n_folds, dont_balance=dont_balance)\n",
    "\n",
    "    # Initialize array metrics of size n_folds*4\n",
    "    metrics_train = np.zeros((n_folds, 4))\n",
    "    metrics_train_fixed = np.zeros((n_folds, 4))\n",
    "    metrics_test = np.zeros((n_folds, 4))\n",
    "\n",
    "    for i in range(len(balanced_folds)):\n",
    "        x_train_fold, x_train_fold_fixed, y_train_fold, y_train_fold_fixed, x_test_fold, y_test_fold = balanced_folds[i]\n",
    "\n",
    "        print(\"x\", x_train_fold_fixed.shape)\n",
    "        print(\"y\", y_train_fold_fixed.shape)\n",
    "        print(\"x_test\", x_test.shape)\n",
    "        model.fit(x_train_fold_fixed, y_train_fold_fixed)\n",
    "\n",
    "        # Predict on the train, train_fixed and test set\n",
    "        y_train_pred = model.predict(x_train_fold)\n",
    "        y_train_fixed_pred = model.predict(x_train_fold_fixed)\n",
    "        y_test_pred = model.predict(x_test_fold)\n",
    "\n",
    "        # Calculate the accuracy, precision, recall and F1 score\n",
    "        metrics_train[i] = accuracy_precision_recall_f1(y_train_fold, y_train_pred)\n",
    "        metrics_train_fixed[i] = accuracy_precision_recall_f1(y_train_fold_fixed, y_train_fixed_pred)\n",
    "        metrics_test[i] = accuracy_precision_recall_f1(y_test_fold, y_test_pred)\n",
    "\n",
    "    # take the average\n",
    "    metrics_train = np.mean(metrics_train, axis=0)\n",
    "    metrics_train_fixed = np.mean(metrics_train_fixed, axis=0)\n",
    "    metrics_test = np.mean(metrics_test, axis=0)\n",
    "\n",
    "    return metrics_train, metrics_train_fixed, metrics_test\n",
    "\n",
    "def fit_predict_model(x_train, y_train, x_test, final_features, dont_balance=False, model=CategoricalNB()):\n",
    "    y_train_mapped = (1 + y_train[:, 1]) / 2\n",
    "    x_train_encoded, x_train_encoded_fixed, y_train_fixed, x_test_encoded = cleaning_x_pipeline(x_train, y_train_mapped, x_test, final_features, dont_balance=dont_balance, n_folds=0)\n",
    "\n",
    "    model.fit(x_train_encoded_fixed, y_train_fixed)\n",
    "    y_pred = model.predict(x_test_encoded)\n",
    "    y_train_pred = model.predict(x_train_encoded)\n",
    "\n",
    "    # print metrics for the training set\n",
    "    accuracy, precision, recall, f1 = accuracy_precision_recall_f1(y_train_mapped, y_train_pred)\n",
    "    print(f\"Training set: accuracy={accuracy:.2f}, precision={precision:.2f}, recall={recall:.2f}, F1={f1:.5f}\")\n",
    "\n",
    "    return y_pred, y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nx_r = np.random.randint(20, size=(1000, 3))\\ny_r = np.random.randint(2, size=(1000))\\nx = np.array([[1, 2, 0],\\n              [4, 5, 10],\\n              [7, 8, 0],\\n              [10, 11, 12],\\n              [13, 14, 3]])\\n\\ny = np.array([0, 1, 0, 1, 0])\\ny2 = np.array([1, 1, 1, 1, 1])\\n\\n\\nrf = RandomForest(n_trees=20, max_depth=5, min_samples_split=2)\\na = rf.fit(x_r, y_r)\\npred = rf.predict(x_r)\\nprint(np.mean(np.abs(pred-y_r)))\\n\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "class RandomForest:\n",
    "\n",
    "    class Tree:\n",
    "\n",
    "        def __init__(self, max_depth=3, min_sample_split=2):\n",
    "            self.max_depth = max_depth\n",
    "            self.min_sample_split = min_sample_split\n",
    "            self.root = None\n",
    "\n",
    "        def fit(self, x, y, depth=0):\n",
    "\n",
    "            if len(y) == 0:\n",
    "                return None\n",
    "\n",
    "            \n",
    "            if x.shape[0] < self.min_sample_split or len(set(y)) == 1 or depth >= self.max_depth:\n",
    "                leaf_value = self._calculate_leaf_value(y)\n",
    "                self.root = RandomForest.TreeNode(value=leaf_value)\n",
    "                return self.root\n",
    "                \n",
    "            \n",
    "            best_features, best_threshold = self._best_split(x, y)\n",
    "\n",
    "            left_x = x[:, best_features] <= best_threshold\n",
    "            right_x = x[:, best_features] > best_threshold\n",
    "\n",
    "            left_subtree = self.fit(x[left_x], y[left_x], depth + 1)\n",
    "            right_subtree = self.fit(x[right_x], y[right_x], depth + 1)\n",
    "\n",
    "            self.root = RandomForest.TreeNode(best_features, best_threshold, left_subtree, right_subtree)\n",
    "            return self.root\n",
    "\n",
    "\n",
    "        def predict(self, x):\n",
    "            return self._traverse_tree(x, self.root)\n",
    "\n",
    "        def _traverse_tree(self, x, treeNode):\n",
    "            \n",
    "            if treeNode.value is not None:\n",
    "                return np.full((x.shape[0],), treeNode.value)  \n",
    "\n",
    "            # Go right or left\n",
    "            left_indices = x[:, treeNode.feature_index] <= treeNode.threshold\n",
    "            right_indices = x[:, treeNode.feature_index] > treeNode.threshold\n",
    "\n",
    "            # empty array for now\n",
    "            predictions = np.empty(x.shape[0])\n",
    "\n",
    "            # predict for left and right subtree\n",
    "            if np.any(left_indices):\n",
    "                predictions[left_indices] = self._traverse_tree(x[left_indices], treeNode.left)\n",
    "            if np.any(right_indices):\n",
    "                predictions[right_indices] = self._traverse_tree(x[right_indices], treeNode.right)\n",
    "\n",
    "            return predictions\n",
    "            \n",
    "\n",
    "        def _calculate_leaf_value(self, y):\n",
    "            value, count = np.unique(y, return_counts=True)\n",
    "            return value[np.argmax(count)]\n",
    "\n",
    "\n",
    "        def _gini_impurity(self, y):\n",
    "            D = len(y)\n",
    "            #print(\"N\", D)\n",
    "            _, count = np.unique(y, return_counts=True, axis=0)\n",
    "            #print(\"Count\", count.shape)\n",
    "            gini = 1 - np.sum((count / D) ** 2)\n",
    "            #print(\"gini\", gini.shape)\n",
    "            return gini\n",
    "    \n",
    "        def _entropy(self, y):\n",
    "            N = y.shape\n",
    "            _, count = np.unique(y, return_counts=True)\n",
    "            p = count/N\n",
    "            \n",
    "            entropy = np.where(p > 0, -p* np.log2(p), 0)\n",
    "            \n",
    "            return entropy.sum()\n",
    "        \n",
    "        def _split(self, x, feature, threshold):\n",
    "            #print(threshold[:,np.newaxis].shape)\n",
    "            #print(x[np.newaxis, :,feature].shape)\n",
    "            left_branch = np.where(x[:,feature] <= threshold)[0]\n",
    "            right_branch = np.where(x[:, feature] > threshold)[0]\n",
    "            #print(\"left_b\", left_branch)\n",
    "            return left_branch, right_branch\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        def _best_split(self, x, y):\n",
    "\n",
    "            best_features = 0\n",
    "            best_threshold = 0\n",
    "\n",
    "            best_gini_impurity = 1\n",
    "\n",
    "            N, D = x.shape\n",
    "\n",
    "            for i in range(D):\n",
    "                unique = np.unique(x[:, i])\n",
    "                unique = np.sort(unique)\n",
    "                \n",
    "                # Calculate midpoint between possible value\n",
    "                threshold = (unique[:-1] + unique[1:])/2\n",
    "                #print(\"threshold\", threshold)\n",
    "\n",
    "                \"\"\"\n",
    "                l, r = self._split(x, i, threshold)\n",
    "                print(\"l\", l)\n",
    "                print(\"r\", r)\n",
    "                y_prime = y[np.newaxis,:] * np.ones(l.shape)\n",
    "                print(\"y'\", y_prime)\n",
    "                print(\"y'\", y_prime)\n",
    "\n",
    "                gini_left = self._gini_impurity(y_prime[l])\n",
    "                gini_right = self._gini_impurity(y_prime[r])\n",
    "                print(gini_left)\n",
    "                print(gini_right)\n",
    "                \n",
    "\n",
    "                break\n",
    "                \"\"\"\n",
    "                \n",
    "                for t in threshold:\n",
    "                    l, r = self._split(x, i, t)\n",
    "                    gini_left = self._gini_impurity(y[l])\n",
    "                    gini_right = self._gini_impurity(y[r])\n",
    "                    gini = (gini_left * len(l) + gini_right * len(r))/N\n",
    "\n",
    "\n",
    "                    if gini < best_gini_impurity:\n",
    "                        best_gini_impurity = gini\n",
    "                        best_features = i\n",
    "                        best_threshold = t\n",
    "                \n",
    "\n",
    "            \n",
    "            return best_features, best_threshold\n",
    "\n",
    "\n",
    "    def __init__(self, n_trees=20, max_depth=10, min_samples_split=2, seed=42):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        # Create the trees\n",
    "        self.list_tree = []\n",
    "        for i in range(self.n_trees):\n",
    "\n",
    "            bootstrap_x, bootstrap_y = self._bootstrap_sample(x, y)\n",
    "\n",
    "            tree = RandomForest.Tree(self.max_depth, self.min_samples_split)\n",
    "\n",
    "            tree.fit(bootstrap_x, bootstrap_y)\n",
    "\n",
    "\n",
    "            self.list_tree.append(tree)\n",
    "        \n",
    "\n",
    "    def predict(self, x):\n",
    "        predictions = np.zeros((len(x), self.n_trees))\n",
    "        for i in range(self.n_trees):\n",
    "            pred = self.list_tree[i].predict(x)\n",
    "            #Convert -1 in 0\n",
    "            predictions[:, i] = np.where(pred == -1, 0, 1)\n",
    "\n",
    "        dominant_prediction = np.apply_along_axis(lambda p: np.bincount(p.astype(int)).argmax(), axis=1, arr=predictions)\n",
    "        # Convert back 0 in -1\n",
    "        dominant_prediction = np.where(dominant_prediction == 0, -1, 1)\n",
    "        return dominant_prediction\n",
    "\n",
    "    \n",
    "    \n",
    "    class TreeNode:\n",
    "\n",
    "        def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
    "            self.feature_index = feature_index\n",
    "            self.threshold = threshold\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "            self.value = value\n",
    "\n",
    "    def _bootstrap_sample(self, x, y):\n",
    "\n",
    "        N, _ = x.shape\n",
    "\n",
    "        bootstrap_indices = self.rng.choice(N, N, replace=True)\n",
    "        \n",
    "        x_bootstrap = x[bootstrap_indices]\n",
    "\n",
    "        y_bootstrap = y[bootstrap_indices]\n",
    "        return x_bootstrap, y_bootstrap\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x_r = np.random.randint(20, size=(1000, 3))\n",
    "y_r = np.random.randint(2, size=(1000))\n",
    "x = np.array([[1, 2, 0],\n",
    "              [4, 5, 10],\n",
    "              [7, 8, 0],\n",
    "              [10, 11, 12],\n",
    "              [13, 14, 3]])\n",
    "\n",
    "y = np.array([0, 1, 0, 1, 0])\n",
    "y2 = np.array([1, 1, 1, 1, 1])\n",
    "\n",
    "\n",
    "rf = RandomForest(n_trees=20, max_depth=5, min_samples_split=2)\n",
    "a = rf.fit(x_r, y_r)\n",
    "pred = rf.predict(x_r)\n",
    "print(np.mean(np.abs(pred-y_r)))\n",
    "\n",
    "\"\"\"\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing by splitting the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 322)\n",
      "(1000,)\n",
      "(1000, 322)\n",
      "(array([-1.,  1.]), array([500, 500]))\n",
      "0.5\n",
      "(array([-1.,  1.]), array([500, 500]))\n"
     ]
    }
   ],
   "source": [
    "size_subset = 1000\n",
    "\n",
    "# Get indices where y_train is -1 and 1\n",
    "indices_minus1 = np.where(y_train[:, 1] == -1)[0][:500]  # First 500 indices where y_train is -1\n",
    "indices_plus1 = np.where(y_train[:, 1] == 1)[0][:500]   # First 500 indices where y_train is 1\n",
    "\n",
    "x_train_subset = np.vstack((x_train[indices_minus1, :], x_train[indices_plus1, :]))\n",
    "y_train_subset = np.hstack((y_train[indices_minus1, 1], y_train[indices_plus1, 1]))\n",
    "\n",
    "x_test_subset = x_test[:size_subset]\n",
    "\n",
    "print(x_train_subset.shape)\n",
    "print(y_train_subset.shape)\n",
    "print(x_test_subset.shape)\n",
    "print(np.unique(y_train_subset, return_counts=True))\n",
    "\n",
    "rf = RandomForest(n_trees=50, max_depth=12, min_samples_split=5)\n",
    "a = rf.fit(x_train_subset, y_train_subset)\n",
    "pred_train = rf.predict(x_train_subset)\n",
    "pred_test = rf.predict(x_test_subset)\n",
    "\n",
    "print(np.mean((abs(pred_train - y_train_subset)/2)))\n",
    "print(np.unique(y_train_subset, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.908\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(abs(pred_train - y_train_subset)/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m n_folds \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      2\u001b[0m final_features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_RFHLTH\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAXVO2_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGENHLTH\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCVDSTRK3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBLOODCHO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSEX\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHLTHPLN1\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m metrics_train, metrics_train_fixed, metrics_test \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdont_balance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_folds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_folds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRandomForest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain set: Accuracy=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics_train[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Precision=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics_train[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Recall=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics_train[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, F1=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics_train[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain set fixed: Accuracy=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics_train_fixed[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Precision=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics_train_fixed[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Recall=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics_train_fixed[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, F1=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics_train_fixed[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[46], line 161\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(x_train, y_train, x_test, final_features, dont_balance, n_folds, model)\u001b[0m\n\u001b[1;32m    158\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(x_train_fold_fixed, y_train_fold_fixed)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# Predict on the train, train_fixed and test set\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m y_train_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_fold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m y_train_fixed_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_train_fold_fixed)\n\u001b[1;32m    163\u001b[0m y_test_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_test_fold)\n",
      "Cell \u001b[0;32mIn[47], line 168\u001b[0m, in \u001b[0;36mRandomForest.predict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    166\u001b[0m predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(x), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_trees))\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_trees):\n\u001b[0;32m--> 168\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_tree\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m#Convert -1 in 0\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     predictions[:, i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(pred \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[47], line 36\u001b[0m, in \u001b[0;36mRandomForest.Tree.predict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_traverse_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[47], line 40\u001b[0m, in \u001b[0;36mRandomForest.Tree._traverse_tree\u001b[0;34m(self, x, treeNode)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_traverse_tree\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, treeNode):\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtreeNode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mfull((x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],), treeNode\u001b[38;5;241m.\u001b[39mvalue)  \n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# Go right or left\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "n_folds = 1\n",
    "final_features = ['_RFHLTH', 'MAXVO2_', 'GENHLTH', 'CVDSTRK3', 'BLOODCHO', 'SEX', 'HLTHPLN1']\n",
    "metrics_train, metrics_train_fixed, metrics_test = evaluate_model(x_train, y_train, x_test, final_features, dont_balance=False, n_folds=n_folds, model=RandomForest(5))\n",
    "\n",
    "print(f\"Train set: Accuracy={metrics_train[0]:.2f}, Precision={metrics_train[1]:.2f}, Recall={metrics_train[2]:.2f}, F1={metrics_train[3]:.5f}\")\n",
    "print(f\"Train set fixed: Accuracy={metrics_train_fixed[0]:.2f}, Precision={metrics_train_fixed[1]:.2f}, Recall={metrics_train_fixed[2]:.2f}, F1={metrics_train_fixed[3]:.5f}\")\n",
    "print(f\"Test set: Accuracy={metrics_test[0]:.2f}, Precision={metrics_test[1]:.2f}, Recall={metrics_test[2]:.2f}, F1={metrics_test[3]:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # greedy algorithm to find the best set of features that maximizes f1_test\n",
    "# # add features greedily one by one until the f1_test stops increasing\n",
    "# from tqdm import tqdm  # Import tqdm for the progress bar\n",
    "\n",
    "# def greedy_feature_selection(x_train, y_train, x_test, features, dont_balance=False):\n",
    "#     features = select_features_with_low_nan_ratio(x_train, features, threshold=0.1)\n",
    "#     n_features = len(features)\n",
    "#     selected_features = []\n",
    "#     remaining_features = features.copy()\n",
    "\n",
    "#     best_f1 = 0\n",
    "#     progress_bar = tqdm(total=n_features, desc=\"Selecting Features\")\n",
    "\n",
    "#     while remaining_features:\n",
    "#         # Track the best feature and F1 score in the current iteration\n",
    "#         best_feature = None\n",
    "#         best_f1_iteration = 0\n",
    "#         # print(remaining_features)\n",
    "#         # Try adding each remaining feature and evaluate F1 score\n",
    "#         for feature in remaining_features:\n",
    "#             current_features = selected_features + [feature]  # Add feature to the selected set\n",
    "#             # print(current_features)\n",
    "\n",
    "#             metrics_train, metrics_train_fixed, metrics_test = evaluate_model(x_train, y_train, x_test, current_features, dont_balance=dont_balance, n_folds=5, model=CategoricalNB())\n",
    "#             f1_test = metrics_test[3]\n",
    "\n",
    "#             # Check if the current F1 score is the best so far\n",
    "#             if f1_test > best_f1_iteration:\n",
    "#                 best_f1_iteration = f1_test\n",
    "#                 best_feature = feature\n",
    "#         # Stop if no improvement is made\n",
    "#         if best_f1_iteration <= best_f1:\n",
    "#             break\n",
    "        \n",
    "#         # Update selected features and remaining features\n",
    "#         selected_features.append(best_feature)\n",
    "#         remaining_features.remove(best_feature)\n",
    "#         best_f1 = best_f1_iteration\n",
    "#         print(f\"Best F1 score: {best_f1}\")\n",
    "#         print(f\"Selected features: {selected_features}\")\n",
    "        \n",
    "#         progress_bar.update(1)  # Update the progress bar\n",
    "        \n",
    "#     progress_bar.close()  # Close the progress bar when done\n",
    "#     return selected_features, best_f1\n",
    "\n",
    "# selected_features, best_f1 = greedy_feature_selection(x_train, y_train, x_test, list(mapping_dict.keys()), dont_balance=False)\n",
    "# print(f\"Selected features: {selected_features}\")\n",
    "\n",
    "# # found ['_RFHLTH', 'MAXVO2_', 'GENHLTH', 'CVDSTRK3', 'BLOODCHO', 'SEX', 'HLTHPLN1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction for the real test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = ['_RFHLTH', 'MAXVO2_', 'GENHLTH', 'CVDSTRK3', 'BLOODCHO', 'SEX', 'HLTHPLN1']\n",
    "y_pred_test, y_train_pred = fit_predict_model(x_train, y_train, x_test, final_features, dont_balance=False, model=RandomForest())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ids = x_test[:,0]\n",
    "y_pred_test_final = 2*y_pred_test-1\n",
    "\n",
    "np.savetxt(\"data/submission_RandomForest_4.csv\", np.array([Ids, y_pred_test_final]).T, delimiter=\",\", fmt=\"%d\", header=\"Id,Prediction\", comments=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project1-grading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
