{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edb3e540",
   "metadata": {},
   "source": [
    "# Project 1 in Machine Learning, Kernel-trick"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ededdb",
   "metadata": {},
   "source": [
    "**Author:** Antoine Cornaz, Zayneb Melloui, Anas Himmi\n",
    "**Date:** 1.11.2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b83d02",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59585ad4",
   "metadata": {},
   "source": [
    "### 1.1 Problem Statement\n",
    "This machine learning project consists of two main phases. The dataset used contains medical records of patients, both healthy and those diagnosed with cardiovascular disease (CVD). The first phase focuses on implementing the functions covered in class and exercise sessions. The second phase aims to optimize the model for the best possible prediction, simulating a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f2125b",
   "metadata": {},
   "source": [
    "### 1.2 Objective\n",
    "\n",
    "The objective of this project is to develop a machine learning model capable of predicting the presence of cardiovascular disease (CVD) in patients based on their medical information. The performance of the model will be evaluated using the following metrics:\n",
    "- **Accuracy**: To measure the overall correctness of the model's predictions.\n",
    "- **F1-Score**: To assess the balance between precision and recall, particularly for the CVD class.\n",
    "- **Precision**: To evaluate the proportion of true positive predictions among all positive predictions.\n",
    "- **Recall**: To measure the proportion of true positive predictions among all actual positive instances.\n",
    "\n",
    "The goal is to optimize the model to achieve the best possible performance on these metrics, but most importantly on the F1-score, simulating real-world predictive scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39570748",
   "metadata": {},
   "source": [
    "### 1.3 Dataset Description\n",
    "The dataset consists of records from 328,135 patients, each described by 321 dimensions containing various medical information. While none of the patients have all 321 medical fields fully populated, 92 key medical attributes are consistently available for every patient. The data originates from the **Behavioral Risk Factor Surveillance System (BRFSS)**, a health-related telephone survey system that gathers state-level data on U.S. residents. The BRFSS focuses on health-related risk behaviors, chronic health conditions, and the usage of preventive health services. This dataset provides a rich source of information for analyzing cardiovascular disease (CVD) and related health factors.\n",
    "\n",
    "The description and definition of each feature can be found in the following link : https://www.cdc.gov/brfss/annual_data/2015/pdf/codebook15_llcp.pdf. We can observe that most features are categorical which will lead us to make all of them categorical, and use algorithms that can handle categorical data or use encoding techniques to convert them to numerical data (e.g., one-hot encoding)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fd1bc8",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba7c851",
   "metadata": {},
   "source": [
    "## 2.1 Import and Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b7de107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from implementations import *\n",
    "# from helpers import *\n",
    "# from visualisation import *\n",
    "# from preprocessing import *\n",
    "# from missing import *\n",
    "# from split import *\n",
    "\n",
    "# import sys\n",
    "\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9aedaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.genfromtxt(\"data/x_train.csv\", delimiter=\",\", skip_header=1)\n",
    "features = np.genfromtxt(\"data/x_train.csv\", delimiter=\",\", dtype=str, max_rows=1)\n",
    "y_train = np.genfromtxt(\"data/y_train.csv\", delimiter=\",\", skip_header=1)\n",
    "y_features = np.genfromtxt(\"data/y_train.csv\", delimiter=\",\", dtype=str, max_rows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52c94f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.genfromtxt(\"data/x_test.csv\",delimiter=\",\", skip_header=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ef8013",
   "metadata": {},
   "source": [
    "## 2.2 Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0777adc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(array, range_min, range_max, n_bins):\n",
    "    \"\"\"\n",
    "    Converts a numerical array into categorical bins based on specified range and number of bins.\n",
    "    Parameters:\n",
    "    array (numpy.ndarray): The input array containing numerical values.\n",
    "    range_min (float): The minimum value of the range to consider for binning.\n",
    "    range_max (float): The maximum value of the range to consider for binning.\n",
    "    n_bins (int): The number of bins to divide the range into.\n",
    "    Returns:\n",
    "    function: A function that takes a value and returns the corresponding bin index or the value itself if it is outside the specified range.\n",
    "              If the value is NaN, it returns -1.\n",
    "    Notes:\n",
    "    - Values outside the specified range are returned as is.\n",
    "    - NaN values are assigned a bin index of -1.\n",
    "    - Bin edges are calculated using quantiles to ensure approximately equal distribution of values across bins.\n",
    "    - The rightmost bin includes values exactly equal to range_max.\n",
    "    \"\"\"\n",
    "    # Filter array to include only values within the specified range\n",
    "    filtered_values = array[(array >= range_min) & (array <= range_max)]\n",
    "    \n",
    "    # Calculate the bin edges using quantiles\n",
    "    bin_edges = np.quantile(filtered_values, np.linspace(0, 1, n_bins + 1))\n",
    "    \n",
    "    def assign_bin(value):\n",
    "        # Check if the value is NaN\n",
    "        if np.isnan(value):\n",
    "            return -1\n",
    "        \n",
    "        # If the value is outside the range, return it as is\n",
    "        if value < range_min or value > range_max:\n",
    "            return value\n",
    "        \n",
    "        # Assign bin based on which range the value falls into\n",
    "        # We use right=True to ensure that values exactly equal to range_max are included in the last bin\n",
    "        return np.digitize(value, bin_edges, right=True)\n",
    "    \n",
    "    return assign_bin\n",
    "\n",
    "\n",
    "\n",
    "# a dictionary that maps the column names to the corresponding mapping function\n",
    "# - For naturally categorical, we map the NaN values to -1\n",
    "# - For numerical, we check the codebook for all possible values \n",
    "#   and map the range of numerical values to a range of bins (e.g. 0-30 to 4 bins) \n",
    "#   then map the rest to their own category (don't know, didn't want to answer, Nan etc.)\n",
    "# We only keep vaguely relevant columns to our problem. We will further refine this list later.\n",
    "\n",
    "mapping_dict = {\n",
    "    \"GENHLTH\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"PHYSHLTH\": to_categorical(array=x_train[:, features==\"PHYSHLTH\"].flatten(), range_min=0, range_max=30, n_bins=4),\n",
    "    \"MENTHLTH\": to_categorical(array=x_train[:, features==\"MENTHLTH\"].flatten(), range_min=0, range_max=30, n_bins=4),\n",
    "    \"POORHLTH\": to_categorical(array=x_train[:, features==\"POORHLTH\"].flatten(), range_min=0, range_max=30, n_bins=4),\n",
    "    \"HLTHPLN1\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"MEDCOST\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"CHECKUP1\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"BPHIGH4\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"BPMEDS\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"BLOODCHO\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"CHOLCHK\": lambda value: value if not np.isnan(value) else -1,\n",
    "    # \"CVDINFR4\": lambda value: 1 if value == 1 else 0,\n",
    "    # \"CVDCRHD4\": lambda value: 1 if value == 1 else 0,\n",
    "    \"TOLDHI2\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"CVDSTRK3\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"ASTHMA3\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"ASTHNOW\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"CHCSCNCR\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"CHCOCNCR\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"CHCCOPD1\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"HAVARTH3\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"ADDEPEV2\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"CHCKIDNY\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"DIABETE3\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"SEX\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"MARITAL\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"EDUCA\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"VETERAN3\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"INCOME2\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"INTERNET\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"WTKG3\": to_categorical(array=x_train[:, features==\"WTKG3\"].flatten(), range_min=23, range_max=295, n_bins=6),\n",
    "    \"QLACTLM2\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"USEEQUIP\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"BLIND\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"DECIDE\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"DIFFWALK\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"DIFFDRES\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"DIFFALON\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"SMOKE100\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"SMOKDAY2\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"LASTSMK2\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"USENOW3\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"AVEDRNK2\": to_categorical(array=x_train[:, features==\"AVEDRNK2\"].flatten(), range_min=1, range_max=76, n_bins=5),\n",
    "    \"DRNK3GE5\": to_categorical(array=x_train[:, features==\"DRNK3GE5\"].flatten(), range_min=1, range_max=76, n_bins=5),\n",
    "    \"EXERANY2\": lambda value : value if not np.isnan(value) else -1,\n",
    "    # \"EXERHMM1\": lambda value: str(value//200) if value <= 959 and value not in [777,999] else -1,\n",
    "    \"LMTJOIN3\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"FLUSHOT6\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"PDIABTST\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"PREDIAB1\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"INSULIN\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"CIMEMLOS\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_RFHLTH\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_HCVU651\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_RFHYPE5\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_CHOLCHK\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_RFCHOL\": lambda value : value if not np.isnan(value) else -1,\n",
    "    # \"_MICHD\": lambda value: value if value <= 2 else -1,\n",
    "    \"_LTASTH1\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_CASTHM1\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_DRDXAR1\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_AGEG5YR\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_AGE_G\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"HTM4\": to_categorical(array=x_train[:, features==\"HTM4\"].flatten(), range_min=0.91, range_max=2.44, n_bins=6),\n",
    "    \"_RFBMI5\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_EDUCAG\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_SMOKER3\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_RFBING5\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_BMI5CAT\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_RFDRHV5\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"FTJUDA1_\": to_categorical(array=x_train[:, features==\"FTJUDA1_\"].flatten(), range_min=0, range_max=99.99, n_bins=4),\n",
    "    \"MAXVO2_\": to_categorical(array=x_train[:, features==\"MAXVO2_\"].flatten(), range_min=0, range_max=50.1, n_bins=6),\n",
    "    \"ACTIN11_\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"ACTIN21_\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_PACAT1\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_PA150R2\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_PA300R2\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_PASTRNG\":  lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_PASTAE1\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_LMTACT1\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_LMTWRK1\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_LMTSCL1\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_INCOMG\": lambda value : value if not np.isnan(value) else -1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfc4756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not necessary because we will do a greedy approach later (unless we want to save time)\n",
    "def select_features_with_low_nan_ratio(X, features_to_check, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Select features from the training data that have a NaN ratio below a specified threshold.\n",
    "\n",
    "    Parameters:\n",
    "    X (numpy.ndarray): The feature data array.\n",
    "    features_to_check (list): List of features to check for NaN ratios.\n",
    "    threshold (float): The maximum allowable NaN ratio for a feature to be selected. Default is 0.1.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of features that have a NaN ratio below the specified threshold.\n",
    "    \"\"\"\n",
    "    nan_ratios = {}\n",
    "    for feature in features_to_check:\n",
    "        nan_ratios[feature] = np.sum(np.isnan(X[:, features == feature])) / len(X)\n",
    "\n",
    "    selected_features = [feature for feature in nan_ratios if nan_ratios[feature] < threshold]\n",
    "\n",
    "    # print(f\"Selected {len(selected_features)} features over {len(features_to_check)}\")\n",
    "    # print(nan_ratios)\n",
    "    return selected_features\n",
    "\n",
    "# This is the function that applies the mapping to the selected features\n",
    "def apply_mapping(X, selected_features, mapping_dict):\n",
    "    \"\"\"\n",
    "    Applies a mapping function to selected features in the training data.\n",
    "\n",
    "    Parameters:\n",
    "    X (numpy.ndarray): The feature data array of shape (n_samples, n_features).\n",
    "    selected_features (list): A list of feature indices to which the mapping functions will be applied (they need to be keys of mapping_dict)\n",
    "    mapping_dict (dict): A dictionary where keys are feature indices and values are functions that map feature values.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: A new array with the same number of samples as X but only the selected features, \n",
    "                   with the mapping functions applied to each feature.\n",
    "    \"\"\"\n",
    "    X_filtered = np.zeros((X.shape[0], len(selected_features)))\n",
    "    for feature in selected_features:\n",
    "        feature_values = X[:, features == feature].flatten()\n",
    "        if feature_values.size > 0:\n",
    "            X_filtered[:, selected_features.index(feature)] = np.array([mapping_dict[feature](value) for value in feature_values])\n",
    "    return X_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12e69a5",
   "metadata": {},
   "source": [
    "We can observe that the target variable is imbalanced, with a higher number of healthy patients compared to those diagnosed with CVD. This imbalance can affect the model's performance, particularly in terms of recall and F1-score. Therefore, we will address this issue by oversampling the minority class by duplicating randomly selected instances from the CVD class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3a76385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of Y=1: 8.83%\n"
     ]
    }
   ],
   "source": [
    "# Proportion of Y=1 in the training set vs Y=-1\n",
    "number_of_ones = np.sum(y_train == 1)\n",
    "number_of_minus_ones = np.sum(y_train == -1)\n",
    "total = len(y_train)\n",
    "print(f\"Proportion of Y=1: {number_of_ones/total*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1abfe516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_class_imbalance(X, y, target_value=1, dont_balance=False):\n",
    "    \"\"\"\n",
    "    Fix class imbalance by oversampling the minority class or undersampling the majority class.\n",
    "    \n",
    "    Parameters:\n",
    "    X (numpy.ndarray): Feature matrix of shape (n_samples, n_features)\n",
    "    y (numpy.ndarray): Target vector of shape (n_samples,), containing values -1 and 1\n",
    "    target_value (int): Class value to balance to (default is 1)\n",
    "    dont_balance (bool): If True, the function will not balance the classes (default is False)\n",
    "    \n",
    "    Returns:\n",
    "    X_balanced (numpy.ndarray): Feature matrix with balanced classes\n",
    "    y_balanced (numpy.ndarray): Balanced target vector\n",
    "    \"\"\"\n",
    "    if dont_balance:\n",
    "        return X, y\n",
    "\n",
    "    # Separate samples by class\n",
    "    class_1_indices = np.where(y == target_value)[0]\n",
    "    class_minus_1_indices = np.where(y != target_value)[0]\n",
    "    \n",
    "    # Find class counts\n",
    "    class_1_count = len(class_1_indices)\n",
    "    class_minus_1_count = len(class_minus_1_indices)\n",
    "    \n",
    "    if class_1_count == class_minus_1_count:\n",
    "        # If classes are already balanced, return the original data\n",
    "        return X, y\n",
    "    \n",
    "    elif class_1_count < class_minus_1_count:\n",
    "        # If class 1 is the minority, oversample class 1\n",
    "        oversample_size = class_minus_1_count - class_1_count\n",
    "        oversampled_indices = np.random.choice(class_1_indices, oversample_size, replace=True)\n",
    "        new_indices = np.concatenate([np.arange(len(y)), oversampled_indices])\n",
    "    else:\n",
    "        # If class -1 is the minority, oversample class -1\n",
    "        oversample_size = class_1_count - class_minus_1_count\n",
    "        oversampled_indices = np.random.choice(class_minus_1_indices, oversample_size, replace=True)\n",
    "        new_indices = np.concatenate([np.arange(len(y)), oversampled_indices])\n",
    "    \n",
    "    # Create the balanced dataset\n",
    "    X_balanced = X[new_indices]\n",
    "    y_balanced = y[new_indices]\n",
    "    \n",
    "    return X_balanced, y_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2669e659",
   "metadata": {},
   "source": [
    "We will implement here a LabelEncoder to simplify the design of algorithms and a OneHotEncoder to convert the categorical features into numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87658261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(X, selected_features):\n",
    "    \"\"\"\n",
    "    One-hot encodes the selected features in the input matrix X.\n",
    "\n",
    "    Parameters:\n",
    "    X (ndarray): The input feature matrix of shape (n_samples, n_features).\n",
    "    selected_features (list): A list of selected feature indices for one-hot encoding.\n",
    "\n",
    "    Returns:\n",
    "    ndarray: A matrix containing the one-hot encoded columns.\n",
    "    \"\"\"\n",
    "    # Initialize a list to collect the one-hot encoded columns\n",
    "    one_hot_encoded_list = []\n",
    "\n",
    "    # Iterate over the final selected features\n",
    "    for i, feature in enumerate(selected_features):\n",
    "        # Extract feature values for the current feature\n",
    "        feature_values = X[:, i]\n",
    "        \n",
    "        # Get unique values for this feature\n",
    "        unique_values = np.unique(feature_values)\n",
    "        \n",
    "        # One-hot encode by creating a column for each unique value and append to the list\n",
    "        one_hot_encoded_columns = [(feature_values == value).astype(int).reshape(-1, 1) for value in unique_values]\n",
    "        one_hot_encoded_list.extend(one_hot_encoded_columns)\n",
    "    \n",
    "    # Concatenate all one-hot encoded columns horizontally\n",
    "    X_OHE = np.hstack(one_hot_encoded_list)\n",
    "    \n",
    "    return X_OHE\n",
    "\n",
    "def label_encode(X, selected_features):\n",
    "    \"\"\"\n",
    "    Label encodes the selected features in the input matrix X.\n",
    "\n",
    "    Parameters:\n",
    "    X (ndarray): The input feature matrix of shape (n_samples, n_features).\n",
    "    selected_features (list): A list of selected feature indices for label encoding.\n",
    "\n",
    "    Returns:\n",
    "    ndarray: A matrix containing the label encoded columns.\n",
    "    \"\"\"\n",
    "    # Initialize a list to collect the label encoded columns\n",
    "    label_encoded_list = []\n",
    "\n",
    "    # Iterate over the final selected features\n",
    "    for i, feature in enumerate(selected_features):\n",
    "        # Extract feature values for the current feature\n",
    "        feature_values = X[:, i]\n",
    "        \n",
    "        # Get unique values for this feature\n",
    "        unique_values = np.unique(feature_values)\n",
    "        \n",
    "        # Label encode by mapping each unique value to an integer from 0 to n_categories - 1\n",
    "        label_encoded_column = np.array([np.where(unique_values == value)[0][0] for value in feature_values]).reshape(-1, 1)\n",
    "        label_encoded_list.append(label_encoded_column)\n",
    "    \n",
    "    # Concatenate all label encoded columns horizontally\n",
    "    X_LE = np.hstack(label_encoded_list)\n",
    "    \n",
    "    return X_LE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673cb021",
   "metadata": {},
   "source": [
    "We also will map the target variable to 0 and 1, where 0 represents healthy patients and 1 represents patients diagnosed with CVD because the logistic regression algorithm requires it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ecb692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_y_to_0_1(y):\n",
    "    \"\"\"\n",
    "    Maps the target values from -1 and 1 to 0 and 1.\n",
    "\n",
    "    Parameters:\n",
    "    y (ndarray): The target vector containing values -1 and 1.\n",
    "\n",
    "    Returns:\n",
    "    ndarray: The target vector with values 0 and 1.\n",
    "    \"\"\"\n",
    "    return (y + 1) // 2\n",
    "\n",
    "def map_y_to_minus_1_1(y):\n",
    "    \"\"\"\n",
    "    Maps the target values from 0 and 1 to -1 and 1.\n",
    "\n",
    "    Parameters:\n",
    "    y (ndarray): The target vector containing values 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "    ndarray: The target vector with values -1 and 1.\n",
    "    \"\"\"\n",
    "    return 2 * y - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb224bb",
   "metadata": {},
   "source": [
    "## 2.3 Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0422441e",
   "metadata": {},
   "source": [
    "We calculate the normalized mutual information between each feature and the target variable to identify the most relevant features for predicting CVD. This information will guide us in selecting the most important features for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "414d05f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_feature_entropies(x_train_filtered, y_train, selected_features):\n",
    "    # Function to calculate entropy\n",
    "    def calculate_entropy(probabilities):\n",
    "        # Calculate entropy: H(X) = -sum(p(x) * log2(p(x)))\n",
    "        assert np.isclose(np.sum(probabilities), 1), f\"Probabilities do not sum to 1: {np.sum(probabilities)}\"\n",
    "        return -np.sum([p * np.log2(p) for p in probabilities if p > 0])\n",
    "\n",
    "    entropies = []\n",
    "    entropies_normalized = []\n",
    "    joint_entropies = []\n",
    "    joint_entropies_normalized = []\n",
    "    mutual_informations = []\n",
    "    NMI = []\n",
    "\n",
    "    # Filter the rows based on target values\n",
    "    Y1 = x_train_filtered[y_train[:, 1] == 1]\n",
    "    Y_minus1 = x_train_filtered[y_train[:, 1] == -1]\n",
    "\n",
    "    # Total counts for probabilities\n",
    "    count_Y1 = len(Y1)\n",
    "    count_Y_minus1 = len(Y_minus1)\n",
    "    total_count = count_Y1 + count_Y_minus1\n",
    "\n",
    "    # Calculate probabilities of Y\n",
    "    p_Y1 = count_Y1 / total_count\n",
    "    p_Y_minus1 = count_Y_minus1 / total_count\n",
    "\n",
    "    # Entropy of Y\n",
    "    entropy_Y = calculate_entropy([p_Y1, p_Y_minus1])\n",
    "\n",
    "    # Iterate over the indices of the features\n",
    "    for feature in selected_features:\n",
    "        feature_index = selected_features.index(feature)\n",
    "        # Get the feature values for the entire dataset (to find all unique values)\n",
    "        all_feature_values = x_train_filtered[:, feature_index]\n",
    "        unique_values_all, counts_all = np.unique(all_feature_values, return_counts=True)\n",
    "\n",
    "        # Get the feature values for the filtered rows where Y=1\n",
    "        feature_values_Y1 = Y1[:, feature_index]\n",
    "        non_nan_values_Y1 = feature_values_Y1[~np.isnan(feature_values_Y1)]\n",
    "\n",
    "        # Get the counts for each unique value in filtered rows where Y=1\n",
    "        unique_values_filtered_Y1, counts_filtered_Y1 = np.unique(non_nan_values_Y1, return_counts=True)\n",
    "\n",
    "        # Create a dictionary for filtered counts where Y=1\n",
    "        filtered_count_dict_Y1 = dict(zip(unique_values_filtered_Y1, counts_filtered_Y1))\n",
    "\n",
    "        # Initialize the probabilities list for Y=1\n",
    "        probabilities_Y1 = []\n",
    "\n",
    "        # Calculate probabilities for Y=1\n",
    "        for value in unique_values_all:\n",
    "            count = filtered_count_dict_Y1.get(value, 0)  # Default to 0 if not found\n",
    "            probabilities_Y1.append(count / np.sum(counts_filtered_Y1) if np.sum(counts_filtered_Y1) > 0 else 0)  # Probabilities\n",
    "\n",
    "        # Compute the entropy for Y=1\n",
    "        entropy_Y1 = calculate_entropy(probabilities_Y1)\n",
    "\n",
    "        # Get the feature values for the filtered rows where Y=-1\n",
    "        feature_values_Y_minus1 = Y_minus1[:, feature_index]\n",
    "        non_nan_values_Y_minus1 = feature_values_Y_minus1[~np.isnan(feature_values_Y_minus1)]\n",
    "\n",
    "        # Get the counts for each unique value in filtered rows where Y=-1\n",
    "        unique_values_filtered_Y_minus1, counts_filtered_Y_minus1 = np.unique(non_nan_values_Y_minus1, return_counts=True)\n",
    "\n",
    "        # Create a dictionary for filtered counts where Y=-1\n",
    "        filtered_count_dict_Y_minus1 = dict(zip(unique_values_filtered_Y_minus1, counts_filtered_Y_minus1))\n",
    "\n",
    "        # Initialize the probabilities list for Y=-1\n",
    "        probabilities_Y_minus1 = []\n",
    "\n",
    "        # Calculate probabilities for Y=-1\n",
    "        for value in unique_values_all:\n",
    "            count = filtered_count_dict_Y_minus1.get(value, 0)  # Default to 0 if not found\n",
    "            probabilities_Y_minus1.append(count / np.sum(counts_filtered_Y_minus1) if np.sum(counts_filtered_Y_minus1) > 0 else 0)  # Probabilities\n",
    "\n",
    "        # Compute the entropy for Y=-1\n",
    "        entropy_Y_minus1 = calculate_entropy(probabilities_Y_minus1)\n",
    "\n",
    "        # Calculate the joint entropy H(f|Y)\n",
    "        joint_entropy = (p_Y1 * entropy_Y1) + (p_Y_minus1 * entropy_Y_minus1)\n",
    "\n",
    "        # Normalize entropies\n",
    "        entropy_normalized_Y1 = entropy_Y1 / np.log2(len(unique_values_all)) if len(unique_values_all) > 1 else 0\n",
    "        entropy_normalized_Y_minus1 = entropy_Y_minus1 / np.log2(len(unique_values_all)) if len(unique_values_all) > 1 else 0\n",
    "\n",
    "        # Append results\n",
    "        entropies.append((entropy_Y1, entropy_Y_minus1))\n",
    "        entropies_normalized.append((entropy_normalized_Y1, entropy_normalized_Y_minus1))\n",
    "        joint_entropies.append(joint_entropy)\n",
    "        joint_entropies_normalized.append(joint_entropy / np.log2(len(unique_values_all)) if len(unique_values_all) > 1 else 0)\n",
    "\n",
    "        # Mutual information = H(F) - H(F|Y)\n",
    "        entropy_feature = calculate_entropy(counts_all / np.sum(counts_all))\n",
    "        mutual_information = (entropy_feature - joint_entropy) / np.log2(len(unique_values_all))\n",
    "        mutual_informations.append(mutual_information)\n",
    "\n",
    "        # Normalized mutual information\n",
    "        NMI.append(2 * mutual_information / (entropy_Y + entropy_feature))\n",
    "\n",
    "    return entropies, entropies_normalized, joint_entropies, joint_entropies_normalized, mutual_informations, NMI\n",
    "\n",
    "all_features = list(mapping_dict.keys())\n",
    "x_train_filtered = apply_mapping(x_train, all_features, mapping_dict)\n",
    "\n",
    "entropies, entropies_normalized, joint_entropies, joint_entropies_normalized, mutual_informations, NMI = calculate_feature_entropies(x_train_filtered, y_train, all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94a92826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5400431686644592\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(NMI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9745643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('GENHLTH', 0.011372890308499821), ('PHYSHLTH', 0.005606052144614916), ('MENTHLTH', 0.0010073867855778646), ('POORHLTH', 0.00366851995405314), ('HLTHPLN1', 0.0015925699351870635), ('MEDCOST', 0.0003081076536101091), ('CHECKUP1', 0.0021290749718682936), ('BPHIGH4', 0.015498997023659125), ('BPMEDS', 0.019267635898967392), ('BLOODCHO', 0.006956033953880671), ('CHOLCHK', 0.005693564802715609), ('TOLDHI2', 0.01237341942537317), ('CVDSTRK3', 0.02816824772263489), ('ASTHMA3', 0.0016497472672407046), ('ASTHNOW', 0.001564798171259762), ('CHCSCNCR', 0.004113376120738742), ('CHCOCNCR', 0.004828329350552296), ('CHCCOPD1', 0.021124291494311827), ('HAVARTH3', 0.011756126222333537), ('ADDEPEV2', 0.0027441504990411212), ('CHCKIDNY', 0.01350056865769127), ('DIABETE3', 0.011512512856022677), ('SEX', 0.005068909697124422), ('MARITAL', 0.0036540298199253386), ('EDUCA', 0.0015638338373940759), ('VETERAN3', 0.007317295465853785), ('INCOME2', 0.0015029038610149315), ('INTERNET', 0.009257085421347786), ('WTKG3', 0.0006124469027556226), ('QLACTLM2', 0.012989793638438387), ('USEEQUIP', 0.01518444957438256), ('BLIND', 0.005602380525391611), ('DECIDE', 0.004502010995828507), ('DIFFWALK', 0.017294079085055866), ('DIFFDRES', 0.006862727090511622), ('DIFFALON', 0.008661778532511311), ('SMOKE100', 0.004283333022436428), ('SMOKDAY2', 0.0035474391026389723), ('LASTSMK2', 0.0025937222231816937), ('USENOW3', 8.25378160613786e-05), ('AVEDRNK2', 0.0017719976700132155), ('DRNK3GE5', 0.0018443695480622883), ('EXERANY2', 0.0021789508659736875), ('LMTJOIN3', 0.009536485916695606), ('FLUSHOT6', 0.001668950817294751), ('PDIABTST', 0.0003987987663804933), ('PREDIAB1', 0.0005714805860370831), ('INSULIN', 0.01034776187521754), ('CIMEMLOS', 0.00332734715483717), ('_RFHLTH', 0.035215673915062834), ('_HCVU651', 0.020068173974302144), ('_RFHYPE5', 0.029866905590159806), ('_CHOLCHK', 0.006542831509135432), ('_RFCHOL', 0.014384006800966437), ('_LTASTH1', 0.002070215310565521), ('_CASTHM1', 0.002883255475691825), ('_DRDXAR1', 0.017237080712807665), ('_AGEG5YR', 0.005506602147614817), ('_AGE_G', 0.011063525075486915), ('HTM4', 0.00019534356672562523), ('_RFBMI5', 0.002118934577652099), ('_EDUCAG', 0.0019454876391061604), ('_SMOKER3', 0.0035832050630977548), ('_RFBING5', 0.0031751087990385886), ('_BMI5CAT', 0.001239903358431248), ('_RFDRHV5', 0.000713196171711271), ('FTJUDA1_', 0.00033767287952966835), ('MAXVO2_', 0.008830781079372949), ('ACTIN11_', 0.004064853064096468), ('ACTIN21_', 0.0026795956069726695), ('_PACAT1', 0.0014868274291499621), ('_PA150R2', 0.001901911990568184), ('_PA300R2', 0.0019614223114472816), ('_PASTRNG', 0.001058228178274276), ('_PASTAE1', 0.00104694288765896), ('_LMTACT1', 0.009796385021022145), ('_LMTWRK1', 0.009392148522540125), ('_LMTSCL1', 0.008644681820631543), ('_INCOMG', 0.002368967153438779)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "# print the entropies with the corresponding feature\n",
    "print(list(zip(all_features, NMI)))\n",
    "\n",
    "# plot the NMI in seaborn in a bar plot (sorted by entropy)\n",
    "# Sort the features by entropy\n",
    "sorted_indices = np.argsort(NMI)[::-1]\n",
    "sorted_features = [all_features[i] for i in sorted_indices]\n",
    "sorted_entropies = [NMI[i] for i in sorted_indices]\n",
    "\n",
    "# Plot the entropies\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.barplot(x=sorted_features, y=sorted_entropies)\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Normalized Mutual Information')\n",
    "plt.title('NMI of features given Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493e173f",
   "metadata": {},
   "source": [
    "We observe from this that the most dependent features to the target are :\n",
    "- _RFHLTH : Adults with good or better health\n",
    "- _RFHYPE5 : Adults who have been told they have high blood pressure by a doctor, nurse, or other health professional\n",
    "- CVDSTRK3 : \"(Ever told) you had a stroke\"\n",
    "- CHCCOPD1 : \"(Ever told) you have Chronic Obstructive Pulmonary Disease or COPD, emphysema or chronic bronchitis?\"\n",
    "\n",
    "Because our implementations are slow when using all the features, we will use only the most important features to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f72f58a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 10 features out of 79\n",
      "['BPHIGH4', 'BPMEDS', 'CVDSTRK3', 'CHCCOPD1', 'USEEQUIP', 'DIFFWALK', '_RFHLTH', '_HCVU651', '_RFHYPE5', '_DRDXAR1']\n"
     ]
    }
   ],
   "source": [
    "# keep features with NMI > 0.015\n",
    "features_good_nmi = [feature for feature, nmi in zip(all_features, NMI) if nmi > 0.015]\n",
    "print(f\"Selected {len(features_good_nmi)} features out of {len(all_features)}\")\n",
    "print(features_good_nmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b730160",
   "metadata": {},
   "source": [
    "# 3. Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6ca6cd",
   "metadata": {},
   "source": [
    "## 3.1 Splitting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69d7b395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_k_folds(x, y, n_folds=4):\n",
    "    \"\"\"\n",
    "    Splits the dataset into k folds for cross-validation.\n",
    "    Parameters:\n",
    "    x (numpy.ndarray): The input features of the dataset.\n",
    "    y (numpy.ndarray): The target labels of the dataset.\n",
    "    n_folds (int, optional): The number of folds to split the data into. Default is 5.\n",
    "    Returns:\n",
    "    list of tuples: A list where each tuple contains four elements:\n",
    "        - x_train (numpy.ndarray): Training set features for the current fold.\n",
    "        - y_train (numpy.ndarray): Training set labels for the current fold.\n",
    "        - x_test (numpy.ndarray): Test set features for the current fold.\n",
    "        - y_test (numpy.ndarray): Test set labels for the current fold.\n",
    "    \"\"\"\n",
    "    # Shuffle the data\n",
    "    indices = np.random.permutation(x.shape[0])\n",
    "    \n",
    "    # Split indices into n equal-sized parts\n",
    "    fold_sizes = np.full(n_folds, x.shape[0] // n_folds, dtype=int)  # Base size of each fold\n",
    "    fold_sizes[:x.shape[0] % n_folds] += 1  # Distribute the remainder\n",
    "\n",
    "    current = 0\n",
    "    folds = []\n",
    "    for fold_size in fold_sizes:\n",
    "        start, stop = current, current + fold_size\n",
    "        test_indices = indices[start:stop]  # Select current fold as test set\n",
    "        train_indices = np.concatenate([indices[:start], indices[stop:]])  # Rest are training\n",
    "        \n",
    "        x_train, y_train = x[train_indices], y[train_indices]\n",
    "        x_test, y_test = x[test_indices], y[test_indices]\n",
    "        folds.append((x_train, y_train, x_test, y_test))\n",
    "        \n",
    "        current = stop\n",
    "\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3398b7a",
   "metadata": {},
   "source": [
    "# 5. Model Selection and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e8f636",
   "metadata": {},
   "source": [
    "## 5.1 Implementing the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9547fcf",
   "metadata": {},
   "source": [
    "We will implement and compare the following models:\n",
    "- **Logistic Regression**\n",
    "- **Categorical Naive Bayes Classifier**\n",
    "- **Random Forest Classifier**\n",
    "\n",
    "This is because they are relatively simple to implement, they can handle categorical data (except for Logistic Regression that will need one hot encoding), and they are suitable for binary classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "debdd2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, max_iters=1000, gamma=0.01, lambda_=0.1):\n",
    "        self.max_iters = max_iters\n",
    "        self.gamma = gamma\n",
    "        self.lambda_ = lambda_\n",
    "        self.w = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.w, _ = reg_logistic_regression(y, X, self.lambda_, np.zeros(X.shape[1]), self.max_iters, self.gamma)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([1 if p > 0.5 else 0 for p in sigmoid(X @ self.w)])\n",
    "    \n",
    "class CategoricalNaiveBayes:\n",
    "    def __init__(self, smoothing=1e-6):\n",
    "        self.class_probs = None\n",
    "        self.feature_probs = None\n",
    "        self.classes = None\n",
    "        self.smoothing = smoothing  # Smoothing factor for Laplace smoothing\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Get the unique classes and initialize dictionaries to store probabilities\n",
    "        self.classes = np.unique(y)\n",
    "        num_classes = len(self.classes)\n",
    "        \n",
    "        # Calculate class probabilities P(y)\n",
    "        self.class_probs = np.array([(y == c).mean() for c in self.classes])\n",
    "\n",
    "        # Calculate feature probabilities P(X|y) for each feature and class\n",
    "        num_features = X.shape[1]\n",
    "        self.feature_probs = {}\n",
    "\n",
    "        for feature_idx in range(num_features):\n",
    "            feature_values = np.unique(X[:, feature_idx])\n",
    "            self.feature_probs[feature_idx] = {}\n",
    "            \n",
    "            for class_idx, class_value in enumerate(self.classes):\n",
    "                self.feature_probs[feature_idx][class_value] = {}\n",
    "                \n",
    "                # Filter X by class to get only relevant rows\n",
    "                X_class = X[y == class_value, feature_idx]\n",
    "                \n",
    "                for value in feature_values:\n",
    "                    # Calculate P(feature=value | class) with smoothing\n",
    "                    prob = ((X_class == value).sum() + self.smoothing) / (len(X_class) + len(feature_values) * self.smoothing)\n",
    "                    self.feature_probs[feature_idx][class_value][value] = prob\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Initialize predictions list\n",
    "        predictions = []\n",
    "        \n",
    "        for row in X:\n",
    "            # Calculate log-probabilities for each class\n",
    "            class_log_probs = []\n",
    "            \n",
    "            for class_idx, class_value in enumerate(self.classes):\n",
    "                # Start with log of the class probability\n",
    "                log_prob = np.log(self.class_probs[class_idx])\n",
    "                \n",
    "                for feature_idx in range(len(row)):\n",
    "                    feature_value = row[feature_idx]\n",
    "                    # Get the feature probability P(X|y), apply smoothing if not present\n",
    "                    feature_prob = self.feature_probs[feature_idx][class_value].get(feature_value, self.smoothing)\n",
    "                    log_prob += np.log(feature_prob)\n",
    "                \n",
    "                class_log_probs.append(log_prob)\n",
    "            \n",
    "            # Predict the class with the highest log-probability\n",
    "            predictions.append(self.classes[np.argmax(class_log_probs)])\n",
    "        \n",
    "        return np.array(predictions)\n",
    "    \n",
    "class RandomForest:\n",
    "\n",
    "    class Tree:\n",
    "\n",
    "        def __init__(self, max_depth=3, min_sample_split=2):\n",
    "            self.max_depth = max_depth\n",
    "            self.min_sample_split = min_sample_split\n",
    "            self.root = None\n",
    "\n",
    "        def fit(self, x, y, depth=0):\n",
    "            if len(y) == 0:\n",
    "                return None\n",
    "            \n",
    "            if x.shape[0] < self.min_sample_split or len(set(y)) == 1 or depth >= self.max_depth:\n",
    "                leaf_value = self._calculate_leaf_value(y)\n",
    "                self.root = RandomForest.TreeNode(value=leaf_value)\n",
    "                return self.root\n",
    "        \n",
    "            best_features, best_threshold = self._best_split(x, y)\n",
    "\n",
    "            left_x = x[:, best_features] <= best_threshold\n",
    "            right_x = x[:, best_features] > best_threshold\n",
    "\n",
    "            left_subtree = self.fit(x[left_x], y[left_x], depth + 1)\n",
    "            right_subtree = self.fit(x[right_x], y[right_x], depth + 1)\n",
    "\n",
    "            self.root = RandomForest.TreeNode(best_features, best_threshold, left_subtree, right_subtree)\n",
    "            return self.root\n",
    "\n",
    "\n",
    "        def predict(self, x):\n",
    "            return self._traverse_tree(x, self.root)\n",
    "\n",
    "        def _traverse_tree(self, x, treeNode):\n",
    "            if treeNode.value is not None:\n",
    "                return np.full((x.shape[0],), treeNode.value)  \n",
    "\n",
    "            # Go right or left\n",
    "            left_indices = x[:, treeNode.feature_index] <= treeNode.threshold\n",
    "            right_indices = x[:, treeNode.feature_index] > treeNode.threshold\n",
    "\n",
    "            # empty array for now\n",
    "            predictions = np.empty(x.shape[0])\n",
    "\n",
    "            # predict for left and right subtree\n",
    "            if np.any(left_indices):\n",
    "                predictions[left_indices] = self._traverse_tree(x[left_indices], treeNode.left)\n",
    "            if np.any(right_indices):\n",
    "                predictions[right_indices] = self._traverse_tree(x[right_indices], treeNode.right)\n",
    "\n",
    "            return predictions\n",
    "            \n",
    "        def _calculate_leaf_value(self, y):\n",
    "            value, count = np.unique(y, return_counts=True)\n",
    "            return value[np.argmax(count)]\n",
    "\n",
    "        def _gini_impurity(self, y):\n",
    "            D = len(y)\n",
    "            #print(\"N\", D)\n",
    "            _, count = np.unique(y, return_counts=True, axis=0)\n",
    "            #print(\"Count\", count.shape)\n",
    "            gini = 1 - np.sum((count / D) ** 2)\n",
    "            #print(\"gini\", gini.shape)\n",
    "            return gini\n",
    "    \n",
    "        def _entropy(self, y):\n",
    "            N = y.shape\n",
    "            _, count = np.unique(y, return_counts=True)\n",
    "            p = count/N\n",
    "            \n",
    "            entropy = np.where(p > 0, -p* np.log2(p), 0)\n",
    "            \n",
    "            return entropy.sum()\n",
    "        \n",
    "        def _split(self, x, feature, threshold):\n",
    "            #print(threshold[:,np.newaxis].shape)\n",
    "            #print(x[np.newaxis, :,feature].shape)\n",
    "            left_branch = np.where(x[:,feature] <= threshold)[0]\n",
    "            right_branch = np.where(x[:, feature] > threshold)[0]\n",
    "            #print(\"left_b\", left_branch)\n",
    "            return left_branch, right_branch\n",
    "        \n",
    "        def _best_split(self, x, y):\n",
    "            best_features = 0\n",
    "            best_threshold = 0\n",
    "            best_gini_impurity = 1\n",
    "\n",
    "            N, D = x.shape\n",
    "\n",
    "            for i in range(D):\n",
    "                unique = np.unique(x[:, i])\n",
    "                unique = np.sort(unique)\n",
    "                \n",
    "                # Calculate midpoint between possible value\n",
    "                threshold = (unique[:-1] + unique[1:])/2\n",
    "                #print(\"threshold\", threshold)\n",
    "\n",
    "                \"\"\"\n",
    "                l, r = self._split(x, i, threshold)\n",
    "                print(\"l\", l)\n",
    "                print(\"r\", r)\n",
    "                y_prime = y[np.newaxis,:] * np.ones(l.shape)\n",
    "                print(\"y'\", y_prime)\n",
    "                print(\"y'\", y_prime)\n",
    "\n",
    "                gini_left = self._gini_impurity(y_prime[l])\n",
    "                gini_right = self._gini_impurity(y_prime[r])\n",
    "                print(gini_left)\n",
    "                print(gini_right)\n",
    "                \n",
    "\n",
    "                break\n",
    "                \"\"\"\n",
    "                for t in threshold:\n",
    "                    l, r = self._split(x, i, t)\n",
    "                    gini_left = self._gini_impurity(y[l])\n",
    "                    gini_right = self._gini_impurity(y[r])\n",
    "                    gini = (gini_left * len(l) + gini_right * len(r))/N\n",
    "                    if gini < best_gini_impurity:\n",
    "                        best_gini_impurity = gini\n",
    "                        best_features = i\n",
    "                        best_threshold = t \n",
    "            return best_features, best_threshold\n",
    "\n",
    "    def __init__(self, n_trees=20, max_depth=10, min_samples_split=2, seed=42):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        # Create the trees\n",
    "        self.list_tree = []\n",
    "        for i in range(self.n_trees):\n",
    "            bootstrap_x, bootstrap_y = self._bootstrap_sample(x, y)\n",
    "            tree = RandomForest.Tree(self.max_depth, self.min_samples_split)\n",
    "            tree.fit(bootstrap_x, bootstrap_y)\n",
    "            self.list_tree.append(tree)\n",
    "        \n",
    "\n",
    "    def predict(self, x):\n",
    "        predictions = np.zeros((len(x), self.n_trees))\n",
    "        for i in range(self.n_trees):\n",
    "            pred = self.list_tree[i].predict(x)\n",
    "            #Convert -1 in 0\n",
    "            predictions[:, i] = np.where(pred == -1, 0, 1)\n",
    "        dominant_prediction = np.apply_along_axis(lambda p: np.bincount(p.astype(int)).argmax(), axis=1, arr=predictions)\n",
    "        # Convert back 0 in -1\n",
    "        dominant_prediction = np.where(dominant_prediction == 0, -1, 1)\n",
    "        return dominant_prediction\n",
    "\n",
    "    class TreeNode:\n",
    "        def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
    "            self.feature_index = feature_index\n",
    "            self.threshold = threshold\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "            self.value = value\n",
    "\n",
    "    def _bootstrap_sample(self, x, y):\n",
    "        N, _ = x.shape\n",
    "\n",
    "        bootstrap_indices = self.rng.choice(N, N, replace=True)\n",
    "        \n",
    "        x_bootstrap = x[bootstrap_indices]\n",
    "\n",
    "        y_bootstrap = y[bootstrap_indices]\n",
    "        return x_bootstrap, y_bootstrap\n",
    "\n",
    "    \"\"\"\n",
    "    x_r = np.random.randint(20, size=(1000, 3))\n",
    "    y_r = np.random.randint(2, size=(1000))\n",
    "    x = np.array([[1, 2, 0],\n",
    "                [4, 5, 10],\n",
    "                [7, 8, 0],\n",
    "                [10, 11, 12],\n",
    "                [13, 14, 3]])\n",
    "\n",
    "    y = np.array([0, 1, 0, 1, 0])\n",
    "    y2 = np.array([1, 1, 1, 1, 1])\n",
    "\n",
    "\n",
    "    rf = RandomForest(n_trees=20, max_depth=5, min_samples_split=2)\n",
    "    a = rf.fit(x_r, y_r)\n",
    "    pred = rf.predict(x_r)\n",
    "    print(np.mean(np.abs(pred-y_r)))\n",
    "\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89f2816",
   "metadata": {},
   "source": [
    "## 5.2 Training and Commparing the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f64f390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_precision_recall_f1(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate accuracy, precision, recall, and F1 score for binary classification (y in {0,1}, 1 being positive).\n",
    "\n",
    "    Parameters:\n",
    "    y_true (array-like): True binary labels.\n",
    "    y_pred (array-like): Predicted binary labels.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - accuracy (float): The accuracy of the predictions.\n",
    "        - precision (float): The precision of the predictions.\n",
    "        - recall (float): The recall of the predictions.\n",
    "        - f1 (float): The F1 score of the predictions.\n",
    "    \"\"\"\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    accuracy = (tp + tn) / len(y_true)\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def cleaning_x_pipeline(x_train, y_train, x_test, features, nan_threshold=0.1, n_folds=4, dont_balance=False,oneHot=False):\n",
    "    np.random.seed(41)\n",
    "\n",
    "    selected_features = select_features_with_low_nan_ratio(x_train, features, threshold=nan_threshold)\n",
    "    print(\"Features with low NaN ratio selected. Number of features:\", len(selected_features))\n",
    "\n",
    "    # cleaning\n",
    "    x_train_filtered_mapped = apply_mapping(x_train, selected_features, mapping_dict)\n",
    "    x_test_filtered_mapped = apply_mapping(x_test, selected_features, mapping_dict)\n",
    "    print(\"Features mapped to categorical values.\")\n",
    "\n",
    "    if oneHot:\n",
    "        # One-hot encoding\n",
    "        combined = np.vstack((x_train_filtered_mapped, x_test_filtered_mapped))\n",
    "        combined_encoded = one_hot_encode(combined, selected_features)\n",
    "        x_train_encoded = combined_encoded[:len(x_train_filtered_mapped)]\n",
    "        x_test_encoded = combined_encoded[len(x_train_filtered_mapped):]\n",
    "        print(\"Features one-hot encoded.\")\n",
    "    else:\n",
    "        # Label encoding\n",
    "        # we need to combine the training and test set so that the encoding is the same for both\n",
    "        combined = np.vstack((x_train_filtered_mapped, x_test_filtered_mapped))\n",
    "        combined_encoded = label_encode(combined, selected_features)\n",
    "        x_train_encoded = combined_encoded[:len(x_train_filtered_mapped)]\n",
    "        x_test_encoded = combined_encoded[len(x_train_filtered_mapped):]\n",
    "        print(\"Features label encoded.\")\n",
    "\n",
    "    # no splitting (for submission)\n",
    "    if n_folds==0:\n",
    "        # fix class imbalance in the training set\n",
    "        x_train_encoded_fixed, y_train_fixed = fix_class_imbalance(x_train_encoded, y_train, target_value=1, dont_balance=dont_balance)\n",
    "        print(\"Class imbalance fixed.\")\n",
    "\n",
    "        return x_train_encoded, x_train_encoded_fixed, y_train_fixed, x_test_encoded\n",
    "    # splitting (for cross-validation)\n",
    "    else:\n",
    "        # split the data into k folds\n",
    "        folds = split_data_k_folds(x_train_encoded, y_train, n_folds=n_folds)\n",
    "        print(\"Data split into k folds.\")\n",
    "        balanced_folds = []\n",
    "\n",
    "        for x_train_fold, y_train_fold, x_test_fold, y_test_fold in folds:\n",
    "            # fix class imbalance in the training set\n",
    "            x_train_fold_fixed, y_train_fold_fixed = fix_class_imbalance(x_train_fold, y_train_fold, target_value=1, dont_balance=dont_balance)\n",
    "            balanced_folds.append((x_train_fold, x_train_fold_fixed, y_train_fold, y_train_fold_fixed, x_test_fold, y_test_fold))\n",
    "\n",
    "        print(\"Class imbalance fixed.\")\n",
    "\n",
    "        return balanced_folds, x_test_encoded\n",
    "    \n",
    "def evaluate_model(x_train, y_train, x_test, final_features, nan_threshold=0.1, dont_balance=False, n_folds=4, model=CategoricalNaiveBayes(), oneHot=False):\n",
    "    # process the data and split it into k folds\n",
    "    y_train_mapped = map_y_to_0_1(y_train[:,1])\n",
    "    balanced_folds, x_test_encoded = cleaning_x_pipeline(x_train, y_train_mapped, x_test, final_features, nan_threshold=nan_threshold, n_folds=n_folds, dont_balance=dont_balance, oneHot=oneHot)\n",
    "\n",
    "    # Initialize array metrics of size n_folds*4\n",
    "    metrics_train = np.zeros((n_folds, 4))\n",
    "    metrics_train_fixed = np.zeros((n_folds, 4))\n",
    "    metrics_test = np.zeros((n_folds, 4))\n",
    "\n",
    "    for i in range(len(balanced_folds)):\n",
    "        x_train_fold, x_train_fold_fixed, y_train_fold, y_train_fold_fixed, x_test_fold, y_test_fold = balanced_folds[i]\n",
    "        model.fit(x_train_fold_fixed, y_train_fold_fixed)\n",
    "\n",
    "        # Predict on the train, train_fixed and test set\n",
    "        y_train_pred = model.predict(x_train_fold)\n",
    "        y_train_fixed_pred = model.predict(x_train_fold_fixed)\n",
    "        y_test_pred = model.predict(x_test_fold)\n",
    "\n",
    "        # Calculate the accuracy, precision, recall and F1 score\n",
    "        metrics_train[i] = accuracy_precision_recall_f1(y_train_fold, y_train_pred)\n",
    "        metrics_train_fixed[i] = accuracy_precision_recall_f1(y_train_fold_fixed, y_train_fixed_pred)\n",
    "        metrics_test[i] = accuracy_precision_recall_f1(y_test_fold, y_test_pred)\n",
    "\n",
    "        print(f\"Fold {i+1} completed.\")\n",
    "\n",
    "    # take the average\n",
    "    metrics_train = np.mean(metrics_train, axis=0) # scores on the original trains et\n",
    "    metrics_train_fixed = np.mean(metrics_train_fixed, axis=0) # scores on the train set with class imbalance fixed\n",
    "    metrics_test = np.mean(metrics_test, axis=0) # scores on the test set\n",
    "\n",
    "    return metrics_train, metrics_train_fixed, metrics_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4028adfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with low NaN ratio selected. Number of features: 9\n",
      "Features mapped to categorical values.\n",
      "Features label encoded.\n",
      "Data split into k folds.\n",
      "Class imbalance fixed.\n",
      "Fold 1 completed.\n",
      "Fold 2 completed.\n",
      "Fold 3 completed.\n",
      "Fold 4 completed.\n",
      "Logistic Regression\n",
      "Train set: Accuracy=0.72, Precision=0.21, Recall=0.76, F1=0.32563\n",
      "Train set fixed: Accuracy=0.74, Precision=0.73, Recall=0.76, F1=0.74619\n",
      "Test set: Accuracy=0.72, Precision=0.21, Recall=0.76, F1=0.32546\n"
     ]
    }
   ],
   "source": [
    "# Categorical Naive Bayes\n",
    "all_features = features_good_nmi\n",
    "metrics_train_LR, metrics_train_fixed_LR, metrics_test_LR = evaluate_model(x_train, y_train, x_test, all_features, nan_threshold=0.1, dont_balance=False, n_folds=4, model=CategoricalNaiveBayes(), oneHot=False)\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Train set: Accuracy={metrics_train_LR[0]:.2f}, Precision={metrics_train_LR[1]:.2f}, Recall={metrics_train_LR[2]:.2f}, F1={metrics_train_LR[3]:.5f}\")\n",
    "print(f\"Train set fixed: Accuracy={metrics_train_fixed_LR[0]:.2f}, Precision={metrics_train_fixed_LR[1]:.2f}, Recall={metrics_train_fixed_LR[2]:.2f}, F1={metrics_train_fixed_LR[3]:.5f}\")\n",
    "print(f\"Test set: Accuracy={metrics_test_LR[0]:.2f}, Precision={metrics_test_LR[1]:.2f}, Recall={metrics_test_LR[2]:.2f}, F1={metrics_test_LR[3]:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce0aafdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with low NaN ratio selected. Number of features: 9\n",
      "Features mapped to categorical values.\n",
      "Features one-hot encoded.\n",
      "Data split into k folds.\n",
      "Class imbalance fixed.\n",
      "Regularized Logistic Regression(0/999): loss=0.6931471805599448, w0=0.0, w1=0.0009335138601867029\n",
      "Regularized Logistic Regression(1/999): loss=0.6922357483362718, w0=0.0, w1=0.0018644451208529769\n",
      "Regularized Logistic Regression(2/999): loss=0.6913329012301928, w0=0.0, w1=0.0027927820900686853\n",
      "Regularized Logistic Regression(3/999): loss=0.6904385392135903, w0=0.0, w1=0.0037185134748623254\n",
      "Regularized Logistic Regression(4/999): loss=0.6895525638034692, w0=0.0, w1=0.004641628374958861\n",
      "Regularized Logistic Regression(5/999): loss=0.6886748780318085, w0=0.0, w1=0.005562116276586646\n",
      "Regularized Logistic Regression(6/999): loss=0.6878053864160388, w0=0.0, w1=0.0064799670463471125\n",
      "Regularized Logistic Regression(7/999): loss=0.686943994930124, w0=0.0, w1=0.00739517092514978\n",
      "Regularized Logistic Regression(8/999): loss=0.6860906109762498, w0=0.0, w1=0.008307718522210829\n",
      "Regularized Logistic Regression(9/999): loss=0.6852451433570951, w0=0.0, w1=0.00921760080911874\n",
      "Regularized Logistic Regression(10/999): loss=0.6844075022486901, w0=0.0, w1=0.010124809113964224\n",
      "Regularized Logistic Regression(11/999): loss=0.6835775991738441, w0=0.0, w1=0.011029335115531353\n",
      "Regularized Logistic Regression(12/999): loss=0.6827553469761297, w0=0.0, w1=0.011931170837559348\n",
      "Regularized Logistic Regression(13/999): loss=0.6819406597944216, w0=0.0, w1=0.012830308643064961\n",
      "Regularized Logistic Regression(14/999): loss=0.6811334530379748, w0=0.0, w1=0.01372674122873114\n",
      "Regularized Logistic Regression(15/999): loss=0.6803336433620348, w0=0.0, w1=0.014620461619358205\n",
      "Regularized Logistic Regression(16/999): loss=0.6795411486439664, w0=0.0, w1=0.015511463162381568\n",
      "Regularized Logistic Regression(17/999): loss=0.6787558879598946, w0=0.0, w1=0.016399739522451433\n",
      "Regularized Logistic Regression(18/999): loss=0.67797778156185, w0=0.0, w1=0.017285284676077962\n",
      "Regularized Logistic Regression(19/999): loss=0.6772067508554024, w0=0.0, w1=0.018168092906339098\n",
      "Regularized Logistic Regression(20/999): loss=0.6764427183777755, w0=0.0, w1=0.019048158797653744\n",
      "Regularized Logistic Regression(21/999): loss=0.6756856077764409, w0=0.0, w1=0.019925477230612436\n",
      "Regularized Logistic Regression(22/999): loss=0.674935343788168, w0=0.0, w1=0.020800043376877658\n",
      "Regularized Logistic Regression(23/999): loss=0.6741918522185348, w0=0.0, w1=0.021671852694142638\n",
      "Regularized Logistic Regression(24/999): loss=0.6734550599218782, w0=0.0, w1=0.02254090092115382\n",
      "Regularized Logistic Regression(25/999): loss=0.6727248947816827, w0=0.0, w1=0.023407184072794785\n",
      "Regularized Logistic Regression(26/999): loss=0.6720012856914003, w0=0.0, w1=0.02427069843523109\n",
      "Regularized Logistic Regression(27/999): loss=0.6712841625356766, w0=0.0, w1=0.025131440561117314\n",
      "Regularized Logistic Regression(28/999): loss=0.670573456172001, w0=0.0, w1=0.025989407264864837\n",
      "Regularized Logistic Regression(29/999): loss=0.6698690984127471, w0=0.0, w1=0.02684459561796904\n",
      "Regularized Logistic Regression(30/999): loss=0.6691710220076093, w0=0.0, w1=0.02769700294439861\n",
      "Regularized Logistic Regression(31/999): loss=0.6684791606264258, w0=0.0, w1=0.02854662681604276\n",
      "Regularized Logistic Regression(32/999): loss=0.6677934488423679, w0=0.0, w1=0.02939346504821715\n",
      "Regularized Logistic Regression(33/999): loss=0.6671138221155104, w0=0.0, w1=0.03023751569523124\n",
      "Regularized Logistic Regression(34/999): loss=0.6664402167767519, w0=0.0, w1=0.031078777046011207\n",
      "Regularized Logistic Regression(35/999): loss=0.6657725700120868, w0=0.0, w1=0.03191724761978396\n",
      "Regularized Logistic Regression(36/999): loss=0.6651108198472264, w0=0.0, w1=0.032752926161815216\n",
      "Regularized Logistic Regression(37/999): loss=0.6644549051325527, w0=0.0, w1=0.0335858116392082\n",
      "Regularized Logistic Regression(38/999): loss=0.6638047655284022, w0=0.0, w1=0.034415903236755314\n",
      "Regularized Logistic Regression(39/999): loss=0.6631603414906719, w0=0.0, w1=0.03524320035285004\n",
      "Regularized Logistic Regression(40/999): loss=0.6625215742567357, w0=0.0, w1=0.03606770259545185\n",
      "Regularized Logistic Regression(41/999): loss=0.6618884058316744, w0=0.0, w1=0.03688940977810694\n",
      "Regularized Logistic Regression(42/999): loss=0.6612607789747995, w0=0.0, w1=0.03770832191602408\n",
      "Regularized Logistic Regression(43/999): loss=0.6606386371864749, w0=0.0, w1=0.03852443922220483\n",
      "Regularized Logistic Regression(44/999): loss=0.6600219246952239, w0=0.0, w1=0.03933776210362606\n",
      "Regularized Logistic Regression(45/999): loss=0.6594105864451146, w0=0.0, w1=0.04014829115748172\n",
      "Regularized Logistic Regression(46/999): loss=0.6588045680834225, w0=0.0, w1=0.04095602716746751\n",
      "Regularized Logistic Regression(47/999): loss=0.6582038159485567, w0=0.0, w1=0.04176097110012667\n",
      "Regularized Logistic Regression(48/999): loss=0.6576082770582505, w0=0.0, w1=0.0425631241012444\n",
      "Regularized Logistic Regression(49/999): loss=0.6570178990980053, w0=0.0, w1=0.043362487492291775\n",
      "Regularized Logistic Regression(50/999): loss=0.6564326304097836, w0=0.0, w1=0.044159062766925196\n",
      "Regularized Logistic Regression(51/999): loss=0.655852419980948, w0=0.0, w1=0.044952851587532296\n",
      "Regularized Logistic Regression(52/999): loss=0.6552772174334346, w0=0.0, w1=0.04574385578182829\n",
      "Regularized Logistic Regression(53/999): loss=0.6547069730131582, w0=0.0, w1=0.046532077339504166\n",
      "Regularized Logistic Regression(54/999): loss=0.6541416375796497, w0=0.0, w1=0.04731751840892109\n",
      "Regularized Logistic Regression(55/999): loss=0.6535811625959072, w0=0.0, w1=0.04810018129385499\n",
      "Regularized Logistic Regression(56/999): loss=0.6530255001184699, w0=0.0, w1=0.04888006845028747\n",
      "Regularized Logistic Regression(57/999): loss=0.6524746027876982, w0=0.0, w1=0.04965718248324505\n",
      "Regularized Logistic Regression(58/999): loss=0.6519284238182641, w0=0.0, w1=0.050431526143686234\n",
      "Regularized Logistic Regression(59/999): loss=0.6513869169898379, w0=0.0, w1=0.05120310232543233\n",
      "Regularized Logistic Regression(60/999): loss=0.6508500366379749, w0=0.0, w1=0.051971914062146865\n",
      "Regularized Logistic Regression(61/999): loss=0.6503177376451942, w0=0.0, w1=0.05273796452435951\n",
      "Regularized Logistic Regression(62/999): loss=0.649789975432241, w0=0.0, w1=0.053501257016533\n",
      "Regularized Logistic Regression(63/999): loss=0.6492667059495345, w0=0.0, w1=0.05426179497417901\n",
      "Regularized Logistic Regression(64/999): loss=0.6487478856687937, w0=0.0, w1=0.0550195819610112\n",
      "Regularized Logistic Regression(65/999): loss=0.6482334715748371, w0=0.0, w1=0.05577462166614782\n",
      "Regularized Logistic Regression(66/999): loss=0.6477234211575533, w0=0.0, w1=0.056526917901355174\n",
      "Regularized Logistic Regression(67/999): loss=0.6472176924040335, w0=0.0, w1=0.05727647459833117\n",
      "Regularized Logistic Regression(68/999): loss=0.6467162437908718, w0=0.0, w1=0.05802329580603486\n",
      "Regularized Logistic Regression(69/999): loss=0.6462190342766194, w0=0.0, w1=0.05876738568805336\n",
      "Regularized Logistic Regression(70/999): loss=0.6457260232943943, w0=0.0, w1=0.059508748520013116\n",
      "Regularized Logistic Regression(71/999): loss=0.6452371707446403, w0=0.0, w1=0.06024738868702841\n",
      "Regularized Logistic Regression(72/999): loss=0.6447524369880371, w0=0.0, w1=0.0609833106811924\n",
      "Regularized Logistic Regression(73/999): loss=0.6442717828385488, w0=0.0, w1=0.06171651909910815\n",
      "Regularized Logistic Regression(74/999): loss=0.6437951695566155, w0=0.0, w1=0.062447018639454004\n",
      "Regularized Logistic Regression(75/999): loss=0.6433225588424794, w0=0.0, w1=0.06317481410059186\n",
      "Regularized Logistic Regression(76/999): loss=0.6428539128296465, w0=0.0, w1=0.06389991037821496\n",
      "Regularized Logistic Regression(77/999): loss=0.6423891940784782, w0=0.0, w1=0.0646223124630266\n",
      "Regularized Logistic Regression(78/999): loss=0.6419283655699091, w0=0.0, w1=0.06534202543846221\n",
      "Regularized Logistic Regression(79/999): loss=0.6414713906992875, w0=0.0, w1=0.06605905447844548\n",
      "Regularized Logistic Regression(80/999): loss=0.641018233270341, w0=0.0, w1=0.06677340484517988\n",
      "Regularized Logistic Regression(81/999): loss=0.640568857489258, w0=0.0, w1=0.06748508188697802\n",
      "Regularized Logistic Regression(82/999): loss=0.6401232279588835, w0=0.0, w1=0.06819409103612568\n",
      "Regularized Logistic Regression(83/999): loss=0.63968130967303, w0=0.0, w1=0.06890043780677871\n",
      "Regularized Logistic Regression(84/999): loss=0.6392430680108963, w0=0.0, w1=0.0696041277928966\n",
      "Regularized Logistic Regression(85/999): loss=0.6388084687315947, w0=0.0, w1=0.07030516666620888\n",
      "Regularized Logistic Regression(86/999): loss=0.6383774779687839, w0=0.0, w1=0.07100356017421587\n",
      "Regularized Logistic Regression(87/999): loss=0.6379500622254042, w0=0.0, w1=0.07169931413822023\n",
      "Regularized Logistic Regression(88/999): loss=0.6375261883685095, w0=0.0, w1=0.07239243445139512\n",
      "Regularized Logistic Regression(89/999): loss=0.6371058236242012, w0=0.0, w1=0.07308292707687987\n",
      "Regularized Logistic Regression(90/999): loss=0.6366889355726555, w0=0.0, w1=0.07377079804590958\n",
      "Regularized Logistic Regression(91/999): loss=0.6362754921432433, w0=0.0, w1=0.07445605345597978\n",
      "Regularized Logistic Regression(92/999): loss=0.6358654616097409, w0=0.0, w1=0.07513869946903275\n",
      "Regularized Logistic Regression(93/999): loss=0.6354588125856322, w0=0.0, w1=0.075818742309683\n",
      "Regularized Logistic Regression(94/999): loss=0.6350555140194947, w0=0.0, w1=0.07649618826347014\n",
      "Regularized Logistic Regression(95/999): loss=0.6346555351904688, w0=0.0, w1=0.07717104367513926\n",
      "Regularized Logistic Regression(96/999): loss=0.634258845703816, w0=0.0, w1=0.0778433149469525\n",
      "Regularized Logistic Regression(97/999): loss=0.6338654154865516, w0=0.0, w1=0.07851300853703011\n",
      "Regularized Logistic Regression(98/999): loss=0.6334752147831587, w0=0.0, w1=0.07918013095771778\n",
      "Regularized Logistic Regression(99/999): loss=0.63308821415138, w0=0.0, w1=0.07984468877398458\n",
      "Regularized Logistic Regression(100/999): loss=0.6327043844580837, w0=0.0, w1=0.08050668860184616\n",
      "Regularized Logistic Regression(101/999): loss=0.6323236968752042, w0=0.0, w1=0.08116613710681624\n",
      "Regularized Logistic Regression(102/999): loss=0.6319461228757539, w0=0.0, w1=0.081823041002385\n",
      "Regularized Logistic Regression(103/999): loss=0.6315716342299046, w0=0.0, w1=0.08247740704852566\n",
      "Regularized Logistic Regression(104/999): loss=0.6312002030011391, w0=0.0, w1=0.08312924205022217\n",
      "Regularized Logistic Regression(105/999): loss=0.6308318015424697, w0=0.0, w1=0.08377855285603085\n",
      "Regularized Logistic Regression(106/999): loss=0.630466402492721, w0=0.0, w1=0.08442534635665853\n",
      "Regularized Logistic Regression(107/999): loss=0.630103978772878, w0=0.0, w1=0.08506962948357012\n",
      "Regularized Logistic Regression(108/999): loss=0.6297445035824972, w0=0.0, w1=0.08571140920762241\n",
      "Regularized Logistic Regression(109/999): loss=0.6293879503961768, w0=0.0, w1=0.08635069253771964\n",
      "Regularized Logistic Regression(110/999): loss=0.6290342929600893, w0=0.0, w1=0.08698748651949369\n",
      "Regularized Logistic Regression(111/999): loss=0.6286835052885723, w0=0.0, w1=0.08762179823400754\n",
      "Regularized Logistic Regression(112/999): loss=0.6283355616607738, w0=0.0, w1=0.08825363479648232\n",
      "Regularized Logistic Regression(113/999): loss=0.6279904366173591, w0=0.0, w1=0.0888830033550473\n",
      "Regularized Logistic Regression(114/999): loss=0.6276481049572643, w0=0.0, w1=0.0895099110895141\n",
      "Regularized Logistic Regression(115/999): loss=0.6273085417345127, w0=0.0, w1=0.09013436521016861\n",
      "Regularized Logistic Regression(116/999): loss=0.6269717222550764, w0=0.0, w1=0.09075637295659016\n",
      "Regularized Logistic Regression(117/999): loss=0.6266376220737927, w0=0.0, w1=0.09137594159648996\n",
      "Regularized Logistic Regression(118/999): loss=0.6263062169913278, w0=0.0, w1=0.0919930784245691\n",
      "Regularized Logistic Regression(119/999): loss=0.6259774830511944, w0=0.0, w1=0.09260779076140083\n",
      "Regularized Logistic Regression(120/999): loss=0.625651396536812, w0=0.0, w1=0.09322008595233063\n",
      "Regularized Logistic Regression(121/999): loss=0.6253279339686173, w0=0.0, w1=0.09382997136639848\n",
      "Regularized Logistic Regression(122/999): loss=0.6250070721012198, w0=0.0, w1=0.09443745439528134\n",
      "Regularized Logistic Regression(123/999): loss=0.6246887879206022, w0=0.0, w1=0.09504254245225474\n",
      "Regularized Logistic Regression(124/999): loss=0.6243730586413658, w0=0.0, w1=0.09564524297117079\n",
      "Regularized Logistic Regression(125/999): loss=0.6240598617040153, w0=0.0, w1=0.09624556340546242\n",
      "Regularized Logistic Regression(126/999): loss=0.6237491747722925, w0=0.0, w1=0.09684351122715742\n",
      "Regularized Logistic Regression(127/999): loss=0.6234409757305451, w0=0.0, w1=0.09743909392592193\n",
      "Regularized Logistic Regression(128/999): loss=0.6231352426811398, w0=0.0, w1=0.09803231900810958\n",
      "Regularized Logistic Regression(129/999): loss=0.6228319539419113, w0=0.0, w1=0.09862319399584077\n",
      "Regularized Logistic Regression(130/999): loss=0.6225310880436545, w0=0.0, w1=0.0992117264260896\n",
      "Regularized Logistic Regression(131/999): loss=0.6222326237276521, w0=0.0, w1=0.09979792384979701\n",
      "Regularized Logistic Regression(132/999): loss=0.6219365399432398, w0=0.0, w1=0.10038179383099291\n",
      "Regularized Logistic Regression(133/999): loss=0.6216428158454077, w0=0.0, w1=0.10096334394594199\n",
      "Regularized Logistic Regression(134/999): loss=0.6213514307924384, w0=0.0, w1=0.10154258178230144\n",
      "Regularized Logistic Regression(135/999): loss=0.6210623643435786, w0=0.0, w1=0.10211951493829856\n",
      "Regularized Logistic Regression(136/999): loss=0.620775596256748, w0=0.0, w1=0.10269415102192127\n",
      "Regularized Logistic Regression(137/999): loss=0.6204911064862773, w0=0.0, w1=0.10326649765012684\n",
      "Regularized Logistic Regression(138/999): loss=0.6202088751806833, w0=0.0, w1=0.10383656244806491\n",
      "Regularized Logistic Regression(139/999): loss=0.6199288826804734, w0=0.0, w1=0.10440435304831795\n",
      "Regularized Logistic Regression(140/999): loss=0.6196511095159851, w0=0.0, w1=0.10496987709015308\n",
      "Regularized Logistic Regression(141/999): loss=0.6193755364052527, w0=0.0, w1=0.10553314221879359\n",
      "Regularized Logistic Regression(142/999): loss=0.619102144251906, w0=0.0, w1=0.10609415608469967\n",
      "Regularized Logistic Regression(143/999): loss=0.6188309141431011, w0=0.0, w1=0.1066529263428682\n",
      "Regularized Logistic Regression(144/999): loss=0.6185618273474767, w0=0.0, w1=0.10720946065214566\n",
      "Regularized Logistic Regression(145/999): loss=0.6182948653131414, w0=0.0, w1=0.10776376667455236\n",
      "Regularized Logistic Regression(146/999): loss=0.6180300096656896, w0=0.0, w1=0.10831585207462362\n",
      "Regularized Logistic Regression(147/999): loss=0.6177672422062432, w0=0.0, w1=0.10886572451876296\n",
      "Regularized Logistic Regression(148/999): loss=0.6175065449095216, w0=0.0, w1=0.10941339167460834\n",
      "Regularized Logistic Regression(149/999): loss=0.6172478999219384, w0=0.0, w1=0.10995886121041269\n",
      "Regularized Logistic Regression(150/999): loss=0.6169912895597237, w0=0.0, w1=0.1105021407944352\n",
      "Regularized Logistic Regression(151/999): loss=0.6167366963070737, w0=0.0, w1=0.11104323809434925\n",
      "Regularized Logistic Regression(152/999): loss=0.6164841028143215, w0=0.0, w1=0.1115821607766568\n",
      "Regularized Logistic Regression(153/999): loss=0.6162334918961369, w0=0.0, w1=0.11211891650612094\n",
      "Regularized Logistic Regression(154/999): loss=0.6159848465297486, w0=0.0, w1=0.1126535129452066\n",
      "Regularized Logistic Regression(155/999): loss=0.6157381498531906, w0=0.0, w1=0.11318595775353554\n",
      "Regularized Logistic Regression(156/999): loss=0.615493385163571, w0=0.0, w1=0.11371625858735014\n",
      "Regularized Logistic Regression(157/999): loss=0.6152505359153653, w0=0.0, w1=0.11424442309899155\n",
      "Regularized Logistic Regression(158/999): loss=0.6150095857187308, w0=0.0, w1=0.11477045893638743\n",
      "Regularized Logistic Regression(159/999): loss=0.6147705183378447, w0=0.0, w1=0.11529437374255226\n",
      "Regularized Logistic Regression(160/999): loss=0.6145333176892622, w0=0.0, w1=0.11581617515509646\n",
      "Regularized Logistic Regression(161/999): loss=0.6142979678402984, w0=0.0, w1=0.1163358708057485\n",
      "Regularized Logistic Regression(162/999): loss=0.6140644530074281, w0=0.0, w1=0.11685346831988444\n",
      "Regularized Logistic Regression(163/999): loss=0.6138327575547072, w0=0.0, w1=0.11736897531607059\n",
      "Regularized Logistic Regression(164/999): loss=0.61360286599222, w0=0.0, w1=0.11788239940561615\n",
      "Regularized Logistic Regression(165/999): loss=0.6133747629745349, w0=0.0, w1=0.11839374819213495\n",
      "Regularized Logistic Regression(166/999): loss=0.6131484332991907, w0=0.0, w1=0.11890302927111748\n",
      "Regularized Logistic Regression(167/999): loss=0.6129238619051972, w0=0.0, w1=0.11941025022951081\n",
      "Regularized Logistic Regression(168/999): loss=0.6127010338715538, w0=0.0, w1=0.11991541864531183\n",
      "Regularized Logistic Regression(169/999): loss=0.6124799344157913, w0=0.0, w1=0.12041854208716674\n",
      "Regularized Logistic Regression(170/999): loss=0.612260548892525, w0=0.0, w1=0.12091962811397867\n",
      "Regularized Logistic Regression(171/999): loss=0.6120428627920333, w0=0.0, w1=0.12141868427453019\n",
      "Regularized Logistic Regression(172/999): loss=0.6118268617388499, w0=0.0, w1=0.12191571810710812\n",
      "Regularized Logistic Regression(173/999): loss=0.6116125314903731, w0=0.0, w1=0.12241073713913901\n",
      "Regularized Logistic Regression(174/999): loss=0.6113998579354936, w0=0.0, w1=0.12290374888683611\n",
      "Regularized Logistic Regression(175/999): loss=0.6111888270932394, w0=0.0, w1=0.1233947608548519\n",
      "Regularized Logistic Regression(176/999): loss=0.6109794251114375, w0=0.0, w1=0.1238837805359379\n",
      "Regularized Logistic Regression(177/999): loss=0.6107716382653893, w0=0.0, w1=0.1243708154106155\n",
      "Regularized Logistic Regression(178/999): loss=0.6105654529565658, w0=0.0, w1=0.1248558729468551\n",
      "Regularized Logistic Regression(179/999): loss=0.6103608557113159, w0=0.0, w1=0.1253389605997589\n",
      "Regularized Logistic Regression(180/999): loss=0.6101578331795913, w0=0.0, w1=0.12582008581125662\n",
      "Regularized Logistic Regression(181/999): loss=0.609956372133687, w0=0.0, w1=0.12629925600980255\n",
      "Regularized Logistic Regression(182/999): loss=0.6097564594669962, w0=0.0, w1=0.12677647861008845\n",
      "Regularized Logistic Regression(183/999): loss=0.6095580821927808, w0=0.0, w1=0.12725176101275523\n",
      "Regularized Logistic Regression(184/999): loss=0.6093612274429552, w0=0.0, w1=0.12772511060411673\n",
      "Regularized Logistic Regression(185/999): loss=0.6091658824668869, w0=0.0, w1=0.12819653475589143\n",
      "Regularized Logistic Regression(186/999): loss=0.6089720346302083, w0=0.0, w1=0.12866604082493707\n",
      "Regularized Logistic Regression(187/999): loss=0.6087796714136455, w0=0.0, w1=0.12913363615299434\n",
      "Regularized Logistic Regression(188/999): loss=0.6085887804118588, w0=0.0, w1=0.1295993280664379\n",
      "Regularized Logistic Regression(189/999): loss=0.6083993493322962, w0=0.0, w1=0.13006312387603358\n",
      "Regularized Logistic Regression(190/999): loss=0.6082113659940624, w0=0.0, w1=0.1305250308766995\n",
      "Regularized Logistic Regression(191/999): loss=0.6080248183268007, w0=0.0, w1=0.13098505634727817\n",
      "Regularized Logistic Regression(192/999): loss=0.6078396943695861, w0=0.0, w1=0.13144320755031005\n",
      "Regularized Logistic Regression(193/999): loss=0.6076559822698313, w0=0.0, w1=0.13189949173181773\n",
      "Regularized Logistic Regression(194/999): loss=0.6074736702822082, w0=0.0, w1=0.13235391612109332\n",
      "Regularized Logistic Regression(195/999): loss=0.6072927467675794, w0=0.0, w1=0.13280648793049046\n",
      "Regularized Logistic Regression(196/999): loss=0.6071132001919396, w0=0.0, w1=0.13325721435522775\n",
      "Regularized Logistic Regression(197/999): loss=0.6069350191253754, w0=0.0, w1=0.13370610257318907\n",
      "Regularized Logistic Regression(198/999): loss=0.6067581922410321, w0=0.0, w1=0.13415315974473951\n",
      "Regularized Logistic Regression(199/999): loss=0.6065827083140914, w0=0.0, w1=0.13459839301253698\n",
      "Regularized Logistic Regression(200/999): loss=0.6064085562207657, w0=0.0, w1=0.13504180950135916\n",
      "Regularized Logistic Regression(201/999): loss=0.6062357249372992, w0=0.0, w1=0.1354834163179262\n",
      "Regularized Logistic Regression(202/999): loss=0.606064203538981, w0=0.0, w1=0.1359232205507362\n",
      "Regularized Logistic Regression(203/999): loss=0.6058939811991726, w0=0.0, w1=0.13636122926989988\n",
      "Regularized Logistic Regression(204/999): loss=0.6057250471883403, w0=0.0, w1=0.13679744952698464\n",
      "Regularized Logistic Regression(205/999): loss=0.6055573908731053, w0=0.0, w1=0.13723188835486108\n",
      "Regularized Logistic Regression(206/999): loss=0.6053910017152987, w0=0.0, w1=0.13766455276755588\n",
      "Regularized Logistic Regression(207/999): loss=0.6052258692710291, w0=0.0, w1=0.1380954497601072\n",
      "Regularized Logistic Regression(208/999): loss=0.6050619831897625, w0=0.0, w1=0.13852458630842737\n",
      "Regularized Logistic Regression(209/999): loss=0.6048993332134084, w0=0.0, w1=0.1389519693691692\n",
      "Regularized Logistic Regression(210/999): loss=0.6047379091754194, w0=0.0, w1=0.13937760587959383\n",
      "Regularized Logistic Regression(211/999): loss=0.6045777009998985, w0=0.0, w1=0.13980150275744965\n",
      "Regularized Logistic Regression(212/999): loss=0.6044186987007185, w0=0.0, w1=0.14022366690084753\n",
      "Regularized Logistic Regression(213/999): loss=0.6042608923806476, w0=0.0, w1=0.14064410518814696\n",
      "Regularized Logistic Regression(214/999): loss=0.6041042722304875, w0=0.0, w1=0.14106282447784124\n",
      "Regularized Logistic Regression(215/999): loss=0.6039488285282208, w0=0.0, w1=0.1414798316084507\n",
      "Regularized Logistic Regression(216/999): loss=0.6037945516381649, w0=0.0, w1=0.14189513339841778\n",
      "Regularized Logistic Regression(217/999): loss=0.6036414320101389, w0=0.0, w1=0.14230873664600624\n",
      "Regularized Logistic Regression(218/999): loss=0.603489460178636, w0=0.0, w1=0.14272064812920457\n",
      "Regularized Logistic Regression(219/999): loss=0.6033386267620074, w0=0.0, w1=0.1431308746056331\n",
      "Regularized Logistic Regression(220/999): loss=0.6031889224616532, w0=0.0, w1=0.14353942281245402\n",
      "Regularized Logistic Regression(221/999): loss=0.6030403380612245, w0=0.0, w1=0.14394629946628648\n",
      "Regularized Logistic Regression(222/999): loss=0.6028928644258293, w0=0.0, w1=0.1443515112631236\n",
      "Regularized Logistic Regression(223/999): loss=0.6027464925012535, w0=0.0, w1=0.1447550648782558\n",
      "Regularized Logistic Regression(224/999): loss=0.6026012133131841, w0=0.0, w1=0.14515696696619254\n",
      "Regularized Logistic Regression(225/999): loss=0.6024570179664441, w0=0.0, w1=0.14555722416059266\n",
      "Regularized Logistic Regression(226/999): loss=0.6023138976442353, w0=0.0, w1=0.14595584307419435\n",
      "Regularized Logistic Regression(227/999): loss=0.6021718436073868, w0=0.0, w1=0.14635283029875096\n",
      "Regularized Logistic Regression(228/999): loss=0.6020308471936157, w0=0.0, w1=0.146748192404968\n",
      "Regularized Logistic Regression(229/999): loss=0.6018908998167917, w0=0.0, w1=0.147141935942443\n",
      "Regularized Logistic Regression(230/999): loss=0.6017519929662098, w0=0.0, w1=0.1475340674396116\n",
      "Regularized Logistic Regression(231/999): loss=0.6016141182058751, w0=0.0, w1=0.1479245934036932\n",
      "Regularized Logistic Regression(232/999): loss=0.6014772671737874, w0=0.0, w1=0.14831352032063858\n",
      "Regularized Logistic Regression(233/999): loss=0.6013414315812418, w0=0.0, w1=0.1487008546550858\n",
      "Regularized Logistic Regression(234/999): loss=0.6012066032121297, w0=0.0, w1=0.14908660285031505\n",
      "Regularized Logistic Regression(235/999): loss=0.6010727739222518, w0=0.0, w1=0.14947077132820308\n",
      "Regularized Logistic Regression(236/999): loss=0.600939935638635, w0=0.0, w1=0.14985336648918765\n",
      "Regularized Logistic Regression(237/999): loss=0.6008080803588594, w0=0.0, w1=0.1502343947122304\n",
      "Regularized Logistic Regression(238/999): loss=0.6006772001503891, w0=0.0, w1=0.15061386235478233\n",
      "Regularized Logistic Regression(239/999): loss=0.6005472871499128, w0=0.0, w1=0.15099177575275027\n",
      "Regularized Logistic Regression(240/999): loss=0.6004183335626897, w0=0.0, w1=0.1513681412204704\n",
      "Regularized Logistic Regression(241/999): loss=0.6002903316619025, w0=0.0, w1=0.1517429650506812\n",
      "Regularized Logistic Regression(242/999): loss=0.600163273788018, w0=0.0, w1=0.1521162535144984\n",
      "Regularized Logistic Regression(243/999): loss=0.6000371523481518, w0=0.0, w1=0.1524880128613935\n",
      "Regularized Logistic Regression(244/999): loss=0.5999119598154432, w0=0.0, w1=0.1528582493191734\n",
      "Regularized Logistic Regression(245/999): loss=0.5997876887284322, w0=0.0, w1=0.15322696909396338\n",
      "Regularized Logistic Regression(246/999): loss=0.5996643316904477, w0=0.0, w1=0.15359417837019207\n",
      "Regularized Logistic Regression(247/999): loss=0.5995418813689984, w0=0.0, w1=0.153959883310577\n",
      "Regularized Logistic Regression(248/999): loss=0.5994203304951706, w0=0.0, w1=0.15432409005611508\n",
      "Regularized Logistic Regression(249/999): loss=0.5992996718630333, w0=0.0, w1=0.15468680472607074\n",
      "Regularized Logistic Regression(250/999): loss=0.5991798983290496, w0=0.0, w1=0.15504803341797516\n",
      "Regularized Logistic Regression(251/999): loss=0.5990610028114911, w0=0.0, w1=0.15540778220761292\n",
      "Regularized Logistic Regression(252/999): loss=0.5989429782898626, w0=0.0, w1=0.15576605714902458\n",
      "Regularized Logistic Regression(253/999): loss=0.598825817804329, w0=0.0, w1=0.15612286427450234\n",
      "Regularized Logistic Regression(254/999): loss=0.5987095144551506, w0=0.0, w1=0.1564782095945935\n",
      "Regularized Logistic Regression(255/999): loss=0.5985940614021226, w0=0.0, w1=0.15683209909809914\n",
      "Regularized Logistic Regression(256/999): loss=0.5984794518640202, w0=0.0, w1=0.1571845387520801\n",
      "Regularized Logistic Regression(257/999): loss=0.5983656791180512, w0=0.0, w1=0.1575355345018637\n",
      "Regularized Logistic Regression(258/999): loss=0.5982527364993114, w0=0.0, w1=0.15788509227104908\n",
      "Regularized Logistic Regression(259/999): loss=0.5981406174002476, w0=0.0, w1=0.15823321796151835\n",
      "Regularized Logistic Regression(260/999): loss=0.5980293152701256, w0=0.0, w1=0.1585799174534443\n",
      "Regularized Logistic Regression(261/999): loss=0.5979188236145027, w0=0.0, w1=0.15892519660530596\n",
      "Regularized Logistic Regression(262/999): loss=0.5978091359947073, w0=0.0, w1=0.1592690612539002\n",
      "Regularized Logistic Regression(263/999): loss=0.5977002460273204, w0=0.0, w1=0.1596115172143579\n",
      "Regularized Logistic Regression(264/999): loss=0.597592147383666, w0=0.0, w1=0.15995257028016052\n",
      "Regularized Logistic Regression(265/999): loss=0.5974848337893055, w0=0.0, w1=0.16029222622315728\n",
      "Regularized Logistic Regression(266/999): loss=0.5973782990235348, w0=0.0, w1=0.16063049079358716\n",
      "Regularized Logistic Regression(267/999): loss=0.5972725369188914, w0=0.0, w1=0.16096736972009623\n",
      "Regularized Logistic Regression(268/999): loss=0.5971675413606593, w0=0.0, w1=0.16130286870976454\n",
      "Regularized Logistic Regression(269/999): loss=0.5970633062863874, w0=0.0, w1=0.16163699344812477\n",
      "Regularized Logistic Regression(270/999): loss=0.596959825685405, w0=0.0, w1=0.16196974959918972\n",
      "Regularized Logistic Regression(271/999): loss=0.5968570935983459, w0=0.0, w1=0.16230114280547916\n",
      "Regularized Logistic Regression(272/999): loss=0.5967551041166783, w0=0.0, w1=0.16263117868804464\n",
      "Regularized Logistic Regression(273/999): loss=0.5966538513822368, w0=0.0, w1=0.16295986284649983\n",
      "Regularized Logistic Regression(274/999): loss=0.5965533295867591, w0=0.0, w1=0.1632872008590494\n",
      "Regularized Logistic Regression(275/999): loss=0.5964535329714299, w0=0.0, w1=0.16361319828252002\n",
      "Regularized Logistic Regression(276/999): loss=0.5963544558264265, w0=0.0, w1=0.16393786065239302\n",
      "Regularized Logistic Regression(277/999): loss=0.5962560924904708, w0=0.0, w1=0.1642611934828358\n",
      "Regularized Logistic Regression(278/999): loss=0.5961584373503847, w0=0.0, w1=0.16458320226673537\n",
      "Regularized Logistic Regression(279/999): loss=0.5960614848406497, w0=0.0, w1=0.1649038924757361\n",
      "Regularized Logistic Regression(280/999): loss=0.5959652294429723, w0=0.0, w1=0.16522326956027308\n",
      "Regularized Logistic Regression(281/999): loss=0.5958696656858512, w0=0.0, w1=0.16554133894960993\n",
      "Regularized Logistic Regression(282/999): loss=0.5957747881441526, w0=0.0, w1=0.16585810605187745\n",
      "Regularized Logistic Regression(283/999): loss=0.5956805914386856, w0=0.0, w1=0.1661735762541117\n",
      "Regularized Logistic Regression(284/999): loss=0.5955870702357834, w0=0.0, w1=0.16648775492229415\n",
      "Regularized Logistic Regression(285/999): loss=0.5954942192468902, w0=0.0, w1=0.16680064740139164\n",
      "Regularized Logistic Regression(286/999): loss=0.5954020332281493, w0=0.0, w1=0.16711225901539828\n",
      "Regularized Logistic Regression(287/999): loss=0.5953105069799975, w0=0.0, w1=0.16742259506737725\n",
      "Regularized Logistic Regression(288/999): loss=0.5952196353467619, w0=0.0, w1=0.16773166083950514\n",
      "Regularized Logistic Regression(289/999): loss=0.5951294132162633, w0=0.0, w1=0.1680394615931146\n",
      "Regularized Logistic Regression(290/999): loss=0.5950398355194176, w0=0.0, w1=0.16834600256873944\n",
      "Regularized Logistic Regression(291/999): loss=0.5949508972298501, w0=0.0, w1=0.16865128898616055\n",
      "Regularized Logistic Regression(292/999): loss=0.5948625933635039, w0=0.0, w1=0.1689553260444507\n",
      "Regularized Logistic Regression(293/999): loss=0.59477491897826, w0=0.0, w1=0.16925811892202317\n",
      "Regularized Logistic Regression(294/999): loss=0.5946878691735563, w0=0.0, w1=0.16955967277667794\n",
      "Regularized Logistic Regression(295/999): loss=0.5946014390900122, w0=0.0, w1=0.1698599927456504\n",
      "Regularized Logistic Regression(296/999): loss=0.594515623909056, w0=0.0, w1=0.17015908394566004\n",
      "Regularized Logistic Regression(297/999): loss=0.5944304188525573, w0=0.0, w1=0.17045695147295958\n",
      "Regularized Logistic Regression(298/999): loss=0.5943458191824618, w0=0.0, w1=0.17075360040338608\n",
      "Regularized Logistic Regression(299/999): loss=0.59426182020043, w0=0.0, w1=0.1710490357924115\n",
      "Regularized Logistic Regression(300/999): loss=0.5941784172474779, w0=0.0, w1=0.17134326267519231\n",
      "Regularized Logistic Regression(301/999): loss=0.5940956057036253, w0=0.0, w1=0.17163628606662296\n",
      "Regularized Logistic Regression(302/999): loss=0.5940133809875426, w0=0.0, w1=0.17192811096138805\n",
      "Regularized Logistic Regression(303/999): loss=0.5939317385562045, w0=0.0, w1=0.17221874233401424\n",
      "Regularized Logistic Regression(304/999): loss=0.5938506739045449, w0=0.0, w1=0.17250818513892474\n",
      "Regularized Logistic Regression(305/999): loss=0.5937701825651166, w0=0.0, w1=0.17279644431049337\n",
      "Regularized Logistic Regression(306/999): loss=0.593690260107753, w0=0.0, w1=0.17308352476309732\n",
      "Regularized Logistic Regression(307/999): loss=0.5936109021392355, w0=0.0, w1=0.1733694313911739\n",
      "Regularized Logistic Regression(308/999): loss=0.5935321043029599, w0=0.0, w1=0.17365416906927425\n",
      "Regularized Logistic Regression(309/999): loss=0.5934538622786104, w0=0.0, w1=0.17393774265212122\n",
      "Regularized Logistic Regression(310/999): loss=0.5933761717818341, w0=0.0, w1=0.17422015697466134\n",
      "Regularized Logistic Regression(311/999): loss=0.5932990285639194, w0=0.0, w1=0.17450141685212706\n",
      "Regularized Logistic Regression(312/999): loss=0.5932224284114772, w0=0.0, w1=0.17478152708008868\n",
      "Regularized Logistic Regression(313/999): loss=0.5931463671461257, w0=0.0, w1=0.17506049243451416\n",
      "Regularized Logistic Regression(314/999): loss=0.5930708406241775, w0=0.0, w1=0.17533831767182756\n",
      "Regularized Logistic Regression(315/999): loss=0.5929958447363308, w0=0.0, w1=0.1756150075289648\n",
      "Regularized Logistic Regression(316/999): loss=0.5929213754073628, w0=0.0, w1=0.1758905667234326\n",
      "Regularized Logistic Regression(317/999): loss=0.592847428595825, w0=0.0, w1=0.17616499995336818\n",
      "Regularized Logistic Regression(318/999): loss=0.5927740002937437, w0=0.0, w1=0.17643831189759798\n",
      "Regularized Logistic Regression(319/999): loss=0.5927010865263225, w0=0.0, w1=0.17671050721569653\n",
      "Regularized Logistic Regression(320/999): loss=0.5926286833516442, w0=0.0, w1=0.17698159054804655\n",
      "Regularized Logistic Regression(321/999): loss=0.5925567868603839, w0=0.0, w1=0.17725156651589807\n",
      "Regularized Logistic Regression(322/999): loss=0.592485393175515, w0=0.0, w1=0.17752043972142856\n",
      "Regularized Logistic Regression(323/999): loss=0.5924144984520248, w0=0.0, w1=0.1777882147478055\n",
      "Regularized Logistic Regression(324/999): loss=0.5923440988766298, w0=0.0, w1=0.1780548961592439\n",
      "Regularized Logistic Regression(325/999): loss=0.5922741906674952, w0=0.0, w1=0.17832048850106885\n",
      "Regularized Logistic Regression(326/999): loss=0.5922047700739561, w0=0.0, w1=0.17858499629977598\n",
      "Regularized Logistic Regression(327/999): loss=0.5921358333762401, w0=0.0, w1=0.17884842406309479\n",
      "Regularized Logistic Regression(328/999): loss=0.5920673768851974, w0=0.0, w1=0.17911077628004743\n",
      "Regularized Logistic Regression(329/999): loss=0.5919993969420264, w0=0.0, w1=0.1793720574210112\n",
      "Regularized Logistic Regression(330/999): loss=0.5919318899180092, w0=0.0, w1=0.17963227193778136\n",
      "Regularized Logistic Regression(331/999): loss=0.5918648522142418, w0=0.0, w1=0.17989142426363272\n",
      "Regularized Logistic Regression(332/999): loss=0.5917982802613756, w0=0.0, w1=0.1801495188133818\n",
      "Regularized Logistic Regression(333/999): loss=0.5917321705193532, w0=0.0, w1=0.18040655998344848\n",
      "Regularized Logistic Regression(334/999): loss=0.5916665194771527, w0=0.0, w1=0.18066255215192015\n",
      "Regularized Logistic Regression(335/999): loss=0.59160132365253, w0=0.0, w1=0.18091749967861348\n",
      "Regularized Logistic Regression(336/999): loss=0.5915365795917671, w0=0.0, w1=0.18117140690513742\n",
      "Regularized Logistic Regression(337/999): loss=0.5914722838694212, w0=0.0, w1=0.18142427815495518\n",
      "Regularized Logistic Regression(338/999): loss=0.5914084330880751, w0=0.0, w1=0.181676117733449\n",
      "Regularized Logistic Regression(339/999): loss=0.5913450238780917, w0=0.0, w1=0.18192692992798162\n",
      "Regularized Logistic Regression(340/999): loss=0.5912820528973702, w0=0.0, w1=0.18217671900796012\n",
      "Regularized Logistic Regression(341/999): loss=0.5912195168311047, w0=0.0, w1=0.18242548922490104\n",
      "Regularized Logistic Regression(342/999): loss=0.5911574123915444, w0=0.0, w1=0.18267324481249028\n",
      "Regularized Logistic Regression(343/999): loss=0.5910957363177564, w0=0.0, w1=0.18291998998664952\n",
      "Regularized Logistic Regression(344/999): loss=0.5910344853753924, w0=0.0, w1=0.183165728945599\n",
      "Regularized Logistic Regression(345/999): loss=0.5909736563564536, w0=0.0, w1=0.18341046586992024\n",
      "Regularized Logistic Regression(346/999): loss=0.5909132460790633, w0=0.0, w1=0.18365420492262088\n",
      "Regularized Logistic Regression(347/999): loss=0.590853251387235, w0=0.0, w1=0.18389695024919706\n",
      "Regularized Logistic Regression(348/999): loss=0.5907936691506499, w0=0.0, w1=0.18413870597769907\n",
      "Regularized Logistic Regression(349/999): loss=0.5907344962644309, w0=0.0, w1=0.18437947621879267\n",
      "Regularized Logistic Regression(350/999): loss=0.5906757296489203, w0=0.0, w1=0.18461926506582751\n",
      "Regularized Logistic Regression(351/999): loss=0.5906173662494613, w0=0.0, w1=0.1848580765948943\n",
      "Regularized Logistic Regression(352/999): loss=0.590559403036179, w0=0.0, w1=0.18509591486489366\n",
      "Regularized Logistic Regression(353/999): loss=0.5905018370037658, w0=0.0, w1=0.18533278391759833\n",
      "Regularized Logistic Regression(354/999): loss=0.590444665171267, w0=0.0, w1=0.18556868777771782\n",
      "Regularized Logistic Regression(355/999): loss=0.5903878845818683, w0=0.0, w1=0.1858036304529607\n",
      "Regularized Logistic Regression(356/999): loss=0.5903314923026871, w0=0.0, w1=0.18603761593409948\n",
      "Regularized Logistic Regression(357/999): loss=0.5902754854245648, w0=0.0, w1=0.18627064819503578\n",
      "Regularized Logistic Regression(358/999): loss=0.5902198610618598, w0=0.0, w1=0.18650273119286162\n",
      "Regularized Logistic Regression(359/999): loss=0.5901646163522442, w0=0.0, w1=0.18673386886792476\n",
      "Regularized Logistic Regression(360/999): loss=0.5901097484565014, w0=0.0, w1=0.18696406514389166\n",
      "Regularized Logistic Regression(361/999): loss=0.5900552545583272, w0=0.0, w1=0.1871933239278125\n",
      "Regularized Logistic Regression(362/999): loss=0.5900011318641288, w0=0.0, w1=0.18742164911018397\n",
      "Regularized Logistic Regression(363/999): loss=0.5899473776028304, w0=0.0, w1=0.1876490445650125\n",
      "Regularized Logistic Regression(364/999): loss=0.5898939890256784, w0=0.0, w1=0.18787551414987858\n",
      "Regularized Logistic Regression(365/999): loss=0.5898409634060469, w0=0.0, w1=0.1881010617060007\n",
      "Regularized Logistic Regression(366/999): loss=0.5897882980392481, w0=0.0, w1=0.188325691058298\n",
      "Regularized Logistic Regression(367/999): loss=0.5897359902423428, w0=0.0, w1=0.1885494060154541\n",
      "Regularized Logistic Regression(368/999): loss=0.5896840373539518, w0=0.0, w1=0.18877221036997988\n",
      "Regularized Logistic Regression(369/999): loss=0.589632436734071, w0=0.0, w1=0.18899410789827806\n",
      "Regularized Logistic Regression(370/999): loss=0.5895811857638871, w0=0.0, w1=0.1892151023607046\n",
      "Regularized Logistic Regression(371/999): loss=0.5895302818455951, w0=0.0, w1=0.18943519750163515\n",
      "Regularized Logistic Regression(372/999): loss=0.5894797224022164, w0=0.0, w1=0.18965439704952458\n",
      "Regularized Logistic Regression(373/999): loss=0.5894295048774212, w0=0.0, w1=0.1898727047169704\n",
      "Regularized Logistic Regression(374/999): loss=0.5893796267353512, w0=0.0, w1=0.19009012420077803\n",
      "Regularized Logistic Regression(375/999): loss=0.5893300854604421, w0=0.0, w1=0.19030665918202197\n",
      "Regularized Logistic Regression(376/999): loss=0.5892808785572505, w0=0.0, w1=0.19052231332610772\n",
      "Regularized Logistic Regression(377/999): loss=0.5892320035502815, w0=0.0, w1=0.19073709028283653\n",
      "Regularized Logistic Regression(378/999): loss=0.5891834579838167, w0=0.0, w1=0.19095099368646745\n",
      "Regularized Logistic Regression(379/999): loss=0.589135239421746, w0=0.0, w1=0.19116402715577724\n",
      "Regularized Logistic Regression(380/999): loss=0.5890873454473988, w0=0.0, w1=0.1913761942941256\n",
      "Regularized Logistic Regression(381/999): loss=0.5890397736633769, w0=0.0, w1=0.19158749868951636\n",
      "Regularized Logistic Regression(382/999): loss=0.5889925216913922, w0=0.0, w1=0.1917979439146585\n",
      "Regularized Logistic Regression(383/999): loss=0.5889455871721004, w0=0.0, w1=0.19200753352703048\n",
      "Regularized Logistic Regression(384/999): loss=0.5888989677649409, w0=0.0, w1=0.19221627106893996\n",
      "Regularized Logistic Regression(385/999): loss=0.5888526611479762, w0=0.0, w1=0.19242416006758634\n",
      "Regularized Logistic Regression(386/999): loss=0.588806665017733, w0=0.0, w1=0.19263120403512207\n",
      "Regularized Logistic Regression(387/999): loss=0.5887609770890445, w0=0.0, w1=0.1928374064687145\n",
      "Regularized Logistic Regression(388/999): loss=0.5887155950948934, w0=0.0, w1=0.19304277085060773\n",
      "Regularized Logistic Regression(389/999): loss=0.5886705167862598, w0=0.0, w1=0.19324730064818116\n",
      "Regularized Logistic Regression(390/999): loss=0.5886257399319659, w0=0.0, w1=0.19345099931401447\n",
      "Regularized Logistic Regression(391/999): loss=0.588581262318525, w0=0.0, w1=0.1936538702859458\n",
      "Regularized Logistic Regression(392/999): loss=0.5885370817499911, w0=0.0, w1=0.19385591698713311\n",
      "Regularized Logistic Regression(393/999): loss=0.5884931960478093, w0=0.0, w1=0.1940571428261158\n",
      "Regularized Logistic Regression(394/999): loss=0.5884496030506691, w0=0.0, w1=0.19425755119687302\n",
      "Regularized Logistic Regression(395/999): loss=0.5884063006143573, w0=0.0, w1=0.1944571454788867\n",
      "Regularized Logistic Regression(396/999): loss=0.5883632866116132, w0=0.0, w1=0.19465592903719986\n",
      "Regularized Logistic Regression(397/999): loss=0.5883205589319846, w0=0.0, w1=0.19485390522247686\n",
      "Regularized Logistic Regression(398/999): loss=0.588278115481687, w0=0.0, w1=0.19505107737106459\n",
      "Regularized Logistic Regression(399/999): loss=0.5882359541834589, w0=0.0, w1=0.1952474488050514\n",
      "Regularized Logistic Regression(400/999): loss=0.5881940729764267, w0=0.0, w1=0.1954430228323257\n",
      "Regularized Logistic Regression(401/999): loss=0.5881524698159621, w0=0.0, w1=0.19563780274663695\n",
      "Regularized Logistic Regression(402/999): loss=0.5881111426735468, w0=0.0, w1=0.19583179182765373\n",
      "Regularized Logistic Regression(403/999): loss=0.5880700895366365, w0=0.0, w1=0.19602499334102483\n",
      "Regularized Logistic Regression(404/999): loss=0.5880293084085254, w0=0.0, w1=0.19621741053843472\n",
      "Regularized Logistic Regression(405/999): loss=0.5879887973082119, w0=0.0, w1=0.1964090466576654\n",
      "Regularized Logistic Regression(406/999): loss=0.5879485542702684, w0=0.0, w1=0.19659990492265492\n",
      "Regularized Logistic Regression(407/999): loss=0.5879085773447082, w0=0.0, w1=0.1967899885435533\n",
      "Regularized Logistic Regression(408/999): loss=0.5878688645968555, w0=0.0, w1=0.19697930071678388\n",
      "Regularized Logistic Regression(409/999): loss=0.5878294141072178, w0=0.0, w1=0.19716784462509854\n",
      "Regularized Logistic Regression(410/999): loss=0.5877902239713572, w0=0.0, w1=0.19735562343763802\n",
      "Regularized Logistic Regression(411/999): loss=0.5877512922997635, w0=0.0, w1=0.1975426403099885\n",
      "Regularized Logistic Regression(412/999): loss=0.5877126172177302, w0=0.0, w1=0.1977288983842398\n",
      "Regularized Logistic Regression(413/999): loss=0.5876741968652289, w0=0.0, w1=0.19791440078904174\n",
      "Regularized Logistic Regression(414/999): loss=0.587636029396787, w0=0.0, w1=0.19809915063966244\n",
      "Regularized Logistic Regression(415/999): loss=0.5875981129813643, w0=0.0, w1=0.1982831510380452\n",
      "Regularized Logistic Regression(416/999): loss=0.5875604458022339, w0=0.0, w1=0.19846640507286467\n",
      "Regularized Logistic Regression(417/999): loss=0.5875230260568606, w0=0.0, w1=0.19864891581958474\n",
      "Regularized Logistic Regression(418/999): loss=0.5874858519567834, w0=0.0, w1=0.19883068634051398\n",
      "Regularized Logistic Regression(419/999): loss=0.5874489217274971, w0=0.0, w1=0.1990117196848626\n",
      "Regularized Logistic Regression(420/999): loss=0.5874122336083347, w0=0.0, w1=0.19919201888879845\n",
      "Regularized Logistic Regression(421/999): loss=0.5873757858523544, w0=0.0, w1=0.19937158697550358\n",
      "Regularized Logistic Regression(422/999): loss=0.5873395767262215, w0=0.0, w1=0.19955042695522823\n",
      "Regularized Logistic Regression(423/999): loss=0.5873036045100978, w0=0.0, w1=0.1997285418253482\n",
      "Regularized Logistic Regression(424/999): loss=0.5872678674975272, w0=0.0, w1=0.19990593457042088\n",
      "Regularized Logistic Regression(425/999): loss=0.5872323639953251, w0=0.0, w1=0.20008260816223736\n",
      "Regularized Logistic Regression(426/999): loss=0.5871970923234672, w0=0.0, w1=0.2002585655598803\n",
      "Regularized Logistic Regression(427/999): loss=0.5871620508149799, w0=0.0, w1=0.200433809709778\n",
      "Regularized Logistic Regression(428/999): loss=0.5871272378158319, w0=0.0, w1=0.20060834354576024\n",
      "Regularized Logistic Regression(429/999): loss=0.5870926516848268, w0=0.0, w1=0.20078216998910964\n",
      "Regularized Logistic Regression(430/999): loss=0.5870582907934961, w0=0.0, w1=0.20095529194861986\n",
      "Regularized Logistic Regression(431/999): loss=0.5870241535259936, w0=0.0, w1=0.20112771232064594\n",
      "Regularized Logistic Regression(432/999): loss=0.5869902382789892, w0=0.0, w1=0.20129943398916111\n",
      "Regularized Logistic Regression(433/999): loss=0.5869565434615688, w0=0.0, w1=0.20147045982580944\n",
      "Regularized Logistic Regression(434/999): loss=0.5869230674951259, w0=0.0, w1=0.20164079268995916\n",
      "Regularized Logistic Regression(435/999): loss=0.5868898088132639, w0=0.0, w1=0.20181043542875673\n",
      "Regularized Logistic Regression(436/999): loss=0.5868567658616943, w0=0.0, w1=0.20197939087717892\n",
      "Regularized Logistic Regression(437/999): loss=0.586823937098134, w0=0.0, w1=0.20214766185808644\n",
      "Regularized Logistic Regression(438/999): loss=0.5867913209922083, w0=0.0, w1=0.20231525118227783\n",
      "Regularized Logistic Regression(439/999): loss=0.5867589160253514, w0=0.0, w1=0.20248216164853944\n",
      "Regularized Logistic Regression(440/999): loss=0.5867267206907103, w0=0.0, w1=0.20264839604370014\n",
      "Regularized Logistic Regression(441/999): loss=0.5866947334930451, w0=0.0, w1=0.2028139571426828\n",
      "Regularized Logistic Regression(442/999): loss=0.5866629529486356, w0=0.0, w1=0.20297884770855512\n",
      "Regularized Logistic Regression(443/999): loss=0.5866313775851858, w0=0.0, w1=0.2031430704925833\n",
      "Regularized Logistic Regression(444/999): loss=0.5866000059417291, w0=0.0, w1=0.20330662823428244\n",
      "Regularized Logistic Regression(445/999): loss=0.5865688365685353, w0=0.0, w1=0.20346952366146956\n",
      "Regularized Logistic Regression(446/999): loss=0.586537868027017, w0=0.0, w1=0.20363175949031223\n",
      "Regularized Logistic Regression(447/999): loss=0.5865070988896401, w0=0.0, w1=0.20379333842538114\n",
      "Regularized Logistic Regression(448/999): loss=0.586476527739831, w0=0.0, w1=0.20395426315970067\n",
      "Regularized Logistic Regression(449/999): loss=0.5864461531718871, w0=0.0, w1=0.20411453637480154\n",
      "Regularized Logistic Regression(450/999): loss=0.5864159737908868, w0=0.0, w1=0.2042741607407671\n",
      "Regularized Logistic Regression(451/999): loss=0.5863859882126018, w0=0.0, w1=0.20443313891628698\n",
      "Regularized Logistic Regression(452/999): loss=0.5863561950634097, w0=0.0, w1=0.2045914735487058\n",
      "Regularized Logistic Regression(453/999): loss=0.5863265929802055, w0=0.0, w1=0.2047491672740732\n",
      "Regularized Logistic Regression(454/999): loss=0.5862971806103163, w0=0.0, w1=0.20490622271719328\n",
      "Regularized Logistic Regression(455/999): loss=0.5862679566114157, w0=0.0, w1=0.2050626424916751\n",
      "Regularized Logistic Regression(456/999): loss=0.5862389196514389, w0=0.0, w1=0.20521842919998215\n",
      "Regularized Logistic Regression(457/999): loss=0.5862100684084989, w0=0.0, w1=0.20537358543347822\n",
      "Regularized Logistic Regression(458/999): loss=0.5861814015708032, w0=0.0, w1=0.2055281137724806\n",
      "Regularized Logistic Regression(459/999): loss=0.586152917836571, w0=0.0, w1=0.20568201678630518\n",
      "Regularized Logistic Regression(460/999): loss=0.5861246159139513, w0=0.0, w1=0.205835297033319\n",
      "Regularized Logistic Regression(461/999): loss=0.5860964945209434, w0=0.0, w1=0.20598795706098413\n",
      "Regularized Logistic Regression(462/999): loss=0.5860685523853129, w0=0.0, w1=0.20613999940590763\n",
      "Regularized Logistic Regression(463/999): loss=0.5860407882445161, w0=0.0, w1=0.20629142659389166\n",
      "Regularized Logistic Regression(464/999): loss=0.5860132008456186, w0=0.0, w1=0.2064422411399772\n",
      "Regularized Logistic Regression(465/999): loss=0.5859857889452168, w0=0.0, w1=0.20659244554849446\n",
      "Regularized Logistic Regression(466/999): loss=0.5859585513093624, w0=0.0, w1=0.20674204231310916\n",
      "Regularized Logistic Regression(467/999): loss=0.5859314867134827, w0=0.0, w1=0.206891033916871\n",
      "Regularized Logistic Regression(468/999): loss=0.5859045939423062, w0=0.0, w1=0.207039422832257\n",
      "Regularized Logistic Regression(469/999): loss=0.5858778717897868, w0=0.0, w1=0.20718721152122435\n",
      "Regularized Logistic Regression(470/999): loss=0.5858513190590278, w0=0.0, w1=0.20733440243525017\n",
      "Regularized Logistic Regression(471/999): loss=0.5858249345622083, w0=0.0, w1=0.20748099801538308\n",
      "Regularized Logistic Regression(472/999): loss=0.5857987171205097, w0=0.0, w1=0.20762700069228657\n",
      "Regularized Logistic Regression(473/999): loss=0.5857726655640432, w0=0.0, w1=0.20777241288628648\n",
      "Regularized Logistic Regression(474/999): loss=0.5857467787317743, w0=0.0, w1=0.20791723700741555\n",
      "Regularized Logistic Regression(475/999): loss=0.5857210554714557, w0=0.0, w1=0.20806147545546036\n",
      "Regularized Logistic Regression(476/999): loss=0.5856954946395525, w0=0.0, w1=0.20820513062000592\n",
      "Regularized Logistic Regression(477/999): loss=0.5856700951011735, w0=0.0, w1=0.20834820488048156\n",
      "Regularized Logistic Regression(478/999): loss=0.5856448557300002, w0=0.0, w1=0.20849070060620434\n",
      "Regularized Logistic Regression(479/999): loss=0.5856197754082187, w0=0.0, w1=0.20863262015642597\n",
      "Regularized Logistic Regression(480/999): loss=0.5855948530264499, w0=0.0, w1=0.20877396588037656\n",
      "Regularized Logistic Regression(481/999): loss=0.5855700874836811, w0=0.0, w1=0.20891474011730896\n",
      "Regularized Logistic Regression(482/999): loss=0.5855454776871999, w0=0.0, w1=0.20905494519654383\n",
      "Regularized Logistic Regression(483/999): loss=0.5855210225525264, w0=0.0, w1=0.20919458343751182\n",
      "Regularized Logistic Regression(484/999): loss=0.585496721003346, w0=0.0, w1=0.20933365714979982\n",
      "Regularized Logistic Regression(485/999): loss=0.5854725719714455, w0=0.0, w1=0.2094721686331937\n",
      "Regularized Logistic Regression(486/999): loss=0.5854485743966465, w0=0.0, w1=0.20961012017772182\n",
      "Regularized Logistic Regression(487/999): loss=0.5854247272267397, w0=0.0, w1=0.20974751406369754\n",
      "Regularized Logistic Regression(488/999): loss=0.585401029417424, w0=0.0, w1=0.20988435256176463\n",
      "Regularized Logistic Regression(489/999): loss=0.585377479932239, w0=0.0, w1=0.2100206379329381\n",
      "Regularized Logistic Regression(490/999): loss=0.5853540777425059, w0=0.0, w1=0.21015637242864857\n",
      "Regularized Logistic Regression(491/999): loss=0.5853308218272611, w0=0.0, w1=0.21029155829078278\n",
      "Regularized Logistic Regression(492/999): loss=0.5853077111731977, w0=0.0, w1=0.21042619775172866\n",
      "Regularized Logistic Regression(493/999): loss=0.585284744774602, w0=0.0, w1=0.21056029303441462\n",
      "Regularized Logistic Regression(494/999): loss=0.5852619216332925, w0=0.0, w1=0.21069384635235527\n",
      "Regularized Logistic Regression(495/999): loss=0.5852392407585607, w0=0.0, w1=0.21082685990969022\n",
      "Regularized Logistic Regression(496/999): loss=0.5852167011671113, w0=0.0, w1=0.2109593359012267\n",
      "Regularized Logistic Regression(497/999): loss=0.5851943018830013, w0=0.0, w1=0.21109127651248194\n",
      "Regularized Logistic Regression(498/999): loss=0.5851720419375822, w0=0.0, w1=0.21122268391972318\n",
      "Regularized Logistic Regression(499/999): loss=0.5851499203694417, w0=0.0, w1=0.21135356029000896\n",
      "Regularized Logistic Regression(500/999): loss=0.5851279362243459, w0=0.0, w1=0.2114839077812326\n",
      "Regularized Logistic Regression(501/999): loss=0.5851060885551814, w0=0.0, w1=0.21161372854215926\n",
      "Regularized Logistic Regression(502/999): loss=0.5850843764218994, w0=0.0, w1=0.21174302471246823\n",
      "Regularized Logistic Regression(503/999): loss=0.5850627988914575, w0=0.0, w1=0.21187179842279477\n",
      "Regularized Logistic Regression(504/999): loss=0.5850413550377664, w0=0.0, w1=0.21200005179476744\n",
      "Regularized Logistic Regression(505/999): loss=0.5850200439416325, w0=0.0, w1=0.2121277869410515\n",
      "Regularized Logistic Regression(506/999): loss=0.584998864690703, w0=0.0, w1=0.2122550059653854\n",
      "Regularized Logistic Regression(507/999): loss=0.5849778163794128, w0=0.0, w1=0.21238171096262287\n",
      "Regularized Logistic Regression(508/999): loss=0.5849568981089286, w0=0.0, w1=0.21250790401877195\n",
      "Regularized Logistic Regression(509/999): loss=0.5849361089870979, w0=0.0, w1=0.21263358721103326\n",
      "Regularized Logistic Regression(510/999): loss=0.584915448128393, w0=0.0, w1=0.21275876260784213\n",
      "Regularized Logistic Regression(511/999): loss=0.58489491465386, w0=0.0, w1=0.21288343226890427\n",
      "Regularized Logistic Regression(512/999): loss=0.5848745076910672, w0=0.0, w1=0.21300759824523627\n",
      "Regularized Logistic Regression(513/999): loss=0.5848542263740509, w0=0.0, w1=0.21313126257920378\n",
      "Regularized Logistic Regression(514/999): loss=0.584834069843267, w0=0.0, w1=0.21325442730456004\n",
      "Regularized Logistic Regression(515/999): loss=0.5848140372455374, w0=0.0, w1=0.2133770944464854\n",
      "Regularized Logistic Regression(516/999): loss=0.584794127734002, w0=0.0, w1=0.21349926602162345\n",
      "Regularized Logistic Regression(517/999): loss=0.5847743404680665, w0=0.0, w1=0.21362094403812112\n",
      "Regularized Logistic Regression(518/999): loss=0.584754674613355, w0=0.0, w1=0.2137421304956644\n",
      "Regularized Logistic Regression(519/999): loss=0.5847351293416584, w0=0.0, w1=0.2138628273855178\n",
      "Regularized Logistic Regression(520/999): loss=0.5847157038308872, w0=0.0, w1=0.21398303669056062\n",
      "Regularized Logistic Regression(521/999): loss=0.5846963972650228, w0=0.0, w1=0.21410276038532525\n",
      "Regularized Logistic Regression(522/999): loss=0.5846772088340693, w0=0.0, w1=0.21422200043603307\n",
      "Regularized Logistic Regression(523/999): loss=0.5846581377340068, w0=0.0, w1=0.2143407588006334\n",
      "Regularized Logistic Regression(524/999): loss=0.5846391831667423, w0=0.0, w1=0.2144590374288374\n",
      "Regularized Logistic Regression(525/999): loss=0.584620344340065, w0=0.0, w1=0.21457683826215795\n",
      "Regularized Logistic Regression(526/999): loss=0.5846016204675993, w0=0.0, w1=0.2146941632339433\n",
      "Regularized Logistic Regression(527/999): loss=0.5845830107687581, w0=0.0, w1=0.21481101426941468\n",
      "Regularized Logistic Regression(528/999): loss=0.5845645144686977, w0=0.0, w1=0.21492739328570445\n",
      "Regularized Logistic Regression(529/999): loss=0.5845461307982734, w0=0.0, w1=0.2150433021918875\n",
      "Regularized Logistic Regression(530/999): loss=0.5845278589939928, w0=0.0, w1=0.21515874288902098\n",
      "Regularized Logistic Regression(531/999): loss=0.5845096982979737, w0=0.0, w1=0.2152737172701777\n",
      "Regularized Logistic Regression(532/999): loss=0.5844916479578973, w0=0.0, w1=0.2153882272204828\n",
      "Regularized Logistic Regression(533/999): loss=0.5844737072269673, w0=0.0, w1=0.21550227461715013\n",
      "Regularized Logistic Regression(534/999): loss=0.5844558753638647, w0=0.0, w1=0.2156158613295149\n",
      "Regularized Logistic Regression(535/999): loss=0.5844381516327051, w0=0.0, w1=0.2157289892190696\n",
      "Regularized Logistic Regression(536/999): loss=0.5844205353029968, w0=0.0, w1=0.21584166013950049\n",
      "Regularized Logistic Regression(537/999): loss=0.584403025649597, w0=0.0, w1=0.2159538759367197\n",
      "Regularized Logistic Regression(538/999): loss=0.5843856219526725, w0=0.0, w1=0.2160656384489006\n",
      "Regularized Logistic Regression(539/999): loss=0.5843683234976552, w0=0.0, w1=0.21617694950651523\n",
      "Regularized Logistic Regression(540/999): loss=0.5843511295752031, w0=0.0, w1=0.2162878109323636\n",
      "Regularized Logistic Regression(541/999): loss=0.5843340394811579, w0=0.0, w1=0.21639822454160967\n",
      "Regularized Logistic Regression(542/999): loss=0.5843170525165056, w0=0.0, w1=0.21650819214181716\n",
      "Regularized Logistic Regression(543/999): loss=0.5843001679873359, w0=0.0, w1=0.2166177155329815\n",
      "Regularized Logistic Regression(544/999): loss=0.5842833852048017, w0=0.0, w1=0.21672679650756324\n",
      "Regularized Logistic Regression(545/999): loss=0.5842667034850807, w0=0.0, w1=0.2168354368505223\n",
      "Regularized Logistic Regression(546/999): loss=0.5842501221493356, w0=0.0, w1=0.21694363833935054\n",
      "Regularized Logistic Regression(547/999): loss=0.5842336405236747, w0=0.0, w1=0.21705140274410625\n",
      "Regularized Logistic Regression(548/999): loss=0.5842172579391146, w0=0.0, w1=0.21715873182744477\n",
      "Regularized Logistic Regression(549/999): loss=0.5842009737315412, w0=0.0, w1=0.21726562734465424\n",
      "Regularized Logistic Regression(550/999): loss=0.5841847872416716, w0=0.0, w1=0.2173720910436857\n",
      "Regularized Logistic Regression(551/999): loss=0.5841686978150169, w0=0.0, w1=0.21747812466518712\n",
      "Regularized Logistic Regression(552/999): loss=0.5841527048018449, w0=0.0, w1=0.21758372994253503\n",
      "Regularized Logistic Regression(553/999): loss=0.5841368075571423, w0=0.0, w1=0.21768890860186715\n",
      "Regularized Logistic Regression(554/999): loss=0.5841210054405799, w0=0.0, w1=0.21779366236211511\n",
      "Regularized Logistic Regression(555/999): loss=0.5841052978164746, w0=0.0, w1=0.21789799293503495\n",
      "Regularized Logistic Regression(556/999): loss=0.584089684053753, w0=0.0, w1=0.2180019020252392\n",
      "Regularized Logistic Regression(557/999): loss=0.584074163525918, w0=0.0, w1=0.21810539133023046\n",
      "Regularized Logistic Regression(558/999): loss=0.58405873561101, w0=0.0, w1=0.21820846254042947\n",
      "Regularized Logistic Regression(559/999): loss=0.5840433996915748, w0=0.0, w1=0.21831111733920996\n",
      "Regularized Logistic Regression(560/999): loss=0.5840281551546272, w0=0.0, w1=0.21841335740292825\n",
      "Regularized Logistic Regression(561/999): loss=0.5840130013916157, w0=0.0, w1=0.2185151844009534\n",
      "Regularized Logistic Regression(562/999): loss=0.5839979377983904, w0=0.0, w1=0.21861659999569927\n",
      "Regularized Logistic Regression(563/999): loss=0.5839829637751662, w0=0.0, w1=0.21871760584265557\n",
      "Regularized Logistic Regression(564/999): loss=0.5839680787264916, w0=0.0, w1=0.21881820359041781\n",
      "Regularized Logistic Regression(565/999): loss=0.5839532820612133, w0=0.0, w1=0.2189183948807176\n",
      "Regularized Logistic Regression(566/999): loss=0.5839385731924444, w0=0.0, w1=0.21901818134845408\n",
      "Regularized Logistic Regression(567/999): loss=0.5839239515375305, w0=0.0, w1=0.21911756462172363\n",
      "Regularized Logistic Regression(568/999): loss=0.5839094165180178, w0=0.0, w1=0.21921654632184773\n",
      "Regularized Logistic Regression(569/999): loss=0.5838949675596196, w0=0.0, w1=0.2193151280634072\n",
      "Regularized Logistic Regression(570/999): loss=0.5838806040921858, w0=0.0, w1=0.21941331145426823\n",
      "Regularized Logistic Regression(571/999): loss=0.5838663255496693, w0=0.0, w1=0.21951109809561442\n",
      "Regularized Logistic Regression(572/999): loss=0.583852131370096, w0=0.0, w1=0.21960848958197482\n",
      "Regularized Logistic Regression(573/999): loss=0.5838380209955318, w0=0.0, w1=0.21970548750125368\n",
      "Regularized Logistic Regression(574/999): loss=0.5838239938720532, w0=0.0, w1=0.2198020934347601\n",
      "Regularized Logistic Regression(575/999): loss=0.5838100494497155, w0=0.0, w1=0.21989830895723728\n",
      "Regularized Logistic Regression(576/999): loss=0.5837961871825219, w0=0.0, w1=0.21999413563689024\n",
      "Regularized Logistic Regression(577/999): loss=0.5837824065283946, w0=0.0, w1=0.22008957503541635\n",
      "Regularized Logistic Regression(578/999): loss=0.5837687069491427, w0=0.0, w1=0.22018462870803207\n",
      "Regularized Logistic Regression(579/999): loss=0.5837550879104345, w0=0.0, w1=0.22027929820350378\n",
      "Regularized Logistic Regression(580/999): loss=0.5837415488817663, w0=0.0, w1=0.22037358506417337\n",
      "Regularized Logistic Regression(581/999): loss=0.583728089336434, w0=0.0, w1=0.22046749082599043\n",
      "Regularized Logistic Regression(582/999): loss=0.5837147087515032, w0=0.0, w1=0.22056101701853592\n",
      "Regularized Logistic Regression(583/999): loss=0.5837014066077807, w0=0.0, w1=0.22065416516505434\n",
      "Regularized Logistic Regression(584/999): loss=0.5836881823897867, w0=0.0, w1=0.22074693678247928\n",
      "Regularized Logistic Regression(585/999): loss=0.5836750355857243, w0=0.0, w1=0.22083933338146092\n",
      "Regularized Logistic Regression(586/999): loss=0.5836619656874544, w0=0.0, w1=0.22093135646639422\n",
      "Regularized Logistic Regression(587/999): loss=0.5836489721904646, w0=0.0, w1=0.22102300753544885\n",
      "Regularized Logistic Regression(588/999): loss=0.5836360545938434, w0=0.0, w1=0.2211142880805912\n",
      "Regularized Logistic Regression(589/999): loss=0.5836232124002518, w0=0.0, w1=0.2212051995876174\n",
      "Regularized Logistic Regression(590/999): loss=0.5836104451158973, w0=0.0, w1=0.22129574353617643\n",
      "Regularized Logistic Regression(591/999): loss=0.5835977522505053, w0=0.0, w1=0.2213859213997995\n",
      "Regularized Logistic Regression(592/999): loss=0.5835851333172928, w0=0.0, w1=0.2214757346459246\n",
      "Regularized Logistic Regression(593/999): loss=0.5835725878329431, w0=0.0, w1=0.22156518473592599\n",
      "Regularized Logistic Regression(594/999): loss=0.5835601153175765, w0=0.0, w1=0.2216542731251403\n",
      "Regularized Logistic Regression(595/999): loss=0.5835477152947274, w0=0.0, w1=0.22174300126288882\n",
      "Regularized Logistic Regression(596/999): loss=0.5835353872913164, w0=0.0, w1=0.22183137059251065\n",
      "Regularized Logistic Regression(597/999): loss=0.5835231308376247, w0=0.0, w1=0.2219193825513841\n",
      "Regularized Logistic Regression(598/999): loss=0.5835109454672689, w0=0.0, w1=0.22200703857095397\n",
      "Regularized Logistic Regression(599/999): loss=0.5834988307171758, w0=0.0, w1=0.22209434007675877\n",
      "Regularized Logistic Regression(600/999): loss=0.5834867861275567, w0=0.0, w1=0.22218128848845575\n",
      "Regularized Logistic Regression(601/999): loss=0.5834748112418829, w0=0.0, w1=0.22226788521984622\n",
      "Regularized Logistic Regression(602/999): loss=0.5834629056068615, w0=0.0, w1=0.22235413167890125\n",
      "Regularized Logistic Regression(603/999): loss=0.5834510687724094, w0=0.0, w1=0.22244002926778877\n",
      "Regularized Logistic Regression(604/999): loss=0.5834393002916306, w0=0.0, w1=0.22252557938289633\n",
      "Regularized Logistic Regression(605/999): loss=0.58342759972079, w0=0.0, w1=0.2226107834148599\n",
      "Regularized Logistic Regression(606/999): loss=0.5834159666192926, w0=0.0, w1=0.22269564274858458\n",
      "Regularized Logistic Regression(607/999): loss=0.5834044005496558, w0=0.0, w1=0.22278015876327345\n",
      "Regularized Logistic Regression(608/999): loss=0.5833929010774894, w0=0.0, w1=0.22286433283245213\n",
      "Regularized Logistic Regression(609/999): loss=0.5833814677714705, w0=0.0, w1=0.22294816632399062\n",
      "Regularized Logistic Regression(610/999): loss=0.5833701002033207, w0=0.0, w1=0.2230316606001311\n",
      "Regularized Logistic Regression(611/999): loss=0.5833587979477816, w0=0.0, w1=0.2231148170175108\n",
      "Regularized Logistic Regression(612/999): loss=0.5833475605825953, w0=0.0, w1=0.22319763692718783\n",
      "Regularized Logistic Regression(613/999): loss=0.583336387688479, w0=0.0, w1=0.2232801216746641\n",
      "Regularized Logistic Regression(614/999): loss=0.5833252788491025, w0=0.0, w1=0.22336227259991145\n",
      "Regularized Logistic Regression(615/999): loss=0.5833142336510682, w0=0.0, w1=0.2234440910373938\n",
      "Regularized Logistic Regression(616/999): loss=0.5833032516838873, w0=0.0, w1=0.22352557831609218\n",
      "Regularized Logistic Regression(617/999): loss=0.5832923325399579, w0=0.0, w1=0.2236067357595279\n",
      "Regularized Logistic Regression(618/999): loss=0.5832814758145435, w0=0.0, w1=0.22368756468578757\n",
      "Regularized Logistic Regression(619/999): loss=0.5832706811057516, w0=0.0, w1=0.22376806640754573\n",
      "Regularized Logistic Regression(620/999): loss=0.5832599480145125, w0=0.0, w1=0.22384824223209\n",
      "Regularized Logistic Regression(621/999): loss=0.5832492761445581, w0=0.0, w1=0.2239280934613416\n",
      "Regularized Logistic Regression(622/999): loss=0.5832386651023996, w0=0.0, w1=0.2240076213918816\n",
      "Regularized Logistic Regression(623/999): loss=0.5832281144973083, w0=0.0, w1=0.22408682731497262\n",
      "Regularized Logistic Regression(624/999): loss=0.583217623941294, w0=0.0, w1=0.22416571251658357\n",
      "Regularized Logistic Regression(625/999): loss=0.5832071930490842, w0=0.0, w1=0.22424427827740961\n",
      "Regularized Logistic Regression(626/999): loss=0.5831968214381049, w0=0.0, w1=0.2243225258728984\n",
      "Regularized Logistic Regression(627/999): loss=0.5831865087284598, w0=0.0, w1=0.2244004565732707\n",
      "Regularized Logistic Regression(628/999): loss=0.5831762545429086, w0=0.0, w1=0.22447807164354344\n",
      "Regularized Logistic Regression(629/999): loss=0.5831660585068498, w0=0.0, w1=0.2245553723435551\n",
      "Regularized Logistic Regression(630/999): loss=0.5831559202482997, w0=0.0, w1=0.2246323599279838\n",
      "Regularized Logistic Regression(631/999): loss=0.583145839397872, w0=0.0, w1=0.22470903564637232\n",
      "Regularized Logistic Regression(632/999): loss=0.58313581558876, w0=0.0, w1=0.2247854007431499\n",
      "Regularized Logistic Regression(633/999): loss=0.5831258484567162, w0=0.0, w1=0.2248614564576543\n",
      "Regularized Logistic Regression(634/999): loss=0.5831159376400332, w0=0.0, w1=0.2249372040241542\n",
      "Regularized Logistic Regression(635/999): loss=0.5831060827795259, w0=0.0, w1=0.22501264467187052\n",
      "Regularized Logistic Regression(636/999): loss=0.5830962835185106, w0=0.0, w1=0.22508777962499896\n",
      "Regularized Logistic Regression(637/999): loss=0.5830865395027887, w0=0.0, w1=0.2251626101027319\n",
      "Regularized Logistic Regression(638/999): loss=0.583076850380626, w0=0.0, w1=0.22523713731927933\n",
      "Regularized Logistic Regression(639/999): loss=0.5830672158027361, w0=0.0, w1=0.22531136248389033\n",
      "Regularized Logistic Regression(640/999): loss=0.5830576354222607, w0=0.0, w1=0.225385286800876\n",
      "Regularized Logistic Regression(641/999): loss=0.5830481088947533, w0=0.0, w1=0.22545891146962935\n",
      "Regularized Logistic Regression(642/999): loss=0.5830386358781592, w0=0.0, w1=0.22553223768464678\n",
      "Regularized Logistic Regression(643/999): loss=0.5830292160327984, w0=0.0, w1=0.22560526663555056\n",
      "Regularized Logistic Regression(644/999): loss=0.5830198490213496, w0=0.0, w1=0.22567799950710785\n",
      "Regularized Logistic Regression(645/999): loss=0.5830105345088301, w0=0.0, w1=0.2257504374792531\n",
      "Regularized Logistic Regression(646/999): loss=0.5830012721625799, w0=0.0, w1=0.2258225817271088\n",
      "Regularized Logistic Regression(647/999): loss=0.5829920616522444, w0=0.0, w1=0.2258944334210057\n",
      "Regularized Logistic Regression(648/999): loss=0.5829829026497565, w0=0.0, w1=0.22596599372650375\n",
      "Regularized Logistic Regression(649/999): loss=0.5829737948293209, w0=0.0, w1=0.22603726380441413\n",
      "Regularized Logistic Regression(650/999): loss=0.5829647378673961, w0=0.0, w1=0.22610824481081584\n",
      "Regularized Logistic Regression(651/999): loss=0.582955731442678, w0=0.0, w1=0.22617893789708068\n",
      "Regularized Logistic Regression(652/999): loss=0.5829467752360837, w0=0.0, w1=0.22624934420989112\n",
      "Regularized Logistic Regression(653/999): loss=0.5829378689307347, w0=0.0, w1=0.22631946489126012\n",
      "Regularized Logistic Regression(654/999): loss=0.5829290122119413, w0=0.0, w1=0.22638930107855335\n",
      "Regularized Logistic Regression(655/999): loss=0.5829202047671848, w0=0.0, w1=0.2264588539045062\n",
      "Regularized Logistic Regression(656/999): loss=0.5829114462861025, w0=0.0, w1=0.2265281244972465\n",
      "Regularized Logistic Regression(657/999): loss=0.5829027364604724, w0=0.0, w1=0.22659711398031354\n",
      "Regularized Logistic Regression(658/999): loss=0.582894074984196, w0=0.0, w1=0.22666582347267727\n",
      "Regularized Logistic Regression(659/999): loss=0.5828854615532844, w0=0.0, w1=0.2267342540887576\n",
      "Regularized Logistic Regression(660/999): loss=0.5828768958658403, w0=0.0, w1=0.22680240693844542\n",
      "Regularized Logistic Regression(661/999): loss=0.5828683776220447, w0=0.0, w1=0.22687028312712004\n",
      "Regularized Logistic Regression(662/999): loss=0.5828599065241404, w0=0.0, w1=0.22693788375567145\n",
      "Regularized Logistic Regression(663/999): loss=0.5828514822764177, w0=0.0, w1=0.2270052099205174\n",
      "Regularized Logistic Regression(664/999): loss=0.5828431045851976, w0=0.0, w1=0.22707226271362235\n",
      "Regularized Logistic Regression(665/999): loss=0.5828347731588197, w0=0.0, w1=0.22713904322251782\n",
      "Regularized Logistic Regression(666/999): loss=0.5828264877076245, w0=0.0, w1=0.22720555253032065\n",
      "Regularized Logistic Regression(667/999): loss=0.5828182479439401, w0=0.0, w1=0.2272717917157532\n",
      "Regularized Logistic Regression(668/999): loss=0.5828100535820679, w0=0.0, w1=0.22733776185316062\n",
      "Regularized Logistic Regression(669/999): loss=0.5828019043382665, w0=0.0, w1=0.22740346401253073\n",
      "Regularized Logistic Regression(670/999): loss=0.5827937999307393, w0=0.0, w1=0.2274688992595113\n",
      "Regularized Logistic Regression(671/999): loss=0.5827857400796186, w0=0.0, w1=0.2275340686554295\n",
      "Regularized Logistic Regression(672/999): loss=0.582777724506953, w0=0.0, w1=0.22759897325731282\n",
      "Regularized Logistic Regression(673/999): loss=0.5827697529366912, w0=0.0, w1=0.22766361411790198\n",
      "Regularized Logistic Regression(674/999): loss=0.5827618250946706, w0=0.0, w1=0.2277279922856741\n",
      "Regularized Logistic Regression(675/999): loss=0.5827539407086009, w0=0.0, w1=0.22779210880485867\n",
      "Regularized Logistic Regression(676/999): loss=0.5827460995080527, w0=0.0, w1=0.2278559647154572\n",
      "Regularized Logistic Regression(677/999): loss=0.5827383012244428, w0=0.0, w1=0.227919561053259\n",
      "Regularized Logistic Regression(678/999): loss=0.5827305455910192, w0=0.0, w1=0.2279828988498619\n",
      "Regularized Logistic Regression(679/999): loss=0.5827228323428517, w0=0.0, w1=0.22804597913268812\n",
      "Regularized Logistic Regression(680/999): loss=0.5827151612168141, w0=0.0, w1=0.22810880292500305\n",
      "Regularized Logistic Regression(681/999): loss=0.5827075319515738, w0=0.0, w1=0.22817137124593279\n",
      "Regularized Logistic Regression(682/999): loss=0.5826999442875775, w0=0.0, w1=0.2282336851104812\n",
      "Regularized Logistic Regression(683/999): loss=0.5826923979670395, w0=0.0, w1=0.2282957455295496\n",
      "Regularized Logistic Regression(684/999): loss=0.5826848927339261, w0=0.0, w1=0.22835755350995088\n",
      "Regularized Logistic Regression(685/999): loss=0.5826774283339468, w0=0.0, w1=0.22841911005442986\n",
      "Regularized Logistic Regression(686/999): loss=0.5826700045145377, w0=0.0, w1=0.22848041616167966\n",
      "Regularized Logistic Regression(687/999): loss=0.5826626210248514, w0=0.0, w1=0.22854147282635834\n",
      "Regularized Logistic Regression(688/999): loss=0.5826552776157438, w0=0.0, w1=0.22860228103910765\n",
      "Regularized Logistic Regression(689/999): loss=0.5826479740397611, w0=0.0, w1=0.22866284178656776\n",
      "Regularized Logistic Regression(690/999): loss=0.5826407100511278, w0=0.0, w1=0.22872315605139656\n",
      "Regularized Logistic Regression(691/999): loss=0.5826334854057359, w0=0.0, w1=0.2287832248122846\n",
      "Regularized Logistic Regression(692/999): loss=0.5826262998611299, w0=0.0, w1=0.22884304904397407\n",
      "Regularized Logistic Regression(693/999): loss=0.5826191531764976, w0=0.0, w1=0.22890262971727438\n",
      "Regularized Logistic Regression(694/999): loss=0.5826120451126565, w0=0.0, w1=0.2289619677990792\n",
      "Regularized Logistic Regression(695/999): loss=0.5826049754320428, w0=0.0, w1=0.22902106425238256\n",
      "Regularized Logistic Regression(696/999): loss=0.5825979438986986, w0=0.0, w1=0.22907992003629568\n",
      "Regularized Logistic Regression(697/999): loss=0.5825909502782619, w0=0.0, w1=0.2291385361060638\n",
      "Regularized Logistic Regression(698/999): loss=0.5825839943379527, w0=0.0, w1=0.22919691341308343\n",
      "Regularized Logistic Regression(699/999): loss=0.5825770758465645, w0=0.0, w1=0.22925505290491585\n",
      "Regularized Logistic Regression(700/999): loss=0.5825701945744499, w0=0.0, w1=0.229312955525306\n",
      "Regularized Logistic Regression(701/999): loss=0.5825633502935118, w0=0.0, w1=0.2293706222141973\n",
      "Regularized Logistic Regression(702/999): loss=0.5825565427771897, w0=0.0, w1=0.2294280539077499\n",
      "Regularized Logistic Regression(703/999): loss=0.5825497718004514, w0=0.0, w1=0.2294852515383525\n",
      "Regularized Logistic Regression(704/999): loss=0.5825430371397798, w0=0.0, w1=0.22954221603464334\n",
      "Regularized Logistic Regression(705/999): loss=0.5825363385731619, w0=0.0, w1=0.22959894832152264\n",
      "Regularized Logistic Regression(706/999): loss=0.5825296758800803, w0=0.0, w1=0.22965544932016915\n",
      "Regularized Logistic Regression(707/999): loss=0.5825230488414993, w0=0.0, w1=0.22971171994805628\n",
      "Regularized Logistic Regression(708/999): loss=0.5825164572398565, w0=0.0, w1=0.22976776111896832\n",
      "Regularized Logistic Regression(709/999): loss=0.5825099008590504, w0=0.0, w1=0.2298235737430148\n",
      "Regularized Logistic Regression(710/999): loss=0.5825033794844315, w0=0.0, w1=0.22987915872664705\n",
      "Regularized Logistic Regression(711/999): loss=0.582496892902791, w0=0.0, w1=0.22993451697267267\n",
      "Regularized Logistic Regression(712/999): loss=0.5824904409023504, w0=0.0, w1=0.22998964938027175\n",
      "Regularized Logistic Regression(713/999): loss=0.5824840232727513, w0=0.0, w1=0.23004455684501224\n",
      "Regularized Logistic Regression(714/999): loss=0.5824776398050449, w0=0.0, w1=0.23009924025886466\n",
      "Regularized Logistic Regression(715/999): loss=0.5824712902916825, w0=0.0, w1=0.23015370051021763\n",
      "Regularized Logistic Regression(716/999): loss=0.5824649745265049, w0=0.0, w1=0.23020793848389143\n",
      "Regularized Logistic Regression(717/999): loss=0.5824586923047325, w0=0.0, w1=0.23026195506115693\n",
      "Regularized Logistic Regression(718/999): loss=0.5824524434229558, w0=0.0, w1=0.2303157511197469\n",
      "Regularized Logistic Regression(719/999): loss=0.5824462276791244, w0=0.0, w1=0.23036932753387176\n",
      "Regularized Logistic Regression(720/999): loss=0.5824400448725389, w0=0.0, w1=0.2304226851742352\n",
      "Regularized Logistic Regression(721/999): loss=0.5824338948038389, w0=0.0, w1=0.2304758249080483\n",
      "Regularized Logistic Regression(722/999): loss=0.582427777274997, w0=0.0, w1=0.23052874759904485\n",
      "Regularized Logistic Regression(723/999): loss=0.5824216920893054, w0=0.0, w1=0.23058145410749523\n",
      "Regularized Logistic Regression(724/999): loss=0.5824156390513683, w0=0.0, w1=0.23063394529022208\n",
      "Regularized Logistic Regression(725/999): loss=0.5824096179670925, w0=0.0, w1=0.23068622200061256\n",
      "Regularized Logistic Regression(726/999): loss=0.5824036286436777, w0=0.0, w1=0.23073828508863534\n",
      "Regularized Logistic Regression(727/999): loss=0.5823976708896077, w0=0.0, w1=0.23079013540085383\n",
      "Regularized Logistic Regression(728/999): loss=0.5823917445146409, w0=0.0, w1=0.230841773780441\n",
      "Regularized Logistic Regression(729/999): loss=0.5823858493298003, w0=0.0, w1=0.23089320106719172\n",
      "Regularized Logistic Regression(730/999): loss=0.5823799851473664, w0=0.0, w1=0.23094441809754032\n",
      "Regularized Logistic Regression(731/999): loss=0.5823741517808666, w0=0.0, w1=0.23099542570457124\n",
      "Regularized Logistic Regression(732/999): loss=0.5823683490450671, w0=0.0, w1=0.23104622471803576\n",
      "Regularized Logistic Regression(733/999): loss=0.5823625767559634, w0=0.0, w1=0.23109681596436393\n",
      "Regularized Logistic Regression(734/999): loss=0.5823568347307722, w0=0.0, w1=0.2311472002666797\n",
      "Regularized Logistic Regression(735/999): loss=0.5823511227879222, w0=0.0, w1=0.2311973784448161\n",
      "Regularized Logistic Regression(736/999): loss=0.5823454407470454, w0=0.0, w1=0.23124735131532476\n",
      "Regularized Logistic Regression(737/999): loss=0.5823397884289702, w0=0.0, w1=0.2312971196914938\n",
      "Regularized Logistic Regression(738/999): loss=0.582334165655709, w0=0.0, w1=0.23134668438336026\n",
      "Regularized Logistic Regression(739/999): loss=0.5823285722504541, w0=0.0, w1=0.23139604619772292\n",
      "Regularized Logistic Regression(740/999): loss=0.5823230080375668, w0=0.0, w1=0.23144520593815585\n",
      "Regularized Logistic Regression(741/999): loss=0.5823174728425695, w0=0.0, w1=0.23149416440502393\n",
      "Regularized Logistic Regression(742/999): loss=0.5823119664921381, w0=0.0, w1=0.23154292239549384\n",
      "Regularized Logistic Regression(743/999): loss=0.5823064888140932, w0=0.0, w1=0.23159148070354843\n",
      "Regularized Logistic Regression(744/999): loss=0.5823010396373912, w0=0.0, w1=0.23163984011999986\n",
      "Regularized Logistic Regression(745/999): loss=0.5822956187921183, w0=0.0, w1=0.23168800143250398\n",
      "Regularized Logistic Regression(746/999): loss=0.5822902261094809, w0=0.0, w1=0.2317359654255716\n",
      "Regularized Logistic Regression(747/999): loss=0.5822848614217971, w0=0.0, w1=0.23178373288058157\n",
      "Regularized Logistic Regression(748/999): loss=0.5822795245624914, w0=0.0, w1=0.23183130457579637\n",
      "Regularized Logistic Regression(749/999): loss=0.5822742153660838, w0=0.0, w1=0.23187868128637343\n",
      "Regularized Logistic Regression(750/999): loss=0.5822689336681846, w0=0.0, w1=0.23192586378437738\n",
      "Regularized Logistic Regression(751/999): loss=0.5822636793054846, w0=0.0, w1=0.23197285283879435\n",
      "Regularized Logistic Regression(752/999): loss=0.5822584521157483, w0=0.0, w1=0.2320196492155438\n",
      "Regularized Logistic Regression(753/999): loss=0.582253251937807, w0=0.0, w1=0.2320662536774919\n",
      "Regularized Logistic Regression(754/999): loss=0.58224807861155, w0=0.0, w1=0.23211266698446487\n",
      "Regularized Logistic Regression(755/999): loss=0.5822429319779184, w0=0.0, w1=0.23215888989325925\n",
      "Regularized Logistic Regression(756/999): loss=0.5822378118788964, w0=0.0, w1=0.23220492315765717\n",
      "Regularized Logistic Regression(757/999): loss=0.5822327181575047, w0=0.0, w1=0.23225076752843737\n",
      "Regularized Logistic Regression(758/999): loss=0.5822276506577929, w0=0.0, w1=0.23229642375338833\n",
      "Regularized Logistic Regression(759/999): loss=0.5822226092248319, w0=0.0, w1=0.23234189257731952\n",
      "Regularized Logistic Regression(760/999): loss=0.5822175937047082, w0=0.0, w1=0.23238717474207696\n",
      "Regularized Logistic Regression(761/999): loss=0.582212603944515, w0=0.0, w1=0.2324322709865516\n",
      "Regularized Logistic Regression(762/999): loss=0.5822076397923454, w0=0.0, w1=0.23247718204669282\n",
      "Regularized Logistic Regression(763/999): loss=0.582202701097286, w0=0.0, w1=0.23252190865552325\n",
      "Regularized Logistic Regression(764/999): loss=0.5821977877094101, w0=0.0, w1=0.23256645154314756\n",
      "Regularized Logistic Regression(765/999): loss=0.5821928994797695, w0=0.0, w1=0.2326108114367655\n",
      "Regularized Logistic Regression(766/999): loss=0.5821880362603885, w0=0.0, w1=0.23265498906068557\n",
      "Regularized Logistic Regression(767/999): loss=0.5821831979042574, w0=0.0, w1=0.23269898513633414\n",
      "Regularized Logistic Regression(768/999): loss=0.5821783842653245, w0=0.0, w1=0.2327428003822696\n",
      "Regularized Logistic Regression(769/999): loss=0.5821735951984907, w0=0.0, w1=0.23278643551419448\n",
      "Regularized Logistic Regression(770/999): loss=0.5821688305596018, w0=0.0, w1=0.2328298912449656\n",
      "Regularized Logistic Regression(771/999): loss=0.5821640902054419, w0=0.0, w1=0.23287316828460694\n",
      "Regularized Logistic Regression(772/999): loss=0.5821593739937285, w0=0.0, w1=0.23291626734032053\n",
      "Regularized Logistic Regression(773/999): loss=0.5821546817831036, w0=0.0, w1=0.23295918911650013\n",
      "Regularized Logistic Regression(774/999): loss=0.5821500134331276, w0=0.0, w1=0.23300193431473987\n",
      "Regularized Logistic Regression(775/999): loss=0.582145368804275, w0=0.0, w1=0.233044503633849\n",
      "Regularized Logistic Regression(776/999): loss=0.5821407477579252, w0=0.0, w1=0.2330868977698603\n",
      "Regularized Logistic Regression(777/999): loss=0.5821361501563586, w0=0.0, w1=0.23312911741604425\n",
      "Regularized Logistic Regression(778/999): loss=0.5821315758627478, w0=0.0, w1=0.23317116326291842\n",
      "Regularized Logistic Regression(779/999): loss=0.5821270247411541, w0=0.0, w1=0.23321303599826135\n",
      "Regularized Logistic Regression(780/999): loss=0.5821224966565179, w0=0.0, w1=0.23325473630711988\n",
      "Regularized Logistic Regression(781/999): loss=0.5821179914746568, w0=0.0, w1=0.23329626487182442\n",
      "Regularized Logistic Regression(782/999): loss=0.5821135090622558, w0=0.0, w1=0.23333762237199748\n",
      "Regularized Logistic Regression(783/999): loss=0.582109049286863, w0=0.0, w1=0.23337880948456682\n",
      "Regularized Logistic Regression(784/999): loss=0.5821046120168829, w0=0.0, w1=0.23341982688377522\n",
      "Regularized Logistic Regression(785/999): loss=0.5821001971215709, w0=0.0, w1=0.23346067524119254\n",
      "Regularized Logistic Regression(786/999): loss=0.5820958044710273, w0=0.0, w1=0.23350135522572443\n",
      "Regularized Logistic Regression(787/999): loss=0.5820914339361911, w0=0.0, w1=0.23354186750362707\n",
      "Regularized Logistic Regression(788/999): loss=0.5820870853888346, w0=0.0, w1=0.23358221273851526\n",
      "Regularized Logistic Regression(789/999): loss=0.582082758701557, w0=0.0, w1=0.2336223915913734\n",
      "Regularized Logistic Regression(790/999): loss=0.582078453747779, w0=0.0, w1=0.23366240472056796\n",
      "Regularized Logistic Regression(791/999): loss=0.5820741704017367, w0=0.0, w1=0.23370225278185708\n",
      "Regularized Logistic Regression(792/999): loss=0.5820699085384776, w0=0.0, w1=0.23374193642840155\n",
      "Regularized Logistic Regression(793/999): loss=0.5820656680338521, w0=0.0, w1=0.23378145631077504\n",
      "Regularized Logistic Regression(794/999): loss=0.5820614487645099, w0=0.0, w1=0.2338208130769757\n",
      "Regularized Logistic Regression(795/999): loss=0.5820572506078941, w0=0.0, w1=0.23386000737243678\n",
      "Regularized Logistic Regression(796/999): loss=0.582053073442236, w0=0.0, w1=0.23389903984003516\n",
      "Regularized Logistic Regression(797/999): loss=0.5820489171465484, w0=0.0, w1=0.23393791112010343\n",
      "Regularized Logistic Regression(798/999): loss=0.5820447816006209, w0=0.0, w1=0.23397662185044216\n",
      "Regularized Logistic Regression(799/999): loss=0.5820406666850159, w0=0.0, w1=0.23401517266632685\n",
      "Regularized Logistic Regression(800/999): loss=0.5820365722810603, w0=0.0, w1=0.23405356420051962\n",
      "Regularized Logistic Regression(801/999): loss=0.5820324982708426, w0=0.0, w1=0.23409179708328018\n",
      "Regularized Logistic Regression(802/999): loss=0.582028444537207, w0=0.0, w1=0.2341298719423762\n",
      "Regularized Logistic Regression(803/999): loss=0.5820244109637471, w0=0.0, w1=0.23416778940309146\n",
      "Regularized Logistic Regression(804/999): loss=0.5820203974348027, w0=0.0, w1=0.2342055500882387\n",
      "Regularized Logistic Regression(805/999): loss=0.5820164038354532, w0=0.0, w1=0.2342431546181689\n",
      "Regularized Logistic Regression(806/999): loss=0.5820124300515124, w0=0.0, w1=0.23428060361078\n",
      "Regularized Logistic Regression(807/999): loss=0.5820084759695239, w0=0.0, w1=0.2343178976815291\n",
      "Regularized Logistic Regression(808/999): loss=0.5820045414767567, w0=0.0, w1=0.23435503744344038\n",
      "Regularized Logistic Regression(809/999): loss=0.5820006264611985, w0=0.0, w1=0.23439202350711594\n",
      "Regularized Logistic Regression(810/999): loss=0.5819967308115528, w0=0.0, w1=0.23442885648074666\n",
      "Regularized Logistic Regression(811/999): loss=0.5819928544172319, w0=0.0, w1=0.23446553697012057\n",
      "Regularized Logistic Regression(812/999): loss=0.5819889971683534, w0=0.0, w1=0.23450206557863446\n",
      "Regularized Logistic Regression(813/999): loss=0.5819851589557351, w0=0.0, w1=0.23453844290730097\n",
      "Regularized Logistic Regression(814/999): loss=0.5819813396708897, w0=0.0, w1=0.23457466955476075\n",
      "Regularized Logistic Regression(815/999): loss=0.5819775392060205, w0=0.0, w1=0.23461074611729052\n",
      "Regularized Logistic Regression(816/999): loss=0.5819737574540165, w0=0.0, w1=0.234646673188815\n",
      "Regularized Logistic Regression(817/999): loss=0.5819699943084471, w0=0.0, w1=0.23468245136091323\n",
      "Regularized Logistic Regression(818/999): loss=0.5819662496635587, w0=0.0, w1=0.2347180812228315\n",
      "Regularized Logistic Regression(819/999): loss=0.5819625234142689, w0=0.0, w1=0.23475356336149075\n",
      "Regularized Logistic Regression(820/999): loss=0.581958815456162, w0=0.0, w1=0.23478889836149625\n",
      "Regularized Logistic Regression(821/999): loss=0.5819551256854857, w0=0.0, w1=0.23482408680514766\n",
      "Regularized Logistic Regression(822/999): loss=0.5819514539991439, w0=0.0, w1=0.2348591292724478\n",
      "Regularized Logistic Regression(823/999): loss=0.5819478002946955, w0=0.0, w1=0.23489402634111337\n",
      "Regularized Logistic Regression(824/999): loss=0.581944164470347, w0=0.0, w1=0.23492877858658248\n",
      "Regularized Logistic Regression(825/999): loss=0.5819405464249502, w0=0.0, w1=0.23496338658202515\n",
      "Regularized Logistic Regression(826/999): loss=0.5819369460579963, w0=0.0, w1=0.23499785089835143\n",
      "Regularized Logistic Regression(827/999): loss=0.5819333632696114, w0=0.0, w1=0.23503217210422236\n",
      "Regularized Logistic Regression(828/999): loss=0.5819297979605551, w0=0.0, w1=0.23506635076605784\n",
      "Regularized Logistic Regression(829/999): loss=0.5819262500322109, w0=0.0, w1=0.23510038744804554\n",
      "Regularized Logistic Regression(830/999): loss=0.5819227193865872, w0=0.0, w1=0.23513428271215037\n",
      "Regularized Logistic Regression(831/999): loss=0.5819192059263102, w0=0.0, w1=0.23516803711812356\n",
      "Regularized Logistic Regression(832/999): loss=0.5819157095546192, w0=0.0, w1=0.23520165122351316\n",
      "Regularized Logistic Regression(833/999): loss=0.5819122301753648, w0=0.0, w1=0.23523512558366996\n",
      "Regularized Logistic Regression(834/999): loss=0.5819087676930023, w0=0.0, w1=0.23526846075175828\n",
      "Regularized Logistic Regression(835/999): loss=0.5819053220125887, w0=0.0, w1=0.23530165727876504\n",
      "Regularized Logistic Regression(836/999): loss=0.5819018930397797, w0=0.0, w1=0.23533471571350795\n",
      "Regularized Logistic Regression(837/999): loss=0.581898480680823, w0=0.0, w1=0.23536763660264445\n",
      "Regularized Logistic Regression(838/999): loss=0.5818950848425558, w0=0.0, w1=0.23540042049068072\n",
      "Regularized Logistic Regression(839/999): loss=0.5818917054324017, w0=0.0, w1=0.2354330679199787\n",
      "Regularized Logistic Regression(840/999): loss=0.5818883423583647, w0=0.0, w1=0.23546557943076987\n",
      "Regularized Logistic Regression(841/999): loss=0.5818849955290264, w0=0.0, w1=0.23549795556115638\n",
      "Regularized Logistic Regression(842/999): loss=0.581881664853542, w0=0.0, w1=0.23553019684712606\n",
      "Regularized Logistic Regression(843/999): loss=0.5818783502416361, w0=0.0, w1=0.23556230382255813\n",
      "Regularized Logistic Regression(844/999): loss=0.581875051603599, w0=0.0, w1=0.23559427701923114\n",
      "Regularized Logistic Regression(845/999): loss=0.5818717688502831, w0=0.0, w1=0.23562611696683394\n",
      "Regularized Logistic Regression(846/999): loss=0.5818685018930984, w0=0.0, w1=0.23565782419297263\n",
      "Regularized Logistic Regression(847/999): loss=0.5818652506440095, w0=0.0, w1=0.2356893992231782\n",
      "Regularized Logistic Regression(848/999): loss=0.5818620150155314, w0=0.0, w1=0.2357208425809162\n",
      "Regularized Logistic Regression(849/999): loss=0.581858794920725, w0=0.0, w1=0.23575215478759548\n",
      "Regularized Logistic Regression(850/999): loss=0.5818555902731957, w0=0.0, w1=0.23578333636257434\n",
      "Regularized Logistic Regression(851/999): loss=0.581852400987087, w0=0.0, w1=0.23581438782317143\n",
      "Regularized Logistic Regression(852/999): loss=0.5818492269770785, w0=0.0, w1=0.23584530968467235\n",
      "Regularized Logistic Regression(853/999): loss=0.5818460681583821, w0=0.0, w1=0.2358761024603385\n",
      "Regularized Logistic Regression(854/999): loss=0.5818429244467376, w0=0.0, w1=0.2359067666614151\n",
      "Regularized Logistic Regression(855/999): loss=0.5818397957584096, w0=0.0, w1=0.23593730279713954\n",
      "Regularized Logistic Regression(856/999): loss=0.5818366820101849, w0=0.0, w1=0.23596771137474912\n",
      "Regularized Logistic Regression(857/999): loss=0.5818335831193668, w0=0.0, w1=0.23599799289948817\n",
      "Regularized Logistic Regression(858/999): loss=0.5818304990037739, w0=0.0, w1=0.23602814787461937\n",
      "Regularized Logistic Regression(859/999): loss=0.5818274295817348, w0=0.0, w1=0.23605817680142854\n",
      "Regularized Logistic Regression(860/999): loss=0.5818243747720865, w0=0.0, w1=0.23608808017923322\n",
      "Regularized Logistic Regression(861/999): loss=0.5818213344941687, w0=0.0, w1=0.236117858505392\n",
      "Regularized Logistic Regression(862/999): loss=0.5818183086678219, w0=0.0, w1=0.23614751227531047\n",
      "Regularized Logistic Regression(863/999): loss=0.5818152972133842, w0=0.0, w1=0.23617704198245149\n",
      "Regularized Logistic Regression(864/999): loss=0.5818123000516882, w0=0.0, w1=0.23620644811833996\n",
      "Regularized Logistic Regression(865/999): loss=0.5818093171040545, w0=0.0, w1=0.23623573117257388\n",
      "Regularized Logistic Regression(866/999): loss=0.5818063482922928, w0=0.0, w1=0.23626489163283046\n",
      "Regularized Logistic Regression(867/999): loss=0.5818033935386964, w0=0.0, w1=0.23629392998487217\n",
      "Regularized Logistic Regression(868/999): loss=0.5818004527660388, w0=0.0, w1=0.23632284671255882\n",
      "Regularized Logistic Regression(869/999): loss=0.5817975258975709, w0=0.0, w1=0.23635164229785055\n",
      "Regularized Logistic Regression(870/999): loss=0.5817946128570182, w0=0.0, w1=0.2363803172208185\n",
      "Regularized Logistic Regression(871/999): loss=0.5817917135685766, w0=0.0, w1=0.2364088719596517\n",
      "Regularized Logistic Regression(872/999): loss=0.5817888279569097, w0=0.0, w1=0.23643730699066376\n",
      "Regularized Logistic Regression(873/999): loss=0.5817859559471468, w0=0.0, w1=0.23646562278830158\n",
      "Regularized Logistic Regression(874/999): loss=0.5817830974648772, w0=0.0, w1=0.2364938198251519\n",
      "Regularized Logistic Regression(875/999): loss=0.5817802524361502, w0=0.0, w1=0.23652189857194988\n",
      "Regularized Logistic Regression(876/999): loss=0.5817774207874692, w0=0.0, w1=0.236549859497585\n",
      "Regularized Logistic Regression(877/999): loss=0.5817746024457902, w0=0.0, w1=0.2365777030691107\n",
      "Regularized Logistic Regression(878/999): loss=0.5817717973385188, w0=0.0, w1=0.23660542975174925\n",
      "Regularized Logistic Regression(879/999): loss=0.5817690053935074, w0=0.0, w1=0.23663304000890029\n",
      "Regularized Logistic Regression(880/999): loss=0.5817662265390495, w0=0.0, w1=0.23666053430214942\n",
      "Regularized Logistic Regression(881/999): loss=0.5817634607038817, w0=0.0, w1=0.2366879130912723\n",
      "Regularized Logistic Regression(882/999): loss=0.581760707817176, w0=0.0, w1=0.23671517683424517\n",
      "Regularized Logistic Regression(883/999): loss=0.5817579678085393, w0=0.0, w1=0.2367423259872504\n",
      "Regularized Logistic Regression(884/999): loss=0.5817552406080105, w0=0.0, w1=0.23676936100468354\n",
      "Regularized Logistic Regression(885/999): loss=0.5817525261460564, w0=0.0, w1=0.23679628233916192\n",
      "Regularized Logistic Regression(886/999): loss=0.5817498243535705, w0=0.0, w1=0.2368230904415299\n",
      "Regularized Logistic Regression(887/999): loss=0.5817471351618677, w0=0.0, w1=0.23684978576086693\n",
      "Regularized Logistic Regression(888/999): loss=0.5817444585026851, w0=0.0, w1=0.23687636874449536\n",
      "Regularized Logistic Regression(889/999): loss=0.5817417943081755, w0=0.0, w1=0.23690283983798588\n",
      "Regularized Logistic Regression(890/999): loss=0.5817391425109063, w0=0.0, w1=0.23692919948516525\n",
      "Regularized Logistic Regression(891/999): loss=0.5817365030438577, w0=0.0, w1=0.23695544812812452\n",
      "Regularized Logistic Regression(892/999): loss=0.5817338758404176, w0=0.0, w1=0.2369815862072235\n",
      "Regularized Logistic Regression(893/999): loss=0.5817312608343813, w0=0.0, w1=0.2370076141610996\n",
      "Regularized Logistic Regression(894/999): loss=0.5817286579599471, w0=0.0, w1=0.23703353242667485\n",
      "Regularized Logistic Regression(895/999): loss=0.5817260671517139, w0=0.0, w1=0.23705934143915994\n",
      "Regularized Logistic Regression(896/999): loss=0.5817234883446798, w0=0.0, w1=0.23708504163206562\n",
      "Regularized Logistic Regression(897/999): loss=0.5817209214742376, w0=0.0, w1=0.23711063343720593\n",
      "Regularized Logistic Regression(898/999): loss=0.5817183664761734, w0=0.0, w1=0.2371361172847053\n",
      "Regularized Logistic Regression(899/999): loss=0.5817158232866635, w0=0.0, w1=0.23716149360300812\n",
      "Regularized Logistic Regression(900/999): loss=0.581713291842272, w0=0.0, w1=0.23718676281888232\n",
      "Regularized Logistic Regression(901/999): loss=0.5817107720799486, w0=0.0, w1=0.23721192535742672\n",
      "Regularized Logistic Regression(902/999): loss=0.5817082639370246, w0=0.0, w1=0.2372369816420789\n",
      "Regularized Logistic Regression(903/999): loss=0.5817057673512128, w0=0.0, w1=0.2372619320946205\n",
      "Regularized Logistic Regression(904/999): loss=0.581703282260602, w0=0.0, w1=0.23728677713518478\n",
      "Regularized Logistic Regression(905/999): loss=0.5817008086036568, w0=0.0, w1=0.2373115171822633\n",
      "Regularized Logistic Regression(906/999): loss=0.5816983463192151, w0=0.0, w1=0.23733615265271168\n",
      "Regularized Logistic Regression(907/999): loss=0.5816958953464837, w0=0.0, w1=0.23736068396175553\n",
      "Regularized Logistic Regression(908/999): loss=0.5816934556250375, w0=0.0, w1=0.23738511152299924\n",
      "Regularized Logistic Regression(909/999): loss=0.5816910270948169, w0=0.0, w1=0.2374094357484306\n",
      "Regularized Logistic Regression(910/999): loss=0.5816886096961249, w0=0.0, w1=0.23743365704842792\n",
      "Regularized Logistic Regression(911/999): loss=0.581686203369625, w0=0.0, w1=0.23745777583176603\n",
      "Regularized Logistic Regression(912/999): loss=0.5816838080563379, w0=0.0, w1=0.2374817925056221\n",
      "Regularized Logistic Regression(913/999): loss=0.5816814236976416, w0=0.0, w1=0.237505707475585\n",
      "Regularized Logistic Regression(914/999): loss=0.5816790502352662, w0=0.0, w1=0.23752952114565765\n",
      "Regularized Logistic Regression(915/999): loss=0.5816766876112935, w0=0.0, w1=0.23755323391826502\n",
      "Regularized Logistic Regression(916/999): loss=0.5816743357681529, w0=0.0, w1=0.2375768461942609\n",
      "Regularized Logistic Regression(917/999): loss=0.5816719946486213, w0=0.0, w1=0.23760035837293417\n",
      "Regularized Logistic Regression(918/999): loss=0.5816696641958194, w0=0.0, w1=0.2376237708520142\n",
      "Regularized Logistic Regression(919/999): loss=0.5816673443532093, w0=0.0, w1=0.23764708402767817\n",
      "Regularized Logistic Regression(920/999): loss=0.5816650350645933, w0=0.0, w1=0.23767029829455544\n",
      "Regularized Logistic Regression(921/999): loss=0.5816627362741108, w0=0.0, w1=0.23769341404573524\n",
      "Regularized Logistic Regression(922/999): loss=0.5816604479262357, w0=0.0, w1=0.2377164316727728\n",
      "Regularized Logistic Regression(923/999): loss=0.581658169965776, w0=0.0, w1=0.23773935156569442\n",
      "Regularized Logistic Regression(924/999): loss=0.58165590233787, w0=0.0, w1=0.23776217411300504\n",
      "Regularized Logistic Regression(925/999): loss=0.5816536449879841, w0=0.0, w1=0.23778489970169153\n",
      "Regularized Logistic Regression(926/999): loss=0.5816513978619114, w0=0.0, w1=0.2378075287172316\n",
      "Regularized Logistic Regression(927/999): loss=0.5816491609057693, w0=0.0, w1=0.23783006154360012\n",
      "Regularized Logistic Regression(928/999): loss=0.5816469340659977, w0=0.0, w1=0.23785249856327223\n",
      "Regularized Logistic Regression(929/999): loss=0.5816447172893555, w0=0.0, w1=0.23787484015723107\n",
      "Regularized Logistic Regression(930/999): loss=0.581642510522921, w0=0.0, w1=0.237897086704973\n",
      "Regularized Logistic Regression(931/999): loss=0.5816403137140874, w0=0.0, w1=0.23791923858451486\n",
      "Regularized Logistic Regression(932/999): loss=0.5816381268105615, w0=0.0, w1=0.2379412961723978\n",
      "Regularized Logistic Regression(933/999): loss=0.5816359497603625, w0=0.0, w1=0.23796325984369587\n",
      "Regularized Logistic Regression(934/999): loss=0.5816337825118189, w0=0.0, w1=0.23798512997201826\n",
      "Regularized Logistic Regression(935/999): loss=0.5816316250135667, w0=0.0, w1=0.23800690692951817\n",
      "Regularized Logistic Regression(936/999): loss=0.5816294772145486, w0=0.0, w1=0.23802859108689745\n",
      "Regularized Logistic Regression(937/999): loss=0.5816273390640094, w0=0.0, w1=0.23805018281341228\n",
      "Regularized Logistic Regression(938/999): loss=0.5816252105114966, w0=0.0, w1=0.2380716824768786\n",
      "Regularized Logistic Regression(939/999): loss=0.5816230915068576, w0=0.0, w1=0.23809309044367793\n",
      "Regularized Logistic Regression(940/999): loss=0.5816209820002364, w0=0.0, w1=0.23811440707876524\n",
      "Regularized Logistic Regression(941/999): loss=0.5816188819420742, w0=0.0, w1=0.23813563274567015\n",
      "Regularized Logistic Regression(942/999): loss=0.581616791283105, w0=0.0, w1=0.23815676780650635\n",
      "Regularized Logistic Regression(943/999): loss=0.5816147099743555, w0=0.0, w1=0.2381778126219766\n",
      "Regularized Logistic Regression(944/999): loss=0.5816126379671421, w0=0.0, w1=0.2381987675513763\n",
      "Regularized Logistic Regression(945/999): loss=0.5816105752130695, w0=0.0, w1=0.23821963295260107\n",
      "Regularized Logistic Regression(946/999): loss=0.5816085216640284, w0=0.0, w1=0.23824040918215203\n",
      "Regularized Logistic Regression(947/999): loss=0.5816064772721946, w0=0.0, w1=0.2382610965951396\n",
      "Regularized Logistic Regression(948/999): loss=0.5816044419900266, w0=0.0, w1=0.2382816955452916\n",
      "Regularized Logistic Regression(949/999): loss=0.5816024157702622, w0=0.0, w1=0.23830220638495692\n",
      "Regularized Logistic Regression(950/999): loss=0.5816003985659195, w0=0.0, w1=0.2383226294651115\n",
      "Regularized Logistic Regression(951/999): loss=0.5815983903302935, w0=0.0, w1=0.23834296513536463\n",
      "Regularized Logistic Regression(952/999): loss=0.5815963910169546, w0=0.0, w1=0.2383632137439614\n",
      "Regularized Logistic Regression(953/999): loss=0.5815944005797462, w0=0.0, w1=0.23838337563779252\n",
      "Regularized Logistic Regression(954/999): loss=0.581592418972784, w0=0.0, w1=0.2384034511623956\n",
      "Regularized Logistic Regression(955/999): loss=0.5815904461504535, w0=0.0, w1=0.2384234406619629\n",
      "Regularized Logistic Regression(956/999): loss=0.5815884820674089, w0=0.0, w1=0.23844334447934593\n",
      "Regularized Logistic Regression(957/999): loss=0.5815865266785697, w0=0.0, w1=0.23846316295606057\n",
      "Regularized Logistic Regression(958/999): loss=0.581584579939122, w0=0.0, w1=0.23848289643229298\n",
      "Regularized Logistic Regression(959/999): loss=0.5815826418045137, w0=0.0, w1=0.23850254524690362\n",
      "Regularized Logistic Regression(960/999): loss=0.5815807122304542, w0=0.0, w1=0.2385221097374339\n",
      "Regularized Logistic Regression(961/999): loss=0.5815787911729131, w0=0.0, w1=0.23854159024011048\n",
      "Regularized Logistic Regression(962/999): loss=0.5815768785881179, w0=0.0, w1=0.23856098708985035\n",
      "Regularized Logistic Regression(963/999): loss=0.5815749744325522, w0=0.0, w1=0.23858030062026697\n",
      "Regularized Logistic Regression(964/999): loss=0.5815730786629545, w0=0.0, w1=0.2385995311636741\n",
      "Regularized Logistic Regression(965/999): loss=0.5815711912363163, w0=0.0, w1=0.23861867905109163\n",
      "Regularized Logistic Regression(966/999): loss=0.5815693121098807, w0=0.0, w1=0.23863774461225173\n",
      "Regularized Logistic Regression(967/999): loss=0.5815674412411401, w0=0.0, w1=0.23865672817560135\n",
      "Regularized Logistic Regression(968/999): loss=0.5815655785878361, w0=0.0, w1=0.23867563006831022\n",
      "Regularized Logistic Regression(969/999): loss=0.5815637241079559, w0=0.0, w1=0.2386944506162732\n",
      "Regularized Logistic Regression(970/999): loss=0.5815618777597319, w0=0.0, w1=0.23871319014411674\n",
      "Regularized Logistic Regression(971/999): loss=0.5815600395016407, w0=0.0, w1=0.23873184897520494\n",
      "Regularized Logistic Regression(972/999): loss=0.5815582092924003, w0=0.0, w1=0.23875042743164157\n",
      "Regularized Logistic Regression(973/999): loss=0.5815563870909682, w0=0.0, w1=0.23876892583427703\n",
      "Regularized Logistic Regression(974/999): loss=0.5815545728565428, w0=0.0, w1=0.23878734450271424\n",
      "Regularized Logistic Regression(975/999): loss=0.5815527665485573, w0=0.0, w1=0.23880568375531072\n",
      "Regularized Logistic Regression(976/999): loss=0.581550968126682, w0=0.0, w1=0.23882394390918565\n",
      "Regularized Logistic Regression(977/999): loss=0.5815491775508215, w0=0.0, w1=0.23884212528022414\n",
      "Regularized Logistic Regression(978/999): loss=0.5815473947811126, w0=0.0, w1=0.238860228183082\n",
      "Regularized Logistic Regression(979/999): loss=0.5815456197779236, w0=0.0, w1=0.23887825293119117\n",
      "Regularized Logistic Regression(980/999): loss=0.5815438525018524, w0=0.0, w1=0.23889619983676236\n",
      "Regularized Logistic Regression(981/999): loss=0.5815420929137252, w0=0.0, w1=0.23891406921079258\n",
      "Regularized Logistic Regression(982/999): loss=0.5815403409745955, w0=0.0, w1=0.23893186136307007\n",
      "Regularized Logistic Regression(983/999): loss=0.5815385966457414, w0=0.0, w1=0.23894957660217483\n",
      "Regularized Logistic Regression(984/999): loss=0.5815368598886654, w0=0.0, w1=0.23896721523548906\n",
      "Regularized Logistic Regression(985/999): loss=0.5815351306650918, w0=0.0, w1=0.23898477756919795\n",
      "Regularized Logistic Regression(986/999): loss=0.5815334089369676, w0=0.0, w1=0.23900226390829518\n",
      "Regularized Logistic Regression(987/999): loss=0.5815316946664578, w0=0.0, w1=0.23901967455658876\n",
      "Regularized Logistic Regression(988/999): loss=0.581529987815946, w0=0.0, w1=0.2390370098167056\n",
      "Regularized Logistic Regression(989/999): loss=0.5815282883480336, w0=0.0, w1=0.23905426999009224\n",
      "Regularized Logistic Regression(990/999): loss=0.5815265962255365, w0=0.0, w1=0.2390714553770262\n",
      "Regularized Logistic Regression(991/999): loss=0.581524911411485, w0=0.0, w1=0.2390885662766152\n",
      "Regularized Logistic Regression(992/999): loss=0.5815232338691227, w0=0.0, w1=0.2391056029868044\n",
      "Regularized Logistic Regression(993/999): loss=0.5815215635619033, w0=0.0, w1=0.239122565804379\n",
      "Regularized Logistic Regression(994/999): loss=0.5815199004534919, w0=0.0, w1=0.23913945502497114\n",
      "Regularized Logistic Regression(995/999): loss=0.5815182445077616, w0=0.0, w1=0.2391562709430628\n",
      "Regularized Logistic Regression(996/999): loss=0.5815165956887933, w0=0.0, w1=0.2391730138519911\n",
      "Regularized Logistic Regression(997/999): loss=0.5815149539608735, w0=0.0, w1=0.2391896840439514\n",
      "Regularized Logistic Regression(998/999): loss=0.5815133192884938, w0=0.0, w1=0.23920628181000378\n",
      "Regularized Logistic Regression(999/999): loss=0.5815116916363494, w0=0.0, w1=0.23922280744007546\n",
      "Fold 1 completed.\n",
      "Regularized Logistic Regression(0/999): loss=0.6931471805599447, w0=-1.1148719458083044e-08, w1=0.0009375292653885775\n",
      "Regularized Logistic Regression(1/999): loss=0.6922257336288996, w0=-2.226227987519072e-08, w1=0.0018724506174584567\n",
      "Regularized Logistic Regression(2/999): loss=0.691312966250886, w0=-3.3340900085903364e-08, w1=0.002804752492688402\n",
      "Regularized Logistic Regression(3/999): loss=0.6904087775102095, w0=-4.438479690979781e-08, w1=0.0037344237262342023\n",
      "Regularized Logistic Regression(4/999): loss=0.6895130680422131, w0=-5.5394185167759794e-08, w1=0.004661453545673908\n",
      "Regularized Logistic Regression(5/999): loss=0.6886257400032061, w0=-6.636927769840244e-08, w1=0.005585831564815125\n",
      "Regularized Logistic Regression(6/999): loss=0.6877466970410105, w0=-7.731028537457888e-08, w1=0.006507547777568199\n",
      "Regularized Logistic Regression(7/999): loss=0.6868758442661083, w0=-8.821741711997688e-08, w1=0.007426592551882054\n",
      "Regularized Logistic Regression(8/999): loss=0.6860130882233902, w0=-9.909087992578463e-08, w1=0.008342956623745602\n",
      "Regularized Logistic Regression(9/999): loss=0.6851583368644792, w0=-1.0993087886741613e-07, w1=0.009256631091255445\n",
      "Regularized Logistic Regression(10/999): loss=0.6843114995206375, w0=-1.207376171212866e-07, w1=0.010167607408742455\n",
      "Regularized Logistic Regression(11/999): loss=0.6834724868762387, w0=-1.315112959816272e-07, w1=0.011075877380970588\n",
      "Regularized Logistic Regression(12/999): loss=0.682641210942786, w0=-1.4225211487733e-07, w1=0.011981433157392972\n",
      "Regularized Logistic Regression(13/999): loss=0.6818175850334894, w0=-1.529602713888136e-07, w1=0.012884267226477016\n",
      "Regularized Logistic Regression(14/999): loss=0.6810015237383719, w0=-1.6363596126490115e-07, w1=0.013784372410092631\n",
      "Regularized Logistic Regression(15/999): loss=0.6801929428999013, w0=-1.7427937843970212e-07, w1=0.014681741857962777\n",
      "Regularized Logistic Regression(16/999): loss=0.6793917595891455, w0=-1.848907150494897e-07, w1=0.015576369042179858\n",
      "Regularized Logistic Regression(17/999): loss=0.6785978920824273, w0=-1.954701614495666e-07, w1=0.01646824775178603\n",
      "Regularized Logistic Regression(18/999): loss=0.6778112598384836, w0=-2.0601790623111142e-07, w1=0.017357372087415585\n",
      "Regularized Logistic Regression(19/999): loss=0.6770317834761077, w0=-2.1653413623799923e-07, w1=0.018243736456002614\n",
      "Regularized Logistic Regression(20/999): loss=0.6762593847522755, w0=-2.2701903658358913e-07, w1=0.01912733556554917\n",
      "Regularized Logistic Regression(21/999): loss=0.6754939865407379, w0=-2.3747279066747306e-07, w1=0.020008164419959493\n",
      "Regularized Logistic Regression(22/999): loss=0.6747355128110781, w0=-2.4789558019217944e-07, w1=0.0208862183139351\n",
      "Regularized Logistic Regression(23/999): loss=0.6739838886082096, w0=-2.582875851798259e-07, w1=0.021761492827931148\n",
      "Regularized Logistic Regression(24/999): loss=0.6732390400323333, w0=-2.686489839887159e-07, w1=0.022633983823177647\n",
      "Regularized Logistic Regression(25/999): loss=0.6725008942193134, w0=-2.789799533298745e-07, w1=0.023503687436760002\n",
      "Regularized Logistic Regression(26/999): loss=0.6717693793214841, w0=-2.8928066828351685e-07, w1=0.024370600076764174\n",
      "Regularized Logistic Regression(27/999): loss=0.6710444244888727, w0=-2.9955130231544673e-07, w1=0.02523471841747735\n",
      "Regularized Logistic Regression(28/999): loss=0.6703259598508277, w0=-3.0979202729337946e-07, w1=0.026096039394653856\n",
      "Regularized Logistic Regression(29/999): loss=0.6696139164980452, w0=-3.200030135031852e-07, w1=0.026954560200840746\n",
      "Regularized Logistic Regression(30/999): loss=0.6689082264649914, w0=-3.301844296650492e-07, w1=0.0278102782807625\n",
      "Regularized Logistic Regression(31/999): loss=0.6682088227126977, w0=-3.4033644294954506e-07, w1=0.028663191326765675\n",
      "Regularized Logistic Regression(32/999): loss=0.6675156391119361, w0=-3.5045921899361666e-07, w1=0.029513297274322992\n",
      "Regularized Logistic Regression(33/999): loss=0.6668286104267617, w0=-3.605529219164675e-07, w1=0.030360594297595158\n",
      "Regularized Logistic Regression(34/999): loss=0.666147672298406, w0=-3.7061771433535203e-07, w1=0.031205080805051646\n",
      "Regularized Logistic Regression(35/999): loss=0.6654727612295309, w0=-3.8065375738126756e-07, w1=0.03204675543515092\n",
      "Regularized Logistic Regression(36/999): loss=0.6648038145688131, w0=-3.9066121071454384e-07, w1=0.03288561705207605\n",
      "Regularized Logistic Regression(37/999): loss=0.6641407704958739, w0=-4.0064023254032704e-07, w1=0.03372166474152757\n",
      "Regularized Logistic Regression(38/999): loss=0.663483568006531, w0=-4.1059097962395674e-07, w1=0.03455489780657634\n",
      "Regularized Logistic Regression(39/999): loss=0.6628321468983721, w0=-4.2051360730623307e-07, w1=0.03538531576356571\n",
      "Regularized Logistic Regression(40/999): loss=0.6621864477566433, w0=-4.3040826951857207e-07, w1=0.036212918338077146\n",
      "Regularized Logistic Regression(41/999): loss=0.6615464119404388, w0=-4.4027511879804754e-07, w1=0.03703770546094623\n",
      "Regularized Logistic Regression(42/999): loss=0.6609119815691965, w0=-4.5011430630231744e-07, w1=0.0378596772643341\n",
      "Regularized Logistic Regression(43/999): loss=0.6602830995094814, w0=-4.599259818244333e-07, w1=0.038678834077854884\n",
      "Regularized Logistic Regression(44/999): loss=0.6596597093620545, w0=-4.6971029380753114e-07, w1=0.039495176424756036\n",
      "Regularized Logistic Regression(45/999): loss=0.6590417554492252, w0=-4.794673893594024e-07, w1=0.040308705018151356\n",
      "Regularized Logistic Regression(46/999): loss=0.6584291828024743, w0=-4.891974142669443e-07, w1=0.041119420757309635\n",
      "Regularized Logistic Regression(47/999): loss=0.6578219371503439, w0=-4.989005130104876e-07, w1=0.041927324723989405\n",
      "Regularized Logistic Regression(48/999): loss=0.6572199649065851, w0=-5.085768287780009e-07, w1=0.042732418178834795\n",
      "Regularized Logistic Regression(49/999): loss=0.6566232131585678, w0=-5.182265034791718e-07, w1=0.04353470255781468\n",
      "Regularized Logistic Regression(50/999): loss=0.6560316296559292, w0=-5.278496777593619e-07, w1=0.044334179468716775\n",
      "Regularized Logistic Regression(51/999): loss=0.6554451627994738, w0=-5.374464910134375e-07, w1=0.04513085068769137\n",
      "Regularized Logistic Regression(52/999): loss=0.6548637616303067, w0=-5.470170813994728e-07, w1=0.045924718155845354\n",
      "Regularized Logistic Regression(53/999): loss=0.6542873758191988, w0=-5.565615858523283e-07, w1=0.0467157839758851\n",
      "Regularized Logistic Regression(54/999): loss=0.6537159556561791, w0=-5.660801400970995e-07, w1=0.04750405040880768\n",
      "Regularized Logistic Regression(55/999): loss=0.6531494520403471, w0=-5.75572878662441e-07, w1=0.048289519870642304\n",
      "Regularized Logistic Regression(56/999): loss=0.652587816469902, w0=-5.850399348937601e-07, w1=0.049072194929238686\n",
      "Regularized Logistic Regression(57/999): loss=0.6520310010323834, w0=-5.944814409662852e-07, w1=0.04985207830110267\n",
      "Regularized Logistic Regression(58/999): loss=0.6514789583951155, w0=-6.038975278980043e-07, w1=0.050629172848277317\n",
      "Regularized Logistic Regression(59/999): loss=0.650931641795853, w0=-6.132883255624758e-07, w1=0.05140348157527701\n",
      "Regularized Logistic Regression(60/999): loss=0.6503890050336265, w0=-6.226539627015126e-07, w1=0.05217500762605583\n",
      "Regularized Logistic Regression(61/999): loss=0.6498510024597729, w0=-6.31994566937736e-07, w1=0.05294375428103332\n",
      "Regularized Logistic Regression(62/999): loss=0.6493175889691603, w0=-6.413102647870041e-07, w1=0.05370972495415816\n",
      "Regularized Logistic Regression(63/999): loss=0.6487887199915879, w0=-6.506011816707111e-07, w1=0.054472923190018445\n",
      "Regularized Logistic Regression(64/999): loss=0.6482643514833727, w0=-6.598674419279598e-07, w1=0.05523335266099426\n",
      "Regularized Logistic Regression(65/999): loss=0.6477444399191018, w0=-6.691091688276075e-07, w1=0.05599101716445744\n",
      "Regularized Logistic Regression(66/999): loss=0.6472289422835612, w0=-6.783264845801844e-07, w1=0.05674592062001002\n",
      "Regularized Logistic Regression(67/999): loss=0.646717816063825, w0=-6.875195103496864e-07, w1=0.05749806706676667\n",
      "Regularized Logistic Regression(68/999): loss=0.6462110192415115, w0=-6.966883662652418e-07, w1=0.058247460660680817\n",
      "Regularized Logistic Regression(69/999): loss=0.645708510285195, w0=-7.058331714326528e-07, w1=0.05899410567191003\n",
      "Regularized Logistic Regression(70/999): loss=0.64521024814297, w0=-7.149540439458123e-07, w1=0.059738006482223785\n",
      "Regularized Logistic Regression(71/999): loss=0.6447161922351702, w0=-7.240511008979962e-07, w1=0.06047916758245003\n",
      "Regularized Logistic Regression(72/999): loss=0.6442263024472324, w0=-7.331244583930325e-07, w1=0.06121759356996368\n",
      "Regularized Logistic Regression(73/999): loss=0.643740539122702, w0=-7.421742315563468e-07, w1=0.06195328914621405\n",
      "Regularized Logistic Regression(74/999): loss=0.6432588630563812, w0=-7.512005345458861e-07, w1=0.06268625911428928\n",
      "Regularized Logistic Regression(75/999): loss=0.6427812354876163, w0=-7.602034805629208e-07, w1=0.06341650837652352\n",
      "Regularized Logistic Regression(76/999): loss=0.6423076180937107, w0=-7.691831818627254e-07, w1=0.06414404193213953\n",
      "Regularized Logistic Regression(77/999): loss=0.6418379729834782, w0=-7.781397497651396e-07, w1=0.06486886487492727\n",
      "Regularized Logistic Regression(78/999): loss=0.6413722626909142, w0=-7.870732946650096e-07, w1=0.06559098239096471\n",
      "Regularized Logistic Regression(79/999): loss=0.6409104501689987, w0=-7.959839260425112e-07, w1=0.06631039975636878\n",
      "Regularized Logistic Regression(80/999): loss=0.6404524987836159, w0=-8.048717524733546e-07, w1=0.06702712233508973\n",
      "Regularized Logistic Regression(81/999): loss=0.6399983723075944, w0=-8.137368816388727e-07, w1=0.06774115557673473\n",
      "Regularized Logistic Regression(82/999): loss=0.6395480349148621, w0=-8.225794203359933e-07, w1=0.06845250501443158\n",
      "Regularized Logistic Regression(83/999): loss=0.6391014511747137, w0=-8.313994744870956e-07, w1=0.06916117626272364\n",
      "Regularized Logistic Regression(84/999): loss=0.6386585860461899, w0=-8.40197149149753e-07, w1=0.06986717501550399\n",
      "Regularized Logistic Regression(85/999): loss=0.6382194048725641, w0=-8.489725485263623e-07, w1=0.07057050704397592\n",
      "Regularized Logistic Regression(86/999): loss=0.6377838733759312, w0=-8.577257759736596e-07, w1=0.07127117819465646\n",
      "Regularized Logistic Regression(87/999): loss=0.6373519576519034, w0=-8.664569340121262e-07, w1=0.07196919438740436\n",
      "Regularized Logistic Regression(88/999): loss=0.6369236241644022, w0=-8.751661243352822e-07, w1=0.07266456161348617\n",
      "Regularized Logistic Regression(89/999): loss=0.6364988397405518, w0=-8.838534478188717e-07, w1=0.0733572859336735\n",
      "Regularized Logistic Regression(90/999): loss=0.6360775715656659, w0=-8.925190045299385e-07, w1=0.07404737347637182\n",
      "Regularized Logistic Regression(91/999): loss=0.6356597871783263, w0=-9.011628937357941e-07, w1=0.07473483043577674\n",
      "Regularized Logistic Regression(92/999): loss=0.6352454544655591, w0=-9.097852139128795e-07, w1=0.07541966307006888\n",
      "Regularized Logistic Regression(93/999): loss=0.6348345416580908, w0=-9.183860627555199e-07, w1=0.07610187769963316\n",
      "Regularized Logistic Regression(94/999): loss=0.6344270173256975, w0=-9.269655371845765e-07, w1=0.07678148070531\n",
      "Regularized Logistic Regression(95/999): loss=0.6340228503726402, w0=-9.355237333559924e-07, w1=0.07745847852667843\n",
      "Regularized Logistic Regression(96/999): loss=0.6336220100331783, w0=-9.440607466692371e-07, w1=0.07813287766036571\n",
      "Regularized Logistic Regression(97/999): loss=0.633224465867164, w0=-9.525766717756489e-07, w1=0.07880468465838494\n",
      "Regularized Logistic Regression(98/999): loss=0.6328301877557226, w0=-9.610716025866757e-07, w1=0.07947390612650473\n",
      "Regularized Logistic Regression(99/999): loss=0.6324391458970049, w0=-9.695456322820168e-07, w1=0.08014054872264526\n",
      "Regularized Logistic Regression(100/999): loss=0.6320513108020132, w0=-9.779988533176657e-07, w1=0.08080461915530127\n",
      "Regularized Logistic Regression(101/999): loss=0.6316666532905062, w0=-9.864313574338549e-07, w1=0.08146612418199467\n",
      "Regularized Logistic Regression(102/999): loss=0.6312851444869755, w0=-9.948432356629035e-07, w1=0.08212507060775019\n",
      "Regularized Logistic Regression(103/999): loss=0.6309067558166885, w0=-1.0032345783369705e-06, w1=0.08278146528360177\n",
      "Regularized Logistic Regression(104/999): loss=0.6305314590018044, w0=-1.0116054750957112e-06, w1=0.08343531510512227\n",
      "Regularized Logistic Regression(105/999): loss=0.6301592260575547, w0=-1.0199560148938413e-06, w1=0.08408662701098159\n",
      "Regularized Logistic Regression(106/999): loss=0.629790029288494, w0=-1.0282862860086077e-06, w1=0.08473540798152618\n",
      "Regularized Logistic Regression(107/999): loss=0.6294238412848083, w0=-1.0365963760471676e-06, w1=0.08538166503738902\n",
      "Regularized Logistic Regression(108/999): loss=0.6290606349186929, w0=-1.0448863719538768e-06, w1=0.08602540523812026\n",
      "Regularized Logistic Regression(109/999): loss=0.6287003833407889, w0=-1.0531563600174879e-06, w1=0.08666663568084194\n",
      "Regularized Logistic Regression(110/999): loss=0.6283430599766787, w0=-1.0614064258782603e-06, w1=0.08730536349893105\n",
      "Regularized Logistic Regression(111/999): loss=0.627988638523443, w0=-1.0696366545349823e-06, w1=0.0879415958607228\n",
      "Regularized Logistic Regression(112/999): loss=0.6276370929462739, w0=-1.0778471303519053e-06, w1=0.08857533996823595\n",
      "Regularized Logistic Regression(113/999): loss=0.6272883974751443, w0=-1.0860379370655935e-06, w1=0.08920660305592486\n",
      "Regularized Logistic Regression(114/999): loss=0.626942526601532, w0=-1.0942091577916888e-06, w1=0.0898353923894529\n",
      "Regularized Logistic Regression(115/999): loss=0.6265994550751998, w0=-1.1023608750315898e-06, w1=0.09046171526448549\n",
      "Regularized Logistic Regression(116/999): loss=0.6262591579010239, w0=-1.1104931706790506e-06, w1=0.09108557900550938\n",
      "Regularized Logistic Regression(117/999): loss=0.6259216103358813, w0=-1.118606126026696e-06, w1=0.09170699096466975\n",
      "Regularized Logistic Regression(118/999): loss=0.6255867878855775, w0=-1.1266998217724551e-06, w1=0.09232595852063201\n",
      "Regularized Logistic Regression(119/999): loss=0.6252546663018332, w0=-1.1347743380259173e-06, w1=0.09294248907746283\n",
      "Regularized Logistic Regression(120/999): loss=0.6249252215793126, w0=-1.1428297543146063e-06, w1=0.09355659006353058\n",
      "Regularized Logistic Regression(121/999): loss=0.6245984299527028, w0=-1.1508661495901776e-06, w1=0.09416826893042969\n",
      "Regularized Logistic Regression(122/999): loss=0.6242742678938364, w0=-1.1588836022345386e-06, w1=0.09477753315192117\n",
      "Regularized Logistic Regression(123/999): loss=0.623952712108864, w0=-1.1668821900658922e-06, w1=0.09538439022289506\n",
      "Regularized Logistic Regression(124/999): loss=0.6236337395354647, w0=-1.1748619903447042e-06, w1=0.09598884765835344\n",
      "Regularized Logistic Regression(125/999): loss=0.6233173273401064, w0=-1.182823079779598e-06, w1=0.09659091299241099\n",
      "Regularized Logistic Regression(126/999): loss=0.6230034529153429, w0=-1.1907655345331738e-06, w1=0.0971905937773129\n",
      "Regularized Logistic Regression(127/999): loss=0.6226920938771561, w0=-1.1986894302277562e-06, w1=0.09778789758247565\n",
      "Regularized Logistic Regression(128/999): loss=0.6223832280623391, w0=-1.2065948419510696e-06, w1=0.09838283199354114\n",
      "Regularized Logistic Regression(129/999): loss=0.6220768335259182, w0=-1.214481844261843e-06, w1=0.09897540461145268\n",
      "Regularized Logistic Regression(130/999): loss=0.6217728885386103, w0=-1.2223505111953445e-06, w1=0.0995656230515485\n",
      "Regularized Logistic Regression(131/999): loss=0.6214713715843283, w0=-1.2302009162688464e-06, w1=0.10015349494266979\n",
      "Regularized Logistic Regression(132/999): loss=0.6211722613577123, w0=-1.238033132487022e-06, w1=0.10073902792628806\n",
      "Regularized Logistic Regression(133/999): loss=0.6208755367617066, w0=-1.2458472323472761e-06, w1=0.10132222965565041\n",
      "Regularized Logistic Regression(134/999): loss=0.6205811769051681, w0=-1.2536432878450066e-06, w1=0.10190310779493943\n",
      "Regularized Logistic Regression(135/999): loss=0.6202891611005109, w0=-1.2614213704788027e-06, w1=0.10248167001845031\n",
      "Regularized Logistic Regression(136/999): loss=0.6199994688613867, w0=-1.269181551255576e-06, w1=0.10305792400978364\n",
      "Regularized Logistic Regression(137/999): loss=0.6197120799003969, w0=-1.2769239006956293e-06, w1=0.10363187746105519\n",
      "Regularized Logistic Regression(138/999): loss=0.6194269741268393, w0=-1.2846484888376606e-06, w1=0.10420353807212128\n",
      "Regularized Logistic Regression(139/999): loss=0.6191441316444898, w0=-1.2923553852437057e-06, w1=0.1047729135498191\n",
      "Regularized Logistic Regression(140/999): loss=0.6188635327494092, w0=-1.300044659004018e-06, w1=0.10534001160721895\n",
      "Regularized Logistic Regression(141/999): loss=0.6185851579277888, w0=-1.307716378741888e-06, w1=0.10590483996289977\n",
      "Regularized Logistic Regression(142/999): loss=0.6183089878538226, w0=-1.3153706126184031e-06, w1=0.10646740634023028\n",
      "Regularized Logistic Regression(143/999): loss=0.6180350033876094, w0=-1.3230074283371464e-06, w1=0.10702771846666911\n",
      "Regularized Logistic Regression(144/999): loss=0.6177631855730882, w0=-1.3306268931488384e-06, w1=0.10758578407307984\n",
      "Regularized Logistic Regression(145/999): loss=0.6174935156359977, w0=-1.3382290738559203e-06, w1=0.10814161089305767\n",
      "Regularized Logistic Regression(146/999): loss=0.6172259749818662, w0=-1.34581403681708e-06, w1=0.10869520666226969\n",
      "Regularized Logistic Regression(147/999): loss=0.6169605451940325, w0=-1.3533818479517217e-06, w1=0.10924657911781124\n",
      "Regularized Logistic Regression(148/999): loss=0.616697208031688, w0=-1.3609325727443806e-06, w1=0.10979573599757499\n",
      "Regularized Logistic Regression(149/999): loss=0.6164359454279512, w0=-1.3684662762490808e-06, w1=0.11034268503963036\n",
      "Regularized Logistic Regression(150/999): loss=0.6161767394879657, w0=-1.3759830230936412e-06, w1=0.11088743398161797\n",
      "Regularized Logistic Regression(151/999): loss=0.6159195724870252, w0=-1.3834828774839267e-06, w1=0.11142999056015786\n",
      "Regularized Logistic Regression(152/999): loss=0.6156644268687228, w0=-1.390965903208046e-06, w1=0.1119703625102675\n",
      "Regularized Logistic Regression(153/999): loss=0.615411285243127, w0=-1.3984321636404993e-06, w1=0.11250855756479428\n",
      "Regularized Logistic Regression(154/999): loss=0.6151601303849795, w0=-1.4058817217462718e-06, w1=0.1130445834538596\n",
      "Regularized Logistic Regression(155/999): loss=0.6149109452319206, w0=-1.4133146400848789e-06, w1=0.11357844790431153\n",
      "Regularized Logistic Regression(156/999): loss=0.614663712882736, w0=-1.4207309808143599e-06, w1=0.11411015863919609\n",
      "Regularized Logistic Regression(157/999): loss=0.6144184165956248, w0=-1.4281308056952224e-06, w1=0.11463972337723377\n",
      "Regularized Logistic Regression(158/999): loss=0.6141750397864959, w0=-1.4355141760943385e-06, w1=0.11516714983230844\n",
      "Regularized Logistic Regression(159/999): loss=0.6139335660272812, w0=-1.4428811529887924e-06, w1=0.11569244571297166\n",
      "Regularized Logistic Regression(160/999): loss=0.613693979044276, w0=-1.45023179696968e-06, w1=0.11621561872195144\n",
      "Regularized Logistic Regression(161/999): loss=0.6134562627164922, w0=-1.4575661682458629e-06, w1=0.11673667655567638\n",
      "Regularized Logistic Regression(162/999): loss=0.6132204010740469, w0=-1.4648843266476747e-06, w1=0.11725562690380903\n",
      "Regularized Logistic Regression(163/999): loss=0.6129863782965561, w0=-1.472186331630583e-06, w1=0.1177724774487891\n",
      "Regularized Logistic Regression(164/999): loss=0.6127541787115613, w0=-1.4794722422788048e-06, w1=0.11828723586538686\n",
      "Regularized Logistic Regression(165/999): loss=0.61252378679297, w0=-1.4867421173088793e-06, w1=0.11879990982026767\n",
      "Regularized Logistic Regression(166/999): loss=0.6122951871595159, w0=-1.4939960150731956e-06, w1=0.11931050697156594\n",
      "Regularized Logistic Regression(167/999): loss=0.6120683645732425, w0=-1.5012339935634765e-06, w1=0.11981903496846748\n",
      "Regularized Logistic Regression(168/999): loss=0.6118433039380023, w0=-1.5084561104142216e-06, w1=0.12032550145080319\n",
      "Regularized Logistic Regression(169/999): loss=0.6116199902979761, w0=-1.5156624229061065e-06, w1=0.12082991404865137\n",
      "Regularized Logistic Regression(170/999): loss=0.6113984088362111, w0=-1.5228529879693416e-06, w1=0.12133228038194709\n",
      "Regularized Logistic Regression(171/999): loss=0.6111785448731772, w0=-1.530027862186989e-06, w1=0.12183260806010619\n",
      "Regularized Logistic Regression(172/999): loss=0.6109603838653398, w0=-1.5371871017982387e-06, w1=0.12233090468165231\n",
      "Regularized Logistic Regression(173/999): loss=0.6107439114037527, w0=-1.5443307627016475e-06, w1=0.12282717783385529\n",
      "Regularized Logistic Regression(174/999): loss=0.6105291132126655, w0=-1.551458900458334e-06, w1=0.12332143509237833\n",
      "Regularized Logistic Regression(175/999): loss=0.6103159751481517, w0=-1.558571570295139e-06, w1=0.12381368402093246\n",
      "Regularized Logistic Regression(176/999): loss=0.6101044831967499, w0=-1.565668827107745e-06, w1=0.1243039321709406\n",
      "Regularized Logistic Regression(177/999): loss=0.6098946234741235, w0=-1.5727507254637585e-06, w1=0.12479218708120862\n",
      "Regularized Logistic Regression(178/999): loss=0.6096863822237367, w0=-1.5798173196057562e-06, w1=0.12527845627760506\n",
      "Regularized Logistic Regression(179/999): loss=0.6094797458155456, w0=-1.586868663454292e-06, w1=0.12576274727274916\n",
      "Regularized Logistic Regression(180/999): loss=0.6092747007447066, w0=-1.59390481061087e-06, w1=0.12624506756570403\n",
      "Regularized Logistic Regression(181/999): loss=0.6090712336302969, w0=-1.6009258143608795e-06, w1=0.12672542464168243\n",
      "Regularized Logistic Regression(182/999): loss=0.6088693312140566, w0=-1.6079317276764974e-06, w1=0.1272038259717537\n",
      "Regularized Logistic Regression(183/999): loss=0.6086689803591381, w0=-1.6149226032195523e-06, w1=0.12768027901256374\n",
      "Regularized Logistic Regression(184/999): loss=0.6084701680488765, w0=-1.6218984933443579e-06, w1=0.12815479120605797\n",
      "Regularized Logistic Regression(185/999): loss=0.6082728813855712, w0=-1.6288594501005086e-06, w1=0.12862736997921512\n",
      "Regularized Logistic Regression(186/999): loss=0.6080771075892838, w0=-1.6358055252356449e-06, w1=0.12909802274378365\n",
      "Regularized Logistic Regression(187/999): loss=0.607882833996648, w0=-1.6427367701981838e-06, w1=0.1295667568960289\n",
      "Regularized Logistic Regression(188/999): loss=0.6076900480596954, w0=-1.6496532361400176e-06, w1=0.13003357981648458\n",
      "Regularized Logistic Regression(189/999): loss=0.6074987373446943, w0=-1.6565549739191794e-06, w1=0.13049849886971154\n",
      "Regularized Logistic Regression(190/999): loss=0.6073088895310031, w0=-1.6634420341024785e-06, w1=0.1309615214040628\n",
      "Regularized Logistic Regression(191/999): loss=0.6071204924099336, w0=-1.6703144669681028e-06, w1=0.13142265475145629\n",
      "Regularized Logistic Regression(192/999): loss=0.6069335338836319, w0=-1.6771723225081914e-06, w1=0.13188190622715265\n",
      "Regularized Logistic Regression(193/999): loss=0.6067480019639692, w0=-1.6840156504313764e-06, w1=0.13233928312953688\n",
      "Regularized Logistic Regression(194/999): loss=0.6065638847714461, w0=-1.6908445001652951e-06, w1=0.13279479273991104\n",
      "Regularized Logistic Regression(195/999): loss=0.6063811705341103, w0=-1.6976589208590711e-06, w1=0.13324844232228872\n",
      "Regularized Logistic Regression(196/999): loss=0.6061998475864864, w0=-1.704458961385768e-06, w1=0.13370023912319715\n",
      "Regularized Logistic Regression(197/999): loss=0.6060199043685155, w0=-1.7112446703448132e-06, w1=0.13415019037148385\n",
      "Regularized Logistic Regression(198/999): loss=0.6058413294245121, w0=-1.718016096064393e-06, w1=0.13459830327813072\n",
      "Regularized Logistic Regression(199/999): loss=0.6056641114021267, w0=-1.7247732866038204e-06, w1=0.1350445850360722\n",
      "Regularized Logistic Regression(200/999): loss=0.605488239051325, w0=-1.7315162897558752e-06, w1=0.13548904282001742\n",
      "Regularized Logistic Regression(201/999): loss=0.6053137012233755, w0=-1.7382451530491151e-06, w1=0.1359316837862818\n",
      "Regularized Logistic Regression(202/999): loss=0.6051404868698502, w0=-1.744959923750162e-06, w1=0.13637251507261855\n",
      "Regularized Logistic Regression(203/999): loss=0.6049685850416364, w0=-1.7516606488659608e-06, w1=0.13681154379806115\n",
      "Regularized Logistic Regression(204/999): loss=0.6047979848879578, w0=-1.7583473751460113e-06, w1=0.13724877706276478\n",
      "Regularized Logistic Regression(205/999): loss=0.6046286756554102, w0=-1.7650201490845752e-06, w1=0.13768422194785743\n",
      "Regularized Logistic Regression(206/999): loss=0.6044606466870027, w0=-1.7716790169228568e-06, w1=0.1381178855152926\n",
      "Regularized Logistic Regression(207/999): loss=0.6042938874212137, w0=-1.778324024651159e-06, w1=0.1385497748077094\n",
      "Regularized Logistic Regression(208/999): loss=0.6041283873910585, w0=-1.7849552180110144e-06, w1=0.1389798968482956\n",
      "Regularized Logistic Regression(209/999): loss=0.6039641362231598, w0=-1.7915726424972911e-06, w1=0.13940825864065406\n",
      "Regularized Logistic Regression(210/999): loss=0.6038011236368379, w0=-1.7981763433602751e-06, w1=0.13983486716867755\n",
      "Regularized Logistic Regression(211/999): loss=0.6036393394432044, w0=-1.8047663656077287e-06, w1=0.14025972939642495\n",
      "Regularized Logistic Regression(212/999): loss=0.6034787735442682, w0=-1.8113427540069251e-06, w1=0.14068285226799956\n",
      "Regularized Logistic Regression(213/999): loss=0.6033194159320505, w0=-1.8179055530866594e-06, w1=0.14110424270743802\n",
      "Regularized Logistic Regression(214/999): loss=0.6031612566877116, w0=-1.8244548071392377e-06, w1=0.14152390761859762\n",
      "Regularized Logistic Regression(215/999): loss=0.6030042859806832, w0=-1.8309905602224423e-06, w1=0.14194185388505107\n",
      "Regularized Logistic Regression(216/999): loss=0.6028484940678142, w0=-1.8375128561614754e-06, w1=0.14235808836998315\n",
      "Regularized Logistic Regression(217/999): loss=0.6026938712925226, w0=-1.8440217385508805e-06, w1=0.14277261791609167\n",
      "Regularized Logistic Regression(218/999): loss=0.6025404080839607, w0=-1.850517250756442e-06, w1=0.1431854493454932\n",
      "Regularized Logistic Regression(219/999): loss=0.6023880949561833, w0=-1.8569994359170634e-06, w1=0.1435965894596317\n",
      "Regularized Logistic Regression(220/999): loss=0.6022369225073299, w0=-1.8634683369466243e-06, w1=0.14400604503919273\n",
      "Regularized Logistic Regression(221/999): loss=0.6020868814188145, w0=-1.8699239965358164e-06, w1=0.1444138228440174\n",
      "Regularized Logistic Regression(222/999): loss=0.6019379624545221, w0=-1.8763664571539594e-06, w1=0.14481992961302215\n",
      "Regularized Logistic Regression(223/999): loss=0.6017901564600168, w0=-1.8827957610507955e-06, w1=0.14522437206412384\n",
      "Regularized Logistic Regression(224/999): loss=0.6016434543617549, w0=-1.8892119502582648e-06, w1=0.1456271568941652\n",
      "Regularized Logistic Regression(225/999): loss=0.6014978471663108, w0=-1.8956150665922606e-06, w1=0.14602829077884485\n",
      "Regularized Logistic Regression(226/999): loss=0.6013533259596073, w0=-1.902005151654365e-06, w1=0.1464277803726506\n",
      "Regularized Logistic Regression(227/999): loss=0.6012098819061548, w0=-1.9083822468335655e-06, w1=0.1468256323087955\n",
      "Regularized Logistic Regression(228/999): loss=0.6010675062483004, w0=-1.914746393307952e-06, w1=0.14722185319915793\n",
      "Regularized Logistic Regression(229/999): loss=0.600926190305484, w0=-1.9210976320463954e-06, w1=0.14761644963422418\n",
      "Regularized Logistic Regression(230/999): loss=0.6007859254735013, w0=-1.927436003810208e-06, w1=0.14800942818303414\n",
      "Regularized Logistic Regression(231/999): loss=0.6006467032237751, w0=-1.9337615491547853e-06, w1=0.1484007953931287\n",
      "Regularized Logistic Regression(232/999): loss=0.6005085151026349, w0=-1.940074308431229e-06, w1=0.14879055779050357\n",
      "Regularized Logistic Regression(233/999): loss=0.6003713527306045, w0=-1.9463743217879537e-06, w1=0.14917872187956202\n",
      "Regularized Logistic Regression(234/999): loss=0.6002352078016943, w0=-1.952661629172275e-06, w1=0.14956529414307151\n",
      "Regularized Logistic Regression(235/999): loss=0.6001000720827053, w0=-1.9589362703319802e-06, w1=0.14995028104212563\n",
      "Regularized Logistic Regression(236/999): loss=0.5999659374125366, w0=-1.9651982848168824e-06, w1=0.15033368901610458\n",
      "Regularized Logistic Regression(237/999): loss=0.5998327957015016, w0=-1.971447711980357e-06, w1=0.15071552448264122\n",
      "Regularized Logistic Regression(238/999): loss=0.5997006389306507, w0=-1.977684590980863e-06, w1=0.15109579383758914\n",
      "Regularized Logistic Regression(239/999): loss=0.5995694591511045, w0=-1.983908960783445e-06, w1=0.15147450345499186\n",
      "Regularized Logistic Regression(240/999): loss=0.5994392484833874, w0=-1.9901208601612235e-06, w1=0.151851659687056\n",
      "Regularized Logistic Regression(241/999): loss=0.5993099991167741, w0=-1.9963203276968633e-06, w1=0.1522272688641268\n",
      "Regularized Logistic Regression(242/999): loss=0.5991817033086401, w0=-2.0025074017840314e-06, w1=0.15260133729466416\n",
      "Regularized Logistic Regression(243/999): loss=0.5990543533838198, w0=-2.0086821206288363e-06, w1=0.152973871265225\n",
      "Regularized Logistic Regression(244/999): loss=0.5989279417339692, w0=-2.014844522251253e-06, w1=0.15334487704044306\n",
      "Regularized Logistic Regression(245/999): loss=0.5988024608169379, w0=-2.020994644486532e-06, w1=0.15371436086301365\n",
      "Regularized Logistic Regression(246/999): loss=0.5986779031561473, w0=-2.0271325249865946e-06, w1=0.15408232895367951\n",
      "Regularized Logistic Regression(247/999): loss=0.5985542613399736, w0=-2.033258201221411e-06, w1=0.15444878751122065\n",
      "Regularized Logistic Regression(248/999): loss=0.5984315280211366, w0=-2.039371710480367e-06, w1=0.1548137427124445\n",
      "Regularized Logistic Regression(249/999): loss=0.5983096959160983, w0=-2.0454730898736144e-06, w1=0.15517720071217747\n",
      "Regularized Logistic Regression(250/999): loss=0.5981887578044638, w0=-2.051562376333405e-06, w1=0.15553916764326076\n",
      "Regularized Logistic Regression(251/999): loss=0.5980687065283903, w0=-2.0576396066154168e-06, w1=0.15589964961654657\n",
      "Regularized Logistic Regression(252/999): loss=0.5979495349920029, w0=-2.0637048173000583e-06, w1=0.15625865272089573\n",
      "Regularized Logistic Regression(253/999): loss=0.5978312361608134, w0=-2.0697580447937664e-06, w1=0.1566161830231801\n",
      "Regularized Logistic Regression(254/999): loss=0.5977138030611483, w0=-2.075799325330285e-06, w1=0.1569722465682824\n",
      "Regularized Logistic Regression(255/999): loss=0.59759722877958, w0=-2.081828694971935e-06, w1=0.15732684937910113\n",
      "Regularized Logistic Regression(256/999): loss=0.597481506462366, w0=-2.087846189610867e-06, w1=0.15767999745655706\n",
      "Regularized Logistic Regression(257/999): loss=0.5973666293148926, w0=-2.0938518449703035e-06, w1=0.15803169677959927\n",
      "Regularized Logistic Regression(258/999): loss=0.5972525906011239, w0=-2.0998456966057683e-06, w1=0.15838195330521426\n",
      "Regularized Logistic Regression(259/999): loss=0.5971393836430563, w0=-2.105827779906302e-06, w1=0.15873077296843768\n",
      "Regularized Logistic Regression(260/999): loss=0.5970270018201809, w0=-2.1117981300956644e-06, w1=0.1590781616823662\n",
      "Regularized Logistic Regression(261/999): loss=0.5969154385689472, w0=-2.117756782233528e-06, w1=0.1594241253381703\n",
      "Regularized Logistic Regression(262/999): loss=0.5968046873822372, w0=-2.1237037712166544e-06, w1=0.15976866980511087\n",
      "Regularized Logistic Regression(263/999): loss=0.5966947418088399, w0=-2.129639131780063e-06, w1=0.16011180093055558\n",
      "Regularized Logistic Regression(264/999): loss=0.596585595452933, w0=-2.1355628984981844e-06, w1=0.16045352453999748\n",
      "Regularized Logistic Regression(265/999): loss=0.5964772419735729, w0=-2.141475105786006e-06, w1=0.16079384643707456\n",
      "Regularized Logistic Regression(266/999): loss=0.5963696750841836, w0=-2.1473757879002003e-06, w1=0.16113277240358986\n",
      "Regularized Logistic Regression(267/999): loss=0.596262888552057, w0=-2.153264978940248e-06, w1=0.1614703081995357\n",
      "Regularized Logistic Regression(268/999): loss=0.5961568761978526, w0=-2.159142712849545e-06, w1=0.16180645956311526\n",
      "Regularized Logistic Regression(269/999): loss=0.5960516318951078, w0=-2.1650090234165007e-06, w1=0.16214123221076987\n",
      "Regularized Logistic Regression(270/999): loss=0.5959471495697488, w0=-2.1708639442756253e-06, w1=0.1624746318372054\n",
      "Regularized Logistic Regression(271/999): loss=0.5958434231996065, w0=-2.1767075089086037e-06, w1=0.1628066641154176\n",
      "Regularized Logistic Regression(272/999): loss=0.5957404468139411, w0=-2.1825397506453627e-06, w1=0.1631373346967226\n",
      "Regularized Logistic Regression(273/999): loss=0.5956382144929673, w0=-2.1883607026651238e-06, w1=0.16346664921078632\n",
      "Regularized Logistic Regression(274/999): loss=0.5955367203673865, w0=-2.194170397997448e-06, w1=0.16379461326565506\n",
      "Regularized Logistic Regression(275/999): loss=0.5954359586179218, w0=-2.1999688695232687e-06, w1=0.16412123244779\n",
      "Regularized Logistic Regression(276/999): loss=0.5953359234748612, w0=-2.2057561499759156e-06, w1=0.1644465123220975\n",
      "Regularized Logistic Regression(277/999): loss=0.5952366092175996, w0=-2.211532271942127e-06, w1=0.16477045843196458\n",
      "Regularized Logistic Regression(278/999): loss=0.5951380101741915, w0=-2.217297267863054e-06, w1=0.16509307629929465\n",
      "Regularized Logistic Regression(279/999): loss=0.5950401207209032, w0=-2.2230511700352536e-06, w1=0.16541437142454515\n",
      "Regularized Logistic Regression(280/999): loss=0.5949429352817738, w0=-2.2287940106116724e-06, w1=0.16573434928676284\n",
      "Regularized Logistic Regression(281/999): loss=0.5948464483281755, w0=-2.2345258216026202e-06, w1=0.16605301534362316\n",
      "Regularized Logistic Regression(282/999): loss=0.5947506543783827, w0=-2.2402466348767356e-06, w1=0.1663703750314692\n",
      "Regularized Logistic Regression(283/999): loss=0.594655547997144, w0=-2.2459564821619396e-06, w1=0.16668643376535222\n",
      "Regularized Logistic Regression(284/999): loss=0.5945611237952565, w0=-2.251655395046383e-06, w1=0.16700119693907292\n",
      "Regularized Logistic Regression(285/999): loss=0.594467376429147, w0=-2.2573434049793826e-06, w1=0.16731466992522284\n",
      "Regularized Logistic Regression(286/999): loss=0.5943743006004554, w0=-2.263020543272349e-06, w1=0.16762685807522862\n",
      "Regularized Logistic Regression(287/999): loss=0.5942818910556237, w0=-2.2686868410997055e-06, w1=0.1679377667193943\n",
      "Regularized Logistic Regression(288/999): loss=0.5941901425854883, w0=-2.2743423294997987e-06, w1=0.16824740116694506\n",
      "Regularized Logistic Regression(289/999): loss=0.5940990500248751, w0=-2.2799870393757983e-06, w1=0.16855576670607456\n",
      "Regularized Logistic Regression(290/999): loss=0.5940086082522006, w0=-2.2856210014965917e-06, w1=0.16886286860398958\n",
      "Regularized Logistic Regression(291/999): loss=0.5939188121890758, w0=-2.2912442464976674e-06, w1=0.16916871210695722\n",
      "Regularized Logistic Regression(292/999): loss=0.5938296567999153, w0=-2.29685680488199e-06, w1=0.1694733024403522\n",
      "Regularized Logistic Regression(293/999): loss=0.5937411370915471, w0=-2.30245870702087e-06, w1=0.16977664480870489\n",
      "Regularized Logistic Regression(294/999): loss=0.59365324811283, w0=-2.30804998315482e-06, w1=0.17007874439575094\n",
      "Regularized Logistic Regression(295/999): loss=0.5935659849542723, w0=-2.31363066339441e-06, w1=0.1703796063644797\n",
      "Regularized Logistic Regression(296/999): loss=0.5934793427476545, w0=-2.3192007777211066e-06, w1=0.17067923585718456\n",
      "Regularized Logistic Regression(297/999): loss=0.5933933166656568, w0=-2.324760355988112e-06, w1=0.17097763799551416\n",
      "Regularized Logistic Regression(298/999): loss=0.5933079019214894, w0=-2.3303094279211885e-06, w1=0.1712748178805244\n",
      "Regularized Logistic Regression(299/999): loss=0.5932230937685258, w0=-2.3358480231194816e-06, w1=0.17157078059272843\n",
      "Regularized Logistic Regression(300/999): loss=0.5931388874999409, w0=-2.3413761710563296e-06, w1=0.17186553119215028\n",
      "Regularized Logistic Regression(301/999): loss=0.5930552784483518, w0=-2.3468939010800697e-06, w1=0.17215907471837852\n",
      "Regularized Logistic Regression(302/999): loss=0.5929722619854636, w0=-2.352401242414835e-06, w1=0.1724514161906187\n",
      "Regularized Logistic Regression(303/999): loss=0.5928898335217143, w0=-2.3578982241613448e-06, w1=0.17274256060774937\n",
      "Regularized Logistic Regression(304/999): loss=0.5928079885059302, w0=-2.3633848752976867e-06, w1=0.17303251294837416\n",
      "Regularized Logistic Regression(305/999): loss=0.5927267224249773, w0=-2.368861224680093e-06, w1=0.1733212781708768\n",
      "Regularized Logistic Regression(306/999): loss=0.5926460308034213, w0=-2.374327301043709e-06, w1=0.1736088612134811\n",
      "Regularized Logistic Regression(307/999): loss=0.5925659092031881, w0=-2.379783133003354e-06, w1=0.17389526699430172\n",
      "Regularized Logistic Regression(308/999): loss=0.5924863532232294, w0=-2.3852287490542766e-06, w1=0.17418050041140296\n",
      "Regularized Logistic Regression(309/999): loss=0.5924073584991892, w0=-2.3906641775729015e-06, w1=0.17446456634285637\n",
      "Regularized Logistic Regression(310/999): loss=0.5923289207030767, w0=-2.3960894468175717e-06, w1=0.17474746964679694\n",
      "Regularized Logistic Regression(311/999): loss=0.5922510355429389, w0=-2.4015045849292814e-06, w1=0.1750292151614799\n",
      "Regularized Logistic Regression(312/999): loss=0.5921736987625382, w0=-2.4069096199324054e-06, w1=0.17530980770534166\n",
      "Regularized Logistic Regression(313/999): loss=0.5920969061410346, w0=-2.4123045797354185e-06, w1=0.1755892520770559\n",
      "Regularized Logistic Regression(314/999): loss=0.5920206534926676, w0=-2.4176894921316117e-06, w1=0.17586755305559318\n",
      "Regularized Logistic Regression(315/999): loss=0.5919449366664422, w0=-2.4230643847997995e-06, w1=0.1761447154002795\n",
      "Regularized Logistic Regression(316/999): loss=0.5918697515458214, w0=-2.428429285305023e-06, w1=0.1764207438508566\n",
      "Regularized Logistic Regression(317/999): loss=0.5917950940484161, w0=-2.433784221099245e-06, w1=0.17669564312754163\n",
      "Regularized Logistic Regression(318/999): loss=0.5917209601256807, w0=-2.439129219522039e-06, w1=0.1769694179310871\n",
      "Regularized Logistic Regression(319/999): loss=0.5916473457626136, w0=-2.4444643078012752e-06, w1=0.17724207294284056\n",
      "Regularized Logistic Regression(320/999): loss=0.5915742469774572, w0=-2.449789513053796e-06, w1=0.17751361282480776\n",
      "Regularized Logistic Regression(321/999): loss=0.591501659821401, w0=-2.4551048622860884e-06, w1=0.17778404221971103\n",
      "Regularized Logistic Regression(322/999): loss=0.5914295803782906, w0=-2.4604103823949494e-06, w1=0.17805336575105038\n",
      "Regularized Logistic Regression(323/999): loss=0.5913580047643371, w0=-2.4657061001681463e-06, w1=0.17832158802316692\n",
      "Regularized Logistic Regression(324/999): loss=0.5912869291278282, w0=-2.4709920422850713e-06, w1=0.1785887136213039\n",
      "Regularized Logistic Regression(325/999): loss=0.5912163496488458, w0=-2.47626823531739e-06, w1=0.17885474711166774\n",
      "Regularized Logistic Regression(326/999): loss=0.5911462625389808, w0=-2.481534705729683e-06, w1=0.17911969304149064\n",
      "Regularized Logistic Regression(327/999): loss=0.5910766640410584, w0=-2.486791479880086e-06, w1=0.17938355593909436\n",
      "Regularized Logistic Regression(328/999): loss=0.5910075504288563, w0=-2.492038584020919e-06, w1=0.17964634031394972\n",
      "Regularized Logistic Regression(329/999): loss=0.5909389180068347, w0=-2.497276044299314e-06, w1=0.17990805065674265\n",
      "Regularized Logistic Regression(330/999): loss=0.590870763109863, w0=-2.502503886757837e-06, w1=0.18016869143943529\n",
      "Regularized Logistic Regression(331/999): loss=0.5908030821029513, w0=-2.5077221373351026e-06, w1=0.18042826711532847\n",
      "Regularized Logistic Regression(332/999): loss=0.5907358713809843, w0=-2.512930821866385e-06, w1=0.18068678211912742\n",
      "Regularized Logistic Regression(333/999): loss=0.5906691273684569, w0=-2.5181299660842236e-06, w1=0.18094424086700261\n",
      "Regularized Logistic Regression(334/999): loss=0.5906028465192138, w0=-2.523319595619023e-06, w1=0.18120064775665443\n",
      "Regularized Logistic Regression(335/999): loss=0.5905370253161907, w0=-2.52849973599965e-06, w1=0.1814560071673764\n",
      "Regularized Logistic Regression(336/999): loss=0.590471660271157, w0=-2.53367041265402e-06, w1=0.18171032346012067\n",
      "Regularized Logistic Regression(337/999): loss=0.5904067479244622, w0=-2.538831650909687e-06, w1=0.18196360097755856\n",
      "Regularized Logistic Regression(338/999): loss=0.5903422848447849, w0=-2.5439834759944205e-06, w1=0.18221584404414826\n",
      "Regularized Logistic Regression(339/999): loss=0.5902782676288838, w0=-2.549125913036782e-06, w1=0.1824670569661966\n",
      "Regularized Logistic Regression(340/999): loss=0.5902146929013496, w0=-2.5542589870666977e-06, w1=0.18271724403192313\n",
      "Regularized Logistic Regression(341/999): loss=0.5901515573143614, w0=-2.559382723016021e-06, w1=0.1829664095115262\n",
      "Regularized Logistic Regression(342/999): loss=0.5900888575474456, w0=-2.564497145719097e-06, w1=0.18321455765724648\n",
      "Regularized Logistic Regression(343/999): loss=0.5900265903072328, w0=-2.5696022799133178e-06, w1=0.18346169270342996\n",
      "Regularized Logistic Regression(344/999): loss=0.5899647523272238, w0=-2.5746981502396747e-06, w1=0.18370781886659412\n",
      "Regularized Logistic Regression(345/999): loss=0.5899033403675518, w0=-2.5797847812433064e-06, w1=0.18395294034549228\n",
      "Regularized Logistic Regression(346/999): loss=0.5898423512147505, w0=-2.5848621973740417e-06, w1=0.18419706132117833\n",
      "Regularized Logistic Regression(347/999): loss=0.5897817816815216, w0=-2.589930422986938e-06, w1=0.18444018595706968\n",
      "Regularized Logistic Regression(348/999): loss=0.5897216286065079, w0=-2.5949894823428153e-06, w1=0.1846823183990139\n",
      "Regularized Logistic Regression(349/999): loss=0.5896618888540642, w0=-2.600039399608788e-06, w1=0.18492346277535232\n",
      "Regularized Logistic Regression(350/999): loss=0.589602559314036, w0=-2.605080198858789e-06, w1=0.1851636231969864\n",
      "Regularized Logistic Regression(351/999): loss=0.5895436369015331, w0=-2.6101119040740914e-06, w1=0.1854028037574389\n",
      "Regularized Logistic Regression(352/999): loss=0.5894851185567125, w0=-2.615134539143826e-06, w1=0.18564100853292237\n",
      "Regularized Logistic Regression(353/999): loss=0.5894270012445576, w0=-2.620148127865494e-06, w1=0.18587824158240274\n",
      "Regularized Logistic Regression(354/999): loss=0.5893692819546641, w0=-2.625152693945476e-06, w1=0.18611450694766132\n",
      "Regularized Logistic Regression(355/999): loss=0.5893119577010238, w0=-2.630148260999539e-06, w1=0.18634980865336542\n",
      "Regularized Logistic Regression(356/999): loss=0.5892550255218141, w0=-2.6351348525533334e-06, w1=0.18658415070712647\n",
      "Regularized Logistic Regression(357/999): loss=0.589198482479185, w0=-2.6401124920428933e-06, w1=0.18681753709956953\n",
      "Regularized Logistic Regression(358/999): loss=0.5891423256590539, w0=-2.6450812028151283e-06, w1=0.1870499718043946\n",
      "Regularized Logistic Regression(359/999): loss=0.5890865521708977, w0=-2.6500410081283125e-06, w1=0.18728145877844427\n",
      "Regularized Logistic Regression(360/999): loss=0.5890311591475474, w0=-2.6549919311525697e-06, w1=0.18751200196176696\n",
      "Regularized Logistic Regression(361/999): loss=0.5889761437449879, w0=-2.659933994970355e-06, w1=0.18774160527768013\n",
      "Regularized Logistic Regression(362/999): loss=0.5889215031421542, w0=-2.664867222576933e-06, w1=0.18797027263283772\n",
      "Regularized Logistic Regression(363/999): loss=0.5888672345407352, w0=-2.669791636880851e-06, w1=0.1881980079172914\n",
      "Regularized Logistic Regression(364/999): loss=0.5888133351649754, w0=-2.6747072607044096e-06, w1=0.18842481500455738\n",
      "Regularized Logistic Regression(365/999): loss=0.5887598022614802, w0=-2.679614116784129e-06, w1=0.1886506977516803\n",
      "Regularized Logistic Regression(366/999): loss=0.5887066330990219, w0=-2.6845122277712128e-06, w1=0.1888756599992956\n",
      "Regularized Logistic Regression(367/999): loss=0.5886538249683495, w0=-2.6894016162320065e-06, w1=0.1890997055716971\n",
      "Regularized Logistic Regression(368/999): loss=0.588601375181997, w0=-2.6942823046484536e-06, w1=0.18932283827689914\n",
      "Regularized Logistic Regression(369/999): loss=0.5885492810740972, w0=-2.6991543154185485e-06, w1=0.18954506190669926\n",
      "Regularized Logistic Regression(370/999): loss=0.588497540000193, w0=-2.704017670856785e-06, w1=0.18976638023674575\n",
      "Regularized Logistic Regression(371/999): loss=0.5884461493370569, w0=-2.7088723931946017e-06, w1=0.18998679702659854\n",
      "Regularized Logistic Regression(372/999): loss=0.5883951064825026, w0=-2.7137185045808238e-06, w1=0.19020631601979499\n",
      "Regularized Logistic Regression(373/999): loss=0.5883444088552092, w0=-2.718556027082103e-06, w1=0.19042494094391163\n",
      "Regularized Logistic Regression(374/999): loss=0.5882940538945372, w0=-2.7233849826833514e-06, w1=0.19064267551062966\n",
      "Regularized Logistic Regression(375/999): loss=0.5882440390603543, w0=-2.728205393288175e-06, w1=0.19085952341579782\n",
      "Regularized Logistic Regression(376/999): loss=0.5881943618328569, w0=-2.733017280719301e-06, w1=0.19107548833949548\n",
      "Regularized Logistic Regression(377/999): loss=0.5881450197123954, w0=-2.7378206667190053e-06, w1=0.1912905739460981\n",
      "Regularized Logistic Regression(378/999): loss=0.5880960102193034, w0=-2.7426155729495328e-06, w1=0.19150478388433537\n",
      "Regularized Logistic Regression(379/999): loss=0.5880473308937231, w0=-2.7474020209935187e-06, w1=0.19171812178735975\n",
      "Regularized Logistic Regression(380/999): loss=0.5879989792954384, w0=-2.7521800323544033e-06, w1=0.1919305912728076\n",
      "Regularized Logistic Regression(381/999): loss=0.5879509530037044, w0=-2.756949628456846e-06, w1=0.1921421959428603\n",
      "Regularized Logistic Regression(382/999): loss=0.5879032496170823, w0=-2.7617108306471335e-06, w1=0.1923529393843109\n",
      "Regularized Logistic Regression(383/999): loss=0.5878558667532734, w0=-2.76646366019359e-06, w1=0.1925628251686226\n",
      "Regularized Logistic Regression(384/999): loss=0.5878088020489551, w0=-2.771208138286977e-06, w1=0.19277185685199288\n",
      "Regularized Logistic Regression(385/999): loss=0.5877620531596198, w0=-2.7759442860408976e-06, w1=0.19298003797541816\n",
      "Regularized Logistic Regression(386/999): loss=0.5877156177594126, w0=-2.780672124492194e-06, w1=0.19318737206475234\n",
      "Regularized Logistic Regression(387/999): loss=0.587669493540973, w0=-2.7853916746013406e-06, w1=0.19339386263077096\n",
      "Regularized Logistic Regression(388/999): loss=0.5876236782152781, w0=-2.7901029572528383e-06, w1=0.1935995131692335\n",
      "Regularized Logistic Regression(389/999): loss=0.5875781695114838, w0=-2.7948059932556027e-06, w1=0.19380432716094367\n",
      "Regularized Logistic Regression(390/999): loss=0.5875329651767722, w0=-2.799500803343352e-06, w1=0.19400830807181313\n",
      "Regularized Logistic Regression(391/999): loss=0.5874880629761966, w0=-2.804187408174989e-06, w1=0.19421145935292153\n",
      "Regularized Logistic Regression(392/999): loss=0.5874434606925307, w0=-2.8088658283349838e-06, w1=0.19441378444057872\n",
      "Regularized Logistic Regression(393/999): loss=0.5873991561261163, w0=-2.81353608433375e-06, w1=0.19461528675638534\n",
      "Regularized Logistic Regression(394/999): loss=0.5873551470947147, w0=-2.8181981966080225e-06, w1=0.1948159697072944\n",
      "Regularized Logistic Regression(395/999): loss=0.5873114314333597, w0=-2.8228521855212283e-06, w1=0.1950158366856725\n",
      "Regularized Logistic Regression(396/999): loss=0.5872680069942093, w0=-2.827498071363858e-06, w1=0.1952148910693603\n",
      "Regularized Logistic Regression(397/999): loss=0.5872248716464012, w0=-2.8321358743538335e-06, w1=0.19541313622173329\n",
      "Regularized Logistic Regression(398/999): loss=0.5871820232759084, w0=-2.8367656146368706e-06, w1=0.19561057549176245\n",
      "Regularized Logistic Regression(399/999): loss=0.5871394597853973, w0=-2.841387312286844e-06, w1=0.19580721221407454\n",
      "Regularized Logistic Regression(400/999): loss=0.5870971790940864, w0=-2.846000987306146e-06, w1=0.19600304970901222\n",
      "Regularized Logistic Regression(401/999): loss=0.5870551791376039, w0=-2.850606659626043e-06, w1=0.19619809128269416\n",
      "Regularized Logistic Regression(402/999): loss=0.5870134578678518, w0=-2.8552043491070317e-06, w1=0.19639234022707472\n",
      "Regularized Logistic Regression(403/999): loss=0.5869720132528673, w0=-2.85979407553919e-06, w1=0.19658579982000332\n",
      "Regularized Logistic Regression(404/999): loss=0.5869308432766851, w0=-2.8643758586425278e-06, w1=0.19677847332528517\n",
      "Regularized Logistic Regression(405/999): loss=0.5868899459392046, w0=-2.8689497180673327e-06, w1=0.19697036399274018\n",
      "Regularized Logistic Regression(406/999): loss=0.5868493192560541, w0=-2.873515673394516e-06, w1=0.19716147505826095\n",
      "Regularized Logistic Regression(407/999): loss=0.586808961258459, w0=-2.8780737441359556e-06, w1=0.19735180974387295\n",
      "Regularized Logistic Regression(408/999): loss=0.5867688699931105, w0=-2.882623949734835e-06, w1=0.19754137125779375\n",
      "Regularized Logistic Regression(409/999): loss=0.5867290435220345, w0=-2.887166309565982e-06, w1=0.19773016279449124\n",
      "Regularized Logistic Regression(410/999): loss=0.5866894799224633, w0=-2.891700842936203e-06, w1=0.19791818753474116\n",
      "Regularized Logistic Regression(411/999): loss=0.5866501772867073, w0=-2.8962275690846176e-06, w1=0.19810544864568708\n",
      "Regularized Logistic Regression(412/999): loss=0.5866111337220278, w0=-2.9007465071829878e-06, w1=0.19829194928089774\n",
      "Regularized Logistic Regression(413/999): loss=0.5865723473505121, w0=-2.9052576763360473e-06, w1=0.19847769258042475\n",
      "Regularized Logistic Regression(414/999): loss=0.5865338163089486, w0=-2.9097610955818287e-06, w1=0.1986626816708605\n",
      "Regularized Logistic Regression(415/999): loss=0.5864955387487034, w0=-2.9142567838919856e-06, w1=0.19884691966539633\n",
      "Regularized Logistic Regression(416/999): loss=0.5864575128355979, w0=-2.9187447601721165e-06, w1=0.1990304096638801\n",
      "Regularized Logistic Regression(417/999): loss=0.5864197367497872, w0=-2.923225043262083e-06, w1=0.1992131547528719\n",
      "Regularized Logistic Regression(418/999): loss=0.5863822086856415, w0=-2.927697651936329e-06, w1=0.19939515800570273\n",
      "Regularized Logistic Regression(419/999): loss=0.5863449268516248, w0=-2.932162604904193e-06, w1=0.19957642248253016\n",
      "Regularized Logistic Regression(420/999): loss=0.5863078894701788, w0=-2.9366199208102258e-06, w1=0.19975695123039708\n",
      "Regularized Logistic Regression(421/999): loss=0.5862710947776053, w0=-2.941069618234497e-06, w1=0.1999367472832857\n",
      "Regularized Logistic Regression(422/999): loss=0.5862345410239502, w0=-2.9455117156929084e-06, w1=0.200115813662175\n",
      "Regularized Logistic Regression(423/999): loss=0.5861982264728892, w0=-2.9499462316374978e-06, w1=0.20029415337509673\n",
      "Regularized Logistic Regression(424/999): loss=0.5861621494016128, w0=-2.9543731844567453e-06, w1=0.20047176941719197\n",
      "Regularized Logistic Regression(425/999): loss=0.5861263081007145, w0=-2.958792592475877e-06, w1=0.20064866477076543\n",
      "Regularized Logistic Regression(426/999): loss=0.5860907008740796, w0=-2.9632044739571636e-06, w1=0.20082484240534312\n",
      "Regularized Logistic Regression(427/999): loss=0.5860553260387724, w0=-2.9676088471002224e-06, w1=0.20100030527772472\n",
      "Regularized Logistic Regression(428/999): loss=0.5860201819249283, w0=-2.9720057300423117e-06, w1=0.20117505633204288\n",
      "Regularized Logistic Regression(429/999): loss=0.585985266875644, w0=-2.9763951408586273e-06, w1=0.20134909849981417\n",
      "Regularized Logistic Regression(430/999): loss=0.5859505792468702, w0=-2.980777097562595e-06, w1=0.20152243469999664\n",
      "Regularized Logistic Regression(431/999): loss=0.5859161174073041, w0=-2.9851516181061623e-06, w1=0.20169506783904195\n",
      "Regularized Logistic Regression(432/999): loss=0.585881879738284, w0=-2.9895187203800866e-06, w1=0.20186700081095288\n",
      "Regularized Logistic Regression(433/999): loss=0.5858478646336839, w0=-2.993878422214224e-06, w1=0.2020382364973347\n",
      "Regularized Logistic Regression(434/999): loss=0.5858140704998098, w0=-2.998230741377814e-06, w1=0.2022087777674503\n",
      "Regularized Logistic Regression(435/999): loss=0.5857804957552963, w0=-3.0025756955797626e-06, w1=0.20237862747827462\n",
      "Regularized Logistic Regression(436/999): loss=0.5857471388310042, w0=-3.0069133024689256e-06, w1=0.20254778847454766\n",
      "Regularized Logistic Regression(437/999): loss=0.5857139981699196, w0=-3.0112435796343873e-06, w1=0.20271626358882786\n",
      "Regularized Logistic Regression(438/999): loss=0.5856810722270532, w0=-3.0155665446057397e-06, w1=0.202884055641546\n",
      "Regularized Logistic Regression(439/999): loss=0.5856483594693411, w0=-3.019882214853358e-06, w1=0.20305116744105614\n",
      "Regularized Logistic Regression(440/999): loss=0.5856158583755451, w0=-3.0241906077886755e-06, w1=0.2032176017836912\n",
      "Regularized Logistic Regression(441/999): loss=0.5855835674361566, w0=-3.028491740764458e-06, w1=0.20338336145381367\n",
      "Regularized Logistic Regression(442/999): loss=0.585551485153298, w0=-3.0327856310750718e-06, w1=0.20354844922386958\n",
      "Regularized Logistic Regression(443/999): loss=0.5855196100406285, w0=-3.037072295956756e-06, w1=0.2037128678544397\n",
      "Regularized Logistic Regression(444/999): loss=0.5854879406232466, w0=-3.0413517525878893e-06, w1=0.20387662009429242\n",
      "Regularized Logistic Regression(445/999): loss=0.5854564754375976, w0=-3.045624018089255e-06, w1=0.20403970868043367\n",
      "Regularized Logistic Regression(446/999): loss=0.5854252130313793, w0=-3.0498891095243075e-06, w1=0.20420213633816173\n",
      "Regularized Logistic Regression(447/999): loss=0.5853941519634488, w0=-3.0541470438994327e-06, w1=0.20436390578111496\n",
      "Regularized Logistic Regression(448/999): loss=0.5853632908037321, w0=-3.0583978381642103e-06, w1=0.20452501971132628\n",
      "Regularized Logistic Regression(449/999): loss=0.5853326281331314, w0=-3.0626415092116725e-06, w1=0.20468548081927385\n",
      "Regularized Logistic Regression(450/999): loss=0.5853021625434357, w0=-3.066878073878563e-06, w1=0.20484529178392916\n",
      "Regularized Logistic Regression(451/999): loss=0.5852718926372315, w0=-3.0711075489455916e-06, w1=0.2050044552728109\n",
      "Regularized Logistic Regression(452/999): loss=0.5852418170278127, w0=-3.0753299511376894e-06, w1=0.20516297394203437\n",
      "Regularized Logistic Regression(453/999): loss=0.585211934339095, w0=-3.0795452971242625e-06, w1=0.2053208504363602\n",
      "Regularized Logistic Regression(454/999): loss=0.5851822432055255, w0=-3.0837536035194416e-06, w1=0.2054780873892474\n",
      "Regularized Logistic Regression(455/999): loss=0.5851527422720005, w0=-3.087954886882334e-06, w1=0.20563468742290045\n",
      "Regularized Logistic Regression(456/999): loss=0.5851234301937757, w0=-3.0921491637172698e-06, w1=0.2057906531483218\n",
      "Regularized Logistic Regression(457/999): loss=0.585094305636384, w0=-3.0963364504740507e-06, w1=0.20594598716535803\n",
      "Regularized Logistic Regression(458/999): loss=0.5850653672755504, w0=-3.1005167635481935e-06, w1=0.20610069206275144\n",
      "Regularized Logistic Regression(459/999): loss=0.5850366137971091, w0=-3.1046901192811746e-06, w1=0.20625477041818877\n",
      "Regularized Logistic Regression(460/999): loss=0.5850080438969198, w0=-3.1088565339606723e-06, w1=0.20640822479834955\n",
      "Regularized Logistic Regression(461/999): loss=0.584979656280787, w0=-3.113016023820808e-06, w1=0.20656105775895556\n",
      "Regularized Logistic Regression(462/999): loss=0.5849514496643775, w0=-3.1171686050423833e-06, w1=0.20671327184481936\n",
      "Regularized Logistic Regression(463/999): loss=0.5849234227731412, w0=-3.1213142937531217e-06, w1=0.20686486958989123\n",
      "Regularized Logistic Regression(464/999): loss=0.5848955743422304, w0=-3.1254531060279016e-06, w1=0.20701585351730833\n",
      "Regularized Logistic Regression(465/999): loss=0.5848679031164209, w0=-3.1295850578889924e-06, w1=0.20716622613944363\n",
      "Regularized Logistic Regression(466/999): loss=0.5848404078500332, w0=-3.1337101653062885e-06, w1=0.2073159899579506\n",
      "Regularized Logistic Regression(467/999): loss=0.5848130873068557, w0=-3.1378284441975405e-06, w1=0.20746514746381395\n",
      "Regularized Logistic Regression(468/999): loss=0.5847859402600667, w0=-3.1419399104285873e-06, w1=0.20761370113739552\n",
      "Regularized Logistic Regression(469/999): loss=0.584758965492158, w0=-3.1460445798135845e-06, w1=0.20776165344848171\n",
      "Regularized Logistic Regression(470/999): loss=0.5847321617948603, w0=-3.150142468115232e-06, w1=0.20790900685632988\n",
      "Regularized Logistic Regression(471/999): loss=0.5847055279690663, w0=-3.154233591045002e-06, w1=0.20805576380971524\n",
      "Regularized Logistic Regression(472/999): loss=0.5846790628247577, w0=-3.158317964263362e-06, w1=0.20820192674697816\n",
      "Regularized Logistic Regression(473/999): loss=0.5846527651809312, w0=-3.1623956033800017e-06, w1=0.20834749809607006\n",
      "Regularized Logistic Regression(474/999): loss=0.5846266338655248, w0=-3.1664665239540533e-06, w1=0.2084924802745991\n",
      "Regularized Logistic Regression(475/999): loss=0.5846006677153459, w0=-3.170530741494314e-06, w1=0.20863687568987652\n",
      "Regularized Logistic Regression(476/999): loss=0.5845748655759984, w0=-3.174588271459465e-06, w1=0.20878068673896283\n",
      "Regularized Logistic Regression(477/999): loss=0.5845492263018127, w0=-3.1786391292582907e-06, w1=0.2089239158087124\n",
      "Regularized Logistic Regression(478/999): loss=0.5845237487557747, w0=-3.1826833302498967e-06, w1=0.20906656527581996\n",
      "Regularized Logistic Regression(479/999): loss=0.584498431809455, w0=-3.1867208897439247e-06, w1=0.2092086375068659\n",
      "Regularized Logistic Regression(480/999): loss=0.5844732743429408, w0=-3.190751823000768e-06, w1=0.20935013485835932\n",
      "Regularized Logistic Regression(481/999): loss=0.584448275244765, w0=-3.1947761452317856e-06, w1=0.2094910596767854\n",
      "Regularized Logistic Regression(482/999): loss=0.5844234334118409, w0=-3.1987938715995126e-06, w1=0.2096314142986478\n",
      "Regularized Logistic Regression(483/999): loss=0.5843987477493923, w0=-3.202805017217874e-06, w1=0.2097712010505155\n",
      "Regularized Logistic Regression(484/999): loss=0.5843742171708857, w0=-3.2068095971523926e-06, w1=0.20991042224906403\n",
      "Regularized Logistic Regression(485/999): loss=0.5843498405979676, w0=-3.2108076264203983e-06, w1=0.2100490802011228\n",
      "Regularized Logistic Regression(486/999): loss=0.5843256169603942, w0=-3.2147991199912356e-06, w1=0.2101871772037157\n",
      "Regularized Logistic Regression(487/999): loss=0.5843015451959693, w0=-3.21878409278647e-06, w1=0.21032471554410784\n",
      "Regularized Logistic Regression(488/999): loss=0.5842776242504772, w0=-3.222762559680092e-06, w1=0.21046169749984697\n",
      "Regularized Logistic Regression(489/999): loss=0.5842538530776205, w0=-3.226734535498723e-06, w1=0.2105981253388075\n",
      "Regularized Logistic Regression(490/999): loss=0.5842302306389549, w0=-3.230700035021815e-06, w1=0.21073400131923273\n",
      "Regularized Logistic Regression(491/999): loss=0.5842067559038263, w0=-3.2346590729818555e-06, w1=0.2108693276897801\n",
      "Regularized Logistic Regression(492/999): loss=0.5841834278493092, w0=-3.238611664064566e-06, w1=0.21100410668956152\n",
      "Regularized Logistic Regression(493/999): loss=0.5841602454601437, w0=-3.242557822909101e-06, w1=0.2111383405481874\n",
      "Regularized Logistic Regression(494/999): loss=0.5841372077286743, w0=-3.2464975641082477e-06, w1=0.21127203148580836\n",
      "Regularized Logistic Regression(495/999): loss=0.5841143136547884, w0=-3.2504309022086207e-06, w1=0.21140518171315562\n",
      "Regularized Logistic Regression(496/999): loss=0.5840915622458577, w0=-3.25435785171086e-06, w1=0.21153779343158732\n",
      "Regularized Logistic Regression(497/999): loss=0.5840689525166748, w0=-3.258278427069824e-06, w1=0.21166986883312752\n",
      "Regularized Logistic Regression(498/999): loss=0.5840464834893977, w0=-3.2621926426947848e-06, w1=0.2118014101005068\n",
      "Regularized Logistic Regression(499/999): loss=0.5840241541934879, w0=-3.2661005129496198e-06, w1=0.21193241940720667\n",
      "Regularized Logistic Regression(500/999): loss=0.5840019636656534, w0=-3.2700020521530032e-06, w1=0.21206289891749777\n",
      "Regularized Logistic Regression(501/999): loss=0.5839799109497903, w0=-3.273897274578597e-06, w1=0.21219285078648448\n",
      "Regularized Logistic Regression(502/999): loss=0.5839579950969261, w0=-3.277786194455239e-06, w1=0.21232227716014165\n",
      "Regularized Logistic Regression(503/999): loss=0.5839362151651618, w0=-3.2816688259671326e-06, w1=0.21245118017535936\n",
      "Regularized Logistic Regression(504/999): loss=0.5839145702196155, w0=-3.285545183254033e-06, w1=0.2125795619599794\n",
      "Regularized Logistic Regression(505/999): loss=0.583893059332367, w0=-3.2894152804114346e-06, w1=0.21270742463284018\n",
      "Regularized Logistic Regression(506/999): loss=0.5838716815824023, w0=-3.293279131490754e-06, w1=0.21283477030381312\n",
      "Regularized Logistic Regression(507/999): loss=0.5838504360555578, w0=-3.2971367504995164e-06, w1=0.21296160107384443\n",
      "Regularized Logistic Regression(508/999): loss=0.5838293218444665, w0=-3.300988151401537e-06, w1=0.2130879190349944\n",
      "Regularized Logistic Regression(509/999): loss=0.583808338048504, w0=-3.304833348117104e-06, w1=0.21321372627047758\n",
      "Regularized Logistic Regression(510/999): loss=0.5837874837737342, w0=-3.30867235452316e-06, w1=0.21333902485470152\n",
      "Regularized Logistic Regression(511/999): loss=0.5837667581328564, w0=-3.31250518445348e-06, w1=0.2134638168533069\n",
      "Regularized Logistic Regression(512/999): loss=0.5837461602451528, w0=-3.3163318516988538e-06, w1=0.21358810432320696\n",
      "Regularized Logistic Regression(513/999): loss=0.5837256892364362, w0=-3.3201523700072614e-06, w1=0.21371188931262353\n",
      "Regularized Logistic Regression(514/999): loss=0.5837053442389973, w0=-3.3239667530840505e-06, w1=0.21383517386113002\n",
      "Regularized Logistic Regression(515/999): loss=0.5836851243915548, w0=-3.327775014592114e-06, w1=0.21395795999968667\n",
      "Regularized Logistic Regression(516/999): loss=0.5836650288392035, w0=-3.331577168152064e-06, w1=0.21408024975068035\n",
      "Regularized Logistic Regression(517/999): loss=0.5836450567333635, w0=-3.3353732273424066e-06, w1=0.21420204512796398\n",
      "Regularized Logistic Regression(518/999): loss=0.583625207231731, w0=-3.339163205699714e-06, w1=0.21432334813689216\n",
      "Regularized Logistic Regression(519/999): loss=0.5836054794982276, w0=-3.342947116718799e-06, w1=0.21444416077436027\n",
      "Regularized Logistic Regression(520/999): loss=0.5835858727029521, w0=-3.346724973852884e-06, w1=0.21456448502884282\n",
      "Regularized Logistic Regression(521/999): loss=0.5835663860221311, w0=-3.3504967905137746e-06, w1=0.21468432288043052\n",
      "Regularized Logistic Regression(522/999): loss=0.58354701863807, w0=-3.354262580072025e-06, w1=0.2148036763008659\n",
      "Regularized Logistic Regression(523/999): loss=0.5835277697391061, w0=-3.35802235585711e-06, w1=0.21492254725358506\n",
      "Regularized Logistic Regression(524/999): loss=0.5835086385195601, w0=-3.3617761311575914e-06, w1=0.21504093769374905\n",
      "Regularized Logistic Regression(525/999): loss=0.5834896241796903, w0=-3.3655239192212855e-06, w1=0.21515884956828493\n",
      "Regularized Logistic Regression(526/999): loss=0.5834707259256422, w0=-3.3692657332554285e-06, w1=0.21527628481592118\n",
      "Regularized Logistic Regression(527/999): loss=0.5834519429694076, w0=-3.373001586426841e-06, w1=0.21539324536722373\n",
      "Regularized Logistic Regression(528/999): loss=0.5834332745287735, w0=-3.376731491862093e-06, w1=0.21550973314463287\n",
      "Regularized Logistic Regression(529/999): loss=0.5834147198272795, w0=-3.3804554626476677e-06, w1=0.2156257500625002\n",
      "Regularized Logistic Regression(530/999): loss=0.5833962780941706, w0=-3.384173511830123e-06, w1=0.21574129802712297\n",
      "Regularized Logistic Regression(531/999): loss=0.5833779485643538, w0=-3.3878856524162526e-06, w1=0.21585637893678092\n",
      "Regularized Logistic Regression(532/999): loss=0.5833597304783537, w0=-3.391591897373248e-06, w1=0.21597099468177197\n",
      "Regularized Logistic Regression(533/999): loss=0.5833416230822659, w0=-3.395292259628858e-06, w1=0.2160851471444491\n",
      "Regularized Logistic Regression(534/999): loss=0.5833236256277173, w0=-3.398986752071547e-06, w1=0.2161988381992529\n",
      "Regularized Logistic Regression(535/999): loss=0.5833057373718178, w0=-3.402675387550654e-06, w1=0.2163120697127484\n",
      "Regularized Logistic Regression(536/999): loss=0.5832879575771226, w0=-3.406358178876549e-06, w1=0.216424843543661\n",
      "Regularized Logistic Regression(537/999): loss=0.5832702855115843, w0=-3.4100351388207903e-06, w1=0.21653716154291056\n",
      "Regularized Logistic Regression(538/999): loss=0.5832527204485151, w0=-3.4137062801162806e-06, w1=0.2166490255536452\n",
      "Regularized Logistic Regression(539/999): loss=0.5832352616665414, w0=-3.417371615457421e-06, w1=0.2167604374112773\n",
      "Regularized Logistic Regression(540/999): loss=0.5832179084495648, w0=-3.4210311575002642e-06, w1=0.21687139894351712\n",
      "Regularized Logistic Regression(541/999): loss=0.5832006600867183, w0=-3.42468491886267e-06, w1=0.21698191197040784\n",
      "Regularized Logistic Regression(542/999): loss=0.5831835158723279, w0=-3.4283329121244565e-06, w1=0.21709197830435925\n",
      "Regularized Logistic Regression(543/999): loss=0.5831664751058698, w0=-3.4319751498275515e-06, w1=0.21720159975018077\n",
      "Regularized Logistic Regression(544/999): loss=0.5831495370919321, w0=-3.4356116444761436e-06, w1=0.2173107781051179\n",
      "Regularized Logistic Regression(545/999): loss=0.5831327011401733, w0=-3.4392424085368327e-06, w1=0.21741951515888258\n",
      "Regularized Logistic Regression(546/999): loss=0.583115966565284, w0=-3.442867454438779e-06, w1=0.21752781269368926\n",
      "Regularized Logistic Regression(547/999): loss=0.583099332686946, w0=-3.4464867945738522e-06, w1=0.21763567248428686\n",
      "Regularized Logistic Regression(548/999): loss=0.5830827988297957, w0=-3.450100441296777e-06, w1=0.21774309629799216\n",
      "Regularized Logistic Regression(549/999): loss=0.5830663643233833, w0=-3.4537084069252834e-06, w1=0.2178500858947236\n",
      "Regularized Logistic Regression(550/999): loss=0.583050028502136, w0=-3.4573107037402504e-06, w1=0.21795664302703274\n",
      "Regularized Logistic Regression(551/999): loss=0.5830337907053188, w0=-3.460907343985852e-06, w1=0.21806276944013867\n",
      "Regularized Logistic Regression(552/999): loss=0.5830176502769985, w0=-3.464498339869704e-06, w1=0.21816846687195937\n",
      "Regularized Logistic Regression(553/999): loss=0.5830016065660042, w0=-3.468083703563004e-06, w1=0.21827373705314582\n",
      "Regularized Logistic Regression(554/999): loss=0.5829856589258926, w0=-3.4716634472006793e-06, w1=0.2183785817071101\n",
      "Regularized Logistic Regression(555/999): loss=0.5829698067149096, w0=-3.4752375828815258e-06, w1=0.21848300255006373\n",
      "Regularized Logistic Regression(556/999): loss=0.5829540492959546, w0=-3.478806122668352e-06, w1=0.21858700129104375\n",
      "Regularized Logistic Regression(557/999): loss=0.5829383860365437, w0=-3.482369078588118e-06, w1=0.21869057963194896\n",
      "Regularized Logistic Regression(558/999): loss=0.5829228163087756, w0=-3.485926462632079e-06, w1=0.21879373926756907\n",
      "Regularized Logistic Regression(559/999): loss=0.582907339489294, w0=-3.489478286755922e-06, w1=0.21889648188561617\n",
      "Regularized Logistic Regression(560/999): loss=0.5828919549592533, w0=-3.4930245628799057e-06, w1=0.21899880916675873\n",
      "Regularized Logistic Regression(561/999): loss=0.5828766621042838, w0=-3.4965653028889994e-06, w1=0.21910072278464915\n",
      "Regularized Logistic Regression(562/999): loss=0.5828614603144571, w0=-3.5001005186330205e-06, w1=0.21920222440595752\n",
      "Regularized Logistic Regression(563/999): loss=0.5828463489842516, w0=-3.50363022192677e-06, w1=0.21930331569040243\n",
      "Regularized Logistic Regression(564/999): loss=0.5828313275125176, w0=-3.5071544245501707e-06, w1=0.21940399829078114\n",
      "Regularized Logistic Regression(565/999): loss=0.5828163953024454, w0=-3.5106731382484008e-06, w1=0.21950427385299853\n",
      "Regularized Logistic Regression(566/999): loss=0.5828015517615299, w0=-3.5141863747320297e-06, w1=0.21960414401610112\n",
      "Regularized Logistic Regression(567/999): loss=0.5827867963015382, w0=-3.5176941456771525e-06, w1=0.21970361041230527\n",
      "Regularized Logistic Regression(568/999): loss=0.5827721283384768, w0=-3.5211964627255226e-06, w1=0.2198026746670282\n",
      "Regularized Logistic Regression(569/999): loss=0.5827575472925584, w0=-3.524693337484685e-06, w1=0.21990133839891704\n",
      "Regularized Logistic Regression(570/999): loss=0.5827430525881703, w0=-3.5281847815281082e-06, w1=0.21999960321987932\n",
      "Regularized Logistic Regression(571/999): loss=0.5827286436538409, w0=-3.531670806395316e-06, w1=0.22009747073511327\n",
      "Regularized Logistic Regression(572/999): loss=0.5827143199222086, w0=-3.5351514235920177e-06, w1=0.22019494254313776\n",
      "Regularized Logistic Regression(573/999): loss=0.5827000808299906, w0=-3.5386266445902388e-06, w1=0.22029202023582145\n",
      "Regularized Logistic Regression(574/999): loss=0.5826859258179516, w0=-3.5420964808284506e-06, w1=0.2203887053984105\n",
      "Regularized Logistic Regression(575/999): loss=0.5826718543308703, w0=-3.5455609437116994e-06, w1=0.22048499960956058\n",
      "Regularized Logistic Regression(576/999): loss=0.5826578658175123, w0=-3.5490200446117332e-06, w1=0.22058090444136463\n",
      "Regularized Logistic Regression(577/999): loss=0.5826439597305966, w0=-3.5524737948671312e-06, w1=0.22067642145938243\n",
      "Regularized Logistic Regression(578/999): loss=0.5826301355267661, w0=-3.5559222057834293e-06, w1=0.22077155222266748\n",
      "Regularized Logistic Regression(579/999): loss=0.5826163926665582, w0=-3.559365288633248e-06, w1=0.22086629828379886\n",
      "Regularized Logistic Regression(580/999): loss=0.5826027306143736, w0=-3.5628030546564165e-06, w1=0.22096066118890811\n",
      "Regularized Logistic Regression(581/999): loss=0.5825891488384477, w0=-3.566235515060099e-06, w1=0.221054642477707\n",
      "Regularized Logistic Regression(582/999): loss=0.5825756468108217, w0=-3.5696626810189193e-06, w1=0.2211482436835169\n",
      "Regularized Logistic Regression(583/999): loss=0.5825622240073113, w0=-3.573084563675083e-06, w1=0.2212414663332986\n",
      "Regularized Logistic Regression(584/999): loss=0.58254887990748, w0=-3.5765011741385032e-06, w1=0.22133431194767603\n",
      "Regularized Logistic Regression(585/999): loss=0.5825356139946094, w0=-3.579912523486922e-06, w1=0.2214267820409679\n",
      "Regularized Logistic Regression(586/999): loss=0.5825224257556714, w0=-3.583318622766032e-06, w1=0.22151887812121437\n",
      "Regularized Logistic Regression(587/999): loss=0.5825093146812985, w0=-3.586719482989599e-06, w1=0.221610601690204\n",
      "Regularized Logistic Regression(588/999): loss=0.5824962802657581, w0=-3.5901151151395817e-06, w1=0.22170195424350236\n",
      "Regularized Logistic Regression(589/999): loss=0.5824833220069228, w0=-3.5935055301662543e-06, w1=0.22179293727047958\n",
      "Regularized Logistic Regression(590/999): loss=0.5824704394062432, w0=-3.596890738988323e-06, w1=0.22188355225433695\n",
      "Regularized Logistic Regression(591/999): loss=0.5824576319687227, w0=-3.6002707524930476e-06, w1=0.22197380067213235\n",
      "Regularized Logistic Regression(592/999): loss=0.5824448992028873, w0=-3.6036455815363596e-06, w1=0.22206368399481194\n",
      "Regularized Logistic Regression(593/999): loss=0.5824322406207606, w0=-3.6070152369429795e-06, w1=0.22215320368723276\n",
      "Regularized Logistic Regression(594/999): loss=0.5824196557378365, w0=-3.610379729506535e-06, w1=0.22224236120819113\n",
      "Regularized Logistic Regression(595/999): loss=0.5824071440730533, w0=-3.6137390699896775e-06, w1=0.22233115801044895\n",
      "Regularized Logistic Regression(596/999): loss=0.5823947051487677, w0=-3.617093269124199e-06, w1=0.22241959554076124\n",
      "Regularized Logistic Regression(597/999): loss=0.5823823384907275, w0=-3.6204423376111464e-06, w1=0.22250767523990098\n",
      "Regularized Logistic Regression(598/999): loss=0.5823700436280471, w0=-3.6237862861209393e-06, w1=0.22259539854268676\n",
      "Regularized Logistic Regression(599/999): loss=0.5823578200931813, w0=-3.6271251252934825e-06, w1=0.2226827668780085\n",
      "Regularized Logistic Regression(600/999): loss=0.5823456674218997, w0=-3.6304588657382802e-06, w1=0.22276978166885325\n",
      "Regularized Logistic Regression(601/999): loss=0.5823335851532636, w0=-3.633787518034551e-06, w1=0.22285644433232965\n",
      "Regularized Logistic Regression(602/999): loss=0.582321572829597, w0=-3.637111092731339e-06, w1=0.22294275627969723\n",
      "Regularized Logistic Regression(603/999): loss=0.5823096299964661, w0=-3.6404296003476292e-06, w1=0.2230287189163895\n",
      "Regularized Logistic Regression(604/999): loss=0.5822977562026518, w0=-3.643743051372456e-06, w1=0.2231143336420398\n",
      "Regularized Logistic Regression(605/999): loss=0.5822859510001268, w0=-3.647051456265017e-06, w1=0.22319960185050772\n",
      "Regularized Logistic Regression(606/999): loss=0.5822742139440309, w0=-3.650354825454783e-06, w1=0.2232845249299025\n",
      "Regularized Logistic Regression(607/999): loss=0.5822625445926476, w0=-3.6536531693416094e-06, w1=0.2233691042626105\n",
      "Regularized Logistic Regression(608/999): loss=0.5822509425073794, w0=-3.656946498295844e-06, w1=0.2234533412253196\n",
      "Regularized Logistic Regression(609/999): loss=0.582239407252724, w0=-3.6602348226584385e-06, w1=0.22353723718904206\n",
      "Regularized Logistic Regression(610/999): loss=0.5822279383962536, w0=-3.6635181527410564e-06, w1=0.2236207935191424\n",
      "Regularized Logistic Regression(611/999): loss=0.5822165355085882, w0=-3.666796498826181e-06, w1=0.2237040115753611\n",
      "Regularized Logistic Regression(612/999): loss=0.5822051981633738, w0=-3.6700698711672237e-06, w1=0.22378689271183863\n",
      "Regularized Logistic Regression(613/999): loss=0.5821939259372604, w0=-3.6733382799886315e-06, w1=0.2238694382771387\n",
      "Regularized Logistic Regression(614/999): loss=0.5821827184098795, w0=-3.676601735485993e-06, w1=0.22395164961427638\n",
      "Regularized Logistic Regression(615/999): loss=0.5821715751638192, w0=-3.6798602478261456e-06, w1=0.22403352806073928\n",
      "Regularized Logistic Regression(616/999): loss=0.5821604957846052, w0=-3.6831138271472806e-06, w1=0.22411507494851218\n",
      "Regularized Logistic Regression(617/999): loss=0.5821494798606764, w0=-3.6863624835590493e-06, w1=0.22419629160410187\n",
      "Regularized Logistic Regression(618/999): loss=0.5821385269833638, w0=-3.689606227142667e-06, w1=0.22427717934856028\n",
      "Regularized Logistic Regression(619/999): loss=0.5821276367468681, w0=-3.692845067951018e-06, w1=0.2243577394975092\n",
      "Regularized Logistic Regression(620/999): loss=0.5821168087482395, w0=-3.696079016008758e-06, w1=0.22443797336116367\n",
      "Regularized Logistic Regression(621/999): loss=0.5821060425873545, w0=-3.6993080813124204e-06, w1=0.2245178822443547\n",
      "Regularized Logistic Regression(622/999): loss=0.5820953378668962, w0=-3.702532273830516e-06, w1=0.22459746744655326\n",
      "Regularized Logistic Regression(623/999): loss=0.5820846941923323, w0=-3.7057516035036374e-06, w1=0.22467673026189341\n",
      "Regularized Logistic Regression(624/999): loss=0.5820741111718944, w0=-3.70896608024456e-06, w1=0.22475567197919816\n",
      "Regularized Logistic Regression(625/999): loss=0.5820635884165585, w0=-3.7121757139383445e-06, w1=0.22483429388199822\n",
      "Regularized Logistic Regression(626/999): loss=0.5820531255400211, w0=-3.715380514442436e-06, w1=0.2249125972485584\n",
      "Regularized Logistic Regression(627/999): loss=0.5820427221586824, w0=-3.718580491586767e-06, w1=0.22499058335189895\n",
      "Regularized Logistic Regression(628/999): loss=0.5820323778916253, w0=-3.721775655173856e-06, w1=0.22506825345981796\n",
      "Regularized Logistic Regression(629/999): loss=0.582022092360592, w0=-3.7249660149789067e-06, w1=0.2251456088349168\n",
      "Regularized Logistic Regression(630/999): loss=0.5820118651899702, w0=-3.7281515807499086e-06, w1=0.2252226507346195\n",
      "Regularized Logistic Regression(631/999): loss=0.5820016960067669, w0=-3.7313323622077344e-06, w1=0.225299380411197\n",
      "Regularized Logistic Regression(632/999): loss=0.5819915844405932, w0=-3.734508369046239e-06, w1=0.22537579911178943\n",
      "Regularized Logistic Regression(633/999): loss=0.5819815301236428, w0=-3.7376796109323564e-06, w1=0.2254519080784273\n",
      "Regularized Logistic Regression(634/999): loss=0.581971532690674, w0=-3.7408460975061984e-06, w1=0.22552770854805604\n",
      "Regularized Logistic Regression(635/999): loss=0.5819615917789892, w0=-3.744007838381151e-06, w1=0.2256032017525558\n",
      "Regularized Logistic Regression(636/999): loss=0.581951707028417, w0=-3.7471648431439706e-06, w1=0.22567838891876363\n",
      "Regularized Logistic Regression(637/999): loss=0.5819418780812923, w0=-3.7503171213548793e-06, w1=0.22575327126849645\n",
      "Regularized Logistic Regression(638/999): loss=0.5819321045824388, w0=-3.753464682547662e-06, w1=0.22582785001857344\n",
      "Regularized Logistic Regression(639/999): loss=0.5819223861791492, w0=-3.756607536229761e-06, w1=0.2259021263808349\n",
      "Regularized Logistic Regression(640/999): loss=0.5819127225211689, w0=-3.7597456918823708e-06, w1=0.22597610156216752\n",
      "Regularized Logistic Regression(641/999): loss=0.5819031132606745, w0=-3.7628791589605315e-06, w1=0.22604977676452198\n",
      "Regularized Logistic Regression(642/999): loss=0.581893558052259, w0=-3.766007946893224e-06, w1=0.2261231531849377\n",
      "Regularized Logistic Regression(643/999): loss=0.5818840565529109, w0=-3.7691320650834626e-06, w1=0.22619623201556288\n",
      "Regularized Logistic Regression(644/999): loss=0.581874608421999, w0=-3.772251522908388e-06, w1=0.22626901444367364\n",
      "Regularized Logistic Regression(645/999): loss=0.5818652133212534, w0=-3.775366329719361e-06, w1=0.22634150165169872\n",
      "Regularized Logistic Regression(646/999): loss=0.581855870914748, w0=-3.7784764948420532e-06, w1=0.2264136948172376\n",
      "Regularized Logistic Regression(647/999): loss=0.5818465808688827, w0=-3.7815820275765394e-06, w1=0.2264855951130839\n",
      "Regularized Logistic Regression(648/999): loss=0.5818373428523672, w0=-3.78468293719739e-06, w1=0.22655720370724317\n",
      "Regularized Logistic Regression(649/999): loss=0.5818281565362033, w0=-3.7877792329537595e-06, w1=0.22662852176295645\n",
      "Regularized Logistic Regression(650/999): loss=0.5818190215936677, w0=-3.79087092406948e-06, w1=0.22669955043871878\n",
      "Regularized Logistic Regression(651/999): loss=0.5818099377002957, w0=-3.793958019743149e-06, w1=0.2267702908883016\n",
      "Regularized Logistic Regression(652/999): loss=0.581800904533863, w0=-3.7970405291482204e-06, w1=0.2268407442607705\n",
      "Regularized Logistic Regression(653/999): loss=0.5817919217743721, w0=-3.8001184614330932e-06, w1=0.22691091170050817\n",
      "Regularized Logistic Regression(654/999): loss=0.5817829891040325, w0=-3.8031918257212013e-06, w1=0.2269807943472332\n",
      "Regularized Logistic Regression(655/999): loss=0.581774106207246, w0=-3.8062606311111006e-06, w1=0.2270503933360211\n",
      "Regularized Logistic Regression(656/999): loss=0.581765272770591, w0=-3.8093248866765586e-06, w1=0.227119709797322\n",
      "Regularized Logistic Regression(657/999): loss=0.5817564884828056, w0=-3.812384601466642e-06, w1=0.2271887448569845\n",
      "Regularized Logistic Regression(658/999): loss=0.5817477530347711, w0=-3.815439784505803e-06, w1=0.22725749963627245\n",
      "Regularized Logistic Regression(659/999): loss=0.5817390661194973, w0=-3.818490444793967e-06, w1=0.22732597525188608\n",
      "Regularized Logistic Regression(660/999): loss=0.5817304274321066, w0=-3.821536591306619e-06, w1=0.22739417281598062\n",
      "Regularized Logistic Regression(661/999): loss=0.5817218366698178, w0=-3.8245782329948914e-06, w1=0.22746209343618615\n",
      "Regularized Logistic Regression(662/999): loss=0.5817132935319312, w0=-3.827615378785646e-06, w1=0.22752973821562805\n",
      "Regularized Logistic Regression(663/999): loss=0.581704797719813, w0=-3.830648037581564e-06, w1=0.22759710825294593\n",
      "Regularized Logistic Regression(664/999): loss=0.5816963489368804, w0=-3.8336762182612285e-06, w1=0.2276642046423121\n",
      "Regularized Logistic Regression(665/999): loss=0.5816879468885859, w0=-3.836699929679209e-06, w1=0.22773102847345056\n",
      "Regularized Logistic Regression(666/999): loss=0.5816795912824027, w0=-3.839719180666146e-06, w1=0.22779758083165896\n",
      "Regularized Logistic Regression(667/999): loss=0.5816712818278104, w0=-3.842733980028837e-06, w1=0.227863862797824\n",
      "Regularized Logistic Regression(668/999): loss=0.5816630182362788, w0=-3.8457443365503175e-06, w1=0.22792987544844243\n",
      "Regularized Logistic Regression(669/999): loss=0.5816548002212546, w0=-3.848750258989945e-06, w1=0.22799561985563907\n",
      "Regularized Logistic Regression(670/999): loss=0.5816466274981466, w0=-3.851751756083483e-06, w1=0.22806109708718694\n",
      "Regularized Logistic Regression(671/999): loss=0.581638499784311, w0=-3.854748836543181e-06, w1=0.22812630820652413\n",
      "Regularized Logistic Regression(672/999): loss=0.5816304167990365, w0=-3.857741509057861e-06, w1=0.22819125427277276\n",
      "Regularized Logistic Regression(673/999): loss=0.5816223782635325, w0=-3.860729782292993e-06, w1=0.22825593634075902\n",
      "Regularized Logistic Regression(674/999): loss=0.5816143839009116, w0=-3.863713664890782e-06, w1=0.22832035546103074\n",
      "Regularized Logistic Regression(675/999): loss=0.5816064334361785, w0=-3.866693165470247e-06, w1=0.22838451267987364\n",
      "Regularized Logistic Regression(676/999): loss=0.5815985265962142, w0=-3.869668292627302e-06, w1=0.22844840903933267\n",
      "Regularized Logistic Regression(677/999): loss=0.5815906631097643, w0=-3.872639054934834e-06, w1=0.22851204557722846\n",
      "Regularized Logistic Regression(678/999): loss=0.5815828427074232, w0=-3.875605460942789e-06, w1=0.22857542332717684\n",
      "Regularized Logistic Regression(679/999): loss=0.5815750651216217, w0=-3.878567519178244e-06, w1=0.228638543318604\n",
      "Regularized Logistic Regression(680/999): loss=0.5815673300866142, w0=-3.881525238145491e-06, w1=0.2287014065767679\n",
      "Regularized Logistic Regression(681/999): loss=0.581559637338463, w0=-3.884478626326117e-06, w1=0.2287640141227735\n",
      "Regularized Logistic Regression(682/999): loss=0.5815519866150285, w0=-3.887427692179079e-06, w1=0.22882636697359168\n",
      "Regularized Logistic Regression(683/999): loss=0.5815443776559532, w0=-3.890372444140785e-06, w1=0.22888846614207703\n",
      "Regularized Logistic Regression(684/999): loss=0.5815368102026498, w0=-3.893312890625171e-06, w1=0.22895031263698465\n",
      "Regularized Logistic Regression(685/999): loss=0.5815292839982886, w0=-3.896249040023777e-06, w1=0.22901190746298888\n",
      "Regularized Logistic Regression(686/999): loss=0.5815217987877841, w0=-3.89918090070583e-06, w1=0.22907325162069855\n",
      "Regularized Logistic Regression(687/999): loss=0.5815143543177828, w0=-3.902108481018311e-06, w1=0.22913434610667685\n",
      "Regularized Logistic Regression(688/999): loss=0.5815069503366492, w0=-3.905031789286044e-06, w1=0.22919519191345694\n",
      "Regularized Logistic Regression(689/999): loss=0.5814995865944561, w0=-3.907950833811761e-06, w1=0.22925579002955887\n",
      "Regularized Logistic Regression(690/999): loss=0.5814922628429692, w0=-3.910865622876186e-06, w1=0.2293161414395089\n",
      "Regularized Logistic Regression(691/999): loss=0.5814849788356364, w0=-3.913776164738107e-06, w1=0.22937624712385465\n",
      "Regularized Logistic Regression(692/999): loss=0.5814777343275753, w0=-3.91668246763445e-06, w1=0.2294361080591816\n",
      "Regularized Logistic Regression(693/999): loss=0.5814705290755608, w0=-3.9195845397803575e-06, w1=0.22949572521813183\n",
      "Regularized Logistic Regression(694/999): loss=0.5814633628380133, w0=-3.922482389369261e-06, w1=0.22955509956941933\n",
      "Regularized Logistic Regression(695/999): loss=0.5814562353749868, w0=-3.925376024572957e-06, w1=0.2296142320778467\n",
      "Regularized Logistic Regression(696/999): loss=0.5814491464481564, w0=-3.9282654535416785e-06, w1=0.22967312370432375\n",
      "Regularized Logistic Regression(697/999): loss=0.5814420958208079, w0=-3.931150684404172e-06, w1=0.22973177540588116\n",
      "Regularized Logistic Regression(698/999): loss=0.5814350832578243, w0=-3.934031725267767e-06, w1=0.22979018813568824\n",
      "Regularized Logistic Regression(699/999): loss=0.5814281085256763, w0=-3.936908584218454e-06, w1=0.2298483628430709\n",
      "Regularized Logistic Regression(700/999): loss=0.5814211713924086, w0=-3.939781269320952e-06, w1=0.2299063004735251\n",
      "Regularized Logistic Regression(701/999): loss=0.5814142716276304, w0=-3.942649788618786e-06, w1=0.22996400196873557\n",
      "Regularized Logistic Regression(702/999): loss=0.5814074090025028, w0=-3.945514150134355e-06, w1=0.23002146826659092\n",
      "Regularized Logistic Regression(703/999): loss=0.5814005832897279, w0=-3.9483743618690065e-06, w1=0.2300787003011978\n",
      "Regularized Logistic Regression(704/999): loss=0.5813937942635389, w0=-3.951230431803107e-06, w1=0.23013569900290204\n",
      "Regularized Logistic Regression(705/999): loss=0.5813870416996864, w0=-3.9540823678961135e-06, w1=0.23019246529829995\n",
      "Regularized Logistic Regression(706/999): loss=0.5813803253754297, w0=-3.956930178086646e-06, w1=0.23024900011025562\n",
      "Regularized Logistic Regression(707/999): loss=0.5813736450695255, w0=-3.959773870292554e-06, w1=0.23030530435791746\n",
      "Regularized Logistic Regression(708/999): loss=0.5813670005622159, w0=-3.962613452410992e-06, w1=0.23036137895673278\n",
      "Regularized Logistic Regression(709/999): loss=0.5813603916352196, w0=-3.965448932318487e-06, w1=0.23041722481846405\n",
      "Regularized Logistic Regression(710/999): loss=0.5813538180717194, w0=-3.968280317871007e-06, w1=0.23047284285120578\n",
      "Regularized Logistic Regression(711/999): loss=0.5813472796563524, w0=-3.971107616904033e-06, w1=0.2305282339593968\n",
      "Regularized Logistic Regression(712/999): loss=0.5813407761752001, w0=-3.9739308372326276e-06, w1=0.23058339904383926\n",
      "Regularized Logistic Regression(713/999): loss=0.5813343074157769, w0=-3.976749986651502e-06, w1=0.23063833900171143\n",
      "Regularized Logistic Regression(714/999): loss=0.5813278731670208, w0=-3.979565072935089e-06, w1=0.230693054726584\n",
      "Regularized Logistic Regression(715/999): loss=0.581321473219282, w0=-3.982376103837606e-06, w1=0.2307475471084365\n",
      "Regularized Logistic Regression(716/999): loss=0.5813151073643142, w0=-3.985183087093126e-06, w1=0.23080181703367023\n",
      "Regularized Logistic Regression(717/999): loss=0.5813087753952635, w0=-3.987986030415645e-06, w1=0.23085586538512448\n",
      "Regularized Logistic Regression(718/999): loss=0.5813024771066585, w0=-3.990784941499151e-06, w1=0.23090969304209255\n",
      "Regularized Logistic Regression(719/999): loss=0.5812962122944003, w0=-3.993579828017688e-06, w1=0.23096330088033495\n",
      "Regularized Logistic Regression(720/999): loss=0.5812899807557529, w0=-3.996370697625425e-06, w1=0.23101668977209452\n",
      "Regularized Logistic Regression(721/999): loss=0.581283782289334, w0=-3.999157557956722e-06, w1=0.231069860586113\n",
      "Regularized Logistic Regression(722/999): loss=0.5812776166951037, w0=-4.001940416626198e-06, w1=0.2311228141876438\n",
      "Regularized Logistic Regression(723/999): loss=0.5812714837743567, w0=-4.004719281228794e-06, w1=0.23117555143846671\n",
      "Regularized Logistic Regression(724/999): loss=0.5812653833297111, w0=-4.007494159339841e-06, w1=0.23122807319690603\n",
      "Regularized Logistic Regression(725/999): loss=0.5812593151651002, w0=-4.010265058515127e-06, w1=0.23128038031783923\n",
      "Regularized Logistic Regression(726/999): loss=0.5812532790857624, w0=-4.013031986290958e-06, w1=0.23133247365271692\n",
      "Regularized Logistic Regression(727/999): loss=0.5812472748982315, w0=-4.0157949501842265e-06, w1=0.23138435404957433\n",
      "Regularized Logistic Regression(728/999): loss=0.5812413024103286, w0=-4.018553957692475e-06, w1=0.2314360223530453\n",
      "Regularized Logistic Regression(729/999): loss=0.5812353614311524, w0=-4.021309016293962e-06, w1=0.23148747940437858\n",
      "Regularized Logistic Regression(730/999): loss=0.5812294517710692, w0=-4.024060133447723e-06, w1=0.23153872604145126\n",
      "Regularized Logistic Regression(731/999): loss=0.5812235732417053, w0=-4.026807316593637e-06, w1=0.23158976309878274\n",
      "Regularized Logistic Regression(732/999): loss=0.5812177256559362, w0=-4.029550573152489e-06, w1=0.23164059140754859\n",
      "Regularized Logistic Regression(733/999): loss=0.5812119088278798, w0=-4.032289910526036e-06, w1=0.2316912117955946\n",
      "Regularized Logistic Regression(734/999): loss=0.5812061225728857, w0=-4.035025336097066e-06, w1=0.23174162508745183\n",
      "Regularized Logistic Regression(735/999): loss=0.5812003667075278, w0=-4.037756857229464e-06, w1=0.23179183210434903\n",
      "Regularized Logistic Regression(736/999): loss=0.5811946410495947, w0=-4.040484481268274e-06, w1=0.2318418336642285\n",
      "Regularized Logistic Regression(737/999): loss=0.581188945418081, w0=-4.04320821553976e-06, w1=0.2318916305817565\n",
      "Regularized Logistic Regression(738/999): loss=0.5811832796331794, w0=-4.045928067351471e-06, w1=0.23194122366834052\n",
      "Regularized Logistic Regression(739/999): loss=0.5811776435162712, w0=-4.0486440439923e-06, w1=0.2319906137321412\n",
      "Regularized Logistic Regression(740/999): loss=0.5811720368899193, w0=-4.051356152732547e-06, w1=0.2320398015780856\n",
      "Regularized Logistic Regression(741/999): loss=0.5811664595778582, w0=-4.05406440082398e-06, w1=0.23208878800788255\n",
      "Regularized Logistic Regression(742/999): loss=0.5811609114049867, w0=-4.056768795499897e-06, w1=0.23213757382003372\n",
      "Regularized Logistic Regression(743/999): loss=0.5811553921973589, w0=-4.059469343975184e-06, w1=0.23218615980984955\n",
      "Regularized Logistic Regression(744/999): loss=0.5811499017821773, w0=-4.062166053446381e-06, w1=0.2322345467694609\n",
      "Regularized Logistic Regression(745/999): loss=0.5811444399877826, w0=-4.064858931091737e-06, w1=0.23228273548783096\n",
      "Regularized Logistic Regression(746/999): loss=0.5811390066436471, w0=-4.067547984071274e-06, w1=0.23233072675077293\n",
      "Regularized Logistic Regression(747/999): loss=0.5811336015803675, w0=-4.0702332195268426e-06, w1=0.2323785213409592\n",
      "Regularized Logistic Regression(748/999): loss=0.5811282246296537, w0=-4.0729146445821875e-06, w1=0.2324261200379359\n",
      "Regularized Logistic Regression(749/999): loss=0.5811228756243246, w0=-4.075592266343003e-06, w1=0.2324735236181359\n",
      "Regularized Logistic Regression(750/999): loss=0.5811175543982979, w0=-4.0782660918969904e-06, w1=0.23252073285489253\n",
      "Regularized Logistic Regression(751/999): loss=0.581112260786583, w0=-4.080936128313922e-06, w1=0.23256774851845133\n",
      "Regularized Logistic Regression(752/999): loss=0.5811069946252734, w0=-4.083602382645697e-06, w1=0.23261457137598318\n",
      "Regularized Logistic Regression(753/999): loss=0.5811017557515393, w0=-4.0862648619264004e-06, w1=0.23266120219159714\n",
      "Regularized Logistic Regression(754/999): loss=0.5810965440036193, w0=-4.08892357317236e-06, w1=0.23270764172635391\n",
      "Regularized Logistic Regression(755/999): loss=0.581091359220813, w0=-4.091578523382206e-06, w1=0.2327538907382784\n",
      "Regularized Logistic Regression(756/999): loss=0.5810862012434739, w0=-4.094229719536928e-06, w1=0.23279994998237047\n",
      "Regularized Logistic Regression(757/999): loss=0.5810810699130011, w0=-4.096877168599933e-06, w1=0.23284582021062106\n",
      "Regularized Logistic Regression(758/999): loss=0.581075965071834, w0=-4.099520877517102e-06, w1=0.23289150217202148\n",
      "Regularized Logistic Regression(759/999): loss=0.5810708865634417, w0=-4.102160853216849e-06, w1=0.23293699661257813\n",
      "Regularized Logistic Regression(760/999): loss=0.5810658342323186, w0=-4.104797102610176e-06, w1=0.23298230427532327\n",
      "Regularized Logistic Regression(761/999): loss=0.5810608079239763, w0=-4.107429632590728e-06, w1=0.23302742590032713\n",
      "Regularized Logistic Regression(762/999): loss=0.5810558074849351, w0=-4.110058450034853e-06, w1=0.23307236222471364\n",
      "Regularized Logistic Regression(763/999): loss=0.5810508327627192, w0=-4.112683561801657e-06, w1=0.2331171139826684\n",
      "Regularized Logistic Regression(764/999): loss=0.5810458836058473, w0=-4.115304974733058e-06, w1=0.23316168190545372\n",
      "Regularized Logistic Regression(765/999): loss=0.5810409598638284, w0=-4.117922695653843e-06, w1=0.23320606672141903\n",
      "Regularized Logistic Regression(766/999): loss=0.5810360613871515, w0=-4.1205367313717265e-06, w1=0.23325026915601366\n",
      "Regularized Logistic Regression(767/999): loss=0.5810311880272814, w0=-4.1231470886774e-06, w1=0.23329428993179954\n",
      "Regularized Logistic Regression(768/999): loss=0.5810263396366501, w0=-4.125753774344591e-06, w1=0.23333812976846255\n",
      "Regularized Logistic Regression(769/999): loss=0.5810215160686516, w0=-4.128356795130118e-06, w1=0.23338178938282492\n",
      "Regularized Logistic Regression(770/999): loss=0.5810167171776334, w0=-4.130956157773941e-06, w1=0.2334252694888562\n",
      "Regularized Logistic Regression(771/999): loss=0.5810119428188913, w0=-4.133551868999221e-06, w1=0.2334685707976844\n",
      "Regularized Logistic Regression(772/999): loss=0.5810071928486624, w0=-4.136143935512373e-06, w1=0.23351169401761043\n",
      "Regularized Logistic Regression(773/999): loss=0.5810024671241166, w0=-4.138732364003116e-06, w1=0.2335546398541183\n",
      "Regularized Logistic Regression(774/999): loss=0.5809977655033542, w0=-4.141317161144533e-06, w1=0.23359740900988724\n",
      "Regularized Logistic Regression(775/999): loss=0.5809930878453944, w0=-4.143898333593119e-06, w1=0.2336400021848017\n",
      "Regularized Logistic Regression(776/999): loss=0.5809884340101733, w0=-4.146475887988837e-06, w1=0.2336824200759643\n",
      "Regularized Logistic Regression(777/999): loss=0.5809838038585342, w0=-4.14904983095517e-06, w1=0.23372466337770811\n",
      "Regularized Logistic Regression(778/999): loss=0.5809791972522232, w0=-4.151620169099178e-06, w1=0.23376673278160626\n",
      "Regularized Logistic Regression(779/999): loss=0.580974614053882, w0=-4.154186909011544e-06, w1=0.2338086289764859\n",
      "Regularized Logistic Regression(780/999): loss=0.5809700541270417, w0=-4.15675005726663e-06, w1=0.23385035264843546\n",
      "Regularized Logistic Regression(781/999): loss=0.5809655173361175, w0=-4.15930962042253e-06, w1=0.23389190448082017\n",
      "Regularized Logistic Regression(782/999): loss=0.580961003546401, w0=-4.161865605021122e-06, w1=0.23393328515429157\n",
      "Regularized Logistic Regression(783/999): loss=0.5809565126240552, w0=-4.164418017588118e-06, w1=0.2339744953467969\n",
      "Regularized Logistic Regression(784/999): loss=0.5809520444361084, w0=-4.166966864633117e-06, w1=0.23401553573359546\n",
      "Regularized Logistic Regression(785/999): loss=0.5809475988504468, w0=-4.169512152649655e-06, w1=0.23405640698726346\n",
      "Regularized Logistic Regression(786/999): loss=0.580943175735811, w0=-4.17205388811526e-06, w1=0.23409710977771003\n",
      "Regularized Logistic Regression(787/999): loss=0.5809387749617875, w0=-4.174592077491498e-06, w1=0.23413764477218435\n",
      "Regularized Logistic Regression(788/999): loss=0.5809343963988047, w0=-4.177126727224029e-06, w1=0.23417801263528923\n",
      "Regularized Logistic Regression(789/999): loss=0.5809300399181251, w0=-4.179657843742652e-06, w1=0.23421821402899196\n",
      "Regularized Logistic Regression(790/999): loss=0.5809257053918419, w0=-4.1821854334613614e-06, w1=0.2342582496126336\n",
      "Regularized Logistic Regression(791/999): loss=0.5809213926928716, w0=-4.184709502778393e-06, w1=0.23429812004294073\n",
      "Regularized Logistic Regression(792/999): loss=0.5809171016949483, w0=-4.187230058076275e-06, w1=0.23433782597403652\n",
      "Regularized Logistic Regression(793/999): loss=0.5809128322726184, w0=-4.18974710572188e-06, w1=0.23437736805745232\n",
      "Regularized Logistic Regression(794/999): loss=0.5809085843012352, w0=-4.192260652066472e-06, w1=0.23441674694213416\n",
      "Regularized Logistic Regression(795/999): loss=0.580904357656953, w0=-4.194770703445757e-06, w1=0.23445596327445856\n",
      "Regularized Logistic Regression(796/999): loss=0.5809001522167213, w0=-4.197277266179933e-06, w1=0.23449501769824035\n",
      "Regularized Logistic Regression(797/999): loss=0.5808959678582793, w0=-4.199780346573736e-06, w1=0.23453391085474384\n",
      "Regularized Logistic Regression(798/999): loss=0.5808918044601512, w0=-4.202279950916494e-06, w1=0.23457264338269299\n",
      "Regularized Logistic Regression(799/999): loss=0.5808876619016402, w0=-4.204776085482172e-06, w1=0.23461121591828196\n",
      "Regularized Logistic Regression(800/999): loss=0.5808835400628227, w0=-4.207268756529422e-06, w1=0.23464962909518663\n",
      "Regularized Logistic Regression(801/999): loss=0.5808794388245431, w0=-4.20975797030163e-06, w1=0.2346878835445733\n",
      "Regularized Logistic Regression(802/999): loss=0.5808753580684093, w0=-4.2122437330269655e-06, w1=0.2347259798951085\n",
      "Regularized Logistic Regression(803/999): loss=0.5808712976767867, w0=-4.21472605091843e-06, w1=0.2347639187729722\n",
      "Regularized Logistic Regression(804/999): loss=0.5808672575327932, w0=-4.2172049301739026e-06, w1=0.23480170080186555\n",
      "Regularized Logistic Regression(805/999): loss=0.5808632375202936, w0=-4.219680376976189e-06, w1=0.23483932660302198\n",
      "Regularized Logistic Regression(806/999): loss=0.5808592375238946, w0=-4.222152397493069e-06, w1=0.23487679679521567\n",
      "Regularized Logistic Regression(807/999): loss=0.5808552574289402, w0=-4.224620997877343e-06, w1=0.23491411199477494\n",
      "Regularized Logistic Regression(808/999): loss=0.5808512971215058, w0=-4.227086184266882e-06, w1=0.23495127281558884\n",
      "Regularized Logistic Regression(809/999): loss=0.5808473564883944, w0=-4.2295479627846685e-06, w1=0.23498827986912044\n",
      "Regularized Logistic Regression(810/999): loss=0.5808434354171299, w0=-4.232006339538849e-06, w1=0.2350251337644132\n",
      "Regularized Logistic Regression(811/999): loss=0.5808395337959525, w0=-4.234461320622778e-06, w1=0.23506183510810338\n",
      "Regularized Logistic Regression(812/999): loss=0.5808356515138159, w0=-4.236912912115064e-06, w1=0.2350983845044293\n",
      "Regularized Logistic Regression(813/999): loss=0.5808317884603789, w0=-4.239361120079616e-06, w1=0.23513478255524123\n",
      "Regularized Logistic Regression(814/999): loss=0.5808279445260036, w0=-4.241805950565692e-06, w1=0.23517102986001084\n",
      "Regularized Logistic Regression(815/999): loss=0.5808241196017487, w0=-4.24424740960794e-06, w1=0.2352071270158399\n",
      "Regularized Logistic Regression(816/999): loss=0.5808203135793655, w0=-4.246685503226448e-06, w1=0.23524307461747376\n",
      "Regularized Logistic Regression(817/999): loss=0.5808165263512928, w0=-4.249120237426786e-06, w1=0.23527887325730612\n",
      "Regularized Logistic Regression(818/999): loss=0.5808127578106526, w0=-4.2515516182000545e-06, w1=0.23531452352539145\n",
      "Regularized Logistic Regression(819/999): loss=0.5808090078512447, w0=-4.253979651522927e-06, w1=0.2353500260094544\n",
      "Regularized Logistic Regression(820/999): loss=0.5808052763675428, w0=-4.2564043433576976e-06, w1=0.23538538129489908\n",
      "Regularized Logistic Regression(821/999): loss=0.5808015632546893, w0=-4.258825699652322e-06, w1=0.23542058996481796\n",
      "Regularized Logistic Regression(822/999): loss=0.580797868408491, w0=-4.261243726340466e-06, w1=0.2354556526000017\n",
      "Regularized Logistic Regression(823/999): loss=0.5807941917254148, w0=-4.263658429341548e-06, w1=0.23549056977894858\n",
      "Regularized Logistic Regression(824/999): loss=0.5807905331025824, w0=-4.266069814560782e-06, w1=0.23552534207787382\n",
      "Regularized Logistic Regression(825/999): loss=0.5807868924377664, w0=-4.268477887889225e-06, w1=0.23555997007071927\n",
      "Regularized Logistic Regression(826/999): loss=0.5807832696293854, w0=-4.2708826552038185e-06, w1=0.2355944543291621\n",
      "Regularized Logistic Regression(827/999): loss=0.5807796645765013, w0=-4.273284122367433e-06, w1=0.23562879542262444\n",
      "Regularized Logistic Regression(828/999): loss=0.5807760771788111, w0=-4.27568229522891e-06, w1=0.23566299391828172\n",
      "Regularized Logistic Regression(829/999): loss=0.5807725073366468, w0=-4.27807717962311e-06, w1=0.2356970503810734\n",
      "Regularized Logistic Regression(830/999): loss=0.5807689549509686, w0=-4.280468781370949e-06, w1=0.2357309653737112\n",
      "Regularized Logistic Regression(831/999): loss=0.5807654199233608, w0=-4.282857106279447e-06, w1=0.23576473945668633\n",
      "Regularized Logistic Regression(832/999): loss=0.5807619021560287, w0=-4.285242160141771e-06, w1=0.2357983731882835\n",
      "Regularized Logistic Regression(833/999): loss=0.5807584015517931, w0=-4.287623948737273e-06, w1=0.23583186712458412\n",
      "Regularized Logistic Regression(834/999): loss=0.5807549180140861, w0=-4.2900024778315365e-06, w1=0.23586522181947853\n",
      "Regularized Logistic Regression(835/999): loss=0.5807514514469484, w0=-4.292377753176418e-06, w1=0.23589843782467465\n",
      "Regularized Logistic Regression(836/999): loss=0.5807480017550237, w0=-4.29474978051009e-06, w1=0.23593151568970702\n",
      "Regularized Logistic Regression(837/999): loss=0.5807445688435547, w0=-4.297118565557083e-06, w1=0.23596445596194326\n",
      "Regularized Logistic Regression(838/999): loss=0.5807411526183797, w0=-4.299484114028324e-06, w1=0.23599725918659745\n",
      "Regularized Logistic Regression(839/999): loss=0.5807377529859282, w0=-4.301846431621184e-06, w1=0.23602992590673333\n",
      "Regularized Logistic Regression(840/999): loss=0.5807343698532167, w0=-4.304205524019516e-06, w1=0.2360624566632782\n",
      "Regularized Logistic Regression(841/999): loss=0.5807310031278446, w0=-4.306561396893698e-06, w1=0.23609485199502692\n",
      "Regularized Logistic Regression(842/999): loss=0.5807276527179915, w0=-4.308914055900672e-06, w1=0.23612711243865456\n",
      "Regularized Logistic Regression(843/999): loss=0.5807243185324112, w0=-4.311263506683989e-06, w1=0.23615923852872314\n",
      "Regularized Logistic Regression(844/999): loss=0.5807210004804295, w0=-4.3136097548738476e-06, w1=0.23619123079769058\n",
      "Regularized Logistic Regression(845/999): loss=0.5807176984719387, w0=-4.315952806087133e-06, w1=0.23622308977591788\n",
      "Regularized Logistic Regression(846/999): loss=0.5807144124173959, w0=-4.318292665927464e-06, w1=0.23625481599167986\n",
      "Regularized Logistic Regression(847/999): loss=0.5807111422278176, w0=-4.320629339985225e-06, w1=0.2362864099711719\n",
      "Regularized Logistic Regression(848/999): loss=0.5807078878147758, w0=-4.322962833837615e-06, w1=0.23631787223852022\n",
      "Regularized Logistic Regression(849/999): loss=0.580704649090395, w0=-4.325293153048681e-06, w1=0.23634920331578735\n",
      "Regularized Logistic Regression(850/999): loss=0.5807014259673484, w0=-4.327620303169362e-06, w1=0.23638040372298422\n",
      "Regularized Logistic Regression(851/999): loss=0.5806982183588538, w0=-4.329944289737529e-06, w1=0.23641147397807422\n",
      "Regularized Logistic Regression(852/999): loss=0.58069502617867, w0=-4.332265118278023e-06, w1=0.2364424145969854\n",
      "Regularized Logistic Regression(853/999): loss=0.580691849341093, w0=-4.334582794302695e-06, w1=0.23647322609361618\n",
      "Regularized Logistic Regression(854/999): loss=0.5806886877609527, w0=-4.336897323310446e-06, w1=0.23650390897984414\n",
      "Regularized Logistic Regression(855/999): loss=0.5806855413536092, w0=-4.339208710787267e-06, w1=0.23653446376553564\n",
      "Regularized Logistic Regression(856/999): loss=0.5806824100349492, w0=-4.341516962206277e-06, w1=0.2365648909585523\n",
      "Regularized Logistic Regression(857/999): loss=0.5806792937213824, w0=-4.3438220830277645e-06, w1=0.23659519106475926\n",
      "Regularized Logistic Regression(858/999): loss=0.5806761923298377, w0=-4.3461240786992215e-06, w1=0.23662536458803407\n",
      "Regularized Logistic Regression(859/999): loss=0.5806731057777597, w0=-4.348422954655389e-06, w1=0.23665541203027304\n",
      "Regularized Logistic Regression(860/999): loss=0.5806700339831066, w0=-4.350718716318289e-06, w1=0.23668533389140187\n",
      "Regularized Logistic Regression(861/999): loss=0.5806669768643442, w0=-4.353011369097268e-06, w1=0.2367151306693821\n",
      "Regularized Logistic Regression(862/999): loss=0.580663934340445, w0=-4.355300918389033e-06, w1=0.23674480286021832\n",
      "Regularized Logistic Regression(863/999): loss=0.5806609063308832, w0=-4.35758736957769e-06, w1=0.23677435095796778\n",
      "Regularized Logistic Regression(864/999): loss=0.5806578927556312, w0=-4.359870728034783e-06, w1=0.2368037754547453\n",
      "Regularized Logistic Regression(865/999): loss=0.5806548935351578, w0=-4.362150999119331e-06, w1=0.23683307684073593\n",
      "Regularized Logistic Regression(866/999): loss=0.5806519085904237, w0=-4.364428188177866e-06, w1=0.23686225560419763\n",
      "Regularized Logistic Regression(867/999): loss=0.5806489378428773, w0=-4.366702300544471e-06, w1=0.23689131223147294\n",
      "Regularized Logistic Regression(868/999): loss=0.580645981214454, w0=-4.368973341540816e-06, w1=0.23692024720699478\n",
      "Regularized Logistic Regression(869/999): loss=0.5806430386275706, w0=-4.371241316476199e-06, w1=0.23694906101329308\n",
      "Regularized Logistic Regression(870/999): loss=0.580640110005123, w0=-4.373506230647578e-06, w1=0.2369777541310056\n",
      "Regularized Logistic Regression(871/999): loss=0.5806371952704824, w0=-4.375768089339615e-06, w1=0.23700632703888322\n",
      "Regularized Logistic Regression(872/999): loss=0.5806342943474933, w0=-4.378026897824705e-06, w1=0.23703478021379856\n",
      "Regularized Logistic Regression(873/999): loss=0.5806314071604695, w0=-4.38028266136302e-06, w1=0.23706311413075173\n",
      "Regularized Logistic Regression(874/999): loss=0.5806285336341903, w0=-4.38253538520254e-06, w1=0.237091329262881\n",
      "Regularized Logistic Regression(875/999): loss=0.5806256736938991, w0=-4.384785074579095e-06, w1=0.237119426081467\n",
      "Regularized Logistic Regression(876/999): loss=0.5806228272652987, w0=-4.3870317347163966e-06, w1=0.2371474050559421\n",
      "Regularized Logistic Regression(877/999): loss=0.5806199942745485, w0=-4.389275370826076e-06, w1=0.23717526665389846\n",
      "Regularized Logistic Regression(878/999): loss=0.5806171746482627, w0=-4.391515988107723e-06, w1=0.2372030113410928\n",
      "Regularized Logistic Regression(879/999): loss=0.5806143683135051, w0=-4.393753591748917e-06, w1=0.2372306395814578\n",
      "Regularized Logistic Regression(880/999): loss=0.580611575197789, w0=-4.395988186925267e-06, w1=0.23725815183710483\n",
      "Regularized Logistic Regression(881/999): loss=0.5806087952290705, w0=-4.398219778800444e-06, w1=0.23728554856833367\n",
      "Regularized Logistic Regression(882/999): loss=0.5806060283357494, w0=-4.400448372526221e-06, w1=0.23731283023364078\n",
      "Regularized Logistic Regression(883/999): loss=0.5806032744466628, w0=-4.402673973242504e-06, w1=0.23733999728972438\n",
      "Regularized Logistic Regression(884/999): loss=0.5806005334910846, w0=-4.4048965860773714e-06, w1=0.23736705019149404\n",
      "Regularized Logistic Regression(885/999): loss=0.5805978053987225, w0=-4.407116216147106e-06, w1=0.23739398939207465\n",
      "Regularized Logistic Regression(886/999): loss=0.5805950900997122, w0=-4.409332868556231e-06, w1=0.2374208153428164\n",
      "Regularized Logistic Regression(887/999): loss=0.5805923875246182, w0=-4.4115465483975455e-06, w1=0.23744752849330025\n",
      "Regularized Logistic Regression(888/999): loss=0.5805896976044295, w0=-4.4137572607521606e-06, w1=0.2374741292913473\n",
      "Regularized Logistic Regression(889/999): loss=0.5805870202705562, w0=-4.415965010689532e-06, w1=0.2375006181830221\n",
      "Regularized Logistic Regression(890/999): loss=0.5805843554548269, w0=-4.418169803267496e-06, w1=0.23752699561264345\n",
      "Regularized Logistic Regression(891/999): loss=0.5805817030894871, w0=-4.4203716435323016e-06, w1=0.23755326202278906\n",
      "Regularized Logistic Regression(892/999): loss=0.580579063107195, w0=-4.422570536518649e-06, w1=0.23757941785430306\n",
      "Regularized Logistic Regression(893/999): loss=0.5805764354410187, w0=-4.424766487249721e-06, w1=0.2376054635463037\n",
      "Regularized Logistic Regression(894/999): loss=0.5805738200244352, w0=-4.426959500737218e-06, w1=0.23763139953618911\n",
      "Regularized Logistic Regression(895/999): loss=0.5805712167913261, w0=-4.429149581981391e-06, w1=0.23765722625964525\n",
      "Regularized Logistic Regression(896/999): loss=0.5805686256759749, w0=-4.431336735971076e-06, w1=0.23768294415065128\n",
      "Regularized Logistic Regression(897/999): loss=0.5805660466130652, w0=-4.433520967683729e-06, w1=0.23770855364148985\n",
      "Regularized Logistic Regression(898/999): loss=0.5805634795376772, w0=-4.435702282085458e-06, w1=0.23773405516274845\n",
      "Regularized Logistic Regression(899/999): loss=0.5805609243852862, w0=-4.437880684131058e-06, w1=0.23775944914333041\n",
      "Regularized Logistic Regression(900/999): loss=0.5805583810917587, w0=-4.4400561787640425e-06, w1=0.2377847360104606\n",
      "Regularized Logistic Regression(901/999): loss=0.5805558495933503, w0=-4.442228770916678e-06, w1=0.23780991618969194\n",
      "Regularized Logistic Regression(902/999): loss=0.580553329826704, w0=-4.444398465510017e-06, w1=0.23783499010491196\n",
      "Regularized Logistic Regression(903/999): loss=0.5805508217288455, w0=-4.446565267453932e-06, w1=0.23785995817834982\n",
      "Regularized Logistic Regression(904/999): loss=0.580548325237183, w0=-4.448729181647146e-06, w1=0.23788482083058243\n",
      "Regularized Logistic Regression(905/999): loss=0.5805458402895036, w0=-4.450890212977268e-06, w1=0.23790957848054067\n",
      "Regularized Logistic Regression(906/999): loss=0.5805433668239706, w0=-4.453048366320824e-06, w1=0.23793423154551716\n",
      "Regularized Logistic Regression(907/999): loss=0.5805409047791208, w0=-4.455203646543289e-06, w1=0.2379587804411724\n",
      "Regularized Logistic Regression(908/999): loss=0.5805384540938632, w0=-4.4573560584991204e-06, w1=0.23798322558154159\n",
      "Regularized Logistic Regression(909/999): loss=0.5805360147074763, w0=-4.459505607031793e-06, w1=0.23800756737903958\n",
      "Regularized Logistic Regression(910/999): loss=0.5805335865596034, w0=-4.461652296973827e-06, w1=0.23803180624446865\n",
      "Regularized Logistic Regression(911/999): loss=0.5805311695902539, w0=-4.463796133146819e-06, w1=0.23805594258702506\n",
      "Regularized Logistic Regression(912/999): loss=0.5805287637397974, w0=-4.465937120361481e-06, w1=0.2380799768143056\n",
      "Regularized Logistic Regression(913/999): loss=0.5805263689489643, w0=-4.468075263417666e-06, w1=0.2381039093323123\n",
      "Regularized Logistic Regression(914/999): loss=0.5805239851588413, w0=-4.470210567104402e-06, w1=0.23812774054546154\n",
      "Regularized Logistic Regression(915/999): loss=0.5805216123108694, w0=-4.472343036199922e-06, w1=0.23815147085658683\n",
      "Regularized Logistic Regression(916/999): loss=0.5805192503468428, w0=-4.4744726754716984e-06, w1=0.2381751006669482\n",
      "Regularized Logistic Regression(917/999): loss=0.5805168992089053, w0=-4.4765994896764725e-06, w1=0.23819863037623695\n",
      "Regularized Logistic Regression(918/999): loss=0.5805145588395478, w0=-4.478723483560287e-06, w1=0.23822206038258278\n",
      "Regularized Logistic Regression(919/999): loss=0.5805122291816076, w0=-4.480844661858514e-06, w1=0.23824539108255918\n",
      "Regularized Logistic Regression(920/999): loss=0.5805099101782651, w0=-4.482963029295891e-06, w1=0.2382686228711906\n",
      "Regularized Logistic Regression(921/999): loss=0.5805076017730406, w0=-4.485078590586548e-06, w1=0.23829175614195675\n",
      "Regularized Logistic Regression(922/999): loss=0.5805053039097939, w0=-4.487191350434041e-06, w1=0.2383147912868006\n",
      "Regularized Logistic Regression(923/999): loss=0.5805030165327214, w0=-4.4893013135313806e-06, w1=0.23833772869613393\n",
      "Regularized Logistic Regression(924/999): loss=0.5805007395863526, w0=-4.4914084845610625e-06, w1=0.23836056875884337\n",
      "Regularized Logistic Regression(925/999): loss=0.5804984730155507, w0=-4.4935128681951e-06, w1=0.23838331186229658\n",
      "Regularized Logistic Regression(926/999): loss=0.580496216765507, w0=-4.495614469095055e-06, w1=0.2384059583923477\n",
      "Regularized Logistic Regression(927/999): loss=0.580493970781742, w0=-4.497713291912065e-06, w1=0.23842850873334454\n",
      "Regularized Logistic Regression(928/999): loss=0.5804917350101005, w0=-4.499809341286875e-06, w1=0.23845096326813275\n",
      "Regularized Logistic Regression(929/999): loss=0.5804895093967519, w0=-4.501902621849869e-06, w1=0.2384733223780645\n",
      "Regularized Logistic Regression(930/999): loss=0.5804872938881858, w0=-4.5039931382210975e-06, w1=0.23849558644300142\n",
      "Regularized Logistic Regression(931/999): loss=0.5804850884312117, w0=-4.506080895010309e-06, w1=0.2385177558413226\n",
      "Regularized Logistic Regression(932/999): loss=0.5804828929729555, w0=-4.5081658968169805e-06, w1=0.23853983094992878\n",
      "Regularized Logistic Regression(933/999): loss=0.5804807074608594, w0=-4.510248148230345e-06, w1=0.23856181214425054\n",
      "Regularized Logistic Regression(934/999): loss=0.5804785318426773, w0=-4.512327653829422e-06, w1=0.2385836997982521\n",
      "Regularized Logistic Regression(935/999): loss=0.5804763660664748, w0=-4.514404418183047e-06, w1=0.23860549428443786\n",
      "Regularized Logistic Regression(936/999): loss=0.5804742100806259, w0=-4.516478445849903e-06, w1=0.2386271959738588\n",
      "Regularized Logistic Regression(937/999): loss=0.5804720638338119, w0=-4.518549741378546e-06, w1=0.23864880523611715\n",
      "Regularized Logistic Regression(938/999): loss=0.5804699272750184, w0=-4.520618309307436e-06, w1=0.2386703224393728\n",
      "Regularized Logistic Regression(939/999): loss=0.5804678003535351, w0=-4.522684154164967e-06, w1=0.23869174795034742\n",
      "Regularized Logistic Regression(940/999): loss=0.5804656830189512, w0=-4.524747280469495e-06, w1=0.23871308213433415\n",
      "Regularized Logistic Regression(941/999): loss=0.5804635752211554, w0=-4.526807692729365e-06, w1=0.23873432535519984\n",
      "Regularized Logistic Regression(942/999): loss=0.5804614769103339, w0=-4.528865395442944e-06, w1=0.23875547797538918\n",
      "Regularized Logistic Regression(943/999): loss=0.5804593880369675, w0=-4.530920393098646e-06, w1=0.23877654035593635\n",
      "Regularized Logistic Regression(944/999): loss=0.5804573085518302, w0=-4.532972690174963e-06, w1=0.23879751285646403\n",
      "Regularized Logistic Regression(945/999): loss=0.5804552384059878, w0=-4.535022291140489e-06, w1=0.23881839583519396\n",
      "Regularized Logistic Regression(946/999): loss=0.5804531775507937, w0=-4.5370692004539565e-06, w1=0.2388391896489487\n",
      "Regularized Logistic Regression(947/999): loss=0.580451125937892, w0=-4.539113422564256e-06, w1=0.23885989465316051\n",
      "Regularized Logistic Regression(948/999): loss=0.5804490835192089, w0=-4.541154961910471e-06, w1=0.23888051120187442\n",
      "Regularized Logistic Regression(949/999): loss=0.5804470502469565, w0=-4.5431938229219e-06, w1=0.23890103964775453\n",
      "Regularized Logistic Regression(950/999): loss=0.5804450260736282, w0=-4.5452300100180904e-06, w1=0.23892148034208954\n",
      "Regularized Logistic Regression(951/999): loss=0.5804430109519986, w0=-4.5472635276088625e-06, w1=0.2389418336347982\n",
      "Regularized Logistic Regression(952/999): loss=0.5804410048351184, w0=-4.5492943800943385e-06, w1=0.23896209987443462\n",
      "Regularized Logistic Regression(953/999): loss=0.5804390076763165, w0=-4.5513225718649695e-06, w1=0.23898227940819464\n",
      "Regularized Logistic Regression(954/999): loss=0.5804370194291958, w0=-4.5533481073015634e-06, w1=0.23900237258191867\n",
      "Regularized Logistic Regression(955/999): loss=0.5804350400476326, w0=-4.555370990775314e-06, w1=0.2390223797401001\n",
      "Regularized Logistic Regression(956/999): loss=0.580433069485774, w0=-4.557391226647826e-06, w1=0.2390423012258878\n",
      "Regularized Logistic Regression(957/999): loss=0.5804311076980362, w0=-4.559408819271143e-06, w1=0.23906213738109347\n",
      "Regularized Logistic Regression(958/999): loss=0.5804291546391035, w0=-4.561423772987775e-06, w1=0.23908188854619705\n",
      "Regularized Logistic Regression(959/999): loss=0.5804272102639265, w0=-4.563436092130727e-06, w1=0.23910155506034997\n",
      "Regularized Logistic Regression(960/999): loss=0.5804252745277192, w0=-4.565445781023522e-06, w1=0.23912113726138134\n",
      "Regularized Logistic Regression(961/999): loss=0.5804233473859585, w0=-4.567452843980232e-06, w1=0.2391406354858048\n",
      "Regularized Logistic Regression(962/999): loss=0.5804214287943821, w0=-4.569457285305503e-06, w1=0.2391600500688219\n",
      "Regularized Logistic Regression(963/999): loss=0.5804195187089867, w0=-4.571459109294581e-06, w1=0.23917938134432756\n",
      "Regularized Logistic Regression(964/999): loss=0.5804176170860268, w0=-4.5734583202333406e-06, w1=0.2391986296449152\n",
      "Regularized Logistic Regression(965/999): loss=0.5804157238820126, w0=-4.57545492239831e-06, w1=0.23921779530188286\n",
      "Regularized Logistic Regression(966/999): loss=0.580413839053708, w0=-4.577448920056699e-06, w1=0.23923687864523593\n",
      "Regularized Logistic Regression(967/999): loss=0.5804119625581298, w0=-4.579440317466422e-06, w1=0.2392558800036954\n",
      "Regularized Logistic Regression(968/999): loss=0.5804100943525461, w0=-4.581429118876129e-06, w1=0.2392747997047004\n",
      "Regularized Logistic Regression(969/999): loss=0.5804082343944732, w0=-4.583415328525228e-06, w1=0.2392936380744148\n",
      "Regularized Logistic Regression(970/999): loss=0.5804063826416762, w0=-4.585398950643913e-06, w1=0.23931239543773097\n",
      "Regularized Logistic Regression(971/999): loss=0.5804045390521655, w0=-4.5873799894531885e-06, w1=0.23933107211827742\n",
      "Regularized Logistic Regression(972/999): loss=0.5804027035841968, w0=-4.589358449164898e-06, w1=0.23934966843841868\n",
      "Regularized Logistic Regression(973/999): loss=0.5804008761962677, w0=-4.591334333981746e-06, w1=0.2393681847192655\n",
      "Regularized Logistic Regression(974/999): loss=0.5803990568471185, w0=-4.593307648097329e-06, w1=0.23938662128067717\n",
      "Regularized Logistic Regression(975/999): loss=0.5803972454957279, w0=-4.595278395696155e-06, w1=0.23940497844126735\n",
      "Regularized Logistic Regression(976/999): loss=0.5803954421013138, w0=-4.597246580953673e-06, w1=0.2394232565184081\n",
      "Regularized Logistic Regression(977/999): loss=0.580393646623331, w0=-4.5992122080363e-06, w1=0.2394414558282359\n",
      "Regularized Logistic Regression(978/999): loss=0.5803918590214691, w0=-4.6011752811014395e-06, w1=0.23945957668565482\n",
      "Regularized Logistic Regression(979/999): loss=0.5803900792556512, w0=-4.603135804297514e-06, w1=0.23947761940434314\n",
      "Regularized Logistic Regression(980/999): loss=0.5803883072860332, w0=-4.6050937817639885e-06, w1=0.23949558429675777\n",
      "Regularized Logistic Regression(981/999): loss=0.5803865430730024, w0=-4.607049217631392e-06, w1=0.23951347167413775\n",
      "Regularized Logistic Regression(982/999): loss=0.580384786577174, w0=-4.609002116021347e-06, w1=0.23953128184651132\n",
      "Regularized Logistic Regression(983/999): loss=0.5803830377593909, w0=-4.6109524810465905e-06, w1=0.23954901512269897\n",
      "Regularized Logistic Regression(984/999): loss=0.5803812965807245, w0=-4.612900316811003e-06, w1=0.23956667181031718\n",
      "Regularized Logistic Regression(985/999): loss=0.5803795630024688, w0=-4.61484562740963e-06, w1=0.2395842522157864\n",
      "Regularized Logistic Regression(986/999): loss=0.5803778369861431, w0=-4.61678841692871e-06, w1=0.23960175664433347\n",
      "Regularized Logistic Regression(987/999): loss=0.5803761184934876, w0=-4.618728689445693e-06, w1=0.2396191853999963\n",
      "Regularized Logistic Regression(988/999): loss=0.580374407486463, w0=-4.620666449029273e-06, w1=0.2396365387856291\n",
      "Regularized Logistic Regression(989/999): loss=0.5803727039272506, w0=-4.622601699739406e-06, w1=0.2396538171029076\n",
      "Regularized Logistic Regression(990/999): loss=0.5803710077782478, w0=-4.624534445627339e-06, w1=0.2396710206523317\n",
      "Regularized Logistic Regression(991/999): loss=0.5803693190020702, w0=-4.626464690735629e-06, w1=0.23968814973323335\n",
      "Regularized Logistic Regression(992/999): loss=0.5803676375615474, w0=-4.628392439098173e-06, w1=0.23970520464377731\n",
      "Regularized Logistic Regression(993/999): loss=0.580365963419723, w0=-4.6303176947402276e-06, w1=0.23972218568096895\n",
      "Regularized Logistic Regression(994/999): loss=0.5803642965398531, w0=-4.6322404616784366e-06, w1=0.2397390931406567\n",
      "Regularized Logistic Regression(995/999): loss=0.5803626368854048, w0=-4.634160743920853e-06, w1=0.2397559273175369\n",
      "Regularized Logistic Regression(996/999): loss=0.580360984420054, w0=-4.636078545466961e-06, w1=0.23977268850515965\n",
      "Regularized Logistic Regression(997/999): loss=0.580359339107687, w0=-4.637993870307704e-06, w1=0.23978937699593195\n",
      "Regularized Logistic Regression(998/999): loss=0.580357700912395, w0=-4.639906722425505e-06, w1=0.23980599308112313\n",
      "Regularized Logistic Regression(999/999): loss=0.580356069798476, w0=-4.641817105794292e-06, w1=0.2398225370508686\n",
      "Fold 2 completed.\n",
      "Regularized Logistic Regression(0/999): loss=0.6931471805599447, w0=-1.1139976026771591e-08, w1=0.0009359139459131884\n",
      "Regularized Logistic Regression(1/999): loss=0.6922304937690199, w0=-2.2244883022189746e-08, w1=0.0018692309699273094\n",
      "Regularized Logistic Regression(2/999): loss=0.6913224914651461, w0=-3.331494064808803e-08, w1=0.0027999393464127826\n",
      "Regularized Logistic Regression(3/999): loss=0.6904230719284447, w0=-4.435036653192528e-08, w1=0.0037280277510784348\n",
      "Regularized Logistic Regression(4/999): loss=0.6895321350140772, w0=-5.535137628335435e-08, w1=0.004653485254675271\n",
      "Regularized Logistic Regression(5/999): loss=0.6886495821215634, w0=-6.631818351089642e-08, w1=0.005576301316764239\n",
      "Regularized Logistic Regression(6/999): loss=0.6877753161647302, w0=-7.725099983870864e-08, w1=0.0064964657795515066\n",
      "Regularized Logistic Regression(7/999): loss=0.6869092415422744, w0=-8.815003492343276e-08, w1=0.00741396886178635\n",
      "Regularized Logistic Regression(8/999): loss=0.6860512641089387, w0=-9.901549647111401e-08, w1=0.008328801152724401\n",
      "Regularized Logistic Regression(9/999): loss=0.6852012911472808, w0=-1.0984759025417865e-07, w1=0.009240953606155641\n",
      "Regularized Logistic Regression(10/999): loss=0.684359231340034, w0=-1.2064652012845984e-07, w1=0.010150417534497282\n",
      "Regularized Logistic Regression(11/999): loss=0.6835249947430423, w0=-1.3141248805026183e-07, w1=0.011057184602951986\n",
      "Regularized Logistic Regression(12/999): loss=0.6826984927587672, w0=-1.4214569409345265e-07, w1=0.01196124682373194\n",
      "Regularized Logistic Regression(13/999): loss=0.68187963811035, w0=-1.5284633646657637e-07, w1=0.012862596550344128\n",
      "Regularized Logistic Regression(14/999): loss=0.6810683448162238, w0=-1.6351461152997545e-07, w1=0.013761226471944709\n",
      "Regularized Logistic Regression(15/999): loss=0.680264528165264, w0=-1.7415071381291578e-07, w1=0.014657129607754334\n",
      "Regularized Logistic Regression(16/999): loss=0.6794681046924674, w0=-1.8475483603070516e-07, w1=0.015550299301539824\n",
      "Regularized Logistic Regression(17/999): loss=0.6786789921551455, w0=-1.9532716910179816e-07, w1=0.016440729216159498\n",
      "Regularized Logistic Regression(18/999): loss=0.6778971095096367, w0=-2.058679021648803e-07, w1=0.017328413328172962\n",
      "Regularized Logistic Regression(19/999): loss=0.6771223768885064, w0=-2.1637722259592367e-07, w1=0.01821334592251394\n",
      "Regularized Logistic Regression(20/999): loss=0.6763547155782461, w0=-2.2685531602520795e-07, w1=0.01909552158722706\n",
      "Regularized Logistic Regression(21/999): loss=0.6755940479974462, w0=-2.3730236635430035e-07, w1=0.01997493520826793\n",
      "Regularized Logistic Regression(22/999): loss=0.6748402976754446, w0=-2.4771855577298845e-07, w1=0.02085158196436637\n",
      "Regularized Logistic Regression(23/999): loss=0.6740933892314322, w0=-2.581040647761599e-07, w1=0.02172545732195241\n",
      "Regularized Logistic Regression(24/999): loss=0.6733532483540129, w0=-2.6845907218062356e-07, w1=0.022596557030143998\n",
      "Regularized Logistic Regression(25/999): loss=0.6726198017812093, w0=-2.7878375514186767e-07, w1=0.02346487711579873\n",
      "Regularized Logistic Regression(26/999): loss=0.6718929772808945, w0=-2.890782891707486e-07, w1=0.024330413878627038\n",
      "Regularized Logistic Regression(27/999): loss=0.6711727036316585, w0=-2.993428481501072e-07, w1=0.025193163886361995\n",
      "Regularized Logistic Regression(28/999): loss=0.6704589106040828, w0=-3.0957760435130696e-07, w1=0.026053123970000147\n",
      "Regularized Logistic Regression(29/999): loss=0.6697515289424293, w0=-3.197827284506908e-07, w1=0.0269102912190916\n",
      "Regularized Logistic Regression(30/999): loss=0.6690504903467237, w0=-3.299583895459517e-07, w1=0.027764662977100823\n",
      "Regularized Logistic Regression(31/999): loss=0.6683557274552269, w0=-3.401047551724143e-07, w1=0.028616236836819783\n",
      "Regularized Logistic Regression(32/999): loss=0.6676671738272989, w0=-3.5022199131922326e-07, w1=0.029465010635845338\n",
      "Regularized Logistic Regression(33/999): loss=0.6669847639266249, w0=-3.6031026244543533e-07, w1=0.03031098245211397\n",
      "Regularized Logistic Regression(34/999): loss=0.6663084331048146, w0=-3.703697314960124e-07, w1=0.031154150599494727\n",
      "Regularized Logistic Regression(35/999): loss=0.6656381175853554, w0=-3.80400559917712e-07, w1=0.03199451362344226\n",
      "Regularized Logistic Regression(36/999): loss=0.6649737544479181, w0=-3.9040290767487256e-07, w1=0.03283207029670745\n",
      "Regularized Logistic Regression(37/999): loss=0.6643152816130057, w0=-4.003769332650919e-07, w1=0.03366681961510276\n",
      "Regularized Logistic Regression(38/999): loss=0.6636626378269332, w0=-4.1032279373479516e-07, w1=0.03449876079332795\n",
      "Regularized Logistic Regression(39/999): loss=0.6630157626471378, w0=-4.202406446946908e-07, w1=0.03532789326085214\n",
      "Regularized Logistic Regression(40/999): loss=0.6623745964278088, w0=-4.301306403351127e-07, w1=0.03615421665784989\n",
      "Regularized Logistic Regression(41/999): loss=0.6617390803058301, w0=-4.399929334412459e-07, w1=0.03697773083119407\n",
      "Regularized Logistic Regression(42/999): loss=0.6611091561870273, w0=-4.498276754082347e-07, w1=0.037798435830503525\n",
      "Regularized Logistic Regression(43/999): loss=0.6604847667327135, w0=-4.596350162561714e-07, w1=0.03861633190424744\n",
      "Regularized Logistic Regression(44/999): loss=0.6598658553465325, w0=-4.694151046449638e-07, w1=0.03943141949590054\n",
      "Regularized Logistic Regression(45/999): loss=0.6592523661615792, w0=-4.791680878890812e-07, w1=0.04024369924015462\n",
      "Regularized Logistic Regression(46/999): loss=0.6586442440278052, w0=-4.888941119721762e-07, w1=0.041053171959182275\n",
      "Regularized Logistic Regression(47/999): loss=0.6580414344996933, w0=-4.985933215615822e-07, w1=0.04185983865895371\n",
      "Regularized Logistic Regression(48/999): loss=0.6574438838241996, w0=-5.082658600226859e-07, w1=0.04266370052560628\n",
      "Regularized Logistic Regression(49/999): loss=0.6568515389289569, w0=-5.179118694331721e-07, w1=0.04346475892186326\n",
      "Regularized Logistic Regression(50/999): loss=0.656264347410731, w0=-5.27531490597143e-07, w1=0.04426301538350759\n",
      "Regularized Logistic Regression(51/999): loss=0.6556822575241228, w0=-5.37124863059108e-07, w1=0.04505847161590535\n",
      "Regularized Logistic Regression(52/999): loss=0.6551052181705176, w0=-5.466921251178461e-07, w1=0.045851129490576105\n",
      "Regularized Logistic Regression(53/999): loss=0.6545331788872659, w0=-5.562334138401391e-07, w1=0.04664099104181834\n",
      "Regularized Logistic Regression(54/999): loss=0.6539660898371, w0=-5.657488650743752e-07, w1=0.047428058463379205\n",
      "Regularized Logistic Regression(55/999): loss=0.6534039017977741, w0=-5.752386134640231e-07, w1=0.04821233410517731\n",
      "Regularized Logistic Regression(56/999): loss=0.6528465661519233, w0=-5.847027924609765e-07, w1=0.04899382047007115\n",
      "Regularized Logistic Regression(57/999): loss=0.6522940348771402, w0=-5.941415343387677e-07, w1=0.049772520210675145\n",
      "Regularized Logistic Regression(58/999): loss=0.6517462605362632, w0=-6.035549702056521e-07, w1=0.050548436126225366\n",
      "Regularized Logistic Regression(59/999): loss=0.6512031962678643, w0=-6.12943230017562e-07, w1=0.051321571159488495\n",
      "Regularized Logistic Regression(60/999): loss=0.650664795776943, w0=-6.223064425909297e-07, w1=0.0520919283937208\n",
      "Regularized Logistic Regression(61/999): loss=0.650131013325813, w0=-6.31644735615382e-07, w1=0.05285951104966891\n",
      "Regularized Logistic Regression(62/999): loss=0.6496018037251802, w0=-6.409582356663035e-07, w1=0.053624322482618565\n",
      "Regularized Logistic Regression(63/999): loss=0.6490771223254066, w0=-6.502470682172707e-07, w1=0.05438636617948916\n",
      "Regularized Logistic Regression(64/999): loss=0.6485569250079574, w0=-6.59511357652357e-07, w1=0.05514564575596694\n",
      "Regularized Logistic Regression(65/999): loss=0.6480411681770274, w0=-6.687512272783086e-07, w1=0.05590216495368855\n",
      "Regularized Logistic Regression(66/999): loss=0.647529808751335, w0=-6.779667993365917e-07, w1=0.05665592763746508\n",
      "Regularized Logistic Regression(67/999): loss=0.6470228041560913, w0=-6.87158195015311e-07, w1=0.05740693779254674\n",
      "Regularized Logistic Regression(68/999): loss=0.6465201123151314, w0=-6.963255344610017e-07, w1=0.05815519952193277\n",
      "Regularized Logistic Regression(69/999): loss=0.646021691643209, w0=-7.054689367902928e-07, w1=0.0589007170437223\n",
      "Regularized Logistic Regression(70/999): loss=0.6455275010384475, w0=-7.145885201014448e-07, w1=0.05964349468850632\n",
      "Regularized Logistic Regression(71/999): loss=0.6450374998749425, w0=-7.236844014857606e-07, w1=0.06038353689679825\n",
      "Regularized Logistic Regression(72/999): loss=0.6445516479955198, w0=-7.327566970388718e-07, w1=0.06112084821651005\n",
      "Regularized Logistic Regression(73/999): loss=0.644069905704635, w0=-7.418055218718989e-07, w1=0.061855433300461865\n",
      "Regularized Logistic Regression(74/999): loss=0.6435922337614185, w0=-7.508309901224891e-07, w1=0.06258729690393629\n",
      "Regularized Logistic Regression(75/999): loss=0.6431185933728579, w0=-7.598332149657287e-07, w1=0.0633164438822676\n",
      "Regularized Logistic Regression(76/999): loss=0.64264894618712, w0=-7.688123086249348e-07, w1=0.06404287918847111\n",
      "Regularized Logistic Regression(77/999): loss=0.6421832542870025, w0=-7.777683823823232e-07, w1=0.06476660787091049\n",
      "Regularized Logistic Regression(78/999): loss=0.6417214801835178, w0=-7.867015465895572e-07, w1=0.06548763507100297\n",
      "Regularized Logistic Regression(79/999): loss=0.6412635868096019, w0=-7.956119106781743e-07, w1=0.06620596602095818\n",
      "Regularized Logistic Regression(80/999): loss=0.6408095375139491, w0=-8.044995831698948e-07, w1=0.06692160604155828\n",
      "Regularized Logistic Regression(81/999): loss=0.640359296054967, w0=-8.133646716868108e-07, w1=0.06763456053997091\n",
      "Regularized Logistic Regression(82/999): loss=0.6399128265948435, w0=-8.222072829614585e-07, w1=0.0683448350075986\n",
      "Regularized Logistic Regression(83/999): loss=0.6394700936937422, w0=-8.31027522846773e-07, w1=0.06905243501796253\n",
      "Regularized Logistic Regression(84/999): loss=0.6390310623040939, w0=-8.39825496325927e-07, w1=0.06975736622462292\n",
      "Regularized Logistic Regression(85/999): loss=0.6385956977650129, w0=-8.486013075220555e-07, w1=0.07045963435913116\n",
      "Regularized Logistic Regression(86/999): loss=0.6381639657968097, w0=-8.573550597078657e-07, w1=0.07115924522901747\n",
      "Regularized Logistic Regression(87/999): loss=0.6377358324956119, w0=-8.660868553151337e-07, w1=0.07185620471580936\n",
      "Regularized Logistic Regression(88/999): loss=0.6373112643280899, w0=-8.747967959440893e-07, w1=0.0725505187730887\n",
      "Regularized Logistic Regression(89/999): loss=0.6368902281262803, w0=-8.834849823726904e-07, w1=0.07324219342457326\n",
      "Regularized Logistic Regression(90/999): loss=0.6364726910825037, w0=-8.921515145657859e-07, w1=0.07393123476223583\n",
      "Regularized Logistic Regression(91/999): loss=0.6360586207443824, w0=-9.007964916841702e-07, w1=0.07461764894445332\n",
      "Regularized Logistic Regression(92/999): loss=0.6356479850099477, w0=-9.094200120935294e-07, w1=0.0753014421941882\n",
      "Regularized Logistic Regression(93/999): loss=0.6352407521228425, w0=-9.180221733732799e-07, w1=0.07598262079719707\n",
      "Regularized Logistic Regression(94/999): loss=0.6348368906676051, w0=-9.266030723253019e-07, w1=0.07666119110027327\n",
      "Regularized Logistic Regression(95/999): loss=0.6344363695650461, w0=-9.351628049825665e-07, w1=0.07733715950951911\n",
      "Regularized Logistic Regression(96/999): loss=0.6340391580677068, w0=-9.43701466617659e-07, w1=0.07801053248864416\n",
      "Regularized Logistic Regression(97/999): loss=0.6336452257554016, w0=-9.522191517511994e-07, w1=0.07868131655729664\n",
      "Regularized Logistic Regression(98/999): loss=0.6332545425308397, w0=-9.607159541601608e-07, w1=0.07934951828942222\n",
      "Regularized Logistic Regression(99/999): loss=0.632867078615326, w0=-9.691919668860863e-07, w1=0.08001514431164826\n",
      "Regularized Logistic Regression(100/999): loss=0.6324828045445413, w0=-9.77647282243206e-07, w1=0.08067820130170181\n",
      "Regularized Logistic Regression(101/999): loss=0.6321016911643933, w0=-9.860819918264549e-07, w1=0.08133869598684873\n",
      "Regularized Logistic Regression(102/999): loss=0.6317237096269455, w0=-9.944961865193933e-07, w1=0.08199663514236423\n",
      "Regularized Logistic Regression(103/999): loss=0.6313488313864146, w0=-1.0028899565020292e-06, w1=0.0826520255900291\n",
      "Regularized Logistic Regression(104/999): loss=0.6309770281952413, w0=-1.0112633912585458e-06, w1=0.08330487419665039\n",
      "Regularized Logistic Regression(105/999): loss=0.630608272100226, w0=-1.0196165795849337e-06, w1=0.08395518787261196\n",
      "Regularized Logistic Regression(106/999): loss=0.6302425354387354, w0=-1.0279496095965282e-06, w1=0.08460297357044608\n",
      "Regularized Logistic Regression(107/999): loss=0.6298797908349729, w0=-1.0362625687354557e-06, w1=0.08524823828343311\n",
      "Regularized Logistic Regression(108/999): loss=0.6295200111963122, w0=-1.0445555437779863e-06, w1=0.08589098904422618\n",
      "Regularized Logistic Regression(109/999): loss=0.6291631697096939, w0=-1.0528286208417965e-06, w1=0.0865312329234981\n",
      "Regularized Logistic Regression(110/999): loss=0.6288092398380851, w0=-1.0610818853931422e-06, w1=0.08716897702861569\n",
      "Regularized Logistic Regression(111/999): loss=0.6284581953169984, w0=-1.0693154222539423e-06, w1=0.08780422850233581\n",
      "Regularized Logistic Regression(112/999): loss=0.6281100101510664, w0=-1.077529315608774e-06, w1=0.08843699452152422\n",
      "Regularized Logistic Regression(113/999): loss=0.6277646586106783, w0=-1.0857236490117832e-06, w1=0.0890672822958976\n",
      "Regularized Logistic Regression(114/999): loss=0.6274221152286683, w0=-1.0938985053935065e-06, w1=0.08969509906679327\n",
      "Regularized Logistic Regression(115/999): loss=0.627082354797062, w0=-1.1020539670676102e-06, w1=0.09032045210595305\n",
      "Regularized Logistic Regression(116/999): loss=0.6267453523638741, w0=-1.1101901157375446e-06, w1=0.09094334871433518\n",
      "Regularized Logistic Regression(117/999): loss=0.6264110832299594, w0=-1.1183070325031156e-06, w1=0.09156379622094747\n",
      "Regularized Logistic Regression(118/999): loss=0.6260795229459147, w0=-1.1264047978669743e-06, w1=0.09218180198169881\n",
      "Regularized Logistic Regression(119/999): loss=0.625750647309034, w0=-1.1344834917410253e-06, w1=0.09279737337827476\n",
      "Regularized Logistic Regression(120/999): loss=0.6254244323603056, w0=-1.142543193452756e-06, w1=0.09341051781703316\n",
      "Regularized Logistic Regression(121/999): loss=0.6251008543814653, w0=-1.1505839817514854e-06, w1=0.09402124272792008\n",
      "Regularized Logistic Regression(122/999): loss=0.6247798898920919, w0=-1.1586059348145356e-06, w1=0.09462955556340527\n",
      "Regularized Logistic Regression(123/999): loss=0.6244615156467495, w0=-1.1666091302533267e-06, w1=0.09523546379743866\n",
      "Regularized Logistic Regression(124/999): loss=0.6241457086321752, w0=-1.1745936451193934e-06, w1=0.09583897492442607\n",
      "Regularized Logistic Regression(125/999): loss=0.6238324460645124, w0=-1.1825595559103292e-06, w1=0.09644009645822432\n",
      "Regularized Logistic Regression(126/999): loss=0.6235217053865827, w0=-1.1905069385756534e-06, w1=0.09703883593115402\n",
      "Regularized Logistic Regression(127/999): loss=0.6232134642652071, w0=-1.1984358685226061e-06, w1=0.0976352008930332\n",
      "Regularized Logistic Regression(128/999): loss=0.6229077005885616, w0=-1.2063464206218705e-06, w1=0.09822919891022787\n",
      "Regularized Logistic Regression(129/999): loss=0.6226043924635777, w0=-1.214238669213223e-06, w1=0.09882083756472124\n",
      "Regularized Logistic Regression(130/999): loss=0.6223035182133811, w0=-1.222112688111113e-06, w1=0.09941012445320095\n",
      "Regularized Logistic Regression(131/999): loss=0.6220050563747713, w0=-1.2299685506101722e-06, w1=0.09999706718616286\n",
      "Regularized Logistic Regression(132/999): loss=0.6217089856957323, w0=-1.2378063294906555e-06, w1=0.10058167338703505\n",
      "Regularized Logistic Regression(133/999): loss=0.6214152851329923, w0=-1.2456260970238132e-06, w1=0.10116395069131279\n",
      "Regularized Logistic Regression(134/999): loss=0.6211239338496094, w0=-1.2534279249771954e-06, w1=0.10174390674571832\n",
      "Regularized Logistic Regression(135/999): loss=0.6208349112125977, w0=-1.2612118846198913e-06, w1=0.10232154920736963\n",
      "Regularized Logistic Regression(136/999): loss=0.6205481967905897, w0=-1.2689780467277015e-06, w1=0.10289688574297078\n",
      "Regularized Logistic Regression(137/999): loss=0.6202637703515295, w0=-1.2767264815882458e-06, w1=0.1034699240280166\n",
      "Regularized Logistic Regression(138/999): loss=0.619981611860403, w0=-1.2844572590060078e-06, w1=0.10404067174601174\n",
      "Regularized Logistic Regression(139/999): loss=0.6197017014769997, w0=-1.2921704483073148e-06, w1=0.10460913658770661\n",
      "Regularized Logistic Regression(140/999): loss=0.6194240195537051, w0=-1.2998661183452567e-06, w1=0.10517532625034687\n",
      "Regularized Logistic Regression(141/999): loss=0.619148546633332, w0=-1.3075443375045426e-06, w1=0.10573924843694267\n",
      "Regularized Logistic Regression(142/999): loss=0.6188752634469706, w0=-1.3152051737062965e-06, w1=0.10630091085554588\n",
      "Regularized Logistic Regression(143/999): loss=0.6186041509118813, w0=-1.3228486944127936e-06, w1=0.10686032121854648\n",
      "Regularized Logistic Regression(144/999): loss=0.6183351901294122, w0=-1.3304749666321367e-06, w1=0.10741748724198154\n",
      "Regularized Logistic Regression(145/999): loss=0.6180683623829444, w0=-1.3380840569228745e-06, w1=0.10797241664485978\n",
      "Regularized Logistic Regression(146/999): loss=0.61780364913587, w0=-1.345676031398562e-06, w1=0.10852511714849797\n",
      "Regularized Logistic Regression(147/999): loss=0.6175410320295952, w0=-1.3532509557322635e-06, w1=0.10907559647587074\n",
      "Regularized Logistic Regression(148/999): loss=0.617280492881573, w0=-1.360808895161001e-06, w1=0.10962386235097693\n",
      "Regularized Logistic Regression(149/999): loss=0.6170220136833623, w0=-1.3683499144901446e-06, w1=0.11016992249821622\n",
      "Regularized Logistic Regression(150/999): loss=0.6167655765987143, w0=-1.3758740780977502e-06, w1=0.11071378464177957\n",
      "Regularized Logistic Regression(151/999): loss=0.6165111639616847, w0=-1.3833814499388421e-06, w1=0.1112554565050512\n",
      "Regularized Logistic Regression(152/999): loss=0.6162587582747722, w0=-1.3908720935496428e-06, w1=0.1117949458100249\n",
      "Regularized Logistic Regression(153/999): loss=0.6160083422070812, w0=-1.3983460720517495e-06, w1=0.1123322602767335\n",
      "Regularized Logistic Regression(154/999): loss=0.6157598985925105, w0=-1.4058034481562591e-06, w1=0.11286740762268678\n",
      "Regularized Logistic Regression(155/999): loss=0.6155134104279664, w0=-1.4132442841678415e-06, w1=0.1134003955623255\n",
      "Regularized Logistic Regression(156/999): loss=0.6152688608715976, w0=-1.4206686419887634e-06, w1=0.11393123180648272\n",
      "Regularized Logistic Regression(157/999): loss=0.6150262332410572, w0=-1.4280765831228604e-06, w1=0.11445992406186127\n",
      "Regularized Logistic Regression(158/999): loss=0.614785511011785, w0=-1.4354681686794615e-06, w1=0.11498648003051946\n",
      "Regularized Logistic Regression(159/999): loss=0.6145466778153127, w0=-1.4428434593772636e-06, w1=0.11551090740936881\n",
      "Regularized Logistic Regression(160/999): loss=0.614309717437592, w0=-1.450202515548159e-06, w1=0.11603321388968266\n",
      "Regularized Logistic Regression(161/999): loss=0.6140746138173475, w0=-1.4575453971410154e-06, w1=0.1165534071566164\n",
      "Regularized Logistic Regression(162/999): loss=0.6138413510444427, w0=-1.4648721637254082e-06, w1=0.11707149488873728\n",
      "Regularized Logistic Regression(163/999): loss=0.6136099133582775, w0=-1.4721828744953082e-06, w1=0.11758748475756449\n",
      "Regularized Logistic Regression(164/999): loss=0.6133802851461991, w0=-1.479477588272723e-06, w1=0.11810138442712052\n",
      "Regularized Logistic Regression(165/999): loss=0.6131524509419378, w0=-1.4867563635112933e-06, w1=0.11861320155349164\n",
      "Regularized Logistic Regression(166/999): loss=0.6129263954240568, w0=-1.4940192582998452e-06, w1=0.11912294378439885\n",
      "Regularized Logistic Regression(167/999): loss=0.6127021034144313, w0=-1.5012663303658991e-06, w1=0.11963061875877769\n",
      "Regularized Logistic Regression(168/999): loss=0.6124795598767365, w0=-1.508497637079135e-06, w1=0.12013623410637012\n",
      "Regularized Logistic Regression(169/999): loss=0.6122587499149624, w0=-1.5157132354548154e-06, w1=0.12063979744732128\n",
      "Regularized Logistic Regression(170/999): loss=0.612039658771945, w0=-1.5229131821571666e-06, w1=0.12114131639179117\n",
      "Regularized Logistic Regression(171/999): loss=0.6118222718279125, w0=-1.5300975335027179e-06, w1=0.12164079853957106\n",
      "Regularized Logistic Regression(172/999): loss=0.6116065745990564, w0=-1.5372663454636007e-06, w1=0.12213825147971\n",
      "Regularized Logistic Regression(173/999): loss=0.6113925527361136, w0=-1.544419673670807e-06, w1=0.12263368279014968\n",
      "Regularized Logistic Regression(174/999): loss=0.611180192022971, w0=-1.5515575734174075e-06, w1=0.12312710003737007\n",
      "Regularized Logistic Regression(175/999): loss=0.6109694783752845, w0=-1.5586800996617317e-06, w1=0.1236185107760425\n",
      "Regularized Logistic Regression(176/999): loss=0.6107603978391167, w0=-1.5657873070305083e-06, w1=0.12410792254868731\n",
      "Regularized Logistic Regression(177/999): loss=0.6105529365895902, w0=-1.5728792498219678e-06, w1=0.12459534288534684\n",
      "Regularized Logistic Regression(178/999): loss=0.6103470809295578, w0=-1.5799559820089075e-06, w1=0.12508077930326061\n",
      "Regularized Logistic Regression(179/999): loss=0.6101428172882881, w0=-1.5870175572417193e-06, w1=0.12556423930655142\n",
      "Regularized Logistic Regression(180/999): loss=0.6099401322201692, w0=-1.5940640288513806e-06, w1=0.12604573038591602\n",
      "Regularized Logistic Regression(181/999): loss=0.6097390124034251, w0=-1.6010954498524092e-06, w1=0.12652526001832828\n",
      "Regularized Logistic Regression(182/999): loss=0.6095394446388506, w0=-1.6081118729457821e-06, w1=0.12700283566674567\n",
      "Regularized Logistic Regression(183/999): loss=0.6093414158485583, w0=-1.6151133505218205e-06, w1=0.12747846477982497\n",
      "Regularized Logistic Regression(184/999): loss=0.609144913074744, w0=-1.6220999346630384e-06, w1=0.12795215479164473\n",
      "Regularized Logistic Regression(185/999): loss=0.6089499234784621, w0=-1.6290716771469573e-06, w1=0.1284239131214352\n",
      "Regularized Logistic Regression(186/999): loss=0.6087564343384213, w0=-1.6360286294488884e-06, w1=0.12889374717331478\n",
      "Regularized Logistic Regression(187/999): loss=0.6085644330497897, w0=-1.642970842744679e-06, w1=0.12936166433603324\n",
      "Regularized Logistic Regression(188/999): loss=0.6083739071230168, w0=-1.649898367913429e-06, w1=0.12982767198272152\n",
      "Regularized Logistic Regression(189/999): loss=0.6081848441826678, w0=-1.6568112555401726e-06, w1=0.13029177747065135\n",
      "Regularized Logistic Regression(190/999): loss=0.6079972319662738, w0=-1.6637095559185283e-06, w1=0.1307539881409942\n",
      "Regularized Logistic Regression(191/999): loss=0.607811058323192, w0=-1.670593319053319e-06, w1=0.13121431131859487\n",
      "Regularized Logistic Regression(192/999): loss=0.6076263112134807, w0=-1.6774625946631583e-06, w1=0.1316727543117457\n",
      "Regularized Logistic Regression(193/999): loss=0.6074429787067899, w0=-1.684317432183009e-06, w1=0.1321293244119679\n",
      "Regularized Logistic Regression(194/999): loss=0.6072610489812622, w0=-1.6911578807667083e-06, w1=0.13258402889380178\n",
      "Regularized Logistic Regression(195/999): loss=0.6070805103224429, w0=-1.6979839892894654e-06, w1=0.13303687501460046\n",
      "Regularized Logistic Regression(196/999): loss=0.6069013511222121, w0=-1.7047958063503286e-06, w1=0.13348787001432724\n",
      "Regularized Logistic Regression(197/999): loss=0.6067235598777199, w0=-1.7115933802746229e-06, w1=0.13393702111536415\n",
      "Regularized Logistic Regression(198/999): loss=0.6065471251903378, w0=-1.71837675911636e-06, w1=0.13438433552232165\n",
      "Regularized Logistic Regression(199/999): loss=0.6063720357646215, w0=-1.7251459906606192e-06, w1=0.13482982042185712\n",
      "Regularized Logistic Regression(200/999): loss=0.6061982804072862, w0=-1.7319011224258995e-06, w1=0.13527348298249497\n",
      "Regularized Logistic Regression(201/999): loss=0.606025848026192, w0=-1.7386422016664464e-06, w1=0.13571533035445474\n",
      "Regularized Logistic Regression(202/999): loss=0.6058547276293429, w0=-1.7453692753745494e-06, w1=0.13615536966948436\n",
      "Regularized Logistic Regression(203/999): loss=0.6056849083238944, w0=-1.7520823902828133e-06, w1=0.13659360804069776\n",
      "Regularized Logistic Regression(204/999): loss=0.6055163793151745, w0=-1.7587815928664035e-06, w1=0.13703005256241743\n",
      "Regularized Logistic Regression(205/999): loss=0.6053491299057135, w0=-1.7654669293452643e-06, w1=0.13746471031002225\n",
      "Regularized Logistic Regression(206/999): loss=0.6051831494942886, w0=-1.7721384456863124e-06, w1=0.13789758833979993\n",
      "Regularized Logistic Regression(207/999): loss=0.6050184275749747, w0=-1.7787961876056045e-06, w1=0.13832869368880432\n",
      "Regularized Logistic Regression(208/999): loss=0.6048549537362076, w0=-1.7854402005704786e-06, w1=0.13875803337471843\n",
      "Regularized Logistic Regression(209/999): loss=0.604692717659858, w0=-1.7920705298016732e-06, w1=0.13918561439571955\n",
      "Regularized Logistic Regression(210/999): loss=0.6045317091203162, w0=-1.7986872202754194e-06, w1=0.13961144373035084\n",
      "Regularized Logistic Regression(211/999): loss=0.6043719179835847, w0=-1.8052903167255107e-06, w1=0.1400355283373966\n",
      "Regularized Logistic Regression(212/999): loss=0.6042133342063838, w0=-1.8118798636453483e-06, w1=0.14045787515576258\n",
      "Regularized Logistic Regression(213/999): loss=0.6040559478352633, w0=-1.8184559052899627e-06, w1=0.1408784911043603\n",
      "Regularized Logistic Regression(214/999): loss=0.6038997490057286, w0=-1.8250184856780127e-06, w1=0.14129738308199272\n",
      "Regularized Logistic Regression(215/999): loss=0.6037447279413723, w0=-1.8315676485937613e-06, w1=0.14171455796724947\n",
      "Regularized Logistic Regression(216/999): loss=0.603590874953017, w0=-1.8381034375890291e-06, w1=0.14213002261840232\n",
      "Regularized Logistic Regression(217/999): loss=0.6034381804378688, w0=-1.8446258959851255e-06, w1=0.14254378387330316\n",
      "Regularized Logistic Regression(218/999): loss=0.6032866348786753, w0=-1.8511350668747573e-06, w1=0.14295584854929147\n",
      "Regularized Logistic Regression(219/999): loss=0.6031362288428993, w0=-1.8576309931239174e-06, w1=0.143366223443099\n",
      "Regularized Logistic Regression(220/999): loss=0.6029869529818966, w0=-1.8641137173737505e-06, w1=0.14377491533076336\n",
      "Regularized Logistic Regression(221/999): loss=0.6028387980301044, w0=-1.8705832820423979e-06, w1=0.14418193096754126\n",
      "Regularized Logistic Regression(222/999): loss=0.6026917548042379, w0=-1.877039729326822e-06, w1=0.14458727708782895\n",
      "Regularized Logistic Regression(223/999): loss=0.602545814202497, w0=-1.8834831012046108e-06, w1=0.14499096040508222\n",
      "Regularized Logistic Regression(224/999): loss=0.602400967203778, w0=-1.8899134394357606e-06, w1=0.14539298761174443\n",
      "Regularized Logistic Regression(225/999): loss=0.6022572048668992, w0=-1.8963307855644402e-06, w1=0.14579336537917362\n",
      "Regularized Logistic Regression(226/999): loss=0.6021145183298297, w0=-1.9027351809207347e-06, w1=0.14619210035757485\n",
      "Regularized Logistic Regression(227/999): loss=0.6019728988089299, w0=-1.9091266666223698e-06, w1=0.1465891991759379\n",
      "Regularized Logistic Regression(228/999): loss=0.6018323375981961, w0=-1.9155052835764173e-06, w1=0.1469846684419708\n",
      "Regularized Logistic Regression(229/999): loss=0.6016928260685191, w0=-1.921871072480981e-06, w1=0.14737851474204675\n",
      "Regularized Logistic Regression(230/999): loss=0.6015543556669449, w0=-1.9282240738268655e-06, w1=0.14777074464114678\n",
      "Regularized Logistic Regression(231/999): loss=0.601416917915947, w0=-1.9345643278992237e-06, w1=0.14816136468280566\n",
      "Regularized Logistic Regression(232/999): loss=0.6012805044127032, w0=-1.940891874779189e-06, w1=0.1485503813890652\n",
      "Regularized Logistic Regression(233/999): loss=0.6011451068283843, w0=-1.947206754345488e-06, w1=0.14893780126042616\n",
      "Regularized Logistic Regression(234/999): loss=0.6010107169074453, w0=-1.953509006276037e-06, w1=0.1493236307758051\n",
      "Regularized Logistic Regression(235/999): loss=0.6008773264669287, w0=-1.9597986700495174e-06, w1=0.14970787639249275\n",
      "Regularized Logistic Regression(236/999): loss=0.6007449273957725, w0=-1.9660757849469383e-06, w1=0.15009054454611598\n",
      "Regularized Logistic Regression(237/999): loss=0.6006135116541248, w0=-1.972340390053179e-06, w1=0.1504716416506022\n",
      "Regularized Logistic Regression(238/999): loss=0.6004830712726681, w0=-1.9785925242585174e-06, w1=0.1508511740981468\n",
      "Regularized Logistic Regression(239/999): loss=0.6003535983519488, w0=-1.984832226260137e-06, w1=0.1512291482591811\n",
      "Regularized Logistic Regression(240/999): loss=0.600225085061714, w0=-1.991059534563625e-06, w1=0.15160557048234433\n",
      "Regularized Logistic Regression(241/999): loss=0.6000975236402565, w0=-1.997274487484445e-06, w1=0.15198044709446037\n",
      "Regularized Logistic Regression(242/999): loss=0.5999709063937637, w0=-2.0034771231494036e-06, w1=0.1523537844005115\n",
      "Regularized Logistic Regression(243/999): loss=0.5998452256956767, w0=-2.009667479498093e-06, w1=0.15272558868361827\n",
      "Regularized Logistic Regression(244/999): loss=0.5997204739860548, w0=-2.015845594284322e-06, w1=0.15309586620502091\n",
      "Regularized Logistic Regression(245/999): loss=0.5995966437709438, w0=-2.0220115050775327e-06, w1=0.1534646232040632\n",
      "Regularized Logistic Regression(246/999): loss=0.5994737276217548, w0=-2.028165249264199e-06, w1=0.1538318658981772\n",
      "Regularized Logistic Regression(247/999): loss=0.5993517181746499, w0=-2.034306864049211e-06, w1=0.1541976004828723\n",
      "Regularized Logistic Regression(248/999): loss=0.5992306081299282, w0=-2.040436386457247e-06, w1=0.1545618331317236\n",
      "Regularized Logistic Regression(249/999): loss=0.5991103902514249, w0=-2.046553853334129e-06, w1=0.1549245699963657\n",
      "Regularized Logistic Regression(250/999): loss=0.598991057365914, w0=-2.0526593013481626e-06, w1=0.1552858172064837\n",
      "Regularized Logistic Regression(251/999): loss=0.5988726023625132, w0=-2.0587527669914657e-06, w1=0.15564558086981253\n",
      "Regularized Logistic Regression(252/999): loss=0.5987550181921053, w0=-2.0648342865812804e-06, w1=0.15600386707213226\n",
      "Regularized Logistic Regression(253/999): loss=0.5986382978667522, w0=-2.070903896261274e-06, w1=0.1563606818772686\n",
      "Regularized Logistic Regression(254/999): loss=0.5985224344591252, w0=-2.076961632002824e-06, w1=0.15671603132709525\n",
      "Regularized Logistic Regression(255/999): loss=0.5984074211019377, w0=-2.083007529606289e-06, w1=0.15706992144153484\n",
      "Regularized Logistic Regression(256/999): loss=0.598293250987381, w0=-2.089041624702271e-06, w1=0.15742235821856665\n",
      "Regularized Logistic Regression(257/999): loss=0.5981799173665702, w0=-2.095063952752857e-06, w1=0.15777334763423215\n",
      "Regularized Logistic Regression(258/999): loss=0.598067413548994, w0=-2.1010745490528565e-06, w1=0.15812289564264181\n",
      "Regularized Logistic Regression(259/999): loss=0.5979557329019687, w0=-2.1070734487310178e-06, w1=0.15847100817598903\n",
      "Regularized Logistic Regression(260/999): loss=0.5978448688501005, w0=-2.1130606867512374e-06, w1=0.15881769114455863\n",
      "Regularized Logistic Regression(261/999): loss=0.5977348148747498, w0=-2.1190362979137557e-06, w1=0.1591629504367406\n",
      "Regularized Logistic Regression(262/999): loss=0.5976255645135043, w0=-2.125000316856338e-06, w1=0.15950679191904757\n",
      "Regularized Logistic Regression(263/999): loss=0.5975171113596565, w0=-2.1309527780554464e-06, w1=0.15984922143612607\n",
      "Regularized Logistic Regression(264/999): loss=0.5974094490616835, w0=-2.1368937158273983e-06, w1=0.16019024481077868\n",
      "Regularized Logistic Regression(265/999): loss=0.5973025713227372, w0=-2.142823164329513e-06, w1=0.16052986784398057\n",
      "Regularized Logistic Regression(266/999): loss=0.5971964719001354, w0=-2.1487411575612465e-06, w1=0.16086809631490265\n",
      "Regularized Logistic Regression(267/999): loss=0.5970911446048599, w0=-2.1546477293653163e-06, w1=0.16120493598093194\n",
      "Regularized Logistic Regression(268/999): loss=0.5969865833010592, w0=-2.1605429134288126e-06, w1=0.1615403925776927\n",
      "Regularized Logistic Regression(269/999): loss=0.5968827819055571, w0=-2.1664267432842984e-06, w1=0.16187447181907547\n",
      "Regularized Logistic Regression(270/999): loss=0.5967797343873636, w0=-2.172299252310902e-06, w1=0.16220717939725984\n",
      "Regularized Logistic Regression(271/999): loss=0.5966774347671948, w0=-2.178160473735393e-06, w1=0.16253852098274232\n",
      "Regularized Logistic Regression(272/999): loss=0.5965758771169937, w0=-2.1840104406332524e-06, w1=0.16286850222436514\n",
      "Regularized Logistic Regression(273/999): loss=0.5964750555594571, w0=-2.189849185929729e-06, w1=0.1631971287493445\n",
      "Regularized Logistic Regression(274/999): loss=0.5963749642675702, w0=-2.195676742400887e-06, w1=0.16352440616330238\n",
      "Regularized Logistic Regression(275/999): loss=0.5962755974641394, w0=-2.20149314267464e-06, w1=0.1638503400502987\n",
      "Regularized Logistic Regression(276/999): loss=0.596176949421336, w0=-2.207298419231781e-06, w1=0.16417493597286212\n",
      "Regularized Logistic Regression(277/999): loss=0.596079014460242, w0=-2.213092604406996e-06, w1=0.16449819947202568\n",
      "Regularized Logistic Regression(278/999): loss=0.5959817869503997, w0=-2.218875730389868e-06, w1=0.16482013606736165\n",
      "Regularized Logistic Regression(279/999): loss=0.5958852613093651, w0=-2.224647829225877e-06, w1=0.16514075125701744\n",
      "Regularized Logistic Regression(280/999): loss=0.5957894320022709, w0=-2.230408932817384e-06, w1=0.16546005051775162\n",
      "Regularized Logistic Regression(281/999): loss=0.595694293541386, w0=-2.2361590729246064e-06, w1=0.16577803930497323\n",
      "Regularized Logistic Regression(282/999): loss=0.5955998404856849, w0=-2.2418982811665878e-06, w1=0.16609472305278042\n",
      "Regularized Logistic Regression(283/999): loss=0.595506067440421, w0=-2.247626589022153e-06, w1=0.16641010717400032\n",
      "Regularized Logistic Regression(284/999): loss=0.5954129690567013, w0=-2.253344027830857e-06, w1=0.16672419706022812\n",
      "Regularized Logistic Regression(285/999): loss=0.5953205400310679, w0=-2.2590506287939246e-06, w1=0.16703699808187095\n",
      "Regularized Logistic Regression(286/999): loss=0.5952287751050825, w0=-2.2647464229751794e-06, w1=0.16734851558818986\n",
      "Regularized Logistic Regression(287/999): loss=0.5951376690649147, w0=-2.2704314413019655e-06, w1=0.16765875490734225\n",
      "Regularized Logistic Regression(288/999): loss=0.5950472167409353, w0=-2.2761057145660584e-06, w1=0.1679677213464263\n",
      "Regularized Logistic Regression(289/999): loss=0.5949574130073134, w0=-2.2817692734245695e-06, w1=0.1682754201915262\n",
      "Regularized Logistic Regression(290/999): loss=0.5948682527816167, w0=-2.2874221484008403e-06, w1=0.16858185670775744\n",
      "Regularized Logistic Regression(291/999): loss=0.5947797310244163, w0=-2.293064369885328e-06, w1=0.16888703613931377\n",
      "Regularized Logistic Regression(292/999): loss=0.5946918427388959, w0=-2.298695968136484e-06, w1=0.16919096370951398\n",
      "Regularized Logistic Regression(293/999): loss=0.5946045829704628, w0=-2.304316973281622e-06, w1=0.16949364462084934\n",
      "Regularized Logistic Regression(294/999): loss=0.5945179468063662, w0=-2.309927415317781e-06, w1=0.16979508405503216\n",
      "Regularized Logistic Regression(295/999): loss=0.5944319293753145, w0=-2.315527324112576e-06, w1=0.1700952871730458\n",
      "Regularized Logistic Regression(296/999): loss=0.5943465258471027, w0=-2.321116729405044e-06, w1=0.1703942591151936\n",
      "Regularized Logistic Regression(297/999): loss=0.5942617314322363, w0=-2.326695660806482e-06, w1=0.17069200500114973\n",
      "Regularized Logistic Regression(298/999): loss=0.5941775413815635, w0=-2.3322641478012734e-06, w1=0.17098852993000982\n",
      "Regularized Logistic Regression(299/999): loss=0.5940939509859104, w0=-2.3378222197477117e-06, w1=0.17128383898034372\n",
      "Regularized Logistic Regression(300/999): loss=0.5940109555757193, w0=-2.343369905878813e-06, w1=0.1715779372102468\n",
      "Regularized Logistic Regression(301/999): loss=0.5939285505206884, w0=-2.348907235303123e-06, w1=0.17187082965739234\n",
      "Regularized Logistic Regression(302/999): loss=0.59384673122942, w0=-2.354434237005515e-06, w1=0.17216252133908497\n",
      "Regularized Logistic Regression(303/999): loss=0.5937654931490665, w0=-2.35995093984798e-06, w1=0.17245301725231632\n",
      "Regularized Logistic Regression(304/999): loss=0.593684831764984, w0=-2.365457372570413e-06, w1=0.17274232237381715\n",
      "Regularized Logistic Regression(305/999): loss=0.5936047426003866, w0=-2.370953563791389e-06, w1=0.17303044166011367\n",
      "Regularized Logistic Regression(306/999): loss=0.593525221216007, w0=-2.37643954200893e-06, w1=0.1733173800475801\n",
      "Regularized Logistic Regression(307/999): loss=0.5934462632097558, w0=-2.3819153356012725e-06, w1=0.17360314245249883\n",
      "Regularized Logistic Regression(308/999): loss=0.593367864216389, w0=-2.3873809728276183e-06, w1=0.17388773377111366\n",
      "Regularized Logistic Regression(309/999): loss=0.5932900199071768, w0=-2.392836481828886e-06, w1=0.17417115887968684\n",
      "Regularized Logistic Regression(310/999): loss=0.5932127259895726, w0=-2.3982818906284516e-06, w1=0.17445342263455654\n",
      "Regularized Logistic Regression(311/999): loss=0.593135978206892, w0=-2.4037172271328845e-06, w1=0.17473452987219407\n",
      "Regularized Logistic Regression(312/999): loss=0.5930597723379865, w0=-2.409142519132676e-06, w1=0.17501448540926187\n",
      "Regularized Logistic Regression(313/999): loss=0.5929841041969288, w0=-2.4145577943029605e-06, w1=0.1752932940426727\n",
      "Regularized Logistic Regression(314/999): loss=0.592908969632694, w0=-2.4199630802042324e-06, w1=0.17557096054964552\n",
      "Regularized Logistic Regression(315/999): loss=0.5928343645288484, w0=-2.4253584042830534e-06, w1=0.17584748968776748\n",
      "Regularized Logistic Regression(316/999): loss=0.5927602848032391, w0=-2.430743793872757e-06, w1=0.1761228861950517\n",
      "Regularized Logistic Regression(317/999): loss=0.592686726407688, w0=-2.436119276194144e-06, w1=0.1763971547899964\n",
      "Regularized Logistic Regression(318/999): loss=0.5926136853276879, w0=-2.441484878356173e-06, w1=0.17667030017164476\n",
      "Regularized Logistic Regression(319/999): loss=0.5925411575821002, w0=-2.446840627356644e-06, w1=0.17694232701964838\n",
      "Regularized Logistic Regression(320/999): loss=0.5924691392228593, w0=-2.452186550082879e-06, w1=0.17721323999432212\n",
      "Regularized Logistic Regression(321/999): loss=0.5923976263346753, w0=-2.457522673312391e-06, w1=0.17748304373670964\n",
      "Regularized Logistic Regression(322/999): loss=0.5923266150347437, w0=-2.4628490237135524e-06, w1=0.17775174286864215\n",
      "Regularized Logistic Regression(323/999): loss=0.5922561014724537, w0=-2.4681656278462544e-06, w1=0.1780193419928013\n",
      "Regularized Logistic Regression(324/999): loss=0.5921860818291029, w0=-2.473472512162562e-06, w1=0.17828584569277856\n",
      "Regularized Logistic Regression(325/999): loss=0.5921165523176134, w0=-2.478769703007364e-06, w1=0.17855125853313725\n",
      "Regularized Logistic Regression(326/999): loss=0.5920475091822498, w0=-2.484057226619014e-06, w1=0.17881558505947642\n",
      "Regularized Logistic Regression(327/999): loss=0.5919789486983416, w0=-2.4893351091299702e-06, w1=0.17907882979849166\n",
      "Regularized Logistic Regression(328/999): loss=0.591910867172005, w0=-2.494603376567427e-06, w1=0.17934099725803793\n",
      "Regularized Logistic Regression(329/999): loss=0.5918432609398729, w0=-2.4998620548539424e-06, w1=0.17960209192719198\n",
      "Regularized Logistic Regression(330/999): loss=0.591776126368821, w0=-2.5051111698080573e-06, w1=0.17986211827631438\n",
      "Regularized Logistic Regression(331/999): loss=0.5917094598557023, w0=-2.510350747144914e-06, w1=0.1801210807571133\n",
      "Regularized Logistic Regression(332/999): loss=0.5916432578270798, w0=-2.515580812476867e-06, w1=0.18037898380270695\n",
      "Regularized Logistic Regression(333/999): loss=0.5915775167389631, w0=-2.5208013913140855e-06, w1=0.1806358318276883\n",
      "Regularized Logistic Regression(334/999): loss=0.5915122330765502, w0=-2.526012509065158e-06, w1=0.18089162922818622\n",
      "Regularized Logistic Regression(335/999): loss=0.5914474033539666, w0=-2.531214191037685e-06, w1=0.18114638038193098\n",
      "Regularized Logistic Regression(336/999): loss=0.5913830241140119, w0=-2.5364064624388697e-06, w1=0.1814000896483163\n",
      "Regularized Logistic Regression(337/999): loss=0.5913190919279045, w0=-2.5415893483761045e-06, w1=0.181652761368464\n",
      "Regularized Logistic Regression(338/999): loss=0.5912556033950325, w0=-2.5467628738575504e-06, w1=0.1819043998652877\n",
      "Regularized Logistic Regression(339/999): loss=0.5911925551427043, w0=-2.551927063792713e-06, w1=0.18215500944355706\n",
      "Regularized Logistic Regression(340/999): loss=0.5911299438259034, w0=-2.5570819429930127e-06, w1=0.18240459438996046\n",
      "Regularized Logistic Regression(341/999): loss=0.5910677661270431, w0=-2.5622275361723515e-06, w1=0.18265315897317227\n",
      "Regularized Logistic Regression(342/999): loss=0.5910060187557262, w0=-2.567363867947674e-06, w1=0.18290070744391196\n",
      "Regularized Logistic Regression(343/999): loss=0.5909446984485054, w0=-2.572490962839524e-06, w1=0.1831472440350139\n",
      "Regularized Logistic Regression(344/999): loss=0.5908838019686468, w0=-2.5776088452725963e-06, w1=0.18339277296148865\n",
      "Regularized Logistic Regression(345/999): loss=0.5908233261058942, w0=-2.582717539576285e-06, w1=0.18363729842058646\n",
      "Regularized Logistic Regression(346/999): loss=0.590763267676237, w0=-2.5878170699852247e-06, w1=0.18388082459186503\n",
      "Regularized Logistic Regression(347/999): loss=0.5907036235216816, w0=-2.5929074606398315e-06, w1=0.18412335563725155\n",
      "Regularized Logistic Regression(348/999): loss=0.5906443905100199, w0=-2.5979887355868353e-06, w1=0.18436489570110695\n",
      "Regularized Logistic Regression(349/999): loss=0.590585565534606, w0=-2.60306091877981e-06, w1=0.18460544891029268\n",
      "Regularized Logistic Regression(350/999): loss=0.5905271455141302, w0=-2.6081240340796992e-06, w1=0.18484501937423373\n",
      "Regularized Logistic Regression(351/999): loss=0.5904691273924, w0=-2.613178105255338e-06, w1=0.18508361118498143\n",
      "Regularized Logistic Regression(352/999): loss=0.5904115081381176, w0=-2.6182231559839685e-06, w1=0.1853212284172835\n",
      "Regularized Logistic Regression(353/999): loss=0.5903542847446644, w0=-2.623259209851753e-06, w1=0.18555787512864308\n",
      "Regularized Logistic Regression(354/999): loss=0.5902974542298832, w0=-2.6282862903542846e-06, w1=0.18579355535938535\n",
      "Regularized Logistic Regression(355/999): loss=0.590241013635866, w0=-2.6333044208970886e-06, w1=0.1860282731327221\n",
      "Regularized Logistic Regression(356/999): loss=0.5901849600287433, w0=-2.638313624796126e-06, w1=0.18626203245481873\n",
      "Regularized Logistic Regression(357/999): loss=0.5901292904984723, w0=-2.6433139252782878e-06, w1=0.18649483731485475\n",
      "Regularized Logistic Regression(358/999): loss=0.5900740021586299, w0=-2.6483053454818897e-06, w1=0.1867266916850902\n",
      "Regularized Logistic Regression(359/999): loss=0.5900190921462083, w0=-2.6532879084571588e-06, w1=0.18695759952093077\n",
      "Regularized Logistic Regression(360/999): loss=0.5899645576214098, w0=-2.65826163716672e-06, w1=0.18718756476099174\n",
      "Regularized Logistic Regression(361/999): loss=0.5899103957674454, w0=-2.663226554486076e-06, w1=0.18741659132716196\n",
      "Regularized Logistic Regression(362/999): loss=0.5898566037903346, w0=-2.668182683204085e-06, w1=0.18764468312466892\n",
      "Regularized Logistic Regression(363/999): loss=0.5898031789187069, w0=-2.6731300460234347e-06, w1=0.1878718440421437\n",
      "Regularized Logistic Regression(364/999): loss=0.589750118403607, w0=-2.678068665561111e-06, w1=0.18809807795168346\n",
      "Regularized Logistic Regression(365/999): loss=0.5896974195182979, w0=-2.6829985643488648e-06, w1=0.1883233887089173\n",
      "Regularized Logistic Regression(366/999): loss=0.58964507955807, w0=-2.687919764833674e-06, w1=0.1885477801530695\n",
      "Regularized Logistic Regression(367/999): loss=0.5895930958400502, w0=-2.6928322893782023e-06, w1=0.18877125610702475\n",
      "Regularized Logistic Regression(368/999): loss=0.5895414657030111, w0=-2.6977361602612566e-06, w1=0.18899382037739054\n",
      "Regularized Logistic Regression(369/999): loss=0.5894901865071861, w0=-2.702631399678235e-06, w1=0.1892154767545634\n",
      "Regularized Logistic Regression(370/999): loss=0.5894392556340811, w0=-2.707518029741579e-06, w1=0.18943622901279017\n",
      "Regularized Logistic Regression(371/999): loss=0.5893886704862924, w0=-2.7123960724812153e-06, w1=0.18965608091023436\n",
      "Regularized Logistic Regression(372/999): loss=0.5893384284873245, w0=-2.7172655498449993e-06, w1=0.18987503618903825\n",
      "Regularized Logistic Regression(373/999): loss=0.5892885270814083, w0=-2.722126483699152e-06, w1=0.19009309857538761\n",
      "Regularized Logistic Regression(374/999): loss=0.5892389637333232, w0=-2.726978895828697e-06, w1=0.19031027177957377\n",
      "Regularized Logistic Regression(375/999): loss=0.5891897359282194, w0=-2.731822807937888e-06, w1=0.19052655949605857\n",
      "Regularized Logistic Regression(376/999): loss=0.5891408411714432, w0=-2.7366582416506403e-06, w1=0.19074196540353683\n",
      "Regularized Logistic Regression(377/999): loss=0.5890922769883615, w0=-2.7414852185109556e-06, w1=0.19095649316500102\n",
      "Regularized Logistic Regression(378/999): loss=0.589044040924191, w0=-2.7463037599833414e-06, w1=0.19117014642780233\n",
      "Regularized Logistic Regression(379/999): loss=0.5889961305438266, w0=-2.751113887453232e-06, w1=0.1913829288237155\n",
      "Regularized Logistic Regression(380/999): loss=0.588948543431672, w0=-2.755915622227403e-06, w1=0.19159484396900017\n",
      "Regularized Logistic Regression(381/999): loss=0.5889012771914726, w0=-2.7607089855343825e-06, w1=0.1918058954644655\n",
      "Regularized Logistic Regression(382/999): loss=0.5888543294461496, w0=-2.7654939985248636e-06, w1=0.1920160868955305\n",
      "Regularized Logistic Regression(383/999): loss=0.5888076978376351, w0=-2.7702706822721074e-06, w1=0.1922254218322903\n",
      "Regularized Logistic Regression(384/999): loss=0.5887613800267075, w0=-2.775039057772347e-06, w1=0.19243390382957481\n",
      "Regularized Logistic Regression(385/999): loss=0.5887153736928336, w0=-2.7797991459451886e-06, w1=0.19264153642701323\n",
      "Regularized Logistic Regression(386/999): loss=0.5886696765340047, w0=-2.7845509676340085e-06, w1=0.1928483231490945\n",
      "Regularized Logistic Regression(387/999): loss=0.5886242862665799, w0=-2.7892945436063468e-06, w1=0.19305426750523202\n",
      "Regularized Logistic Regression(388/999): loss=0.5885792006251293, w0=-2.794029894554299e-06, w1=0.19325937298982362\n",
      "Regularized Logistic Regression(389/999): loss=0.5885344173622772, w0=-2.7987570410949055e-06, w1=0.193463643082313\n",
      "Regularized Logistic Regression(390/999): loss=0.5884899342485479, w0=-2.8034760037705364e-06, w1=0.19366708124725399\n",
      "Regularized Logistic Regression(391/999): loss=0.5884457490722136, w0=-2.8081868030492746e-06, w1=0.19386969093436826\n",
      "Regularized Logistic Regression(392/999): loss=0.5884018596391428, w0=-2.8128894593252964e-06, w1=0.19407147557861015\n",
      "Regularized Logistic Regression(393/999): loss=0.5883582637726488, w0=-2.8175839929192487e-06, w1=0.19427243860022564\n",
      "Regularized Logistic Regression(394/999): loss=0.5883149593133427, w0=-2.822270424078623e-06, w1=0.19447258340481516\n",
      "Regularized Logistic Regression(395/999): loss=0.5882719441189845, w0=-2.8269487729781277e-06, w1=0.19467191338339318\n",
      "Regularized Logistic Regression(396/999): loss=0.5882292160643392, w0=-2.831619059720059e-06, w1=0.19487043191244904\n",
      "Regularized Logistic Regression(397/999): loss=0.58818677304103, w0=-2.8362813043346646e-06, w1=0.19506814235400818\n",
      "Regularized Logistic Regression(398/999): loss=0.5881446129573954, w0=-2.8409355267805105e-06, w1=0.195265048055693\n",
      "Regularized Logistic Regression(399/999): loss=0.5881027337383482, w0=-2.8455817469448407e-06, w1=0.19546115235078163\n",
      "Regularized Logistic Regression(400/999): loss=0.5880611333252331, w0=-2.850219984643937e-06, w1=0.19565645855827044\n",
      "Regularized Logistic Regression(401/999): loss=0.5880198096756888, w0=-2.854850259623474e-06, w1=0.19585096998293097\n",
      "Regularized Logistic Regression(402/999): loss=0.5879787607635077, w0=-2.859472591558875e-06, w1=0.19604468991537258\n",
      "Regularized Logistic Regression(403/999): loss=0.5879379845785013, w0=-2.8640870000556616e-06, w1=0.1962376216321002\n",
      "Regularized Logistic Regression(404/999): loss=0.5878974791263629, w0=-2.868693504649803e-06, w1=0.1964297683955754\n",
      "Regularized Logistic Regression(405/999): loss=0.5878572424285331, w0=-2.873292124808062e-06, w1=0.19662113345427404\n",
      "Regularized Logistic Regression(406/999): loss=0.5878172725220677, w0=-2.8778828799283408e-06, w1=0.19681172004274664\n",
      "Regularized Logistic Regression(407/999): loss=0.5877775674595042, w0=-2.8824657893400196e-06, w1=0.19700153138167747\n",
      "Regularized Logistic Regression(408/999): loss=0.5877381253087313, w0=-2.887040872304298e-06, w1=0.19719057067794182\n",
      "Regularized Logistic Regression(409/999): loss=0.5876989441528596, w0=-2.8916081480145307e-06, w1=0.19737884112466678\n",
      "Regularized Logistic Regression(410/999): loss=0.5876600220900932, w0=-2.896167635596563e-06, w1=0.1975663459012878\n",
      "Regularized Logistic Regression(411/999): loss=0.5876213572336014, w0=-2.900719354109062e-06, w1=0.197753088173608\n",
      "Regularized Logistic Regression(412/999): loss=0.5875829477113932, w0=-2.905263322543846e-06, w1=0.19793907109385708\n",
      "Regularized Logistic Regression(413/999): loss=0.5875447916661922, w0=-2.909799559826214e-06, w1=0.19812429780074667\n",
      "Regularized Logistic Regression(414/999): loss=0.587506887255312, w0=-2.9143280848152692e-06, w1=0.19830877141953093\n",
      "Regularized Logistic Regression(415/999): loss=0.5874692326505341, w0=-2.9188489163042435e-06, w1=0.19849249506206249\n",
      "Regularized Logistic Regression(416/999): loss=0.5874318260379859, w0=-2.923362073020817e-06, w1=0.19867547182685089\n",
      "Regularized Logistic Regression(417/999): loss=0.5873946656180198, w0=-2.927867573627438e-06, w1=0.19885770479911774\n",
      "Regularized Logistic Regression(418/999): loss=0.5873577496050943, w0=-2.9323654367216387e-06, w1=0.1990391970508559\n",
      "Regularized Logistic Regression(419/999): loss=0.5873210762276543, w0=-2.9368556808363497e-06, w1=0.19921995164088624\n",
      "Regularized Logistic Regression(420/999): loss=0.587284643728015, w0=-2.9413383244402134e-06, w1=0.19939997161491277\n",
      "Regularized Logistic Regression(421/999): loss=0.587248450362245, w0=-2.945813385937893e-06, w1=0.19957926000558004\n",
      "Regularized Logistic Regression(422/999): loss=0.5872124944000505, w0=-2.9502808836703797e-06, w1=0.19975781983252966\n",
      "Regularized Logistic Regression(423/999): loss=0.5871767741246618, w0=-2.954740835915302e-06, w1=0.19993565410245656\n",
      "Regularized Logistic Regression(424/999): loss=0.5871412878327199, w0=-2.9591932608872256e-06, w1=0.2001127658091633\n",
      "Regularized Logistic Regression(425/999): loss=0.5871060338341628, w0=-2.9636381767379587e-06, w1=0.20028915793361857\n",
      "Regularized Logistic Regression(426/999): loss=0.5870710104521172, w0=-2.96807560155685e-06, w1=0.2004648334440097\n",
      "Regularized Logistic Regression(427/999): loss=0.5870362160227856, w0=-2.9725055533710873e-06, w1=0.20063979529579962\n",
      "Regularized Logistic Regression(428/999): loss=0.5870016488953376, w0=-2.9769280501459935e-06, w1=0.20081404643178197\n",
      "Regularized Logistic Regression(429/999): loss=0.5869673074318031, w0=-2.98134310978532e-06, w1=0.20098758978213577\n",
      "Regularized Logistic Regression(430/999): loss=0.5869331900069626, w0=-2.9857507501315397e-06, w1=0.20116042826448116\n",
      "Regularized Logistic Regression(431/999): loss=0.5868992950082427, w0=-2.9901509889661366e-06, w1=0.20133256478393188\n",
      "Regularized Logistic Regression(432/999): loss=0.5868656208356104, w0=-2.9945438440098943e-06, w1=0.2015040022331521\n",
      "Regularized Logistic Regression(433/999): loss=0.5868321659014673, w0=-2.9989293329231815e-06, w1=0.20167474349240838\n",
      "Regularized Logistic Regression(434/999): loss=0.5867989286305482, w0=-3.003307473306238e-06, w1=0.20184479142962464\n",
      "Regularized Logistic Regression(435/999): loss=0.5867659074598163, w0=-3.007678282699456e-06, w1=0.20201414890043784\n",
      "Regularized Logistic Regression(436/999): loss=0.5867331008383623, w0=-3.0120417785836615e-06, w1=0.2021828187482471\n",
      "Regularized Logistic Regression(437/999): loss=0.5867005072273052, w0=-3.016397978380393e-06, w1=0.20235080380427176\n",
      "Regularized Logistic Regression(438/999): loss=0.5866681250996897, w0=-3.0207468994521785e-06, w1=0.20251810688760158\n",
      "Regularized Logistic Regression(439/999): loss=0.586635952940389, w0=-3.025088559102812e-06, w1=0.20268473080525043\n",
      "Regularized Logistic Regression(440/999): loss=0.5866039892460054, w0=-3.029422974577626e-06, w1=0.2028506783522107\n",
      "Regularized Logistic Regression(441/999): loss=0.5865722325247749, w0=-3.0337501630637623e-06, w1=0.20301595231150416\n",
      "Regularized Logistic Regression(442/999): loss=0.5865406812964682, w0=-3.0380701416904447e-06, w1=0.20318055545423483\n",
      "Regularized Logistic Regression(443/999): loss=0.5865093340922973, w0=-3.042382927529246e-06, w1=0.20334449053964118\n",
      "Regularized Logistic Regression(444/999): loss=0.5864781894548201, w0=-3.046688537594354e-06, w1=0.2035077603151496\n",
      "Regularized Logistic Regression(445/999): loss=0.586447245937846, w0=-3.0509869888428375e-06, w1=0.2036703675164227\n",
      "Regularized Logistic Regression(446/999): loss=0.5864165021063431, w0=-3.0552782981749094e-06, w1=0.20383231486741474\n",
      "Regularized Logistic Regression(447/999): loss=0.5863859565363476, w0=-3.0595624824341874e-06, w1=0.20399360508042141\n",
      "Regularized Logistic Regression(448/999): loss=0.5863556078148691, w0=-3.0638395584079556e-06, w1=0.20415424085613057\n",
      "Regularized Logistic Regression(449/999): loss=0.5863254545398027, w0=-3.0681095428274207e-06, w1=0.20431422488367457\n",
      "Regularized Logistic Regression(450/999): loss=0.5862954953198384, w0=-3.072372452367971e-06, w1=0.20447355984068066\n",
      "Regularized Logistic Regression(451/999): loss=0.586265728774372, w0=-3.0766283036494297e-06, w1=0.204632248393321\n",
      "Regularized Logistic Regression(452/999): loss=0.586236153533416, w0=-3.0808771132363085e-06, w1=0.20479029319636333\n",
      "Regularized Logistic Regression(453/999): loss=0.5862067682375145, w0=-3.085118897638061e-06, w1=0.20494769689322279\n",
      "Regularized Logistic Regression(454/999): loss=0.586177571537654, w0=-3.0893536733093313e-06, w1=0.20510446211600947\n",
      "Regularized Logistic Regression(455/999): loss=0.5861485620951792, w0=-3.0935814566502036e-06, w1=0.2052605914855799\n",
      "Regularized Logistic Regression(456/999): loss=0.5861197385817066, w0=-3.097802264006449e-06, w1=0.20541608761158703\n",
      "Regularized Logistic Regression(457/999): loss=0.586091099679041, w0=-3.1020161116697716e-06, w1=0.2055709530925278\n",
      "Regularized Logistic Regression(458/999): loss=0.5860626440790916, w0=-3.1062230158780527e-06, w1=0.20572519051579433\n",
      "Regularized Logistic Regression(459/999): loss=0.5860343704837879, w0=-3.110422992815593e-06, w1=0.20587880245772242\n",
      "Regularized Logistic Regression(460/999): loss=0.5860062776050002, w0=-3.114616058613354e-06, w1=0.20603179148364006\n",
      "Regularized Logistic Regression(461/999): loss=0.5859783641644551, w0=-3.118802229349198e-06, w1=0.20618416014791666\n",
      "Regularized Logistic Regression(462/999): loss=0.5859506288936566, w0=-3.122981521048125e-06, w1=0.2063359109940112\n",
      "Regularized Logistic Regression(463/999): loss=0.5859230705338053, w0=-3.1271539496825112e-06, w1=0.20648704655452113\n",
      "Regularized Logistic Regression(464/999): loss=0.5858956878357194, w0=-3.131319531172344e-06, w1=0.20663756935122887\n",
      "Regularized Logistic Regression(465/999): loss=0.5858684795597557, w0=-3.1354782813854544e-06, w1=0.20678748189515134\n",
      "Regularized Logistic Regression(466/999): loss=0.5858414444757317, w0=-3.1396302161377516e-06, w1=0.20693678668658785\n",
      "Regularized Logistic Regression(467/999): loss=0.5858145813628478, w0=-3.143775351193452e-06, w1=0.2070854862151659\n",
      "Regularized Logistic Regression(468/999): loss=0.5857878890096123, w0=-3.147913702265311e-06, w1=0.2072335829598903\n",
      "Regularized Logistic Regression(469/999): loss=0.5857613662137637, w0=-3.1520452850148484e-06, w1=0.20738107938918895\n",
      "Regularized Logistic Regression(470/999): loss=0.5857350117821962, w0=-3.156170115052579e-06, w1=0.20752797796095987\n",
      "Regularized Logistic Regression(471/999): loss=0.5857088245308845, w0=-3.160288207938234e-06, w1=0.20767428112262035\n",
      "Regularized Logistic Regression(472/999): loss=0.5856828032848104, w0=-3.164399579180989e-06, w1=0.20781999131114948\n",
      "Regularized Logistic Regression(473/999): loss=0.5856569468778885, w0=-3.168504244239684e-06, w1=0.20796511095313744\n",
      "Regularized Logistic Regression(474/999): loss=0.5856312541528962, w0=-3.172602218523047e-06, w1=0.2081096424648309\n",
      "Regularized Logistic Regression(475/999): loss=0.5856057239613974, w0=-3.176693517389913e-06, w1=0.20825358825217838\n",
      "Regularized Logistic Regression(476/999): loss=0.5855803551636745, w0=-3.1807781561494425e-06, w1=0.20839695071087677\n",
      "Regularized Logistic Regression(477/999): loss=0.5855551466286557, w0=-3.1848561500613406e-06, w1=0.20853973222641697\n",
      "Regularized Logistic Regression(478/999): loss=0.5855300972338461, w0=-3.1889275143360715e-06, w1=0.20868193517412864\n",
      "Regularized Logistic Regression(479/999): loss=0.5855052058652563, w0=-3.192992264135075e-06, w1=0.20882356191922585\n",
      "Regularized Logistic Regression(480/999): loss=0.5854804714173356, w0=-3.1970504145709804e-06, w1=0.20896461481685158\n",
      "Regularized Logistic Regression(481/999): loss=0.5854558927929013, w0=-3.201101980707817e-06, w1=0.20910509621212284\n",
      "Regularized Logistic Regression(482/999): loss=0.5854314689030737, w0=-3.2051469775612267e-06, w1=0.2092450084401761\n",
      "Regularized Logistic Regression(483/999): loss=0.5854071986672054, w0=-3.2091854200986753e-06, w1=0.2093843538262104\n",
      "Regularized Logistic Regression(484/999): loss=0.5853830810128177, w0=-3.2132173232396585e-06, w1=0.20952313468553174\n",
      "Regularized Logistic Regression(485/999): loss=0.5853591148755322, w0=-3.2172427018559116e-06, w1=0.20966135332359936\n",
      "Regularized Logistic Regression(486/999): loss=0.5853352991990075, w0=-3.2212615707716154e-06, w1=0.20979901203606544\n",
      "Regularized Logistic Regression(487/999): loss=0.585311632934872, w0=-3.225273944763601e-06, w1=0.209936113108823\n",
      "Regularized Logistic Regression(488/999): loss=0.5852881150426611, w0=-3.229279838561553e-06, w1=0.21007265881804524\n",
      "Regularized Logistic Regression(489/999): loss=0.5852647444897536, w0=-3.233279266848214e-06, w1=0.2102086514302333\n",
      "Regularized Logistic Regression(490/999): loss=0.5852415202513068, w0=-3.237272244259586e-06, w1=0.21034409320225694\n",
      "Regularized Logistic Regression(491/999): loss=0.5852184413101952, w0=-3.241258785385129e-06, w1=0.210478986381397\n",
      "Regularized Logistic Regression(492/999): loss=0.5851955066569474, w0=-3.2452389047679624e-06, w1=0.21061333320538916\n",
      "Regularized Logistic Regression(493/999): loss=0.585172715289685, w0=-3.2492126169050625e-06, w1=0.21074713590246572\n",
      "Regularized Logistic Regression(494/999): loss=0.5851500662140613, w0=-3.2531799362474587e-06, w1=0.21088039669139924\n",
      "Regularized Logistic Regression(495/999): loss=0.5851275584432016, w0=-3.2571408772004305e-06, w1=0.21101311778154325\n",
      "Regularized Logistic Regression(496/999): loss=0.5851051909976407, w0=-3.261095454123702e-06, w1=0.21114530137287535\n",
      "Regularized Logistic Regression(497/999): loss=0.585082962905266, w0=-3.265043681331636e-06, w1=0.21127694965603835\n",
      "Regularized Logistic Regression(498/999): loss=0.5850608732012571, w0=-3.268985573093424e-06, w1=0.21140806481238264\n",
      "Regularized Logistic Regression(499/999): loss=0.5850389209280269, w0=-3.2729211436332822e-06, w1=0.21153864901400646\n",
      "Regularized Logistic Regression(500/999): loss=0.5850171051351649, w0=-3.276850407130638e-06, w1=0.21166870442379826\n",
      "Regularized Logistic Regression(501/999): loss=0.5849954248793792, w0=-3.2807733777203214e-06, w1=0.21179823319547703\n",
      "Regularized Logistic Regression(502/999): loss=0.5849738792244374, w0=-3.284690069492752e-06, w1=0.21192723747363373\n",
      "Regularized Logistic Regression(503/999): loss=0.5849524672411133, w0=-3.2886004964941275e-06, w1=0.2120557193937717\n",
      "Regularized Logistic Regression(504/999): loss=0.5849311880071286, w0=-3.292504672726609e-06, w1=0.21218368108234775\n",
      "Regularized Logistic Regression(505/999): loss=0.5849100406070984, w0=-3.2964026121485057e-06, w1=0.21231112465681184\n",
      "Regularized Logistic Regression(506/999): loss=0.5848890241324756, w0=-3.3002943286744607e-06, w1=0.21243805222564752\n",
      "Regularized Logistic Regression(507/999): loss=0.584868137681496, w0=-3.304179836175632e-06, w1=0.2125644658884124\n",
      "Regularized Logistic Regression(508/999): loss=0.5848473803591241, w0=-3.3080591484798757e-06, w1=0.21269036773577663\n",
      "Regularized Logistic Regression(509/999): loss=0.5848267512770005, w0=-3.311932279371927e-06, w1=0.21281575984956475\n",
      "Regularized Logistic Regression(510/999): loss=0.5848062495533871, w0=-3.3157992425935796e-06, w1=0.2129406443027921\n",
      "Regularized Logistic Regression(511/999): loss=0.5847858743131146, w0=-3.3196600518438657e-06, w1=0.2130650231597073\n",
      "Regularized Logistic Regression(512/999): loss=0.5847656246875313, w0=-3.3235147207792327e-06, w1=0.21318889847582972\n",
      "Regularized Logistic Regression(513/999): loss=0.5847454998144506, w0=-3.327363263013721e-06, w1=0.2133122722979888\n",
      "Regularized Logistic Regression(514/999): loss=0.5847254988380977, w0=-3.3312056921191406e-06, w1=0.21343514666436228\n",
      "Regularized Logistic Regression(515/999): loss=0.5847056209090614, w0=-3.3350420216252455e-06, w1=0.21355752360451458\n",
      "Regularized Logistic Regression(516/999): loss=0.5846858651842419, w0=-3.3388722650199074e-06, w1=0.2136794051394371\n",
      "Regularized Logistic Regression(517/999): loss=0.5846662308268008, w0=-3.34269643574929e-06, w1=0.21380079328158466\n",
      "Regularized Logistic Regression(518/999): loss=0.5846467170061115, w0=-3.3465145472180208e-06, w1=0.2139216900349144\n",
      "Regularized Logistic Regression(519/999): loss=0.5846273228977101, w0=-3.3503266127893613e-06, w1=0.21404209739492328\n",
      "Regularized Logistic Regression(520/999): loss=0.5846080476832453, w0=-3.3541326457853795e-06, w1=0.2141620173486861\n",
      "Regularized Logistic Regression(521/999): loss=0.5845888905504316, w0=-3.3579326594871177e-06, w1=0.21428145187489228\n",
      "Regularized Logistic Regression(522/999): loss=0.584569850693, w0=-3.3617266671347615e-06, w1=0.21440040294388468\n",
      "Regularized Logistic Regression(523/999): loss=0.5845509273106512, w0=-3.3655146819278073e-06, w1=0.21451887251769647\n",
      "Regularized Logistic Regression(524/999): loss=0.5845321196090069, w0=-3.3692967170252296e-06, w1=0.21463686255008646\n",
      "Regularized Logistic Regression(525/999): loss=0.5845134267995645, w0=-3.3730727855456457e-06, w1=0.21475437498657818\n",
      "Regularized Logistic Regression(526/999): loss=0.5844948480996499, w0=-3.3768429005674818e-06, w1=0.214871411764496\n",
      "Regularized Logistic Regression(527/999): loss=0.5844763827323715, w0=-3.380607075129136e-06, w1=0.2149879748130009\n",
      "Regularized Logistic Regression(528/999): loss=0.5844580299265731, w0=-3.3843653222291423e-06, w1=0.21510406605312785\n",
      "Regularized Logistic Regression(529/999): loss=0.5844397889167922, w0=-3.388117654826332e-06, w1=0.21521968739782074\n",
      "Regularized Logistic Regression(530/999): loss=0.584421658943211, w0=-3.391864085839996e-06, w1=0.21533484075197046\n",
      "Regularized Logistic Regression(531/999): loss=0.5844036392516136, w0=-3.3956046281500443e-06, w1=0.21544952801244818\n",
      "Regularized Logistic Regression(532/999): loss=0.5843857290933431, w0=-3.399339294597167e-06, w1=0.21556375106814432\n",
      "Regularized Logistic Regression(533/999): loss=0.584367927725255, w0=-3.403068097982992e-06, w1=0.2156775118000008\n",
      "Regularized Logistic Regression(534/999): loss=0.5843502344096763, w0=-3.4067910510702436e-06, w1=0.2157908120810483\n",
      "Regularized Logistic Regression(535/999): loss=0.5843326484143604, w0=-3.410508166582899e-06, w1=0.21590365377644205\n",
      "Regularized Logistic Regression(536/999): loss=0.584315169012446, w0=-3.4142194572063453e-06, w1=0.21601603874349504\n",
      "Regularized Logistic Regression(537/999): loss=0.5842977954824138, w0=-3.4179249355875354e-06, w1=0.21612796883171492\n",
      "Regularized Logistic Regression(538/999): loss=0.5842805271080446, w0=-3.421624614335141e-06, w1=0.21623944588283664\n",
      "Regularized Logistic Regression(539/999): loss=0.5842633631783777, w0=-3.425318506019708e-06, w1=0.21635047173085822\n",
      "Regularized Logistic Regression(540/999): loss=0.5842463029876704, w0=-3.429006623173809e-06, w1=0.2164610482020767\n",
      "Regularized Logistic Regression(541/999): loss=0.5842293458353555, w0=-3.432688978292196e-06, w1=0.21657117711511936\n",
      "Regularized Logistic Regression(542/999): loss=0.5842124910260017, w0=-3.4363655838319507e-06, w1=0.21668086028097996\n",
      "Regularized Logistic Regression(543/999): loss=0.5841957378692726, w0=-3.440036452212636e-06, w1=0.21679009950305284\n",
      "Regularized Logistic Regression(544/999): loss=0.5841790856798886, w0=-3.443701595816446e-06, w1=0.2168988965771642\n",
      "Regularized Logistic Regression(545/999): loss=0.5841625337775846, w0=-3.447361026988355e-06, w1=0.21700725329160922\n",
      "Regularized Logistic Regression(546/999): loss=0.5841460814870728, w0=-3.4510147580362653e-06, w1=0.21711517142718378\n",
      "Regularized Logistic Regression(547/999): loss=0.5841297281380025, w0=-3.4546628012311555e-06, w1=0.2172226527572172\n",
      "Regularized Logistic Regression(548/999): loss=0.5841134730649222, w0=-3.4583051688072267e-06, w1=0.2173296990476072\n",
      "Regularized Logistic Regression(549/999): loss=0.5840973156072412, w0=-3.4619418729620487e-06, w1=0.21743631205685074\n",
      "Regularized Logistic Regression(550/999): loss=0.5840812551091915, w0=-3.465572925856705e-06, w1=0.21754249353607932\n",
      "Regularized Logistic Regression(551/999): loss=0.5840652909197899, w0=-3.4691983396159384e-06, w1=0.21764824522908968\n",
      "Regularized Logistic Regression(552/999): loss=0.5840494223928003, w0=-3.472818126328293e-06, w1=0.2177535688723785\n",
      "Regularized Logistic Regression(553/999): loss=0.5840336488866986, w0=-3.4764322980462585e-06, w1=0.2178584661951716\n",
      "Regularized Logistic Regression(554/999): loss=0.5840179697646336, w0=-3.4800408667864124e-06, w1=0.21796293891945975\n",
      "Regularized Logistic Regression(555/999): loss=0.5840023843943917, w0=-3.4836438445295614e-06, w1=0.21806698876002883\n",
      "Regularized Logistic Regression(556/999): loss=0.5839868921483604, w0=-3.487241243220882e-06, w1=0.21817061742449234\n",
      "Regularized Logistic Regression(557/999): loss=0.5839714924034931, w0=-3.490833074770061e-06, w1=0.2182738266133223\n",
      "Regularized Logistic Regression(558/999): loss=0.583956184541273, w0=-3.4944193510514343e-06, w1=0.2183766180198831\n",
      "Regularized Logistic Regression(559/999): loss=0.5839409679476772, w0=-3.4980000839041266e-06, w1=0.21847899333046136\n",
      "Regularized Logistic Regression(560/999): loss=0.5839258420131447, w0=-3.501575285132188e-06, w1=0.21858095422429658\n",
      "Regularized Logistic Regression(561/999): loss=0.5839108061325368, w0=-3.5051449665047333e-06, w1=0.21868250237361497\n",
      "Regularized Logistic Regression(562/999): loss=0.5838958597051073, w0=-3.5087091397560766e-06, w1=0.21878363944365828\n",
      "Regularized Logistic Regression(563/999): loss=0.5838810021344658, w0=-3.5122678165858677e-06, w1=0.21888436709271678\n",
      "Regularized Logistic Regression(564/999): loss=0.5838662328285464, w0=-3.5158210086592282e-06, w1=0.21898468697215726\n",
      "Regularized Logistic Regression(565/999): loss=0.58385155119957, w0=-3.519368727606886e-06, w1=0.21908460072645572\n",
      "Regularized Logistic Regression(566/999): loss=0.5838369566640148, w0=-3.522910985025307e-06, w1=0.21918410999322888\n",
      "Regularized Logistic Regression(567/999): loss=0.5838224486425828, w0=-3.5264477924768312e-06, w1=0.21928321640326115\n",
      "Regularized Logistic Regression(568/999): loss=0.5838080265601651, w0=-3.529979161489804e-06, w1=0.21938192158053876\n",
      "Regularized Logistic Regression(569/999): loss=0.5837936898458113, w0=-3.5335051035587073e-06, w1=0.21948022714227697\n",
      "Regularized Logistic Regression(570/999): loss=0.5837794379326963, w0=-3.5370256301442914e-06, w1=0.2195781346989517\n",
      "Regularized Logistic Regression(571/999): loss=0.5837652702580901, w0=-3.540540752673706e-06, w1=0.21967564585432817\n",
      "Regularized Logistic Regression(572/999): loss=0.5837511862633237, w0=-3.5440504825406296e-06, w1=0.21977276220549216\n",
      "Regularized Logistic Regression(573/999): loss=0.5837371853937591, w0=-3.5475548311053983e-06, w1=0.21986948534287726\n",
      "Regularized Logistic Regression(574/999): loss=0.5837232670987585, w0=-3.5510538096951348e-06, w1=0.2199658168502971\n",
      "Regularized Logistic Regression(575/999): loss=0.5837094308316528, w0=-3.554547429603877e-06, w1=0.22006175830497213\n",
      "Regularized Logistic Regression(576/999): loss=0.5836956760497111, w0=-3.5580357020927037e-06, w1=0.22015731127756025\n",
      "Regularized Logistic Regression(577/999): loss=0.5836820022141104, w0=-3.5615186383898636e-06, w1=0.2202524773321849\n",
      "Regularized Logistic Regression(578/999): loss=0.5836684087899057, w0=-3.564996249690899e-06, w1=0.22034725802646504\n",
      "Regularized Logistic Regression(579/999): loss=0.5836548952459992, w0=-3.5684685471587723e-06, w1=0.2204416549115435\n",
      "Regularized Logistic Regression(580/999): loss=0.5836414610551118, w0=-3.5719355419239915e-06, w1=0.22053566953211415\n",
      "Regularized Logistic Regression(581/999): loss=0.583628105693754, w0=-3.5753972450847335e-06, w1=0.2206293034264529\n",
      "Regularized Logistic Regression(582/999): loss=0.5836148286421955, w0=-3.578853667706968e-06, w1=0.2207225581264428\n",
      "Regularized Logistic Regression(583/999): loss=0.5836016293844365, w0=-3.58230482082458e-06, w1=0.22081543515760563\n",
      "Regularized Logistic Regression(584/999): loss=0.5835885074081797, w0=-3.585750715439493e-06, w1=0.22090793603912828\n",
      "Regularized Logistic Regression(585/999): loss=0.5835754622048026, w0=-3.58919136252179e-06, w1=0.22100006228389077\n",
      "Regularized Logistic Regression(586/999): loss=0.5835624932693266, w0=-3.5926267730098363e-06, w1=0.2210918153984941\n",
      "Regularized Logistic Regression(587/999): loss=0.5835496001003916, w0=-3.5960569578103973e-06, w1=0.2211831968832879\n",
      "Regularized Logistic Regression(588/999): loss=0.5835367822002279, w0=-3.5994819277987607e-06, w1=0.2212742082323979\n",
      "Regularized Logistic Regression(589/999): loss=0.5835240390746275, w0=-3.602901693818856e-06, w1=0.22136485093375358\n",
      "Regularized Logistic Regression(590/999): loss=0.5835113702329171, w0=-3.606316266683372e-06, w1=0.2214551264691159\n",
      "Regularized Logistic Regression(591/999): loss=0.5834987751879326, w0=-3.609725657173876e-06, w1=0.22154503631410352\n",
      "Regularized Logistic Regression(592/999): loss=0.583486253455991, w0=-3.6131298760409317e-06, w1=0.22163458193821978\n",
      "Regularized Logistic Regression(593/999): loss=0.5834738045568625, w0=-3.616528934004215e-06, w1=0.2217237648048816\n",
      "Regularized Logistic Regression(594/999): loss=0.5834614280137465, w0=-3.6199228417526324e-06, w1=0.22181258637144285\n",
      "Regularized Logistic Regression(595/999): loss=0.5834491233532446, w0=-3.623311609944435e-06, w1=0.22190104808922498\n",
      "Regularized Logistic Regression(596/999): loss=0.583436890105334, w0=-3.6266952492073367e-06, w1=0.22198915140354\n",
      "Regularized Logistic Regression(597/999): loss=0.5834247278033421, w0=-3.630073770138626e-06, w1=0.22207689775371828\n",
      "Regularized Logistic Regression(598/999): loss=0.5834126359839211, w0=-3.633447183305282e-06, w1=0.22216428857313644\n",
      "Regularized Logistic Regression(599/999): loss=0.5834006141870225, w0=-3.6368154992440887e-06, w1=0.2222513252892414\n",
      "Regularized Logistic Regression(600/999): loss=0.5833886619558716, w0=-3.6401787284617467e-06, w1=0.22233800932357625\n",
      "Regularized Logistic Regression(601/999): loss=0.5833767788369434, w0=-3.643536881434987e-06, w1=0.22242434209180764\n",
      "Regularized Logistic Regression(602/999): loss=0.5833649643799373, w0=-3.6468899686106834e-06, w1=0.22251032500375076\n",
      "Regularized Logistic Regression(603/999): loss=0.5833532181377529, w0=-3.6502380004059626e-06, w1=0.22259595946339555\n",
      "Regularized Logistic Regression(604/999): loss=0.583341539666465, w0=-3.653580987208317e-06, w1=0.22268124686893195\n",
      "Regularized Logistic Regression(605/999): loss=0.5833299285252991, w0=-3.656918939375714e-06, w1=0.22276618861277317\n",
      "Regularized Logistic Regression(606/999): loss=0.5833183842766099, w0=-3.6602518672367075e-06, w1=0.22285078608158623\n",
      "Regularized Logistic Regression(607/999): loss=0.5833069064858547, w0=-3.663579781090546e-06, w1=0.22293504065631276\n",
      "Regularized Logistic Regression(608/999): loss=0.5832954947215709, w0=-3.666902691207282e-06, w1=0.2230189537121951\n",
      "Regularized Logistic Regression(609/999): loss=0.583284148555354, w0=-3.6702206078278815e-06, w1=0.223102526618801\n",
      "Regularized Logistic Regression(610/999): loss=0.5832728675618315, w0=-3.67353354116433e-06, w1=0.22318576074005117\n",
      "Regularized Logistic Regression(611/999): loss=0.5832616513186419, w0=-3.6768415013997413e-06, w1=0.22326865743424013\n",
      "Regularized Logistic Regression(612/999): loss=0.5832504994064127, w0=-3.680144498688465e-06, w1=0.22335121805406222\n",
      "Regularized Logistic Regression(613/999): loss=0.5832394114087349, w0=-3.6834425431561916e-06, w1=0.2234334439466367\n",
      "Regularized Logistic Regression(614/999): loss=0.5832283869121436, w0=-3.6867356449000592e-06, w1=0.22351533645353266\n",
      "Regularized Logistic Regression(615/999): loss=0.5832174255060937, w0=-3.6900238139887586e-06, w1=0.22359689691079224\n",
      "Regularized Logistic Regression(616/999): loss=0.5832065267829385, w0=-3.693307060462639e-06, w1=0.22367812664895448\n",
      "Regularized Logistic Regression(617/999): loss=0.5831956903379076, w0=-3.696585394333811e-06, w1=0.2237590269930807\n",
      "Regularized Logistic Regression(618/999): loss=0.583184915769086, w0=-3.6998588255862526e-06, w1=0.22383959926277647\n",
      "Regularized Logistic Regression(619/999): loss=0.5831742026773913, w0=-3.7031273641759113e-06, w1=0.22391984477221813\n",
      "Regularized Logistic Regression(620/999): loss=0.5831635506665528, w0=-3.706391020030807e-06, w1=0.22399976483017336\n",
      "Regularized Logistic Regression(621/999): loss=0.5831529593430912, w0=-3.7096498030511353e-06, w1=0.2240793607400287\n",
      "Regularized Logistic Regression(622/999): loss=0.5831424283162956, w0=-3.7129037231093694e-06, w1=0.22415863379980874\n",
      "Regularized Logistic Regression(623/999): loss=0.5831319571982044, w0=-3.7161527900503613e-06, w1=0.22423758530220195\n",
      "Regularized Logistic Regression(624/999): loss=0.5831215456035842, w0=-3.7193970136914436e-06, w1=0.22431621653458383\n",
      "Regularized Logistic Regression(625/999): loss=0.5831111931499082, w0=-3.7226364038225293e-06, w1=0.22439452877903948\n",
      "Regularized Logistic Regression(626/999): loss=0.5831008994573373, w0=-3.725870970206212e-06, w1=0.2244725233123868\n",
      "Regularized Logistic Regression(627/999): loss=0.5830906641486993, w0=-3.729100722577867e-06, w1=0.22455020140620133\n",
      "Regularized Logistic Regression(628/999): loss=0.5830804868494677, w0=-3.7323256706457487e-06, w1=0.22462756432683456\n",
      "Regularized Logistic Regression(629/999): loss=0.5830703671877443, w0=-3.7355458240910906e-06, w1=0.22470461333544078\n",
      "Regularized Logistic Regression(630/999): loss=0.5830603047942369, w0=-3.7387611925682036e-06, w1=0.2247813496879988\n",
      "Regularized Logistic Regression(631/999): loss=0.5830502993022413, w0=-3.741971785704573e-06, w1=0.22485777463533355\n",
      "Regularized Logistic Regression(632/999): loss=0.5830403503476216, w0=-3.7451776131009567e-06, w1=0.22493388942313877\n",
      "Regularized Logistic Regression(633/999): loss=0.58303045756879, w0=-3.7483786843314823e-06, w1=0.22500969529199877\n",
      "Regularized Logistic Regression(634/999): loss=0.58302062060669, w0=-3.751575008943743e-06, w1=0.22508519347741335\n",
      "Regularized Logistic Regression(635/999): loss=0.5830108391047739, w0=-3.7547665964588945e-06, w1=0.2251603852098162\n",
      "Regularized Logistic Regression(636/999): loss=0.5830011127089879, w0=-3.75795345637175e-06, w1=0.22523527171459953\n",
      "Regularized Logistic Regression(637/999): loss=0.5829914410677499, w0=-3.761135598150876e-06, w1=0.2253098542121349\n",
      "Regularized Logistic Regression(638/999): loss=0.5829818238319338, w0=-3.7643130312386867e-06, w1=0.22538413391779494\n",
      "Regularized Logistic Regression(639/999): loss=0.5829722606548488, w0=-3.7674857650515384e-06, w1=0.22545811204197508\n",
      "Regularized Logistic Regression(640/999): loss=0.5829627511922234, w0=-3.7706538089798235e-06, w1=0.22553178979011643\n",
      "Regularized Logistic Regression(641/999): loss=0.5829532951021861, w0=-3.7738171723880637e-06, w1=0.22560516836272504\n",
      "Regularized Logistic Regression(642/999): loss=0.5829438920452464, w0=-3.776975864615004e-06, w1=0.2256782489553943\n",
      "Regularized Logistic Regression(643/999): loss=0.5829345416842806, w0=-3.780129894973705e-06, w1=0.22575103275882671\n",
      "Regularized Logistic Regression(644/999): loss=0.5829252436845093, w0=-3.783279272751634e-06, w1=0.22582352095885624\n",
      "Regularized Logistic Regression(645/999): loss=0.5829159977134838, w0=-3.786424007210759e-06, w1=0.22589571473646466\n",
      "Regularized Logistic Regression(646/999): loss=0.5829068034410672, w0=-3.7895641075876376e-06, w1=0.2259676152678083\n",
      "Regularized Logistic Regression(647/999): loss=0.5828976605394166, w0=-3.7926995830935103e-06, w1=0.22603922372423618\n",
      "Regularized Logistic Regression(648/999): loss=0.5828885686829676, w0=-3.7958304429143897e-06, w1=0.22611054127231003\n",
      "Regularized Logistic Regression(649/999): loss=0.5828795275484159, w0=-3.7989566962111505e-06, w1=0.22618156907382664\n",
      "Regularized Logistic Regression(650/999): loss=0.5828705368147008, w0=-3.802078352119621e-06, w1=0.226252308285838\n",
      "Regularized Logistic Regression(651/999): loss=0.5828615961629896, w0=-3.805195419750671e-06, w1=0.2263227600606708\n",
      "Regularized Logistic Regression(652/999): loss=0.5828527052766593, w0=-3.8083079081903003e-06, w1=0.22639292554594853\n",
      "Regularized Logistic Regression(653/999): loss=0.5828438638412812, w0=-3.811415826499729e-06, w1=0.22646280588460993\n",
      "Regularized Logistic Regression(654/999): loss=0.5828350715446049, w0=-3.814519183715485e-06, w1=0.22653240221493146\n",
      "Regularized Logistic Regression(655/999): loss=0.582826328076541, w0=-3.81761798884949e-06, w1=0.2266017156705458\n",
      "Regularized Logistic Regression(656/999): loss=0.5828176331291463, w0=-3.820712250889151e-06, w1=0.2266707473804621\n",
      "Regularized Logistic Regression(657/999): loss=0.5828089863966069, w0=-3.823801978797442e-06, w1=0.22673949846908603\n",
      "Regularized Logistic Regression(658/999): loss=0.5828003875752228, w0=-3.826887181512995e-06, w1=0.2268079700562396\n",
      "Regularized Logistic Regression(659/999): loss=0.5827918363633928, w0=-3.829967867950186e-06, w1=0.22687616325718205\n",
      "Regularized Logistic Regression(660/999): loss=0.5827833324615979, w0=-3.833044046999216e-06, w1=0.2269440791826268\n",
      "Regularized Logistic Regression(661/999): loss=0.582774875572386, w0=-3.8361157275262035e-06, w1=0.22701171893876362\n",
      "Regularized Logistic Regression(662/999): loss=0.5827664654003581, w0=-3.839182918373263e-06, w1=0.2270790836272782\n",
      "Regularized Logistic Regression(663/999): loss=0.5827581016521514, w0=-3.842245628358597e-06, w1=0.22714617434536824\n",
      "Regularized Logistic Regression(664/999): loss=0.5827497840364244, w0=-3.84530386627657e-06, w1=0.2272129921857665\n",
      "Regularized Logistic Regression(665/999): loss=0.5827415122638431, w0=-3.848357640897805e-06, w1=0.22727953823675878\n",
      "Regularized Logistic Regression(666/999): loss=0.5827332860470652, w0=-3.851406960969256e-06, w1=0.22734581358220143\n",
      "Regularized Logistic Regression(667/999): loss=0.5827251051007255, w0=-3.854451835214299e-06, w1=0.22741181930154475\n",
      "Regularized Logistic Regression(668/999): loss=0.5827169691414213, w0=-3.85749227233281e-06, w1=0.22747755646984572\n",
      "Regularized Logistic Regression(669/999): loss=0.5827088778876994, w0=-3.8605282810012495e-06, w1=0.22754302615779134\n",
      "Regularized Logistic Regression(670/999): loss=0.5827008310600387, w0=-3.863559869872745e-06, w1=0.22760822943171738\n",
      "Regularized Logistic Regression(671/999): loss=0.5826928283808388, w0=-3.866587047577173e-06, w1=0.22767316735362403\n",
      "Regularized Logistic Regression(672/999): loss=0.5826848695744041, w0=-3.869609822721238e-06, w1=0.22773784098119743\n",
      "Regularized Logistic Regression(673/999): loss=0.5826769543669309, w0=-3.872628203888556e-06, w1=0.22780225136782634\n",
      "Regularized Logistic Regression(674/999): loss=0.5826690824864925, w0=-3.875642199639735e-06, w1=0.2278663995626216\n",
      "Regularized Logistic Regression(675/999): loss=0.5826612536630267, w0=-3.878651818512454e-06, w1=0.2279302866104344\n",
      "Regularized Logistic Regression(676/999): loss=0.5826534676283198, w0=-3.881657069021544e-06, w1=0.2279939135518746\n",
      "Regularized Logistic Regression(677/999): loss=0.5826457241159961, w0=-3.884657959659067e-06, w1=0.22805728142332807\n",
      "Regularized Logistic Regression(678/999): loss=0.5826380228615019, w0=-3.887654498894397e-06, w1=0.22812039125697547\n",
      "Regularized Logistic Regression(679/999): loss=0.5826303636020934, w0=-3.8906466951742975e-06, w1=0.22818324408081053\n",
      "Regularized Logistic Regression(680/999): loss=0.5826227460768223, w0=-3.893634556922999e-06, w1=0.2282458409186567\n",
      "Regularized Logistic Regression(681/999): loss=0.5826151700265239, w0=-3.89661809254228e-06, w1=0.2283081827901859\n",
      "Regularized Logistic Regression(682/999): loss=0.5826076351938035, w0=-3.899597310411543e-06, w1=0.2283702707109369\n",
      "Regularized Logistic Regression(683/999): loss=0.5826001413230233, w0=-3.902572218887894e-06, w1=0.22843210569233263\n",
      "Regularized Logistic Regression(684/999): loss=0.5825926881602892, w0=-3.905542826306217e-06, w1=0.22849368874169637\n",
      "Regularized Logistic Regression(685/999): loss=0.582585275453439, w0=-3.908509140979253e-06, w1=0.22855502086227086\n",
      "Regularized Logistic Regression(686/999): loss=0.582577902952029, w0=-3.911471171197675e-06, w1=0.22861610305323513\n",
      "Regularized Logistic Regression(687/999): loss=0.5825705704073217, w0=-3.914428925230167e-06, w1=0.22867693630972177\n",
      "Regularized Logistic Regression(688/999): loss=0.5825632775722729, w0=-3.917382411323497e-06, w1=0.22873752162283553\n",
      "Regularized Logistic Regression(689/999): loss=0.5825560242015196, w0=-3.920331637702595e-06, w1=0.2287978599796678\n",
      "Regularized Logistic Regression(690/999): loss=0.5825488100513678, w0=-3.923276612570627e-06, w1=0.22885795236331596\n",
      "Regularized Logistic Regression(691/999): loss=0.5825416348797811, w0=-3.926217344109069e-06, w1=0.22891779975289905\n",
      "Regularized Logistic Regression(692/999): loss=0.5825344984463668, w0=-3.929153840477783e-06, w1=0.22897740312357656\n",
      "Regularized Logistic Regression(693/999): loss=0.5825274005123647, w0=-3.9320861098150925e-06, w1=0.22903676344656332\n",
      "Regularized Logistic Regression(694/999): loss=0.5825203408406364, w0=-3.935014160237855e-06, w1=0.22909588168914774\n",
      "Regularized Logistic Regression(695/999): loss=0.5825133191956512, w0=-3.9379379998415335e-06, w1=0.22915475881470698\n",
      "Regularized Logistic Regression(696/999): loss=0.5825063353434768, w0=-3.940857636700275e-06, w1=0.22921339578272532\n",
      "Regularized Logistic Regression(697/999): loss=0.5824993890517657, w0=-3.943773078866979e-06, w1=0.22927179354880914\n",
      "Regularized Logistic Regression(698/999): loss=0.5824924800897447, w0=-3.946684334373374e-06, w1=0.22932995306470494\n",
      "Regularized Logistic Regression(699/999): loss=0.5824856082282032, w0=-3.949591411230085e-06, w1=0.22938787527831486\n",
      "Regularized Logistic Regression(700/999): loss=0.5824787732394815, w0=-3.952494317426712e-06, w1=0.2294455611337129\n",
      "Regularized Logistic Regression(701/999): loss=0.5824719748974607, w0=-3.955393060931896e-06, w1=0.22950301157116182\n",
      "Regularized Logistic Regression(702/999): loss=0.5824652129775496, w0=-3.958287649693394e-06, w1=0.2295602275271291\n",
      "Regularized Logistic Regression(703/999): loss=0.5824584872566753, w0=-3.96117809163815e-06, w1=0.22961720993430268\n",
      "Regularized Logistic Regression(704/999): loss=0.5824517975132716, w0=-3.964064394672364e-06, w1=0.22967395972160604\n",
      "Regularized Logistic Regression(705/999): loss=0.5824451435272671, w0=-3.966946566681563e-06, w1=0.22973047781421704\n",
      "Regularized Logistic Regression(706/999): loss=0.5824385250800765, w0=-3.969824615530674e-06, w1=0.2297867651335818\n",
      "Regularized Logistic Regression(707/999): loss=0.5824319419545877, w0=-3.972698549064092e-06, w1=0.22984282259743055\n",
      "Regularized Logistic Regression(708/999): loss=0.5824253939351526, w0=-3.9755683751057485e-06, w1=0.2298986511197937\n",
      "Regularized Logistic Regression(709/999): loss=0.5824188808075752, w0=-3.978434101459183e-06, w1=0.2299542516110177\n",
      "Regularized Logistic Regression(710/999): loss=0.5824124023591019, w0=-3.9812957359076125e-06, w1=0.23000962497778046\n",
      "Regularized Logistic Regression(711/999): loss=0.5824059583784117, w0=-3.984153286213998e-06, w1=0.23006477212310705\n",
      "Regularized Logistic Regression(712/999): loss=0.5823995486556044, w0=-3.987006760121114e-06, w1=0.2301196939463841\n",
      "Regularized Logistic Regression(713/999): loss=0.5823931729821907, w0=-3.9898561653516184e-06, w1=0.23017439134337742\n",
      "Regularized Logistic Regression(714/999): loss=0.5823868311510838, w0=-3.992701509608119e-06, w1=0.23022886520624497\n",
      "Regularized Logistic Regression(715/999): loss=0.5823805229565855, w0=-3.995542800573241e-06, w1=0.23028311642355379\n",
      "Regularized Logistic Regression(716/999): loss=0.5823742481943802, w0=-3.998380045909696e-06, w1=0.23033714588029364\n",
      "Regularized Logistic Regression(717/999): loss=0.5823680066615227, w0=-4.001213253260345e-06, w1=0.23039095445789393\n",
      "Regularized Logistic Regression(718/999): loss=0.582361798156428, w0=-4.004042430248273e-06, w1=0.23044454303423756\n",
      "Regularized Logistic Regression(719/999): loss=0.582355622478864, w0=-4.006867584476847e-06, w1=0.23049791248367607\n",
      "Regularized Logistic Regression(720/999): loss=0.5823494794299384, w0=-4.009688723529788e-06, w1=0.23055106367704514\n",
      "Regularized Logistic Regression(721/999): loss=0.5823433688120908, w0=-4.0125058549712355e-06, w1=0.23060399748167762\n",
      "Regularized Logistic Regression(722/999): loss=0.5823372904290839, w0=-4.015318986345811e-06, w1=0.23065671476142144\n",
      "Regularized Logistic Regression(723/999): loss=0.5823312440859922, w0=-4.0181281251786875e-06, w1=0.23070921637665046\n",
      "Regularized Logistic Regression(724/999): loss=0.582325229589194, w0=-4.020933278975651e-06, w1=0.23076150318428298\n",
      "Regularized Logistic Regression(725/999): loss=0.5823192467463609, w0=-4.023734455223169e-06, w1=0.23081357603779346\n",
      "Regularized Logistic Regression(726/999): loss=0.5823132953664492, w0=-4.026531661388452e-06, w1=0.2308654357872287\n",
      "Regularized Logistic Regression(727/999): loss=0.5823073752596901, w0=-4.029324904919519e-06, w1=0.230917083279221\n",
      "Regularized Logistic Regression(728/999): loss=0.5823014862375822, w0=-4.032114193245263e-06, w1=0.23096851935700358\n",
      "Regularized Logistic Regression(729/999): loss=0.5822956281128797, w0=-4.034899533775512e-06, w1=0.23101974486042426\n",
      "Regularized Logistic Regression(730/999): loss=0.5822898006995856, w0=-4.037680933901096e-06, w1=0.23107076062595988\n",
      "Regularized Logistic Regression(731/999): loss=0.5822840038129415, w0=-4.040458400993908e-06, w1=0.23112156748672955\n",
      "Regularized Logistic Regression(732/999): loss=0.5822782372694196, w0=-4.043231942406967e-06, w1=0.23117216627251203\n",
      "Regularized Logistic Regression(733/999): loss=0.5822725008867133, w0=-4.046001565474482e-06, w1=0.23122255780975562\n",
      "Regularized Logistic Regression(734/999): loss=0.5822667944837285, w0=-4.048767277511915e-06, w1=0.23127274292159503\n",
      "Regularized Logistic Regression(735/999): loss=0.5822611178805753, w0=-4.051529085816041e-06, w1=0.23132272242786306\n",
      "Regularized Logistic Regression(736/999): loss=0.5822554708985587, w0=-4.054286997665013e-06, w1=0.2313724971451071\n",
      "Regularized Logistic Regression(737/999): loss=0.5822498533601705, w0=-4.0570410203184204e-06, w1=0.23142206788659994\n",
      "Regularized Logistic Regression(738/999): loss=0.5822442650890816, w0=-4.059791161017354e-06, w1=0.23147143546235646\n",
      "Regularized Logistic Regression(739/999): loss=0.5822387059101317, w0=-4.0625374269844646e-06, w1=0.23152060067914532\n",
      "Regularized Logistic Regression(740/999): loss=0.5822331756493222, w0=-4.065279825424026e-06, w1=0.23156956434050263\n",
      "Regularized Logistic Regression(741/999): loss=0.5822276741338085, w0=-4.068018363521995e-06, w1=0.23161832724674683\n",
      "Regularized Logistic Regression(742/999): loss=0.5822222011918895, w0=-4.07075304844607e-06, w1=0.2316668901949909\n",
      "Regularized Logistic Regression(743/999): loss=0.5822167566530025, w0=-4.073483887345756e-06, w1=0.2317152539791561\n",
      "Regularized Logistic Regression(744/999): loss=0.5822113403477123, w0=-4.07621088735242e-06, w1=0.2317634193899863\n",
      "Regularized Logistic Regression(745/999): loss=0.5822059521077051, w0=-4.078934055579353e-06, w1=0.23181138721505953\n",
      "Regularized Logistic Regression(746/999): loss=0.5822005917657794, w0=-4.08165339912183e-06, w1=0.23185915823880265\n",
      "Regularized Logistic Regression(747/999): loss=0.5821952591558379, w0=-4.084368925057166e-06, w1=0.2319067332425046\n",
      "Regularized Logistic Regression(748/999): loss=0.5821899541128812, w0=-4.0870806404447815e-06, w1=0.23195411300432756\n",
      "Regularized Logistic Regression(749/999): loss=0.5821846764729983, w0=-4.089788552326254e-06, w1=0.23200129829932212\n",
      "Regularized Logistic Regression(750/999): loss=0.5821794260733595, w0=-4.092492667725382e-06, w1=0.2320482898994398\n",
      "Regularized Logistic Regression(751/999): loss=0.5821742027522092, w0=-4.095192993648241e-06, w1=0.23209508857354597\n",
      "Regularized Logistic Regression(752/999): loss=0.5821690063488573, w0=-4.09788953708324e-06, w1=0.2321416950874321\n",
      "Regularized Logistic Regression(753/999): loss=0.5821638367036721, w0=-4.100582305001186e-06, w1=0.23218811020383\n",
      "Regularized Logistic Regression(754/999): loss=0.5821586936580734, w0=-4.103271304355332e-06, w1=0.23223433468242202\n",
      "Regularized Logistic Regression(755/999): loss=0.5821535770545235, w0=-4.105956542081443e-06, w1=0.2322803692798564\n",
      "Regularized Logistic Regression(756/999): loss=0.582148486736522, w0=-4.1086380250978485e-06, w1=0.23232621474975904\n",
      "Regularized Logistic Regression(757/999): loss=0.582143422548596, w0=-4.1113157603055e-06, w1=0.2323718718427456\n",
      "Regularized Logistic Regression(758/999): loss=0.5821383843362941, w0=-4.113989754588029e-06, w1=0.23241734130643438\n",
      "Regularized Logistic Regression(759/999): loss=0.5821333719461798, w0=-4.116660014811803e-06, w1=0.23246262388546013\n",
      "Regularized Logistic Regression(760/999): loss=0.5821283852258231, w0=-4.119326547825983e-06, w1=0.23250772032148342\n",
      "Regularized Logistic Regression(761/999): loss=0.5821234240237942, w0=-4.121989360462576e-06, w1=0.23255263135320522\n",
      "Regularized Logistic Regression(762/999): loss=0.5821184881896555, w0=-4.124648459536494e-06, w1=0.23259735771637968\n",
      "Regularized Logistic Regression(763/999): loss=0.5821135775739557, w0=-4.127303851845608e-06, w1=0.23264190014382544\n",
      "Regularized Logistic Regression(764/999): loss=0.5821086920282218, w0=-4.129955544170806e-06, w1=0.23268625936543835\n",
      "Regularized Logistic Regression(765/999): loss=0.5821038314049527, w0=-4.132603543276044e-06, w1=0.2327304361082009\n",
      "Regularized Logistic Regression(766/999): loss=0.5820989955576132, w0=-4.135247855908404e-06, w1=0.23277443109619889\n",
      "Regularized Logistic Regression(767/999): loss=0.5820941843406249, w0=-4.137888488798149e-06, w1=0.23281824505063026\n",
      "Regularized Logistic Regression(768/999): loss=0.5820893976093618, w0=-4.140525448658775e-06, w1=0.23286187868981834\n",
      "Regularized Logistic Regression(769/999): loss=0.5820846352201429, w0=-4.143158742187066e-06, w1=0.232905332729224\n",
      "Regularized Logistic Regression(770/999): loss=0.5820798970302236, w0=-4.145788376063151e-06, w1=0.23294860788145672\n",
      "Regularized Logistic Regression(771/999): loss=0.5820751828977926, w0=-4.148414356950554e-06, w1=0.23299170485628637\n",
      "Regularized Logistic Regression(772/999): loss=0.5820704926819624, w0=-4.151036691496249e-06, w1=0.23303462436065622\n",
      "Regularized Logistic Regression(773/999): loss=0.5820658262427648, w0=-4.153655386330714e-06, w1=0.23307736709869267\n",
      "Regularized Logistic Regression(774/999): loss=0.5820611834411419, w0=-4.156270448067986e-06, w1=0.2331199337717187\n",
      "Regularized Logistic Regression(775/999): loss=0.5820565641389428, w0=-4.158881883305708e-06, w1=0.23316232507826512\n",
      "Regularized Logistic Regression(776/999): loss=0.5820519681989152, w0=-4.1614896986251895e-06, w1=0.23320454171408148\n",
      "Regularized Logistic Regression(777/999): loss=0.5820473954846993, w0=-4.164093900591452e-06, w1=0.233246584372149\n",
      "Regularized Logistic Regression(778/999): loss=0.5820428458608227, w0=-4.166694495753285e-06, w1=0.23328845374269022\n",
      "Regularized Logistic Regression(779/999): loss=0.5820383191926918, w0=-4.1692914906433015e-06, w1=0.2333301505131802\n",
      "Regularized Logistic Regression(780/999): loss=0.5820338153465887, w0=-4.171884891777982e-06, w1=0.23337167536836143\n",
      "Regularized Logistic Regression(781/999): loss=0.5820293341896629, w0=-4.174474705657732e-06, w1=0.23341302899025093\n",
      "Regularized Logistic Regression(782/999): loss=0.5820248755899249, w0=-4.177060938766932e-06, w1=0.2334542120581527\n",
      "Regularized Logistic Regression(783/999): loss=0.5820204394162427, w0=-4.17964359757399e-06, w1=0.23349522524867172\n",
      "Regularized Logistic Regression(784/999): loss=0.5820160255383328, w0=-4.182222688531389e-06, w1=0.23353606923572143\n",
      "Regularized Logistic Regression(785/999): loss=0.5820116338267564, w0=-4.1847982180757445e-06, w1=0.23357674469053577\n",
      "Regularized Logistic Regression(786/999): loss=0.582007264152912, w0=-4.187370192627847e-06, w1=0.2336172522816823\n",
      "Regularized Logistic Regression(787/999): loss=0.5820029163890311, w0=-4.189938618592719e-06, w1=0.2336575926750702\n",
      "Regularized Logistic Regression(788/999): loss=0.5819985904081708, w0=-4.192503502359664e-06, w1=0.23369776653396426\n",
      "Regularized Logistic Regression(789/999): loss=0.5819942860842084, w0=-4.195064850302313e-06, w1=0.23373777451899325\n",
      "Regularized Logistic Regression(790/999): loss=0.581990003291838, w0=-4.19762266877868e-06, w1=0.2337776172881625\n",
      "Regularized Logistic Regression(791/999): loss=0.5819857419065597, w0=-4.200176964131207e-06, w1=0.2338172954968627\n",
      "Regularized Logistic Regression(792/999): loss=0.5819815018046797, w0=-4.202727742686817e-06, w1=0.23385680979788367\n",
      "Regularized Logistic Regression(793/999): loss=0.5819772828633004, w0=-4.205275010756962e-06, w1=0.23389616084142262\n",
      "Regularized Logistic Regression(794/999): loss=0.5819730849603177, w0=-4.207818774637671e-06, w1=0.23393534927509588\n",
      "Regularized Logistic Regression(795/999): loss=0.5819689079744134, w0=-4.210359040609601e-06, w1=0.23397437574394897\n",
      "Regularized Logistic Regression(796/999): loss=0.5819647517850506, w0=-4.212895814938085e-06, w1=0.23401324089046802\n",
      "Regularized Logistic Regression(797/999): loss=0.5819606162724688, w0=-4.215429103873179e-06, w1=0.23405194535458956\n",
      "Regularized Logistic Regression(798/999): loss=0.5819565013176773, w0=-4.217958913649714e-06, w1=0.23409048977371144\n",
      "Regularized Logistic Regression(799/999): loss=0.5819524068024504, w0=-4.22048525048734e-06, w1=0.23412887478270336\n",
      "Regularized Logistic Regression(800/999): loss=0.5819483326093232, w0=-4.223008120590578e-06, w1=0.23416710101391633\n",
      "Regularized Logistic Regression(801/999): loss=0.5819442786215838, w0=-4.225527530148867e-06, w1=0.23420516909719397\n",
      "Regularized Logistic Regression(802/999): loss=0.5819402447232701, w0=-4.228043485336608e-06, w1=0.23424307965988203\n",
      "Regularized Logistic Regression(803/999): loss=0.5819362307991649, w0=-4.230555992313214e-06, w1=0.23428083332684124\n",
      "Regularized Logistic Regression(804/999): loss=0.5819322367347879, w0=-4.233065057223161e-06, w1=0.23431843072045377\n",
      "Regularized Logistic Regression(805/999): loss=0.5819282624163944, w0=-4.235570686196028e-06, w1=0.23435587246063572\n",
      "Regularized Logistic Regression(806/999): loss=0.5819243077309669, w0=-4.238072885346551e-06, w1=0.23439315916484557\n",
      "Regularized Logistic Regression(807/999): loss=0.5819203725662122, w0=-4.240571660774662e-06, w1=0.23443029144809716\n",
      "Regularized Logistic Regression(808/999): loss=0.5819164568105551, w0=-4.243067018565545e-06, w1=0.23446726992296632\n",
      "Regularized Logistic Regression(809/999): loss=0.5819125603531352, w0=-4.245558964789672e-06, w1=0.23450409519960308\n",
      "Regularized Logistic Regression(810/999): loss=0.5819086830837984, w0=-4.2480475055028586e-06, w1=0.23454076788574219\n",
      "Regularized Logistic Regression(811/999): loss=0.5819048248930969, w0=-4.250532646746303e-06, w1=0.23457728858670965\n",
      "Regularized Logistic Regression(812/999): loss=0.5819009856722795, w0=-4.253014394546638e-06, w1=0.23461365790543576\n",
      "Regularized Logistic Regression(813/999): loss=0.5818971653132908, w0=-4.25549275491597e-06, w1=0.23464987644246402\n",
      "Regularized Logistic Regression(814/999): loss=0.5818933637087631, w0=-4.25796773385193e-06, w1=0.23468594479596183\n",
      "Regularized Logistic Regression(815/999): loss=0.581889580752014, w0=-4.260439337337716e-06, w1=0.23472186356172728\n",
      "Regularized Logistic Regression(816/999): loss=0.5818858163370404, w0=-4.26290757134214e-06, w1=0.2347576333332024\n",
      "Regularized Logistic Regression(817/999): loss=0.5818820703585146, w0=-4.265372441819672e-06, w1=0.2347932547014801\n",
      "Regularized Logistic Regression(818/999): loss=0.5818783427117791, w0=-4.267833954710484e-06, w1=0.2348287282553159\n",
      "Regularized Logistic Regression(819/999): loss=0.5818746332928417, w0=-4.2702921159404945e-06, w1=0.2348640545811351\n",
      "Regularized Logistic Regression(820/999): loss=0.5818709419983716, w0=-4.2727469314214165e-06, w1=0.2348992342630457\n",
      "Regularized Logistic Regression(821/999): loss=0.5818672687256948, w0=-4.275198407050798e-06, w1=0.23493426788284452\n",
      "Regularized Logistic Regression(822/999): loss=0.5818636133727886, w0=-4.277646548712068e-06, w1=0.23496915602002802\n",
      "Regularized Logistic Regression(823/999): loss=0.581859975838279, w0=-4.280091362274577e-06, w1=0.2350038992518023\n",
      "Regularized Logistic Regression(824/999): loss=0.5818563560214339, w0=-4.282532853593649e-06, w1=0.23503849815309025\n",
      "Regularized Logistic Regression(825/999): loss=0.5818527538221606, w0=-4.284971028510614e-06, w1=0.2350729532965451\n",
      "Regularized Logistic Regression(826/999): loss=0.5818491691410003, w0=-4.287405892852862e-06, w1=0.23510726525255432\n",
      "Regularized Logistic Regression(827/999): loss=0.5818456018791244, w0=-4.289837452433877e-06, w1=0.23514143458925382\n",
      "Regularized Logistic Regression(828/999): loss=0.5818420519383296, w0=-4.292265713053289e-06, w1=0.2351754618725335\n",
      "Regularized Logistic Regression(829/999): loss=0.5818385192210336, w0=-4.294690680496911e-06, w1=0.23520934766604837\n",
      "Regularized Logistic Regression(830/999): loss=0.5818350036302722, w0=-4.2971123605367816e-06, w1=0.23524309253122716\n",
      "Regularized Logistic Regression(831/999): loss=0.5818315050696927, w0=-4.299530758931212e-06, w1=0.23527669702728124\n",
      "Regularized Logistic Regression(832/999): loss=0.5818280234435518, w0=-4.3019458814248244e-06, w1=0.23531016171121338\n",
      "Regularized Logistic Regression(833/999): loss=0.5818245586567099, w0=-4.304357733748596e-06, w1=0.23534348713782718\n",
      "Regularized Logistic Regression(834/999): loss=0.5818211106146284, w0=-4.306766321619903e-06, w1=0.2353766738597375\n",
      "Regularized Logistic Regression(835/999): loss=0.5818176792233641, w0=-4.309171650742557e-06, w1=0.2354097224273757\n",
      "Regularized Logistic Regression(836/999): loss=0.5818142643895662, w0=-4.311573726806853e-06, w1=0.23544263338900198\n",
      "Regularized Logistic Regression(837/999): loss=0.5818108660204723, w0=-4.313972555489609e-06, w1=0.23547540729071204\n",
      "Regularized Logistic Regression(838/999): loss=0.5818074840239031, w0=-4.316368142454205e-06, w1=0.2355080446764479\n",
      "Regularized Logistic Regression(839/999): loss=0.5818041183082596, w0=-4.318760493350628e-06, w1=0.23554054608800468\n",
      "Regularized Logistic Regression(840/999): loss=0.5818007687825196, w0=-4.321149613815512e-06, w1=0.23557291206504083\n",
      "Regularized Logistic Regression(841/999): loss=0.5817974353562319, w0=-4.323535509472176e-06, w1=0.23560514314508516\n",
      "Regularized Logistic Regression(842/999): loss=0.5817941179395139, w0=-4.32591818593067e-06, w1=0.23563723986354748\n",
      "Regularized Logistic Regression(843/999): loss=0.5817908164430472, w0=-4.328297648787814e-06, w1=0.23566920275372588\n",
      "Regularized Logistic Regression(844/999): loss=0.5817875307780734, w0=-4.330673903627235e-06, w1=0.23570103234681544\n",
      "Regularized Logistic Regression(845/999): loss=0.581784260856392, w0=-4.3330469560194135e-06, w1=0.23573272917191823\n",
      "Regularized Logistic Regression(846/999): loss=0.5817810065903539, w0=-4.335416811521719e-06, w1=0.23576429375604863\n",
      "Regularized Logistic Regression(847/999): loss=0.5817777678928601, w0=-4.337783475678452e-06, w1=0.23579572662414514\n",
      "Regularized Logistic Regression(848/999): loss=0.5817745446773558, w0=-4.340146954020884e-06, w1=0.2358270282990762\n",
      "Regularized Logistic Regression(849/999): loss=0.5817713368578283, w0=-4.342507252067297e-06, w1=0.23585819930165106\n",
      "Regularized Logistic Regression(850/999): loss=0.5817681443488041, w0=-4.344864375323023e-06, w1=0.23588924015062646\n",
      "Regularized Logistic Regression(851/999): loss=0.5817649670653415, w0=-4.347218329280486e-06, w1=0.23592015136271513\n",
      "Regularized Logistic Regression(852/999): loss=0.5817618049230308, w0=-4.3495691194192366e-06, w1=0.23595093345259432\n",
      "Regularized Logistic Regression(853/999): loss=0.5817586578379894, w0=-4.351916751205995e-06, w1=0.23598158693291482\n",
      "Regularized Logistic Regression(854/999): loss=0.5817555257268581, w0=-4.354261230094689e-06, w1=0.23601211231430705\n",
      "Regularized Logistic Regression(855/999): loss=0.5817524085067967, w0=-4.356602561526493e-06, w1=0.23604251010539135\n",
      "Regularized Logistic Regression(856/999): loss=0.5817493060954821, w0=-4.3589407509298665e-06, w1=0.23607278081278538\n",
      "Regularized Logistic Regression(857/999): loss=0.581746218411104, w0=-4.3612758037205934e-06, w1=0.2361029249411121\n",
      "Regularized Logistic Regression(858/999): loss=0.5817431453723613, w0=-4.363607725301821e-06, w1=0.2361329429930078\n",
      "Regularized Logistic Regression(859/999): loss=0.5817400868984585, w0=-4.365936521064095e-06, w1=0.2361628354691306\n",
      "Regularized Logistic Regression(860/999): loss=0.581737042909103, w0=-4.368262196385403e-06, w1=0.23619260286816782\n",
      "Regularized Logistic Regression(861/999): loss=0.5817340133245009, w0=-4.370584756631209e-06, w1=0.2362222456868439\n",
      "Regularized Logistic Regression(862/999): loss=0.5817309980653547, w0=-4.372904207154493e-06, w1=0.23625176441992898\n",
      "Regularized Logistic Regression(863/999): loss=0.5817279970528579, w0=-4.375220553295785e-06, w1=0.236281159560247\n",
      "Regularized Logistic Regression(864/999): loss=0.5817250102086945, w0=-4.37753380038321e-06, w1=0.23631043159868279\n",
      "Regularized Logistic Regression(865/999): loss=0.581722037455033, w0=-4.379843953732519e-06, w1=0.23633958102419017\n",
      "Regularized Logistic Regression(866/999): loss=0.5817190787145251, w0=-4.382151018647127e-06, w1=0.23636860832379974\n",
      "Regularized Logistic Regression(867/999): loss=0.581716133910301, w0=-4.384455000418156e-06, w1=0.236397513982626\n",
      "Regularized Logistic Regression(868/999): loss=0.5817132029659668, w0=-4.386755904324465e-06, w1=0.23642629848387658\n",
      "Regularized Logistic Regression(869/999): loss=0.5817102858056026, w0=-4.38905373563269e-06, w1=0.23645496230885885\n",
      "Regularized Logistic Regression(870/999): loss=0.5817073823537563, w0=-4.391348499597283e-06, w1=0.23648350593698877\n",
      "Regularized Logistic Regression(871/999): loss=0.5817044925354429, w0=-4.393640201460545e-06, w1=0.23651192984579564\n",
      "Regularized Logistic Regression(872/999): loss=0.58170161627614, w0=-4.395928846452663e-06, w1=0.2365402345109332\n",
      "Regularized Logistic Regression(873/999): loss=0.581698753501787, w0=-4.398214439791751e-06, w1=0.23656842040618484\n",
      "Regularized Logistic Regression(874/999): loss=0.5816959041387779, w0=-4.400496986683881e-06, w1=0.23659648800347283\n",
      "Regularized Logistic Regression(875/999): loss=0.5816930681139625, w0=-4.40277649232312e-06, w1=0.23662443777286402\n",
      "Regularized Logistic Regression(876/999): loss=0.5816902453546402, w0=-4.405052961891568e-06, w1=0.23665227018257864\n",
      "Regularized Logistic Regression(877/999): loss=0.5816874357885591, w0=-4.407326400559393e-06, w1=0.2366799856989974\n",
      "Regularized Logistic Regression(878/999): loss=0.5816846393439112, w0=-4.409596813484867e-06, w1=0.23670758478666878\n",
      "Regularized Logistic Regression(879/999): loss=0.5816818559493313, w0=-4.411864205814402e-06, w1=0.2367350679083156\n",
      "Regularized Logistic Regression(880/999): loss=0.581679085533892, w0=-4.414128582682584e-06, w1=0.23676243552484585\n",
      "Regularized Logistic Regression(881/999): loss=0.5816763280271027, w0=-4.41638994921221e-06, w1=0.2367896880953532\n",
      "Regularized Logistic Regression(882/999): loss=0.5816735833589054, w0=-4.418648310514321e-06, w1=0.23681682607713234\n",
      "Regularized Logistic Regression(883/999): loss=0.5816708514596716, w0=-4.420903671688241e-06, w1=0.23684384992568017\n",
      "Regularized Logistic Regression(884/999): loss=0.581668132260201, w0=-4.423156037821608e-06, w1=0.23687076009470612\n",
      "Regularized Logistic Regression(885/999): loss=0.581665425691717, w0=-4.425405413990412e-06, w1=0.23689755703613796\n",
      "Regularized Logistic Regression(886/999): loss=0.5816627316858645, w0=-4.4276518052590264e-06, w1=0.23692424120012906\n",
      "Regularized Logistic Regression(887/999): loss=0.5816600501747075, w0=-4.429895216680246e-06, w1=0.23695081303506646\n",
      "Regularized Logistic Regression(888/999): loss=0.581657381090725, w0=-4.4321356532953205e-06, w1=0.2369772729875773\n",
      "Regularized Logistic Regression(889/999): loss=0.5816547243668095, w0=-4.434373120133986e-06, w1=0.23700362150253604\n",
      "Regularized Logistic Regression(890/999): loss=0.5816520799362641, w0=-4.436607622214504e-06, w1=0.2370298590230712\n",
      "Regularized Logistic Regression(891/999): loss=0.5816494477327991, w0=-4.438839164543693e-06, w1=0.23705598599057204\n",
      "Regularized Logistic Regression(892/999): loss=0.5816468276905299, w0=-4.44106775211696e-06, w1=0.23708200284469727\n",
      "Regularized Logistic Regression(893/999): loss=0.5816442197439731, w0=-4.44329338991834e-06, w1=0.2371079100233797\n",
      "Regularized Logistic Regression(894/999): loss=0.5816416238280465, w0=-4.445516082920526e-06, w1=0.2371337079628346\n",
      "Regularized Logistic Regression(895/999): loss=0.5816390398780628, w0=-4.447735836084903e-06, w1=0.23715939709756653\n",
      "Regularized Logistic Regression(896/999): loss=0.58163646782973, w0=-4.449952654361582e-06, w1=0.2371849778603756\n",
      "Regularized Logistic Regression(897/999): loss=0.5816339076191477, w0=-4.452166542689432e-06, w1=0.23721045068236532\n",
      "Regularized Logistic Regression(898/999): loss=0.5816313591828034, w0=-4.454377505996117e-06, w1=0.23723581599294755\n",
      "Regularized Logistic Regression(899/999): loss=0.5816288224575719, w0=-4.456585549198124e-06, w1=0.23726107421985115\n",
      "Regularized Logistic Regression(900/999): loss=0.5816262973807106, w0=-4.458790677200801e-06, w1=0.23728622578912728\n",
      "Regularized Logistic Regression(901/999): loss=0.5816237838898596, w0=-4.460992894898387e-06, w1=0.23731127112515774\n",
      "Regularized Logistic Regression(902/999): loss=0.5816212819230365, w0=-4.463192207174043e-06, w1=0.2373362106506603\n",
      "Regularized Logistic Regression(903/999): loss=0.5816187914186355, w0=-4.465388618899891e-06, w1=0.23736104478669623\n",
      "Regularized Logistic Regression(904/999): loss=0.5816163123154239, w0=-4.467582134937038e-06, w1=0.23738577395267524\n",
      "Regularized Logistic Regression(905/999): loss=0.5816138445525406, w0=-4.469772760135616e-06, w1=0.23741039856636484\n",
      "Regularized Logistic Regression(906/999): loss=0.5816113880694935, w0=-4.471960499334812e-06, w1=0.23743491904389494\n",
      "Regularized Logistic Regression(907/999): loss=0.5816089428061558, w0=-4.474145357362897e-06, w1=0.23745933579976425\n",
      "Regularized Logistic Regression(908/999): loss=0.5816065087027654, w0=-4.476327339037262e-06, w1=0.23748364924684764\n",
      "Regularized Logistic Regression(909/999): loss=0.5816040856999212, w0=-4.478506449164448e-06, w1=0.2375078597964032\n",
      "Regularized Logistic Regression(910/999): loss=0.581601673738581, w0=-4.480682692540178e-06, w1=0.2375319678580767\n",
      "Regularized Logistic Regression(911/999): loss=0.5815992727600591, w0=-4.48285607394939e-06, w1=0.23755597383991048\n",
      "Regularized Logistic Regression(912/999): loss=0.5815968827060244, w0=-4.485026598166268e-06, w1=0.23757987814834758\n",
      "Regularized Logistic Regression(913/999): loss=0.5815945035184976, w0=-4.487194269954272e-06, w1=0.23760368118823916\n",
      "Regularized Logistic Regression(914/999): loss=0.5815921351398492, w0=-4.489359094066171e-06, w1=0.23762738336285183\n",
      "Regularized Logistic Regression(915/999): loss=0.5815897775127964, w0=-4.491521075244074e-06, w1=0.2376509850738718\n",
      "Regularized Logistic Regression(916/999): loss=0.5815874305804017, w0=-4.493680218219462e-06, w1=0.2376744867214137\n",
      "Regularized Logistic Regression(917/999): loss=0.5815850942860706, w0=-4.495836527713216e-06, w1=0.23769788870402495\n",
      "Regularized Logistic Regression(918/999): loss=0.5815827685735484, w0=-4.497990008435653e-06, w1=0.23772119141869152\n",
      "Regularized Logistic Regression(919/999): loss=0.5815804533869188, w0=-4.500140665086551e-06, w1=0.23774439526084706\n",
      "Regularized Logistic Regression(920/999): loss=0.5815781486706018, w0=-4.502288502355184e-06, w1=0.2377675006243756\n",
      "Regularized Logistic Regression(921/999): loss=0.5815758543693508, w0=-4.504433524920352e-06, w1=0.23779050790162037\n",
      "Regularized Logistic Regression(922/999): loss=0.5815735704282511, w0=-4.5065757374504095e-06, w1=0.23781341748338936\n",
      "Regularized Logistic Regression(923/999): loss=0.5815712967927165, w0=-4.508715144603298e-06, w1=0.23783622975896002\n",
      "Regularized Logistic Regression(924/999): loss=0.5815690334084886, w0=-4.510851751026574e-06, w1=0.237858945116086\n",
      "Regularized Logistic Regression(925/999): loss=0.5815667802216342, w0=-4.512985561357443e-06, w1=0.23788156394100507\n",
      "Regularized Logistic Regression(926/999): loss=0.5815645371785431, w0=-4.5151165802227855e-06, w1=0.23790408661844342\n",
      "Regularized Logistic Regression(927/999): loss=0.581562304225925, w0=-4.5172448122391895e-06, w1=0.23792651353162086\n",
      "Regularized Logistic Regression(928/999): loss=0.5815600813108087, w0=-4.519370262012979e-06, w1=0.23794884506225858\n",
      "Regularized Logistic Regression(929/999): loss=0.5815578683805402, w0=-4.521492934140245e-06, w1=0.23797108159058392\n",
      "Regularized Logistic Regression(930/999): loss=0.5815556653827788, w0=-4.523612833206873e-06, w1=0.23799322349533703\n",
      "Regularized Logistic Regression(931/999): loss=0.5815534722654974, w0=-4.525729963788576e-06, w1=0.2380152711537778\n",
      "Regularized Logistic Regression(932/999): loss=0.5815512889769787, w0=-4.527844330450921e-06, w1=0.23803722494168866\n",
      "Regularized Logistic Regression(933/999): loss=0.5815491154658129, w0=-4.5299559377493594e-06, w1=0.23805908523338273\n",
      "Regularized Logistic Regression(934/999): loss=0.5815469516808984, w0=-4.532064790229256e-06, w1=0.23808085240171023\n",
      "Regularized Logistic Regression(935/999): loss=0.5815447975714362, w0=-4.534170892425918e-06, w1=0.23810252681806227\n",
      "Regularized Logistic Regression(936/999): loss=0.5815426530869304, w0=-4.536274248864626e-06, w1=0.23812410885237809\n",
      "Regularized Logistic Regression(937/999): loss=0.5815405181771856, w0=-4.5383748640606585e-06, w1=0.2381455988731504\n",
      "Regularized Logistic Regression(938/999): loss=0.581538392792304, w0=-4.540472742519326e-06, w1=0.2381669972474312\n",
      "Regularized Logistic Regression(939/999): loss=0.5815362768826854, w0=-4.5425678887359945e-06, w1=0.23818830434083765\n",
      "Regularized Logistic Regression(940/999): loss=0.5815341703990221, w0=-4.54466030719612e-06, w1=0.23820952051755656\n",
      "Regularized Logistic Regression(941/999): loss=0.5815320732923016, w0=-4.546750002375272e-06, w1=0.23823064614035186\n",
      "Regularized Logistic Regression(942/999): loss=0.5815299855138001, w0=-4.548836978739164e-06, w1=0.23825168157057025\n",
      "Regularized Logistic Regression(943/999): loss=0.5815279070150826, w0=-4.55092124074368e-06, w1=0.23827262716814446\n",
      "Regularized Logistic Regression(944/999): loss=0.5815258377480022, w0=-4.553002792834909e-06, w1=0.23829348329160038\n",
      "Regularized Logistic Regression(945/999): loss=0.5815237776646959, w0=-4.555081639449162e-06, w1=0.2383142502980644\n",
      "Regularized Logistic Regression(946/999): loss=0.5815217267175841, w0=-4.557157785013012e-06, w1=0.23833492854326566\n",
      "Regularized Logistic Regression(947/999): loss=0.5815196848593688, w0=-4.559231233943313e-06, w1=0.23835551838154387\n",
      "Regularized Logistic Regression(948/999): loss=0.5815176520430309, w0=-4.561301990647233e-06, w1=0.23837602016585335\n",
      "Regularized Logistic Regression(949/999): loss=0.5815156282218295, w0=-4.563370059522278e-06, w1=0.2383964342477702\n",
      "Regularized Logistic Regression(950/999): loss=0.581513613349299, w0=-4.5654354449563245e-06, w1=0.2384167609774974\n",
      "Regularized Logistic Regression(951/999): loss=0.5815116073792485, w0=-4.567498151327641e-06, w1=0.23843700070386825\n",
      "Regularized Logistic Regression(952/999): loss=0.5815096102657586, w0=-4.569558183004921e-06, w1=0.2384571537743535\n",
      "Regularized Logistic Regression(953/999): loss=0.5815076219631811, w0=-4.5716155443473064e-06, w1=0.23847722053506656\n",
      "Regularized Logistic Regression(954/999): loss=0.5815056424261358, w0=-4.573670239704416e-06, w1=0.23849720133076943\n",
      "Regularized Logistic Regression(955/999): loss=0.58150367160951, w0=-4.575722273416375e-06, w1=0.2385170965048771\n",
      "Regularized Logistic Regression(956/999): loss=0.581501709468456, w0=-4.577771649813838e-06, w1=0.2385369063994627\n",
      "Regularized Logistic Regression(957/999): loss=0.58149975595839, w0=-4.579818373218018e-06, w1=0.2385566313552642\n",
      "Regularized Logistic Regression(958/999): loss=0.5814978110349893, w0=-4.581862447940715e-06, w1=0.23857627171168733\n",
      "Regularized Logistic Regression(959/999): loss=0.5814958746541925, w0=-4.583903878284338e-06, w1=0.23859582780681382\n",
      "Regularized Logistic Regression(960/999): loss=0.5814939467721951, w0=-4.585942668541937e-06, w1=0.23861529997740452\n",
      "Regularized Logistic Regression(961/999): loss=0.5814920273454504, w0=-4.5879788229972276e-06, w1=0.2386346885589054\n",
      "Regularized Logistic Regression(962/999): loss=0.5814901163306665, w0=-4.590012345924615e-06, w1=0.23865399388545247\n",
      "Regularized Logistic Regression(963/999): loss=0.5814882136848054, w0=-4.592043241589224e-06, w1=0.23867321628987678\n",
      "Regularized Logistic Regression(964/999): loss=0.5814863193650802, w0=-4.594071514246924e-06, w1=0.2386923561037103\n",
      "Regularized Logistic Regression(965/999): loss=0.5814844333289545, w0=-4.5960971681443546e-06, w1=0.2387114136571903\n",
      "Regularized Logistic Regression(966/999): loss=0.5814825555341406, w0=-4.598120207518952e-06, w1=0.23873038927926452\n",
      "Regularized Logistic Regression(967/999): loss=0.5814806859385976, w0=-4.600140636598976e-06, w1=0.2387492832975978\n",
      "Regularized Logistic Regression(968/999): loss=0.5814788245005302, w0=-4.602158459603535e-06, w1=0.23876809603857443\n",
      "Regularized Logistic Regression(969/999): loss=0.5814769711783867, w0=-4.604173680742612e-06, w1=0.23878682782730498\n",
      "Regularized Logistic Regression(970/999): loss=0.5814751259308576, w0=-4.606186304217091e-06, w1=0.23880547898763238\n",
      "Regularized Logistic Regression(971/999): loss=0.5814732887168739, w0=-4.608196334218781e-06, w1=0.23882404984213262\n",
      "Regularized Logistic Regression(972/999): loss=0.581471459495606, w0=-4.610203774930444e-06, w1=0.23884254071212466\n",
      "Regularized Logistic Regression(973/999): loss=0.5814696382264615, w0=-4.612208630525819e-06, w1=0.23886095191767356\n",
      "Regularized Logistic Regression(974/999): loss=0.5814678248690851, w0=-4.614210905169647e-06, w1=0.23887928377759482\n",
      "Regularized Logistic Regression(975/999): loss=0.5814660193833544, w0=-4.616210603017698e-06, w1=0.2388975366094598\n",
      "Regularized Logistic Regression(976/999): loss=0.5814642217293816, w0=-4.618207728216795e-06, w1=0.2389157107295997\n",
      "Regularized Logistic Regression(977/999): loss=0.5814624318675089, w0=-4.620202284904838e-06, w1=0.2389338064531128\n",
      "Regularized Logistic Regression(978/999): loss=0.5814606497583101, w0=-4.622194277210834e-06, w1=0.238951824093866\n",
      "Regularized Logistic Regression(979/999): loss=0.5814588753625858, w0=-4.624183709254914e-06, w1=0.23896976396450362\n",
      "Regularized Logistic Regression(980/999): loss=0.5814571086413655, w0=-4.6261705851483665e-06, w1=0.23898762637644907\n",
      "Regularized Logistic Regression(981/999): loss=0.581455349555903, w0=-4.628154908993656e-06, w1=0.23900541163991004\n",
      "Regularized Logistic Regression(982/999): loss=0.5814535980676769, w0=-4.630136684884451e-06, w1=0.23902312006388488\n",
      "Regularized Logistic Regression(983/999): loss=0.5814518541383885, w0=-4.632115916905646e-06, w1=0.2390407519561659\n",
      "Regularized Logistic Regression(984/999): loss=0.5814501177299599, w0=-4.634092609133389e-06, w1=0.23905830762334493\n",
      "Regularized Logistic Regression(985/999): loss=0.5814483888045339, w0=-4.6360667656351045e-06, w1=0.23907578737081714\n",
      "Regularized Logistic Regression(986/999): loss=0.5814466673244718, w0=-4.638038390469518e-06, w1=0.2390931915027869\n",
      "Regularized Logistic Regression(987/999): loss=0.5814449532523508, w0=-4.6400074876866805e-06, w1=0.23911052032227273\n",
      "Regularized Logistic Regression(988/999): loss=0.5814432465509654, w0=-4.641974061327993e-06, w1=0.23912777413110964\n",
      "Regularized Logistic Regression(989/999): loss=0.5814415471833229, w0=-4.643938115426228e-06, w1=0.23914495322995596\n",
      "Regularized Logistic Regression(990/999): loss=0.5814398551126446, w0=-4.645899654005559e-06, w1=0.2391620579182979\n",
      "Regularized Logistic Regression(991/999): loss=0.5814381703023636, w0=-4.647858681081578e-06, w1=0.23917908849445213\n",
      "Regularized Logistic Regression(992/999): loss=0.5814364927161225, w0=-4.649815200661326e-06, w1=0.2391960452555733\n",
      "Regularized Logistic Regression(993/999): loss=0.5814348223177731, w0=-4.6517692167433114e-06, w1=0.23921292849765563\n",
      "Regularized Logistic Regression(994/999): loss=0.581433159071375, w0=-4.6537207333175354e-06, w1=0.23922973851553997\n",
      "Regularized Logistic Regression(995/999): loss=0.5814315029411935, w0=-4.655669754365518e-06, w1=0.2392464756029177\n",
      "Regularized Logistic Regression(996/999): loss=0.5814298538917, w0=-4.657616283860318e-06, w1=0.23926314005233437\n",
      "Regularized Logistic Regression(997/999): loss=0.5814282118875675, w0=-4.659560325766558e-06, w1=0.23927973215519457\n",
      "Regularized Logistic Regression(998/999): loss=0.5814265768936738, w0=-4.661501884040451e-06, w1=0.23929625220176626\n",
      "Regularized Logistic Regression(999/999): loss=0.581424948875096, w0=-4.663440962629817e-06, w1=0.239312700481186\n",
      "Fold 3 completed.\n",
      "Regularized Logistic Regression(0/999): loss=0.6931471805599448, w0=-1.114608504908736e-08, w1=0.0009351565356184294\n",
      "Regularized Logistic Regression(1/999): loss=0.6922294541035326, w0=-2.225705392193722e-08, w1=0.0018677218005680942\n",
      "Regularized Logistic Regression(2/999): loss=0.691320385633213, w0=-3.3333125897030855e-08, w1=0.0027976840390327115\n",
      "Regularized Logistic Regression(3/999): loss=0.6904198740340913, w0=-4.43745182253036e-08, w1=0.0037250318961919915\n",
      "Regularized Logistic Regression(4/999): loss=0.6895278197557938, w0=-5.53814461466537e-08, w1=0.004649754411936883\n",
      "Regularized Logistic Regression(5/999): loss=0.6886441247819559, w0=-6.63541229065558e-08, w1=0.005571841014651611\n",
      "Regularized Logistic Regression(6/999): loss=0.6877686926003427, w0=-7.729275977276709e-08, w1=0.006491281515056601\n",
      "Regularized Logistic Regression(7/999): loss=0.6869014281735812, w0=-8.819756605211416e-08, w1=0.0074080661001165865\n",
      "Regularized Logistic Regression(8/999): loss=0.686042237910501, w0=-9.906874910734921e-08, w1=0.008322185327017555\n",
      "Regularized Logistic Regression(9/999): loss=0.6851910296380684, w0=-1.0990651437406464e-07, w1=0.00923363011720335\n",
      "Regularized Logistic Regression(10/999): loss=0.6843477125739091, w0=-1.2071106537765562e-07, w1=0.010142391750480817\n",
      "Regularized Logistic Regression(11/999): loss=0.6835121972994024, w0=-1.314826037503206e-07, w1=0.011048461859187123\n",
      "Regularized Logistic Regression(12/999): loss=0.6826843957333398, w0=-1.4222132924808996e-07, w1=0.011951832422424669\n",
      "Regularized Logistic Regression(13/999): loss=0.6818642211061519, w0=-1.5292743976787397e-07, w1=0.01285249576035826\n",
      "Regularized Logistic Regression(14/999): loss=0.6810515879346646, w0=-1.636011313645211e-07, w1=0.013750444528578109\n",
      "Regularized Logistic Regression(15/999): loss=0.6802464119974007, w0=-1.7424259826787804e-07, w1=0.014645671712526476\n",
      "Regularized Logistic Regression(16/999): loss=0.6794486103104085, w0=-1.8485203289984397e-07, w1=0.015538170621989315\n",
      "Regularized Logistic Regression(17/999): loss=0.6786581011036013, w0=-1.9542962589141076e-07, w1=0.016427934885652556\n",
      "Regularized Logistic Regression(18/999): loss=0.6778748037976094, w0=-2.0597556609968236e-07, w1=0.017314958445719158\n",
      "Regularized Logistic Regression(19/999): loss=0.6770986389811227, w0=-2.1649004062486578e-07, w1=0.018199235552595682\n",
      "Regularized Logistic Regression(20/999): loss=0.6763295283887273, w0=-2.2697323482722776e-07, w1=0.019080760759635943\n",
      "Regularized Logistic Regression(21/999): loss=0.6755673948792169, w0=-2.3742533234401024e-07, w1=0.0199595289179525\n",
      "Regularized Logistic Regression(22/999): loss=0.6748121624143731, w0=-2.4784651510629887e-07, w1=0.020835535171288243\n",
      "Regularized Logistic Regression(23/999): loss=0.6740637560382015, w0=-2.5823696335583855e-07, w1=0.021708774950953014\n",
      "Regularized Logistic Regression(24/999): loss=0.6733221018566264, w0=-2.685968556617909e-07, w1=0.022579243970821176\n",
      "Regularized Logistic Regression(25/999): loss=0.6725871270176178, w0=-2.789263689374285e-07, w1=0.023446938222389597\n",
      "Regularized Logistic Regression(26/999): loss=0.6718587596917536, w0=-2.892256784567605e-07, w1=0.024311853969900956\n",
      "Regularized Logistic Regression(27/999): loss=0.6711369290532053, w0=-2.994949578710856e-07, w1=0.025173987745526028\n",
      "Regularized Logistic Regression(28/999): loss=0.6704215652611393, w0=-3.097343792254676e-07, w1=0.02603333634460798\n",
      "Regularized Logistic Regression(29/999): loss=0.669712599441518, w0=-3.1994411297512956e-07, w1=0.026889896820965598\n",
      "Regularized Logistic Regression(30/999): loss=0.6690099636693007, w0=-3.301243280017623e-07, w1=0.02774366648225996\n",
      "Regularized Logistic Regression(31/999): loss=0.6683135909510413, w0=-3.402751916297442e-07, w1=0.028594642885416743\n",
      "Regularized Logistic Regression(32/999): loss=0.6676234152078494, w0=-3.5039686964226833e-07, w1=0.029442823832112223\n",
      "Regularized Logistic Regression(33/999): loss=0.6669393712587387, w0=-3.6048952629737336e-07, w1=0.030288207364314663\n",
      "Regularized Logistic Regression(34/999): loss=0.6662613948043307, w0=-3.70553324343876e-07, w1=0.031130791759887585\n",
      "Regularized Logistic Regression(35/999): loss=0.6655894224109163, w0=-3.8058842503720116e-07, w1=0.0319705755282489\n",
      "Regularized Logistic Regression(36/999): loss=0.6649233914948688, w0=-3.905949881551082e-07, w1=0.03280755740608871\n",
      "Regularized Logistic Regression(37/999): loss=0.6642632403073926, w0=-4.0057317201330923e-07, w1=0.03364173635314421\n",
      "Regularized Logistic Regression(38/999): loss=0.6636089079196078, w0=-4.105231334809788e-07, w1=0.03447311154803192\n",
      "Regularized Logistic Regression(39/999): loss=0.66296033420796, w0=-4.2044502799615156e-07, w1=0.03530168238413496\n",
      "Regularized Logistic Regression(40/999): loss=0.6623174598399477, w0=-4.3033900958100624e-07, w1=0.03612744846554995\n",
      "Regularized Logistic Regression(41/999): loss=0.6616802262601622, w0=-4.4020523085703424e-07, w1=0.03695040960308314\n",
      "Regularized Logistic Regression(42/999): loss=0.661048575676628, w0=-4.5004384306009073e-07, w1=0.03777056581030858\n",
      "Regularized Logistic Regression(43/999): loss=0.6604224510474453, w0=-4.598549960553271e-07, w1=0.03858791729967629\n",
      "Regularized Logistic Regression(44/999): loss=0.6598017960677208, w0=-4.696388383520028e-07, w1=0.039402464478674994\n",
      "Regularized Logistic Regression(45/999): loss=0.65918655515678, w0=-4.793955171181757e-07, w1=0.040214207946048484\n",
      "Regularized Logistic Regression(46/999): loss=0.6585766734456605, w0=-4.891251781952691e-07, w1=0.041023148488069226\n",
      "Regularized Logistic Regression(47/999): loss=0.657972096764875, w0=-4.988279661125155e-07, w1=0.041829287074857516\n",
      "Regularized Logistic Regression(48/999): loss=0.6573727716324379, w0=-5.085040241012743e-07, w1=0.042632624856758804\n",
      "Regularized Logistic Regression(49/999): loss=0.6567786452421516, w0=-5.181534941092247e-07, w1=0.043433163160768534\n",
      "Regularized Logistic Regression(50/999): loss=0.6561896654521454, w0=-5.277765168144312e-07, w1=0.04423090348700989\n",
      "Regularized Logistic Regression(51/999): loss=0.6556057807736592, w0=-5.373732316392817e-07, w1=0.04502584750526455\n",
      "Regularized Logistic Regression(52/999): loss=0.6550269403600709, w0=-5.469437767642987e-07, w1=0.04581799705154693\n",
      "Regularized Logistic Regression(53/999): loss=0.654453093996159, w0=-5.564882891418205e-07, w1=0.046607354124736315\n",
      "Regularized Logistic Regression(54/999): loss=0.6538841920875912, w0=-5.660069045095552e-07, w1=0.047393920883250756\n",
      "Regularized Logistic Regression(55/999): loss=0.6533201856506402, w0=-5.754997574040044e-07, w1=0.04817769964177533\n",
      "Regularized Logistic Regression(56/999): loss=0.6527610263021216, w0=-5.849669811737581e-07, w1=0.048958692868034316\n",
      "Regularized Logistic Regression(57/999): loss=0.6522066662495387, w0=-5.944087079926598e-07, w1=0.049736903179614815\n",
      "Regularized Logistic Regression(58/999): loss=0.6516570582814419, w0=-6.038250688728424e-07, w1=0.05051233334083229\n",
      "Regularized Logistic Regression(59/999): loss=0.6511121557579915, w0=-6.132161936776339e-07, w1=0.0512849862596489\n",
      "Regularized Logistic Regression(60/999): loss=0.6505719126017147, w0=-6.22582211134334e-07, w1=0.0520548649846325\n",
      "Regularized Logistic Regression(61/999): loss=0.6500362832884625, w0=-6.31923248846861e-07, w1=0.05282197270196662\n",
      "Regularized Logistic Regression(62/999): loss=0.6495052228385566, w0=-6.412394333082694e-07, w1=0.05358631273249747\n",
      "Regularized Logistic Regression(63/999): loss=0.648978686808117, w0=-6.505308899131381e-07, w1=0.05434788852883574\n",
      "Regularized Logistic Regression(64/999): loss=0.6484566312805696, w0=-6.597977429698306e-07, w1=0.055106703672495115\n",
      "Regularized Logistic Regression(65/999): loss=0.6479390128583424, w0=-6.690401157126256e-07, w1=0.05586276187107716\n",
      "Regularized Logistic Regression(66/999): loss=0.6474257886547157, w0=-6.782581303137205e-07, w1=0.056616066955497764\n",
      "Regularized Logistic Regression(67/999): loss=0.646916916285857, w0=-6.874519078951067e-07, w1=0.0573666228772582\n",
      "Regularized Logistic Regression(68/999): loss=0.6464123538630094, w0=-6.966215685403176e-07, w1=0.05811443370575665\n",
      "Regularized Logistic Regression(69/999): loss=0.6459120599848458, w0=-7.057672313060501e-07, w1=0.05885950362564297\n",
      "Regularized Logistic Regression(70/999): loss=0.6454159937299773, w0=-7.148890142336597e-07, w1=0.059601836934212886\n",
      "Regularized Logistic Regression(71/999): loss=0.644924114649613, w0=-7.239870343605308e-07, w1=0.06034143803884402\n",
      "Regularized Logistic Regression(72/999): loss=0.6444363827603761, w0=-7.330614077313207e-07, w1=0.061078311454470756\n",
      "Regularized Logistic Regression(73/999): loss=0.6439527585372531, w0=-7.421122494090803e-07, w1=0.06181246180110165\n",
      "Regularized Logistic Regression(74/999): loss=0.6434732029066969, w0=-7.511396734862509e-07, w1=0.0625438938013718\n",
      "Regularized Logistic Regression(75/999): loss=0.6429976772398623, w0=-7.601437930955382e-07, w1=0.06327261227813712\n",
      "Regularized Logistic Regression(76/999): loss=0.6425261433459798, w0=-7.691247204206635e-07, w1=0.06399862215210651\n",
      "Regularized Logistic Regression(77/999): loss=0.6420585634658558, w0=-7.780825667069947e-07, w1=0.06472192843950955\n",
      "Regularized Logistic Regression(78/999): loss=0.6415949002655105, w0=-7.870174422720551e-07, w1=0.06544253624980566\n",
      "Regularized Logistic Regression(79/999): loss=0.6411351168299357, w0=-7.959294565159143e-07, w1=0.06616045078342586\n",
      "Regularized Logistic Regression(80/999): loss=0.6406791766569757, w0=-8.048187179314582e-07, w1=0.06687567732955309\n",
      "Regularized Logistic Regression(81/999): loss=0.640227043651331, w0=-8.136853341145422e-07, w1=0.06758822126393743\n",
      "Regularized Logistic Regression(82/999): loss=0.6397786821186764, w0=-8.225294117740268e-07, w1=0.06829808804674785\n",
      "Regularized Logistic Regression(83/999): loss=0.6393340567598957, w0=-8.313510567416964e-07, w1=0.0690052832204599\n",
      "Regularized Logistic Regression(84/999): loss=0.638893132665425, w0=-8.401503739820633e-07, w1=0.06970981240777303\n",
      "Regularized Logistic Regression(85/999): loss=0.6384558753097106, w0=-8.489274676020573e-07, w1=0.0704116813095685\n",
      "Regularized Logistic Regression(86/999): loss=0.6380222505457647, w0=-8.576824408606003e-07, w1=0.07111089570289734\n",
      "Regularized Logistic Regression(87/999): loss=0.6375922245998338, w0=-8.664153961780703e-07, w1=0.07180746143900045\n",
      "Regularized Logistic Regression(88/999): loss=0.6371657640661614, w0=-8.751264351456519e-07, w1=0.07250138444136546\n",
      "Regularized Logistic Regression(89/999): loss=0.6367428359018574, w0=-8.838156585345774e-07, w1=0.07319267070381202\n",
      "Regularized Logistic Regression(90/999): loss=0.6363234074218552, w0=-8.92483166305257e-07, w1=0.07388132628861165\n",
      "Regularized Logistic Regression(91/999): loss=0.6359074462939699, w0=-9.011290576163015e-07, w1=0.0745673573246368\n",
      "Regularized Logistic Regression(92/999): loss=0.6354949205340471, w0=-9.097534308334359e-07, w1=0.07525077000554313\n",
      "Regularized Logistic Regression(93/999): loss=0.6350857985012001, w0=-9.183563835383069e-07, w1=0.07593157058798185\n",
      "Regularized Logistic Regression(94/999): loss=0.6346800488931362, w0=-9.269380125371852e-07, w1=0.07660976538984043\n",
      "Regularized Logistic Regression(95/999): loss=0.6342776407415726, w0=-9.354984138695616e-07, w1=0.07728536078851668\n",
      "Regularized Logistic Regression(96/999): loss=0.6338785434077274, w0=-9.440376828166405e-07, w1=0.07795836321921848\n",
      "Regularized Logistic Regression(97/999): loss=0.6334827265779011, w0=-9.525559139097303e-07, w1=0.07862877917329388\n",
      "Regularized Logistic Regression(98/999): loss=0.6330901602591338, w0=-9.610532009385316e-07, w1=0.07929661519659384\n",
      "Regularized Logistic Regression(99/999): loss=0.6327008147749429, w0=-9.69529636959326e-07, w1=0.07996187788785476\n",
      "Regularized Logistic Regression(100/999): loss=0.6323146607611356, w0=-9.779853143030634e-07, w1=0.08062457389711761\n",
      "Regularized Logistic Regression(101/999): loss=0.6319316691616957, w0=-9.864203245833523e-07, w1=0.08128470992416624\n",
      "Regularized Logistic Regression(102/999): loss=0.6315518112247465, w0=-9.948347587043512e-07, w1=0.08194229271700157\n",
      "Regularized Logistic Regression(103/999): loss=0.6311750584985792, w0=-1.003228706868564e-06, w1=0.08259732907033442\n",
      "Regularized Logistic Regression(104/999): loss=0.6308013828277588, w0=-1.01160225858454e-06, w1=0.08324982582411031\n",
      "Regularized Logistic Regression(105/999): loss=0.6304307563492905, w0=-1.0199555026744787e-06, w1=0.08389978986205554\n",
      "Regularized Logistic Regression(106/999): loss=0.6300631514888586, w0=-1.0282885272817411e-06, w1=0.0845472281102539\n",
      "Regularized Logistic Regression(107/999): loss=0.6296985409571257, w0=-1.036601419878269e-06, w1=0.08519214753574479\n",
      "Regularized Logistic Regression(108/999): loss=0.6293368977461004, w0=-1.044894267271913e-06, w1=0.0858345551451453\n",
      "Regularized Logistic Regression(109/999): loss=0.6289781951255607, w0=-1.0531671556136679e-06, w1=0.08647445798330193\n",
      "Regularized Logistic Regression(110/999): loss=0.6286224066395453, w0=-1.0614201704048212e-06, w1=0.08711186313196052\n",
      "Regularized Logistic Regression(111/999): loss=0.6282695061028998, w0=-1.0696533965040115e-06, w1=0.08774677770846487\n",
      "Regularized Logistic Regression(112/999): loss=0.6279194675978821, w0=-1.0778669181342002e-06, w1=0.08837920886447447\n",
      "Regularized Logistic Regression(113/999): loss=0.6275722654708242, w0=-1.0860608188895555e-06, w1=0.08900916378470906\n",
      "Regularized Logistic Regression(114/999): loss=0.6272278743288506, w0=-1.0942351817422523e-06, w1=0.08963664968571121\n",
      "Regularized Logistic Regression(115/999): loss=0.6268862690366513, w0=-1.1023900890491865e-06, w1=0.0902616738146384\n",
      "Regularized Logistic Regression(116/999): loss=0.6265474247133057, w0=-1.110525622558606e-06, w1=0.09088424344807038\n",
      "Regularized Logistic Regression(117/999): loss=0.626211316729161, w0=-1.1186418634166592e-06, w1=0.09150436589084072\n",
      "Regularized Logistic Regression(118/999): loss=0.6258779207027597, w0=-1.126738892173862e-06, w1=0.09212204847489246\n",
      "Regularized Logistic Regression(119/999): loss=0.6255472124978187, w0=-1.1348167887914833e-06, w1=0.09273729855814812\n",
      "Regularized Logistic Regression(120/999): loss=0.6252191682202534, w0=-1.1428756326478519e-06, w1=0.09335012352340995\n",
      "Regularized Logistic Regression(121/999): loss=0.6248937642152564, w0=-1.1509155025445837e-06, w1=0.09396053077727001\n",
      "Regularized Logistic Regression(122/999): loss=0.6245709770644144, w0=-1.158936476712732e-06, w1=0.09456852774905057\n",
      "Regularized Logistic Regression(123/999): loss=0.6242507835828771, w0=-1.1669386328188595e-06, w1=0.0951741218897572\n",
      "Regularized Logistic Regression(124/999): loss=0.6239331608165668, w0=-1.1749220479710356e-06, w1=0.09577732067105314\n",
      "Regularized Logistic Regression(125/999): loss=0.6236180860394352, w0=-1.182886798724758e-06, w1=0.0963781315842566\n",
      "Regularized Logistic Regression(126/999): loss=0.6233055367507602, w0=-1.1908329610888e-06, w1=0.09697656213934985\n",
      "Regularized Logistic Regression(127/999): loss=0.6229954906724857, w0=-1.198760610530986e-06, w1=0.09757261986401526\n",
      "Regularized Logistic Regression(128/999): loss=0.6226879257466029, w0=-1.206669821983892e-06, w1=0.09816631230268325\n",
      "Regularized Logistic Regression(129/999): loss=0.6223828201325688, w0=-1.214560669850477e-06, w1=0.09875764701560047\n",
      "Regularized Logistic Regression(130/999): loss=0.6220801522047721, w0=-1.2224332280096428e-06, w1=0.09934663157791807\n",
      "Regularized Logistic Regression(131/999): loss=0.6217799005500266, w0=-1.2302875698217245e-06, w1=0.09993327357879389\n",
      "Regularized Logistic Regression(132/999): loss=0.621482043965109, w0=-1.2381237681339115e-06, w1=0.10051758062051423\n",
      "Regularized Logistic Regression(133/999): loss=0.6211865614543344, w0=-1.2459418952856016e-06, w1=0.10109956031763168\n",
      "Regularized Logistic Regression(134/999): loss=0.6208934322271648, w0=-1.2537420231136865e-06, w1=0.10167922029611969\n",
      "Regularized Logistic Regression(135/999): loss=0.6206026356958554, w0=-1.2615242229577729e-06, w1=0.1022565681925444\n",
      "Regularized Logistic Regression(136/999): loss=0.6203141514731334, w0=-1.2692885656653365e-06, w1=0.10283161165325222\n",
      "Regularized Logistic Regression(137/999): loss=0.6200279593699132, w0=-1.2770351215968125e-06, w1=0.10340435833357177\n",
      "Regularized Logistic Regression(138/999): loss=0.6197440393930439, w0=-1.284763960630621e-06, w1=0.1039748158970366\n",
      "Regularized Logistic Regression(139/999): loss=0.6194623717430905, w0=-1.2924751521681316e-06, w1=0.10454299201461667\n",
      "Regularized Logistic Regression(140/999): loss=0.6191829368121455, w0=-1.300168765138563e-06, w1=0.10510889436396822\n",
      "Regularized Logistic Regression(141/999): loss=0.6189057151816731, w0=-1.3078448680038247e-06, w1=0.10567253062870101\n",
      "Regularized Logistic Regression(142/999): loss=0.6186306876203859, w0=-1.3155035287632938e-06, w1=0.10623390849765663\n",
      "Regularized Logistic Regression(143/999): loss=0.6183578350821465, w0=-1.3231448149585364e-06, w1=0.10679303566420202\n",
      "Regularized Logistic Regression(144/999): loss=0.6180871387039074, w0=-1.3307687936779667e-06, w1=0.10734991982553804\n",
      "Regularized Logistic Regression(145/999): loss=0.617818579803671, w0=-1.338375531561449e-06, w1=0.10790456868202324\n",
      "Regularized Logistic Regression(146/999): loss=0.6175521398784839, w0=-1.3459650948048432e-06, w1=0.1084569899365088\n",
      "Regularized Logistic Regression(147/999): loss=0.6172878006024578, w0=-1.3535375491644915e-06, w1=0.10900719129368946\n",
      "Regularized Logistic Regression(148/999): loss=0.6170255438248182, w0=-1.36109295996165e-06, w1=0.10955518045946423\n",
      "Regularized Logistic Regression(149/999): loss=0.6167653515679786, w0=-1.3686313920868663e-06, w1=0.11010096514031603\n",
      "Regularized Logistic Regression(150/999): loss=0.6165072060256424, w0=-1.3761529100042997e-06, w1=0.11064455304269954\n",
      "Regularized Logistic Regression(151/999): loss=0.6162510895609304, w0=-1.38365757775599e-06, w1=0.11118595187244246\n",
      "Regularized Logistic Regression(152/999): loss=0.6159969847045355, w0=-1.3911454589660723e-06, w1=0.11172516933416149\n",
      "Regularized Logistic Regression(153/999): loss=0.6157448741528995, w0=-1.3986166168449384e-06, w1=0.11226221313068747\n",
      "Regularized Logistic Regression(154/999): loss=0.6154947407664161, w0=-1.4060711141933484e-06, w1=0.11279709096250679\n",
      "Regularized Logistic Regression(155/999): loss=0.6152465675676584, w0=-1.413509013406489e-06, w1=0.11332981052721065\n",
      "Regularized Logistic Regression(156/999): loss=0.6150003377396303, w0=-1.4209303764779831e-06, w1=0.11386037951895694\n",
      "Regularized Logistic Regression(157/999): loss=0.6147560346240384, w0=-1.4283352650038486e-06, w1=0.11438880562794737\n",
      "Regularized Logistic Regression(158/999): loss=0.6145136417195911, w0=-1.4357237401864085e-06, w1=0.11491509653990942\n",
      "Regularized Logistic Regression(159/999): loss=0.6142731426803164, w0=-1.4430958628381525e-06, w1=0.11543925993559469\n",
      "Regularized Logistic Regression(160/999): loss=0.6140345213139049, w0=-1.450451693385551e-06, w1=0.11596130349028727\n",
      "Regularized Logistic Regression(161/999): loss=0.6137977615800708, w0=-1.4577912918728216e-06, w1=0.11648123487332106\n",
      "Regularized Logistic Regression(162/999): loss=0.6135628475889385, w0=-1.4651147179656486e-06, w1=0.1169990617476078\n",
      "Regularized Logistic Regression(163/999): loss=0.6133297635994465, w0=-1.4724220309548574e-06, w1=0.1175147917691787\n",
      "Regularized Logistic Regression(164/999): loss=0.6130984940177726, w0=-1.4797132897600437e-06, w1=0.11802843258673293\n",
      "Regularized Logistic Regression(165/999): loss=0.612869023395783, w0=-1.486988552933157e-06, w1=0.11853999184119647\n",
      "Regularized Logistic Regression(166/999): loss=0.6126413364294944, w0=-1.4942478786620408e-06, w1=0.11904947716529189\n",
      "Regularized Logistic Regression(167/999): loss=0.6124154179575633, w0=-1.5014913247739283e-06, w1=0.11955689618312086\n",
      "Regularized Logistic Regression(168/999): loss=0.6121912529597872, w0=-1.5087189487388969e-06, w1=0.120062256509748\n",
      "Regularized Logistic Regression(169/999): loss=0.6119688265556309, w0=-1.5159308076732781e-06, w1=0.12056556575080318\n",
      "Regularized Logistic Regression(170/999): loss=0.6117481240027667, w0=-1.5231269583430278e-06, w1=0.12106683150208654\n",
      "Regularized Logistic Regression(171/999): loss=0.6115291306956372, w0=-1.5303074571670531e-06, w1=0.12156606134918692\n",
      "Regularized Logistic Regression(172/999): loss=0.6113118321640313, w0=-1.5374723602205005e-06, w1=0.12206326286710552\n",
      "Regularized Logistic Regression(173/999): loss=0.6110962140716808, w0=-1.5446217232380022e-06, w1=0.12255844361989222\n",
      "Regularized Logistic Regression(174/999): loss=0.6108822622148745, w0=-1.551755601616884e-06, w1=0.12305161116028725\n",
      "Regularized Logistic Regression(175/999): loss=0.6106699625210903, w0=-1.5588740504203339e-06, w1=0.12354277302937278\n",
      "Regularized Logistic Regression(176/999): loss=0.6104593010476376, w0=-1.565977124380531e-06, w1=0.12403193675623304\n",
      "Regularized Logistic Regression(177/999): loss=0.6102502639803257, w0=-1.5730648779017381e-06, w1=0.12451910985762231\n",
      "Regularized Logistic Regression(178/999): loss=0.6100428376321433, w0=-1.5801373650633558e-06, w1=0.12500429983764036\n",
      "Regularized Logistic Regression(179/999): loss=0.6098370084419525, w0=-1.587194639622939e-06, w1=0.12548751418741785\n",
      "Regularized Logistic Regression(180/999): loss=0.6096327629732036, w0=-1.5942367550191783e-06, w1=0.1259687603848048\n",
      "Regularized Logistic Regression(181/999): loss=0.6094300879126602, w0=-1.6012637643748441e-06, w1=0.1264480458940729\n",
      "Regularized Logistic Regression(182/999): loss=0.6092289700691438, w0=-1.608275720499696e-06, w1=0.12692537816562252\n",
      "Regularized Logistic Regression(183/999): loss=0.6090293963722898, w0=-1.6152726758933566e-06, w1=0.12740076463569325\n",
      "Regularized Logistic Regression(184/999): loss=0.6088313538713223, w0=-1.6222546827481508e-06, w1=0.12787421272608931\n",
      "Regularized Logistic Regression(185/999): loss=0.6086348297338398, w0=-1.6292217929519107e-06, w1=0.12834572984390608\n",
      "Regularized Logistic Regression(186/999): loss=0.6084398112446168, w0=-1.6361740580907477e-06, w1=0.12881532338126575\n",
      "Regularized Logistic Regression(187/999): loss=0.6082462858044206, w0=-1.6431115294517906e-06, w1=0.12928300071505958\n",
      "Regularized Logistic Regression(188/999): loss=0.6080542409288404, w0=-1.650034258025891e-06, w1=0.12974876920669828\n",
      "Regularized Logistic Regression(189/999): loss=0.60786366424713, w0=-1.6569422945102966e-06, w1=0.13021263620186502\n",
      "Regularized Logistic Regression(190/999): loss=0.6076745435010664, w0=-1.6638356893112916e-06, w1=0.13067460903028025\n",
      "Regularized Logistic Regression(191/999): loss=0.607486866543818, w0=-1.6707144925468083e-06, w1=0.13113469500546832\n",
      "Regularized Logistic Regression(192/999): loss=0.6073006213388291, w0=-1.677578754049004e-06, w1=0.13159290142453436\n",
      "Regularized Logistic Regression(193/999): loss=0.6071157959587175, w0=-1.6844285233668103e-06, w1=0.13204923556794274\n",
      "Regularized Logistic Regression(194/999): loss=0.606932378584181, w0=-1.6912638497684502e-06, w1=0.13250370469930622\n",
      "Regularized Logistic Regression(195/999): loss=0.60675035750292, w0=-1.6980847822439264e-06, w1=0.1329563160651787\n",
      "Regularized Logistic Regression(196/999): loss=0.6065697211085724, w0=-1.7048913695074806e-06, w1=0.13340707689485162\n",
      "Regularized Logistic Regression(197/999): loss=0.6063904578996588, w0=-1.711683660000022e-06, w1=0.13385599440016166\n",
      "Regularized Logistic Regression(198/999): loss=0.6062125564785411, w0=-1.7184617018915295e-06, w1=0.13430307577529838\n",
      "Regularized Logistic Regression(199/999): loss=0.6060360055503915, w0=-1.7252255430834228e-06, w1=0.13474832819662283\n",
      "Regularized Logistic Regression(200/999): loss=0.6058607939221757, w0=-1.7319752312109094e-06, w1=0.1351917588224841\n",
      "Regularized Logistic Regression(201/999): loss=0.6056869105016457, w0=-1.7387108136453003e-06, w1=0.13563337479304993\n",
      "Regularized Logistic Regression(202/999): loss=0.6055143442963441, w0=-1.745432337496301e-06, w1=0.13607318323013604\n",
      "Regularized Logistic Regression(203/999): loss=0.6053430844126193, w0=-1.7521398496142754e-06, w1=0.13651119123704383\n",
      "Regularized Logistic Regression(204/999): loss=0.6051731200546538, w0=-1.758833396592482e-06, w1=0.13694740589840135\n",
      "Regularized Logistic Regression(205/999): loss=0.605004440523501, w0=-1.7655130247692858e-06, w1=0.1373818342800103\n",
      "Regularized Logistic Regression(206/999): loss=0.604837035216133, w0=-1.7721787802303435e-06, w1=0.13781448342869745\n",
      "Regularized Logistic Regression(207/999): loss=0.6046708936245018, w0=-1.7788307088107634e-06, w1=0.1382453603721707\n",
      "Regularized Logistic Regression(208/999): loss=0.6045060053346065, w0=-1.7854688560972406e-06, w1=0.1386744721188811\n",
      "Regularized Logistic Regression(209/999): loss=0.6043423600255741, w0=-1.7920932674301678e-06, w1=0.13910182565788615\n",
      "Regularized Logistic Regression(210/999): loss=0.6041799474687496, w0=-1.7987039879057203e-06, w1=0.13952742795872178\n",
      "Regularized Logistic Regression(211/999): loss=0.6040187575267956, w0=-1.805301062377919e-06, w1=0.1399512859712761\n",
      "Regularized Logistic Regression(212/999): loss=0.6038587801528021, w0=-1.8118845354606685e-06, w1=0.14037340662566833\n",
      "Regularized Logistic Regression(213/999): loss=0.6037000053894073, w0=-1.8184544515297722e-06, w1=0.14079379683212973\n",
      "Regularized Logistic Regression(214/999): loss=0.6035424233679249, w0=-1.825010854724924e-06, w1=0.14121246348089503\n",
      "Regularized Logistic Regression(215/999): loss=0.6033860243074847, w0=-1.8315537889516771e-06, w1=0.14162941344208904\n",
      "Regularized Logistic Regression(216/999): loss=0.6032307985141794, w0=-1.8380832978833917e-06, w1=0.14204465356562457\n",
      "Regularized Logistic Regression(217/999): loss=0.6030767363802232, w0=-1.844599424963159e-06, w1=0.14245819068109966\n",
      "Regularized Logistic Regression(218/999): loss=0.6029238283831178, w0=-1.8511022134057032e-06, w1=0.1428700315977021\n",
      "Regularized Logistic Regression(219/999): loss=0.6027720650848274, w0=-1.857591706199264e-06, w1=0.14328018310411675\n",
      "Regularized Logistic Regression(220/999): loss=0.6026214371309644, w0=-1.864067946107456e-06, w1=0.1436886519684342\n",
      "Regularized Logistic Regression(221/999): loss=0.6024719352499815, w0=-1.8705309756711067e-06, w1=0.14409544493806573\n",
      "Regularized Logistic Regression(222/999): loss=0.6023235502523745, w0=-1.876980837210076e-06, w1=0.14450056873966044\n",
      "Regularized Logistic Regression(223/999): loss=0.6021762730298921, w0=-1.8834175728250531e-06, w1=0.14490403007902625\n",
      "Regularized Logistic Regression(224/999): loss=0.6020300945547556, w0=-1.8898412243993342e-06, w1=0.14530583564105592\n",
      "Regularized Logistic Regression(225/999): loss=0.6018850058788856, w0=-1.8962518336005803e-06, w1=0.1457059920896548\n",
      "Regularized Logistic Regression(226/999): loss=0.6017409981331382, w0=-1.9026494418825549e-06, w1=0.14610450606766862\n",
      "Regularized Logistic Regression(227/999): loss=0.6015980625265471, w0=-1.9090340904868433e-06, w1=0.1465013841968225\n",
      "Regularized Logistic Regression(228/999): loss=0.6014561903455778, w0=-1.915405820444551e-06, w1=0.1468966330776562\n",
      "Regularized Logistic Regression(229/999): loss=0.601315372953385, w0=-1.921764672577986e-06, w1=0.147290259289465\n",
      "Regularized Logistic Regression(230/999): loss=0.6011756017890808, w0=-1.92811068750232e-06, w1=0.14768226939024443\n",
      "Regularized Logistic Regression(231/999): loss=0.6010368683670104, w0=-1.9344439056272304e-06, w1=0.14807266991663487\n",
      "Regularized Logistic Regression(232/999): loss=0.6008991642760352, w0=-1.9407643671585296e-06, w1=0.14846146738387328\n",
      "Regularized Logistic Regression(233/999): loss=0.6007624811788216, w0=-1.9470721120997686e-06, w1=0.1488486682857455\n",
      "Regularized Logistic Regression(234/999): loss=0.6006268108111412, w0=-1.9533671802538305e-06, w1=0.14923427909454068\n",
      "Regularized Logistic Regression(235/999): loss=0.6004921449811739, w0=-1.959649611224501e-06, w1=0.14961830626100972\n",
      "Regularized Logistic Regression(236/999): loss=0.6003584755688206, w0=-1.9659194444180234e-06, w1=0.15000075621432632\n",
      "Regularized Logistic Regression(237/999): loss=0.6002257945250258, w0=-1.9721767190446394e-06, w1=0.1503816353620495\n",
      "Regularized Logistic Regression(238/999): loss=0.6000940938710995, w0=-1.978421474120108e-06, w1=0.15076095009009044\n",
      "Regularized Logistic Regression(239/999): loss=0.5999633656980561, w0=-1.9846537484672134e-06, w1=0.15113870676268035\n",
      "Regularized Logistic Regression(240/999): loss=0.5998336021659509, w0=-1.990873580717251e-06, w1=0.1515149117223422\n",
      "Regularized Logistic Regression(241/999): loss=0.5997047955032309, w0=-1.9970810093115027e-06, w1=0.15188957128986466\n",
      "Regularized Logistic Regression(242/999): loss=0.5995769380060868, w0=-2.003276072502692e-06, w1=0.15226269176427495\n",
      "Regularized Logistic Regression(243/999): loss=0.5994500220378157, w0=-2.009458808356426e-06, w1=0.15263427942282087\n",
      "Regularized Logistic Regression(244/999): loss=0.5993240400281881, w0=-2.0156292547526212e-06, w1=0.15300434052094866\n",
      "Regularized Logistic Regression(245/999): loss=0.5991989844728215, w0=-2.021787449386913e-06, w1=0.15337288129228574\n",
      "Regularized Logistic Regression(246/999): loss=0.5990748479325625, w0=-2.0279334297720533e-06, w1=0.15373990794862707\n",
      "Regularized Logistic Regression(247/999): loss=0.5989516230328715, w0=-2.0340672332392883e-06, w1=0.1541054266799205\n",
      "Regularized Logistic Regression(248/999): loss=0.5988293024632187, w0=-2.0401888969397254e-06, w1=0.15446944365425724\n",
      "Regularized Logistic Regression(249/999): loss=0.5987078789764825, w0=-2.0462984578456858e-06, w1=0.15483196501786123\n",
      "Regularized Logistic Regression(250/999): loss=0.5985873453883538, w0=-2.0523959527520392e-06, w1=0.15519299689508398\n",
      "Regularized Logistic Regression(251/999): loss=0.5984676945767498, w0=-2.0584814182775287e-06, w1=0.1555525453883991\n",
      "Regularized Logistic Regression(252/999): loss=0.5983489194812313, w0=-2.0645548908660782e-06, w1=0.1559106165783987\n",
      "Regularized Logistic Regression(253/999): loss=0.598231013102425, w0=-2.070616406788089e-06, w1=0.15626721652379216\n",
      "Regularized Logistic Regression(254/999): loss=0.5981139685014552, w0=-2.0766660021417206e-06, w1=0.15662235126140764\n",
      "Regularized Logistic Regression(255/999): loss=0.5979977787993772, w0=-2.082703712854159e-06, w1=0.1569760268061962\n",
      "Regularized Logistic Regression(256/999): loss=0.5978824371766208, w0=-2.0887295746828715e-06, w1=0.15732824915123192\n",
      "Regularized Logistic Regression(257/999): loss=0.5977679368724345, w0=-2.0947436232168493e-06, w1=0.15767902426772037\n",
      "Regularized Logistic Regression(258/999): loss=0.5976542711843394, w0=-2.1007458938778357e-06, w1=0.15802835810500812\n",
      "Regularized Logistic Regression(259/999): loss=0.597541433467587, w0=-2.106736421921543e-06, w1=0.15837625659058874\n",
      "Regularized Logistic Regression(260/999): loss=0.5974294171346228, w0=-2.1127152424388564e-06, w1=0.15872272563011475\n",
      "Regularized Logistic Regression(261/999): loss=0.5973182156545551, w0=-2.118682390357024e-06, w1=0.15906777110741196\n",
      "Regularized Logistic Regression(262/999): loss=0.5972078225526283, w0=-2.1246379004408377e-06, w1=0.15941139888449107\n",
      "Regularized Logistic Regression(263/999): loss=0.597098231409703, w0=-2.130581807293798e-06, w1=0.15975361480156453\n",
      "Regularized Logistic Regression(264/999): loss=0.596989435861742, w0=-2.136514145359272e-06, w1=0.16009442467706303\n",
      "Regularized Logistic Regression(265/999): loss=0.5968814295992984, w0=-2.1424349489216334e-06, w1=0.16043383430765512\n",
      "Regularized Logistic Regression(266/999): loss=0.5967742063670116, w0=-2.148344252107396e-06, w1=0.16077184946826484\n",
      "Regularized Logistic Regression(267/999): loss=0.596667759963107, w0=-2.154242088886334e-06, w1=0.16110847591209615\n",
      "Regularized Logistic Regression(268/999): loss=0.5965620842389018, w0=-2.160128493072589e-06, w1=0.16144371937065266\n",
      "Regularized Logistic Regression(269/999): loss=0.5964571730983144, w0=-2.166003498325769e-06, w1=0.1617775855537625\n",
      "Regularized Logistic Regression(270/999): loss=0.5963530204973799, w0=-2.1718671381520364e-06, w1=0.16211008014960465\n",
      "Regularized Logistic Regression(271/999): loss=0.5962496204437688, w0=-2.177719445905179e-06, w1=0.16244120882473348\n",
      "Regularized Logistic Regression(272/999): loss=0.5961469669963133, w0=-2.1835604547876793e-06, w1=0.1627709772241071\n",
      "Regularized Logistic Regression(273/999): loss=0.5960450542645359, w0=-2.1893901978517662e-06, w1=0.163099390971117\n",
      "Regularized Logistic Regression(274/999): loss=0.5959438764081827, w0=-2.19520870800046e-06, w1=0.16342645566761704\n",
      "Regularized Logistic Regression(275/999): loss=0.5958434276367632, w0=-2.201016017988604e-06, w1=0.16375217689395546\n",
      "Regularized Logistic Regression(276/999): loss=0.5957437022090926, w0=-2.2068121604238894e-06, w1=0.16407656020900654\n",
      "Regularized Logistic Regression(277/999): loss=0.5956446944328404, w0=-2.2125971677678665e-06, w1=0.16439961115020235\n",
      "Regularized Logistic Regression(278/999): loss=0.5955463986640827, w0=-2.218371072336948e-06, w1=0.1647213352335711\n",
      "Regularized Logistic Regression(279/999): loss=0.5954488093068571, w0=-2.2241339063034025e-06, w1=0.16504173795376917\n",
      "Regularized Logistic Regression(280/999): loss=0.5953519208127268, w0=-2.2298857016963383e-06, w1=0.16536082478411876\n",
      "Regularized Logistic Regression(281/999): loss=0.595255727680343, w0=-2.2356264904026744e-06, w1=0.16567860117664424\n",
      "Regularized Logistic Regression(282/999): loss=0.5951602244550172, w0=-2.2413563041681074e-06, w1=0.16599507256211243\n",
      "Regularized Logistic Regression(283/999): loss=0.5950654057282923, w0=-2.247075174598065e-06, w1=0.16631024435006958\n",
      "Regularized Logistic Regression(284/999): loss=0.594971266137524, w0=-2.252783133158651e-06, w1=0.16662412192888398\n",
      "Regularized Logistic Regression(285/999): loss=0.5948778003654609, w0=-2.2584802111775825e-06, w1=0.1669367106657846\n",
      "Regularized Logistic Regression(286/999): loss=0.5947850031398311, w0=-2.264166439845116e-06, w1=0.16724801590690402\n",
      "Regularized Logistic Regression(287/999): loss=0.5946928692329333, w0=-2.2698418502149666e-06, w1=0.16755804297732155\n",
      "Regularized Logistic Regression(288/999): loss=0.594601393461231, w0=-2.2755064732052167e-06, w1=0.16786679718110645\n",
      "Regularized Logistic Regression(289/999): loss=0.5945105706849505, w0=-2.2811603395992175e-06, w1=0.16817428380136226\n",
      "Regularized Logistic Regression(290/999): loss=0.5944203958076851, w0=-2.28680348004648e-06, w1=0.16848050810027101\n",
      "Regularized Logistic Regression(291/999): loss=0.5943308637759984, w0=-2.2924359250635593e-06, w1=0.16878547531914043\n",
      "Regularized Logistic Regression(292/999): loss=0.5942419695790374, w0=-2.2980577050349294e-06, w1=0.16908919067844977\n",
      "Regularized Logistic Regression(293/999): loss=0.5941537082481446, w0=-2.3036688502138503e-06, w1=0.16939165937789918\n",
      "Regularized Logistic Regression(294/999): loss=0.5940660748564758, w0=-2.3092693907232266e-06, w1=0.16969288659645368\n",
      "Regularized Logistic Regression(295/999): loss=0.5939790645186227, w0=-2.314859356556458e-06, w1=0.16999287749239436\n",
      "Regularized Logistic Regression(296/999): loss=0.5938926723902372, w0=-2.320438777578281e-06, w1=0.1702916372033672\n",
      "Regularized Logistic Regression(297/999): loss=0.5938068936676599, w0=-2.326007683525606e-06, w1=0.1705891708464338\n",
      "Regularized Logistic Regression(298/999): loss=0.5937217235875539, w0=-2.33156610400834e-06, w1=0.17088548351812138\n",
      "Regularized Logistic Regression(299/999): loss=0.5936371574265381, w0=-2.3371140685102097e-06, w1=0.17118058029447272\n",
      "Regularized Logistic Regression(300/999): loss=0.5935531905008303, w0=-2.342651606389571e-06, w1=0.1714744662310995\n",
      "Regularized Logistic Regression(301/999): loss=0.5934698181658874, w0=-2.348178746880212e-06, w1=0.17176714636323412\n",
      "Regularized Logistic Regression(302/999): loss=0.5933870358160518, w0=-2.3536955190921525e-06, w1=0.1720586257057815\n",
      "Regularized Logistic Regression(303/999): loss=0.5933048388842039, w0=-2.35920195201243e-06, w1=0.17234890925337437\n",
      "Regularized Logistic Regression(304/999): loss=0.5932232228414126, w0=-2.364698074505883e-06, w1=0.17263800198042564\n",
      "Regularized Logistic Regression(305/999): loss=0.5931421831965933, w0=-2.3701839153159246e-06, w1=0.17292590884118408\n",
      "Regularized Logistic Regression(306/999): loss=0.5930617154961678, w0=-2.3756595030653123e-06, w1=0.17321263476978743\n",
      "Regularized Logistic Regression(307/999): loss=0.5929818153237273, w0=-2.381124866256906e-06, w1=0.1734981846803184\n",
      "Regularized Logistic Regression(308/999): loss=0.5929024782996982, w0=-2.3865800332744235e-06, w1=0.17378256346686158\n",
      "Regularized Logistic Regression(309/999): loss=0.5928237000810136, w0=-2.392025032383185e-06, w1=0.17406577600355716\n",
      "Regularized Logistic Regression(310/999): loss=0.5927454763607839, w0=-2.3974598917308554e-06, w1=0.17434782714466093\n",
      "Regularized Logistic Regression(311/999): loss=0.5926678028679747, w0=-2.402884639348177e-06, w1=0.17462872172459792\n",
      "Regularized Logistic Regression(312/999): loss=0.5925906753670852, w0=-2.4082993031496944e-06, w1=0.1749084645580218\n",
      "Regularized Logistic Regression(313/999): loss=0.59251408965783, w0=-2.413703910934478e-06, w1=0.1751870604398731\n",
      "Regularized Logistic Regression(314/999): loss=0.592438041574825, w0=-2.419098490386834e-06, w1=0.17546451414543573\n",
      "Regularized Logistic Regression(315/999): loss=0.5923625269872762, w0=-2.424483069077015e-06, w1=0.17574083043039523\n",
      "Regularized Logistic Regression(316/999): loss=0.5922875417986693, w0=-2.429857674461918e-06, w1=0.17601601403089975\n",
      "Regularized Logistic Regression(317/999): loss=0.5922130819464652, w0=-2.4352223338857818e-06, w1=0.17629006966361913\n",
      "Regularized Logistic Regression(318/999): loss=0.5921391434017967, w0=-2.4405770745808736e-06, w1=0.17656300202579978\n",
      "Regularized Logistic Regression(319/999): loss=0.5920657221691698, w0=-2.445921923668173e-06, w1=0.17683481579533258\n",
      "Regularized Logistic Regression(320/999): loss=0.5919928142861645, w0=-2.451256908158047e-06, w1=0.17710551563080465\n",
      "Regularized Logistic Regression(321/999): loss=0.5919204158231421, w0=-2.456582054950923e-06, w1=0.17737510617156582\n",
      "Regularized Logistic Regression(322/999): loss=0.5918485228829531, w0=-2.4618973908379504e-06, w1=0.17764359203778593\n",
      "Regularized Logistic Regression(323/999): loss=0.5917771316006492, w0=-2.4672029425016613e-06, w1=0.17791097783051624\n",
      "Regularized Logistic Regression(324/999): loss=0.5917062381431963, w0=-2.4724987365166233e-06, w1=0.17817726813175228\n",
      "Regularized Logistic Regression(325/999): loss=0.5916358387091935, w0=-2.4777847993500866e-06, w1=0.17844246750449244\n",
      "Regularized Logistic Regression(326/999): loss=0.5915659295285896, w0=-2.4830611573626267e-06, w1=0.17870658049280272\n",
      "Regularized Logistic Regression(327/999): loss=0.5914965068624074, w0=-2.48832783680878e-06, w1=0.17896961162187522\n",
      "Regularized Logistic Regression(328/999): loss=0.5914275670024683, w0=-2.4935848638376736e-06, w1=0.17923156539809326\n",
      "Regularized Logistic Regression(329/999): loss=0.5913591062711191, w0=-2.4988322644936528e-06, w1=0.1794924463090916\n",
      "Regularized Logistic Regression(330/999): loss=0.5912911210209627, w0=-2.504070064716899e-06, w1=0.1797522588238189\n",
      "Regularized Logistic Regression(331/999): loss=0.5912236076345896, w0=-2.5092982903440464e-06, w1=0.1800110073926006\n",
      "Regularized Logistic Regression(332/999): loss=0.5911565625243151, w0=-2.514516967108789e-06, w1=0.18026869644720192\n",
      "Regularized Logistic Regression(333/999): loss=0.5910899821319142, w0=-2.5197261206424877e-06, w1=0.1805253304008914\n",
      "Regularized Logistic Regression(334/999): loss=0.5910238629283645, w0=-2.5249257764747667e-06, w1=0.1807809136485018\n",
      "Regularized Logistic Regression(335/999): loss=0.5909582014135867, w0=-2.530115960034109e-06, w1=0.18103545056649573\n",
      "Regularized Logistic Regression(336/999): loss=0.59089299411619, w0=-2.535296696648446e-06, w1=0.18128894551302618\n",
      "Regularized Logistic Regression(337/999): loss=0.5908282375932206, w0=-2.5404680115457397e-06, w1=0.18154140282800302\n",
      "Regularized Logistic Regression(338/999): loss=0.5907639284299105, w0=-2.5456299298545638e-06, w1=0.18179282683315381\n",
      "Regularized Logistic Regression(339/999): loss=0.5907000632394296, w0=-2.5507824766046762e-06, w1=0.18204322183208896\n",
      "Regularized Logistic Regression(340/999): loss=0.5906366386626404, w0=-2.5559256767275906e-06, w1=0.18229259211036636\n",
      "Regularized Logistic Regression(341/999): loss=0.5905736513678564, w0=-2.5610595550571403e-06, w1=0.18254094193555256\n",
      "Regularized Logistic Regression(342/999): loss=0.5905110980505975, w0=-2.5661841363300372e-06, w1=0.1827882755572881\n",
      "Regularized Logistic Regression(343/999): loss=0.5904489754333553, w0=-2.5712994451864293e-06, w1=0.18303459720735052\n",
      "Regularized Logistic Regression(344/999): loss=0.5903872802653541, w0=-2.5764055061704504e-06, w1=0.1832799110997224\n",
      "Regularized Logistic Regression(345/999): loss=0.590326009322318, w0=-2.581502343730767e-06, w1=0.18352422143064967\n",
      "Regularized Logistic Regression(346/999): loss=0.5902651594062382, w0=-2.5865899822211194e-06, w1=0.18376753237870772\n",
      "Regularized Logistic Regression(347/999): loss=0.590204727345144, w0=-2.59166844590086e-06, w1=0.18400984810486928\n",
      "Regularized Logistic Regression(348/999): loss=0.5901447099928738, w0=-2.596737758935486e-06, w1=0.18425117275256306\n",
      "Regularized Logistic Regression(349/999): loss=0.5900851042288514, w0=-2.601797945397168e-06, w1=0.18449151044774179\n",
      "Regularized Logistic Regression(350/999): loss=0.5900259069578614, w0=-2.6068490292652747e-06, w1=0.18473086529894533\n",
      "Regularized Logistic Regression(351/999): loss=0.5899671151098277, w0=-2.611891034426893e-06, w1=0.18496924139736542\n",
      "Regularized Logistic Regression(352/999): loss=0.5899087256395952, w0=-2.616923984677344e-06, w1=0.18520664281690802\n",
      "Regularized Logistic Regression(353/999): loss=0.5898507355267117, w0=-2.6219479037206947e-06, w1=0.18544307361425988\n",
      "Regularized Logistic Regression(354/999): loss=0.5897931417752138, w0=-2.626962815170266e-06, w1=0.18567853782895322\n",
      "Regularized Logistic Regression(355/999): loss=0.5897359414134122, w0=-2.6319687425491356e-06, w1=0.1859130394834278\n",
      "Regularized Logistic Regression(356/999): loss=0.5896791314936818, w0=-2.6369657092906394e-06, w1=0.186146582583098\n",
      "Regularized Logistic Regression(357/999): loss=0.5896227090922518, w0=-2.6419537387388647e-06, w1=0.18637917111641358\n",
      "Regularized Logistic Regression(358/999): loss=0.5895666713089996, w0=-2.646932854149144e-06, w1=0.18661080905492694\n",
      "Regularized Logistic Regression(359/999): loss=0.5895110152672431, w0=-2.651903078688541e-06, w1=0.186841500353357\n",
      "Regularized Logistic Regression(360/999): loss=0.5894557381135401, w0=-2.6568644354363374e-06, w1=0.1870712489496517\n",
      "Regularized Logistic Regression(361/999): loss=0.5894008370174848, w0=-2.6618169473845096e-06, w1=0.18730005876505518\n",
      "Regularized Logistic Regression(362/999): loss=0.5893463091715089, w0=-2.6667606374382077e-06, w1=0.187527933704168\n",
      "Regularized Logistic Regression(363/999): loss=0.5892921517906835, w0=-2.6716955284162274e-06, w1=0.18775487765501533\n",
      "Regularized Logistic Regression(364/999): loss=0.5892383621125235, w0=-2.676621643051478e-06, w1=0.18798089448910799\n",
      "Regularized Logistic Regression(365/999): loss=0.5891849373967926, w0=-2.6815390039914493e-06, w1=0.1882059880615073\n",
      "Regularized Logistic Regression(366/999): loss=0.5891318749253129, w0=-2.686447633798672e-06, w1=0.18843016221088987\n",
      "Regularized Logistic Regression(367/999): loss=0.5890791720017717, w0=-2.6913475549511772e-06, w1=0.18865342075961114\n",
      "Regularized Logistic Regression(368/999): loss=0.5890268259515349, w0=-2.6962387898429495e-06, w1=0.18887576751376825\n",
      "Regularized Logistic Regression(369/999): loss=0.588974834121458, w0=-2.701121360784379e-06, w1=0.18909720626326462\n",
      "Regularized Logistic Regression(370/999): loss=0.5889231938797026, w0=-2.7059952900027085e-06, w1=0.18931774078187447\n",
      "Regularized Logistic Regression(371/999): loss=0.5888719026155516, w0=-2.7108605996424784e-06, w1=0.18953737482730518\n",
      "Regularized Logistic Regression(372/999): loss=0.5888209577392272, w0=-2.7157173117659667e-06, w1=0.18975611214126037\n",
      "Regularized Logistic Regression(373/999): loss=0.5887703566817122, w0=-2.7205654483536264e-06, w1=0.18997395644950507\n",
      "Regularized Logistic Regression(374/999): loss=0.5887200968945692, w0=-2.7254050313045205e-06, w1=0.190190911461927\n",
      "Regularized Logistic Regression(375/999): loss=0.5886701758497646, w0=-2.7302360824367526e-06, w1=0.19040698087260113\n",
      "Regularized Logistic Regression(376/999): loss=0.5886205910394943, w0=-2.735058623487893e-06, w1=0.19062216835985318\n",
      "Regularized Logistic Regression(377/999): loss=0.5885713399760083, w0=-2.739872676115405e-06, w1=0.19083647758632252\n",
      "Regularized Logistic Regression(378/999): loss=0.58852242019144, w0=-2.7446782618970655e-06, w1=0.19104991219902262\n",
      "Regularized Logistic Regression(379/999): loss=0.5884738292376339, w0=-2.749475402331382e-06, w1=0.19126247582940836\n",
      "Regularized Logistic Regression(380/999): loss=0.5884255646859791, w0=-2.754264118838009e-06, w1=0.19147417209343534\n",
      "Regularized Logistic Regression(381/999): loss=0.5883776241272396, w0=-2.7590444327581584e-06, w1=0.1916850045916224\n",
      "Regularized Logistic Regression(382/999): loss=0.5883300051713892, w0=-2.763816365355009e-06, w1=0.1918949769091177\n",
      "Regularized Logistic Regression(383/999): loss=0.5882827054474481, w0=-2.7685799378141118e-06, w1=0.19210409261575587\n",
      "Regularized Logistic Regression(384/999): loss=0.5882357226033186, w0=-2.773335171243793e-06, w1=0.19231235526612495\n",
      "Regularized Logistic Regression(385/999): loss=0.5881890543056251, w0=-2.7780820866755543e-06, w1=0.19251976839962615\n",
      "Regularized Logistic Regression(386/999): loss=0.5881426982395521, w0=-2.7828207050644676e-06, w1=0.1927263355405365\n",
      "Regularized Logistic Regression(387/999): loss=0.5880966521086888, w0=-2.787551047289571e-06, w1=0.1929320601980704\n",
      "Regularized Logistic Regression(388/999): loss=0.5880509136348689, w0=-2.7922731341542588e-06, w1=0.19313694586644073\n",
      "Regularized Logistic Regression(389/999): loss=0.5880054805580179, w0=-2.7969869863866698e-06, w1=0.19334099602492297\n",
      "Regularized Logistic Regression(390/999): loss=0.5879603506359972, w0=-2.8016926246400724e-06, w1=0.1935442141379123\n",
      "Regularized Logistic Regression(391/999): loss=0.5879155216444513, w0=-2.8063900694932474e-06, w1=0.1937466036549906\n",
      "Regularized Logistic Regression(392/999): loss=0.5878709913766578, w0=-2.8110793414508676e-06, w1=0.19394816801098247\n",
      "Regularized Logistic Regression(393/999): loss=0.5878267576433768, w0=-2.8157604609438746e-06, w1=0.19414891062601988\n",
      "Regularized Logistic Regression(394/999): loss=0.5877828182727006, w0=-2.820433448329853e-06, w1=0.1943488349056007\n",
      "Regularized Logistic Regression(395/999): loss=0.5877391711099105, w0=-2.825098323893403e-06, w1=0.19454794424064997\n",
      "Regularized Logistic Regression(396/999): loss=0.5876958140173267, w0=-2.829755107846508e-06, w1=0.19474624200758342\n",
      "Regularized Logistic Regression(397/999): loss=0.5876527448741661, w0=-2.834403820328901e-06, w1=0.1949437315683629\n",
      "Regularized Logistic Regression(398/999): loss=0.5876099615763979, w0=-2.8390444814084284e-06, w1=0.19514041627055928\n",
      "Regularized Logistic Regression(399/999): loss=0.5875674620366025, w0=-2.8436771110814122e-06, w1=0.1953362994474141\n",
      "Regularized Logistic Regression(400/999): loss=0.5875252441838307, w0=-2.8483017292730073e-06, w1=0.19553138441789683\n",
      "Regularized Logistic Regression(401/999): loss=0.5874833059634637, w0=-2.852918355837557e-06, w1=0.19572567448676598\n",
      "Regularized Logistic Regression(402/999): loss=0.5874416453370754, w0=-2.8575270105589483e-06, w1=0.1959191729446291\n",
      "Regularized Logistic Regression(403/999): loss=0.5874002602822959, w0=-2.862127713150961e-06, w1=0.19611188306800106\n",
      "Regularized Logistic Regression(404/999): loss=0.5873591487926744, w0=-2.866720483257617e-06, w1=0.19630380811936496\n",
      "Regularized Logistic Regression(405/999): loss=0.587318308877547, w0=-2.8713053404535267e-06, w1=0.19649495134722922\n",
      "Regularized Logistic Regression(406/999): loss=0.5872777385619008, w0=-2.8758823042442322e-06, w1=0.1966853159861901\n",
      "Regularized Logistic Regression(407/999): loss=0.5872374358862451, w0=-2.8804513940665487e-06, w1=0.19687490525698623\n",
      "Regularized Logistic Regression(408/999): loss=0.5871973989064772, w0=-2.8850126292889033e-06, w1=0.1970637223665595\n",
      "Regularized Logistic Regression(409/999): loss=0.5871576256937558, w0=-2.889566029211671e-06, w1=0.19725177050811438\n",
      "Regularized Logistic Regression(410/999): loss=0.5871181143343698, w0=-2.8941116130675107e-06, w1=0.19743905286117375\n",
      "Regularized Logistic Regression(411/999): loss=0.5870788629296134, w0=-2.8986494000216948e-06, w1=0.19762557259164074\n",
      "Regularized Logistic Regression(412/999): loss=0.5870398695956581, w0=-2.9031794091724404e-06, w1=0.19781133285185182\n",
      "Regularized Logistic Regression(413/999): loss=0.5870011324634296, w0=-2.9077016595512362e-06, w1=0.1979963367806402\n",
      "Regularized Logistic Regression(414/999): loss=0.5869626496784812, w0=-2.9122161701231677e-06, w1=0.19818058750338777\n",
      "Regularized Logistic Regression(415/999): loss=0.5869244194008731, w0=-2.9167229597872395e-06, w1=0.1983640881320865\n",
      "Regularized Logistic Regression(416/999): loss=0.5868864398050507, w0=-2.9212220473766964e-06, w1=0.19854684176539483\n",
      "Regularized Logistic Regression(417/999): loss=0.5868487090797228, w0=-2.9257134516593427e-06, w1=0.198728851488693\n",
      "Regularized Logistic Regression(418/999): loss=0.5868112254277429, w0=-2.9301971913378577e-06, w1=0.19891012037414268\n",
      "Regularized Logistic Regression(419/999): loss=0.5867739870659907, w0=-2.9346732850501096e-06, w1=0.19909065148074204\n",
      "Regularized Logistic Regression(420/999): loss=0.5867369922252547, w0=-2.9391417513694685e-06, w1=0.19927044785438203\n",
      "Regularized Logistic Regression(421/999): loss=0.5867002391501152, w0=-2.9436026088051156e-06, w1=0.19944951252790333\n",
      "Regularized Logistic Regression(422/999): loss=0.5866637260988302, w0=-2.9480558758023523e-06, w1=0.19962784852115292\n",
      "Regularized Logistic Regression(423/999): loss=0.5866274513432203, w0=-2.9525015707429043e-06, w1=0.19980545884103879\n",
      "Regularized Logistic Regression(424/999): loss=0.5865914131685552, w0=-2.9569397119452275e-06, w1=0.19998234648158714\n",
      "Regularized Logistic Regression(425/999): loss=0.5865556098734428, w0=-2.9613703176648077e-06, w1=0.20015851442399868\n",
      "Regularized Logistic Regression(426/999): loss=0.5865200397697163, w0=-2.9657934060944624e-06, w1=0.20033396563669928\n",
      "Regularized Logistic Regression(427/999): loss=0.5864847011823251, w0=-2.970208995364637e-06, w1=0.2005087030754021\n",
      "Regularized Logistic Regression(428/999): loss=0.5864495924492257, w0=-2.9746171035437017e-06, w1=0.2006827296831564\n",
      "Regularized Logistic Regression(429/999): loss=0.5864147119212721, w0=-2.979017748638244e-06, w1=0.20085604839040608\n",
      "Regularized Logistic Regression(430/999): loss=0.5863800579621096, w0=-2.983410948593363e-06, w1=0.20102866211504397\n",
      "Regularized Logistic Regression(431/999): loss=0.5863456289480682, w0=-2.9877967212929572e-06, w1=0.20120057376246467\n",
      "Regularized Logistic Regression(432/999): loss=0.5863114232680573, w0=-2.9921750845600135e-06, w1=0.20137178622562119\n",
      "Regularized Logistic Regression(433/999): loss=0.5862774393234611, w0=-2.996546056156894e-06, w1=0.2015423023850761\n",
      "Regularized Logistic Regression(434/999): loss=0.5862436755280344, w0=-3.0009096537856194e-06, w1=0.20171212510905773\n",
      "Regularized Logistic Regression(435/999): loss=0.5862101303078011, w0=-3.0052658950881523e-06, w1=0.20188125725351364\n",
      "Regularized Logistic Regression(436/999): loss=0.5861768021009517, w0=-3.0096147976466776e-06, w1=0.2020497016621622\n",
      "Regularized Logistic Regression(437/999): loss=0.5861436893577433, w0=-3.0139563789838815e-06, w1=0.2022174611665486\n",
      "Regularized Logistic Regression(438/999): loss=0.5861107905403984, w0=-3.0182906565632286e-06, w1=0.2023845385860952\n",
      "Regularized Logistic Regression(439/999): loss=0.5860781041230068, w0=-3.0226176477892368e-06, w1=0.20255093672815716\n",
      "Regularized Logistic Regression(440/999): loss=0.5860456285914275, w0=-3.0269373700077512e-06, w1=0.2027166583880735\n",
      "Regularized Logistic Regression(441/999): loss=0.58601336244319, w0=-3.0312498405062164e-06, w1=0.20288170634921907\n",
      "Regularized Logistic Regression(442/999): loss=0.5859813041874014, w0=-3.0355550765139456e-06, w1=0.20304608338306002\n",
      "Regularized Logistic Regression(443/999): loss=0.5859494523446449, w0=-3.03985309520239e-06, w1=0.20320979224920252\n",
      "Regularized Logistic Regression(444/999): loss=0.5859178054468914, w0=-3.044143913685405e-06, w1=0.20337283569544432\n",
      "Regularized Logistic Regression(445/999): loss=0.5858863620374021, w0=-3.0484275490195133e-06, w1=0.20353521645782993\n",
      "Regularized Logistic Regression(446/999): loss=0.5858551206706345, w0=-3.0527040182041713e-06, w1=0.20369693726070012\n",
      "Regularized Logistic Regression(447/999): loss=0.5858240799121539, w0=-3.0569733381820293e-06, w1=0.20385800081674332\n",
      "Regularized Logistic Regression(448/999): loss=0.5857932383385381, w0=-3.0612355258391907e-06, w1=0.20401840982704678\n",
      "Regularized Logistic Regression(449/999): loss=0.5857625945372894, w0=-3.0654905980054715e-06, w1=0.204178166981147\n",
      "Regularized Logistic Regression(450/999): loss=0.5857321471067434, w0=-3.0697385714546562e-06, w1=0.20433727495708245\n",
      "Regularized Logistic Regression(451/999): loss=0.5857018946559799, w0=-3.0739794629047535e-06, w1=0.20449573642144253\n",
      "Regularized Logistic Regression(452/999): loss=0.5856718358047357, w0=-3.078213289018249e-06, w1=0.20465355402941762\n",
      "Regularized Logistic Regression(453/999): loss=0.5856419691833159, w0=-3.0824400664023586e-06, w1=0.2048107304248505\n",
      "Regularized Logistic Regression(454/999): loss=0.5856122934325086, w0=-3.0866598116092763e-06, w1=0.20496726824028552\n",
      "Regularized Logistic Regression(455/999): loss=0.5855828072034971, w0=-3.0908725411364254e-06, w1=0.20512317009701872\n",
      "Regularized Logistic Regression(456/999): loss=0.585553509157776, w0=-3.095078271426705e-06, w1=0.20527843860514727\n",
      "Regularized Logistic Regression(457/999): loss=0.5855243979670666, w0=-3.0992770188687356e-06, w1=0.2054330763636188\n",
      "Regularized Logistic Regression(458/999): loss=0.5854954723132323, w0=-3.103468799797103e-06, w1=0.20558708596028127\n",
      "Regularized Logistic Regression(459/999): loss=0.5854667308881973, w0=-3.107653630492602e-06, w1=0.20574046997193013\n",
      "Regularized Logistic Regression(460/999): loss=0.5854381723938618, w0=-3.1118315271824756e-06, w1=0.20589323096435966\n",
      "Regularized Logistic Regression(461/999): loss=0.5854097955420237, w0=-3.116002506040658e-06, w1=0.20604537149240926\n",
      "Regularized Logistic Regression(462/999): loss=0.5853815990542945, w0=-3.1201665831880098e-06, w1=0.2061968941000138\n",
      "Regularized Logistic Regression(463/999): loss=0.5853535816620214, w0=-3.124323774692556e-06, w1=0.2063478013202506\n",
      "Regularized Logistic Regression(464/999): loss=0.5853257421062067, w0=-3.1284740965697222e-06, w1=0.20649809567538813\n",
      "Regularized Logistic Regression(465/999): loss=0.5852980791374303, w0=-3.132617564782567e-06, w1=0.20664777967693274\n",
      "Regularized Logistic Regression(466/999): loss=0.5852705915157702, w0=-3.1367541952420157e-06, w1=0.20679685582567756\n",
      "Regularized Logistic Regression(467/999): loss=0.5852432780107255, w0=-3.140884003807091e-06, w1=0.20694532661174916\n",
      "Regularized Logistic Regression(468/999): loss=0.5852161374011408, w0=-3.1450070062851432e-06, w1=0.2070931945146562\n",
      "Regularized Logistic Regression(469/999): loss=0.5851891684751287, w0=-3.149123218432078e-06, w1=0.20724046200333401\n",
      "Regularized Logistic Regression(470/999): loss=0.5851623700299949, w0=-3.153232655952584e-06, w1=0.20738713153619312\n",
      "Regularized Logistic Regression(471/999): loss=0.5851357408721642, w0=-3.1573353345003575e-06, w1=0.20753320556116614\n",
      "Regularized Logistic Regression(472/999): loss=0.585109279817106, w0=-3.161431269678328e-06, w1=0.20767868651575333\n",
      "Regularized Logistic Regression(473/999): loss=0.5850829856892599, w0=-3.16552047703888e-06, w1=0.20782357682706953\n",
      "Regularized Logistic Regression(474/999): loss=0.5850568573219639, w0=-3.169602972084076e-06, w1=0.20796787891188948\n",
      "Regularized Logistic Regression(475/999): loss=0.5850308935573834, w0=-3.1736787702658744e-06, w1=0.20811159517669345\n",
      "Regularized Logistic Regression(476/999): loss=0.5850050932464365, w0=-3.1777478869863516e-06, w1=0.20825472801771622\n",
      "Regularized Logistic Regression(477/999): loss=0.5849794552487263, w0=-3.181810337597918e-06, w1=0.20839727982098777\n",
      "Regularized Logistic Regression(478/999): loss=0.5849539784324684, w0=-3.185866137403534e-06, w1=0.208539252962382\n",
      "Regularized Logistic Regression(479/999): loss=0.5849286616744229, w0=-3.1899153016569262e-06, w1=0.20868064980766018\n",
      "Regularized Logistic Regression(480/999): loss=0.5849035038598238, w0=-3.1939578455628014e-06, w1=0.20882147271251686\n",
      "Regularized Logistic Regression(481/999): loss=0.5848785038823111, w0=-3.1979937842770577e-06, w1=0.20896172402262408\n",
      "Regularized Logistic Regression(482/999): loss=0.5848536606438636, w0=-3.202023132906998e-06, w1=0.20910140607367564\n",
      "Regularized Logistic Regression(483/999): loss=0.5848289730547304, w0=-3.2060459065115378e-06, w1=0.20924052119143283\n",
      "Regularized Logistic Regression(484/999): loss=0.5848044400333647, w0=-3.210062120101417e-06, w1=0.20937907169176695\n",
      "Regularized Logistic Regression(485/999): loss=0.5847800605063583, w0=-3.2140717886394053e-06, w1=0.20951705988070402\n",
      "Regularized Logistic Regression(486/999): loss=0.5847558334083748, w0=-3.2180749270405093e-06, w1=0.20965448805446907\n",
      "Regularized Logistic Regression(487/999): loss=0.5847317576820851, w0=-3.222071550172179e-06, w1=0.20979135849952862\n",
      "Regularized Logistic Regression(488/999): loss=0.584707832278103, w0=-3.2260616728545095e-06, w1=0.20992767349263555\n",
      "Regularized Logistic Regression(489/999): loss=0.5846840561549219, w0=-3.230045309860447e-06, w1=0.21006343530087077\n",
      "Regularized Logistic Regression(490/999): loss=0.5846604282788495, w0=-3.234022475915988e-06, w1=0.21019864618168702\n",
      "Regularized Logistic Regression(491/999): loss=0.5846369476239479, w0=-3.2379931857003814e-06, w1=0.21033330838295278\n",
      "Regularized Logistic Regression(492/999): loss=0.5846136131719684, w0=-3.2419574538463276e-06, w1=0.21046742414299355\n",
      "Regularized Logistic Regression(493/999): loss=0.5845904239122917, w0=-3.2459152949401764e-06, w1=0.2106009956906334\n",
      "Regularized Logistic Regression(494/999): loss=0.5845673788418656, w0=-3.2498667235221256e-06, w1=0.21073402524524035\n",
      "Regularized Logistic Regression(495/999): loss=0.5845444769651451, w0=-3.2538117540864153e-06, w1=0.21086651501676698\n",
      "Regularized Logistic Regression(496/999): loss=0.5845217172940317, w0=-3.257750401081524e-06, w1=0.21099846720578988\n",
      "Regularized Logistic Regression(497/999): loss=0.5844990988478143, w0=-3.261682678910362e-06, w1=0.21112988400355603\n",
      "Regularized Logistic Regression(498/999): loss=0.5844766206531089, w0=-3.2656086019304647e-06, w1=0.21126076759202117\n",
      "Regularized Logistic Regression(499/999): loss=0.5844542817438013, w0=-3.2695281844541845e-06, w1=0.2113911201438912\n",
      "Regularized Logistic Regression(500/999): loss=0.5844320811609882, w0=-3.27344144074888e-06, w1=0.21152094382266598\n",
      "Regularized Logistic Regression(501/999): loss=0.5844100179529197, w0=-3.277348385037106e-06, w1=0.21165024078267836\n",
      "Regularized Logistic Regression(502/999): loss=0.5843880911749426, w0=-3.2812490314968036e-06, w1=0.21177901316913333\n",
      "Regularized Logistic Regression(503/999): loss=0.5843662998894423, w0=-3.285143394261486e-06, w1=0.21190726311815358\n",
      "Regularized Logistic Regression(504/999): loss=0.584344643165789, w0=-3.289031487420425e-06, w1=0.2120349927568148\n",
      "Regularized Logistic Regression(505/999): loss=0.5843231200802793, w0=-3.2929133250188365e-06, w1=0.21216220420318904\n",
      "Regularized Logistic Regression(506/999): loss=0.5843017297160835, w0=-3.2967889210580657e-06, w1=0.21228889956638508\n",
      "Regularized Logistic Regression(507/999): loss=0.5842804711631888, w0=-3.3006582894957686e-06, w1=0.21241508094658595\n",
      "Regularized Logistic Regression(508/999): loss=0.5842593435183461, w0=-3.3045214442460954e-06, w1=0.21254075043509119\n",
      "Regularized Logistic Regression(509/999): loss=0.584238345885016, w0=-3.308378399179872e-06, w1=0.21266591011435484\n",
      "Regularized Logistic Regression(510/999): loss=0.5842174773733152, w0=-3.3122291681247796e-06, w1=0.21279056205802593\n",
      "Regularized Logistic Regression(511/999): loss=0.5841967370999641, w0=-3.316073764865534e-06, w1=0.21291470833098752\n",
      "Regularized Logistic Regression(512/999): loss=0.5841761241882336, w0=-3.319912203144064e-06, w1=0.21303835098939497\n",
      "Regularized Logistic Regression(513/999): loss=0.584155637767893, w0=-3.3237444966596884e-06, w1=0.2131614920807149\n",
      "Regularized Logistic Regression(514/999): loss=0.5841352769751602, w0=-3.3275706590692937e-06, w1=0.21328413364376542\n",
      "Regularized Logistic Regression(515/999): loss=0.5841150409526477, w0=-3.3313907039875073e-06, w1=0.2134062777087535\n",
      "Regularized Logistic Regression(516/999): loss=0.5840949288493139, w0=-3.3352046449868728e-06, w1=0.21352792629731385\n",
      "Regularized Logistic Regression(517/999): loss=0.5840749398204128, w0=-3.3390124955980237e-06, w1=0.21364908142254585\n",
      "Regularized Logistic Regression(518/999): loss=0.5840550730274425, w0=-3.342814269309856e-06, w1=0.21376974508905364\n",
      "Regularized Logistic Regression(519/999): loss=0.5840353276380981, w0=-3.3466099795696993e-06, w1=0.2138899192929824\n",
      "Regularized Logistic Regression(520/999): loss=0.5840157028262212, w0=-3.3503996397834875e-06, w1=0.2140096060220563\n",
      "Regularized Logistic Regression(521/999): loss=0.5839961977717505, w0=-3.3541832633159284e-06, w1=0.21412880725561628\n",
      "Regularized Logistic Regression(522/999): loss=0.5839768116606756, w0=-3.357960863490673e-06, w1=0.21424752496465857\n",
      "Regularized Logistic Regression(523/999): loss=0.5839575436849885, w0=-3.3617324535904826e-06, w1=0.2143657611118695\n",
      "Regularized Logistic Regression(524/999): loss=0.5839383930426352, w0=-3.365498046857396e-06, w1=0.2144835176516636\n",
      "Regularized Logistic Regression(525/999): loss=0.5839193589374696, w0=-3.3692576564928967e-06, w1=0.214600796530222\n",
      "Regularized Logistic Regression(526/999): loss=0.5839004405792071, w0=-3.3730112956580757e-06, w1=0.21471759968552662\n",
      "Regularized Logistic Regression(527/999): loss=0.5838816371833778, w0=-3.3767589774737982e-06, w1=0.21483392904739756\n",
      "Regularized Logistic Regression(528/999): loss=0.5838629479712816, w0=-3.380500715020865e-06, w1=0.21494978653753102\n",
      "Regularized Logistic Regression(529/999): loss=0.5838443721699407, w0=-3.3842365213401773e-06, w1=0.215065174069533\n",
      "Regularized Logistic Regression(530/999): loss=0.5838259090120577, w0=-3.3879664094328957e-06, w1=0.21518009354895717\n",
      "Regularized Logistic Regression(531/999): loss=0.5838075577359682, w0=-3.3916903922606028e-06, w1=0.21529454687333918\n",
      "Regularized Logistic Regression(532/999): loss=0.5837893175855973, w0=-3.395408482745463e-06, w1=0.2154085359322341\n",
      "Regularized Logistic Regression(533/999): loss=0.5837711878104165, w0=-3.399120693770381e-06, w1=0.21552206260725046\n",
      "Regularized Logistic Regression(534/999): loss=0.583753167665399, w0=-3.4028270381791605e-06, w1=0.2156351287720867\n",
      "Regularized Logistic Regression(535/999): loss=0.5837352564109783, w0=-3.4065275287766613e-06, w1=0.21574773629256583\n",
      "Regularized Logistic Regression(536/999): loss=0.5837174533130021, w0=-3.4102221783289564e-06, w1=0.21585988702666994\n",
      "Regularized Logistic Regression(537/999): loss=0.5836997576426932, w0=-3.413910999563487e-06, w1=0.2159715828245766\n",
      "Regularized Logistic Regression(538/999): loss=0.5836821686766057, w0=-3.417594005169219e-06, w1=0.21608282552869265\n",
      "Regularized Logistic Regression(539/999): loss=0.5836646856965837, w0=-3.4212712077967937e-06, w1=0.21619361697368822\n",
      "Regularized Logistic Regression(540/999): loss=0.5836473079897195, w0=-3.424942620058686e-06, w1=0.216303958986532\n",
      "Regularized Logistic Regression(541/999): loss=0.5836300348483131, w0=-3.4286082545293526e-06, w1=0.21641385338652566\n",
      "Regularized Logistic Regression(542/999): loss=0.5836128655698304, w0=-3.4322681237453867e-06, w1=0.21652330198533734\n",
      "Regularized Logistic Regression(543/999): loss=0.5835957994568641, w0=-3.4359222402056674e-06, w1=0.21663230658703544\n",
      "Regularized Logistic Regression(544/999): loss=0.583578835817094, w0=-3.4395706163715097e-06, w1=0.2167408689881235\n",
      "Regularized Logistic Regression(545/999): loss=0.5835619739632444, w0=-3.443213264666815e-06, w1=0.21684899097757254\n",
      "Regularized Logistic Regression(546/999): loss=0.5835452132130476, w0=-3.446850197478219e-06, w1=0.2169566743368547\n",
      "Regularized Logistic Regression(547/999): loss=0.5835285528892054, w0=-3.45048142715524e-06, w1=0.21706392083997822\n",
      "Regularized Logistic Regression(548/999): loss=0.583511992319346, w0=-3.454106966010424e-06, w1=0.21717073225351763\n",
      "Regularized Logistic Regression(549/999): loss=0.583495530835991, w0=-3.457726826319495e-06, w1=0.2172771103366497\n",
      "Regularized Logistic Regression(550/999): loss=0.5834791677765138, w0=-3.461341020321496e-06, w1=0.2173830568411851\n",
      "Regularized Logistic Regression(551/999): loss=0.5834629024831027, w0=-3.464949560218937e-06, w1=0.2174885735116001\n",
      "Regularized Logistic Regression(552/999): loss=0.5834467343027239, w0=-3.468552458177938e-06, w1=0.21759366208507158\n",
      "Regularized Logistic Regression(553/999): loss=0.583430662587084, w0=-3.4721497263283726e-06, w1=0.21769832429150673\n",
      "Regularized Logistic Regression(554/999): loss=0.5834146866925924, w0=-3.47574137676401e-06, w1=0.21780256185357785\n",
      "Regularized Logistic Regression(555/999): loss=0.5833988059803268, w0=-3.479327421542657e-06, w1=0.21790637648675118\n",
      "Regularized Logistic Regression(556/999): loss=0.5833830198159949, w0=-3.4829078726862995e-06, w1=0.21800976989932225\n",
      "Regularized Logistic Regression(557/999): loss=0.5833673275698993, w0=-3.486482742181243e-06, w1=0.2181127437924462\n",
      "Regularized Logistic Regression(558/999): loss=0.5833517286169014, w0=-3.4900520419782517e-06, w1=0.2182152998601702\n",
      "Regularized Logistic Regression(559/999): loss=0.5833362223363876, w0=-3.493615783992689e-06, w1=0.2183174397894624\n",
      "Regularized Logistic Regression(560/999): loss=0.5833208081122324, w0=-3.497173980104653e-06, w1=0.21841916526024688\n",
      "Regularized Logistic Regression(561/999): loss=0.5833054853327642, w0=-3.500726642159117e-06, w1=0.21852047794543245\n",
      "Regularized Logistic Regression(562/999): loss=0.5832902533907307, w0=-3.504273781966065e-06, w1=0.2186213795109449\n",
      "Regularized Logistic Regression(563/999): loss=0.5832751116832663, w0=-3.5078154113006275e-06, w1=0.2187218716157574\n",
      "Regularized Logistic Regression(564/999): loss=0.5832600596118551, w0=-3.511351541903218e-06, w1=0.21882195591192094\n",
      "Regularized Logistic Regression(565/999): loss=0.5832450965823005, w0=-3.514882185479668e-06, w1=0.2189216340445962\n",
      "Regularized Logistic Regression(566/999): loss=0.5832302220046895, w0=-3.5184073537013597e-06, w1=0.2190209076520833\n",
      "Regularized Logistic Regression(567/999): loss=0.5832154352933608, w0=-3.5219270582053612e-06, w1=0.21911977836585228\n",
      "Regularized Logistic Regression(568/999): loss=0.5832007358668715, w0=-3.5254413105945577e-06, w1=0.21921824781057248\n",
      "Regularized Logistic Regression(569/999): loss=0.5831861231479649, w0=-3.528950122437785e-06, w1=0.21931631760414422\n",
      "Regularized Logistic Regression(570/999): loss=0.5831715965635385, w0=-3.5324535052699607e-06, w1=0.21941398935772768\n",
      "Regularized Logistic Regression(571/999): loss=0.5831571555446106, w0=-3.535951470592214e-06, w1=0.21951126467577287\n",
      "Regularized Logistic Regression(572/999): loss=0.5831427995262897, w0=-3.539444029872018e-06, w1=0.2196081451560495\n",
      "Regularized Logistic Regression(573/999): loss=0.583128527947744, w0=-3.5429311945433173e-06, w1=0.2197046323896762\n",
      "Regularized Logistic Regression(574/999): loss=0.583114340252168, w0=-3.5464129760066574e-06, w1=0.21980072796115033\n",
      "Regularized Logistic Regression(575/999): loss=0.5831002358867523, w0=-3.5498893856293133e-06, w1=0.21989643344837737\n",
      "Regularized Logistic Regression(576/999): loss=0.583086214302654, w0=-3.5533604347454174e-06, w1=0.2199917504226989\n",
      "Regularized Logistic Regression(577/999): loss=0.5830722749549645, w0=-3.5568261346560856e-06, w1=0.22008668044892363\n",
      "Regularized Logistic Regression(578/999): loss=0.5830584173026809, w0=-3.5602864966295447e-06, w1=0.22018122508535318\n",
      "Regularized Logistic Regression(579/999): loss=0.5830446408086749, w0=-3.5637415319012573e-06, w1=0.22027538588381473\n",
      "Regularized Logistic Regression(580/999): loss=0.5830309449396635, w0=-3.5671912516740473e-06, w1=0.22036916438968604\n",
      "Regularized Logistic Regression(581/999): loss=0.5830173291661795, w0=-3.5706356671182247e-06, w1=0.22046256214192536\n",
      "Regularized Logistic Regression(582/999): loss=0.5830037929625421, w0=-3.5740747893717087e-06, w1=0.22055558067310024\n",
      "Regularized Logistic Regression(583/999): loss=0.5829903358068281, w0=-3.5775086295401524e-06, w1=0.22064822150941554\n",
      "Regularized Logistic Regression(584/999): loss=0.5829769571808432, w0=-3.5809371986970636e-06, w1=0.22074048617074107\n",
      "Regularized Logistic Regression(585/999): loss=0.5829636565700935, w0=-3.5843605078839288e-06, w1=0.2208323761706386\n",
      "Regularized Logistic Regression(586/999): loss=0.5829504334637571, w0=-3.587778568110333e-06, w1=0.22092389301639134\n",
      "Regularized Logistic Regression(587/999): loss=0.5829372873546546, w0=-3.591191390354083e-06, w1=0.22101503820903157\n",
      "Regularized Logistic Regression(588/999): loss=0.5829242177392242, w0=-3.594598985561325e-06, w1=0.2211058132433657\n",
      "Regularized Logistic Regression(589/999): loss=0.5829112241174927, w0=-3.598001364646665e-06, w1=0.22119621960800592\n",
      "Regularized Logistic Regression(590/999): loss=0.5828983059930471, w0=-3.601398538493291e-06, w1=0.22128625878539257\n",
      "Regularized Logistic Regression(591/999): loss=0.582885462873008, w0=-3.604790517953087e-06, w1=0.22137593225182606\n",
      "Regularized Logistic Regression(592/999): loss=0.5828726942680038, w0=-3.6081773138467532e-06, w1=0.22146524147749025\n",
      "Regularized Logistic Regression(593/999): loss=0.5828599996921421, w0=-3.6115589369639247e-06, w1=0.221554187926481\n",
      "Regularized Logistic Regression(594/999): loss=0.5828473786629856, w0=-3.6149353980632863e-06, w1=0.22164277305683244\n",
      "Regularized Logistic Regression(595/999): loss=0.5828348307015233, w0=-3.6183067078726896e-06, w1=0.2217309983205432\n",
      "Regularized Logistic Regression(596/999): loss=0.5828223553321457, w0=-3.621672877089269e-06, w1=0.22181886516360513\n",
      "Regularized Logistic Regression(597/999): loss=0.5828099520826188, w0=-3.625033916379557e-06, w1=0.22190637502602728\n",
      "Regularized Logistic Regression(598/999): loss=0.5827976204840588, w0=-3.6283898363795985e-06, w1=0.22199352934186253\n",
      "Regularized Logistic Regression(599/999): loss=0.5827853600709058, w0=-3.6317406476950647e-06, w1=0.222080329539235\n",
      "Regularized Logistic Regression(600/999): loss=0.5827731703808987, w0=-3.6350863609013674e-06, w1=0.22216677704036472\n",
      "Regularized Logistic Regression(601/999): loss=0.5827610509550513, w0=-3.638426986543771e-06, w1=0.22225287326159454\n",
      "Regularized Logistic Regression(602/999): loss=0.5827490013376257, w0=-3.6417625351375062e-06, w1=0.22233861961341514\n",
      "Regularized Logistic Regression(603/999): loss=0.5827370210761093, w0=-3.645093017167881e-06, w1=0.22242401750049107\n",
      "Regularized Logistic Regression(604/999): loss=0.5827251097211892, w0=-3.648418443090393e-06, w1=0.22250906832168638\n",
      "Regularized Logistic Regression(605/999): loss=0.5827132668267284, w0=-3.6517388233308392e-06, w1=0.22259377347008952\n",
      "Regularized Logistic Regression(606/999): loss=0.5827014919497423, w0=-3.655054168285427e-06, w1=0.22267813433304018\n",
      "Regularized Logistic Regression(607/999): loss=0.5826897846503729, w0=-3.6583644883208844e-06, w1=0.22276215229215182\n",
      "Regularized Logistic Regression(608/999): loss=0.582678144491868, w0=-3.6616697937745685e-06, w1=0.22284582872333894\n",
      "Regularized Logistic Regression(609/999): loss=0.582666571040556, w0=-3.664970094954575e-06, w1=0.2229291649968421\n",
      "Regularized Logistic Regression(610/999): loss=0.5826550638658216, w0=-3.668265402139846e-06, w1=0.2230121624772505\n",
      "Regularized Logistic Regression(611/999): loss=0.5826436225400854, w0=-3.6715557255802785e-06, w1=0.2230948225235294\n",
      "Regularized Logistic Regression(612/999): loss=0.5826322466387787, w0=-3.6748410754968305e-06, w1=0.22317714648904327\n",
      "Regularized Logistic Regression(613/999): loss=0.5826209357403224, w0=-3.678121462081629e-06, w1=0.22325913572158007\n",
      "Regularized Logistic Regression(614/999): loss=0.5826096894261025, w0=-3.681396895498076e-06, w1=0.22334079156337655\n",
      "Regularized Logistic Regression(615/999): loss=0.5825985072804505, w0=-3.6846673858809535e-06, w1=0.22342211535114165\n",
      "Regularized Logistic Regression(616/999): loss=0.5825873888906182, w0=-3.6879329433365296e-06, w1=0.22350310841608173\n",
      "Regularized Logistic Regression(617/999): loss=0.5825763338467574, w0=-3.691193577942663e-06, w1=0.22358377208392324\n",
      "Regularized Logistic Regression(618/999): loss=0.582565341741899, w0=-3.694449299748908e-06, w1=0.22366410767493836\n",
      "Regularized Logistic Regression(619/999): loss=0.5825544121719282, w0=-3.6977001187766165e-06, w1=0.22374411650396658\n",
      "Regularized Logistic Regression(620/999): loss=0.5825435447355665, w0=-3.700946045019044e-06, w1=0.22382379988043977\n",
      "Regularized Logistic Regression(621/999): loss=0.582532739034348, w0=-3.7041870884414492e-06, w1=0.22390315910840627\n",
      "Regularized Logistic Regression(622/999): loss=0.5825219946726, w0=-3.7074232589812e-06, w1=0.223982195486553\n",
      "Regularized Logistic Regression(623/999): loss=0.58251131125742, w0=-3.710654566547873e-06, w1=0.22406091030823014\n",
      "Regularized Logistic Regression(624/999): loss=0.5825006883986568, w0=-3.713881021023354e-06, w1=0.22413930486147324\n",
      "Regularized Logistic Regression(625/999): loss=0.582490125708889, w0=-3.7171026322619434e-06, w1=0.2242173804290278\n",
      "Regularized Logistic Regression(626/999): loss=0.5824796228034045, w0=-3.7203194100904518e-06, w1=0.22429513828836992\n",
      "Regularized Logistic Regression(627/999): loss=0.58246917930018, w0=-3.7235313643083025e-06, w1=0.2243725797117322\n",
      "Regularized Logistic Regression(628/999): loss=0.5824587948198618, w0=-3.726738504687631e-06, w1=0.22444970596612454\n",
      "Regularized Logistic Regression(629/999): loss=0.5824484689857453, w0=-3.729940840973383e-06, w1=0.22452651831335638\n",
      "Regularized Logistic Regression(630/999): loss=0.5824382014237537, w0=-3.733138382883415e-06, w1=0.22460301801006244\n",
      "Regularized Logistic Regression(631/999): loss=0.5824279917624212, w0=-3.736331140108591e-06, w1=0.22467920630772212\n",
      "Regularized Logistic Regression(632/999): loss=0.5824178396328716, w0=-3.7395191223128796e-06, w1=0.22475508445268155\n",
      "Regularized Logistic Regression(633/999): loss=0.5824077446687991, w0=-3.742702339133454e-06, w1=0.22483065368617966\n",
      "Regularized Logistic Regression(634/999): loss=0.5823977065064488, w0=-3.7458808001807863e-06, w1=0.22490591524436673\n",
      "Regularized Logistic Regression(635/999): loss=0.5823877247845994, w0=-3.7490545150387447e-06, w1=0.22498087035832817\n",
      "Regularized Logistic Regression(636/999): loss=0.5823777991445421, w0=-3.7522234932646904e-06, w1=0.22505552025410508\n",
      "Regularized Logistic Regression(637/999): loss=0.5823679292300629, w0=-3.7553877443895726e-06, w1=0.22512986615271843\n",
      "Regularized Logistic Regression(638/999): loss=0.5823581146874242, w0=-3.7585472779180223e-06, w1=0.22520390927018863\n",
      "Regularized Logistic Regression(639/999): loss=0.5823483551653462, w0=-3.7617021033284493e-06, w1=0.22527765081755902\n",
      "Regularized Logistic Regression(640/999): loss=0.5823386503149877, w0=-3.764852230073135e-06, w1=0.2253510920009151\n",
      "Regularized Logistic Regression(641/999): loss=0.5823289997899299, w0=-3.767997667578326e-06, w1=0.22542423402140885\n",
      "Regularized Logistic Regression(642/999): loss=0.5823194032461557, w0=-3.7711384252443293e-06, w1=0.22549707807527883\n",
      "Regularized Logistic Regression(643/999): loss=0.5823098603420349, w0=-3.774274512445603e-06, w1=0.22556962535387026\n",
      "Regularized Logistic Regression(644/999): loss=0.582300370738303, w0=-3.777405938530851e-06, w1=0.22564187704365757\n",
      "Regularized Logistic Regression(645/999): loss=0.5822909340980472, w0=-3.7805327128231133e-06, w1=0.22571383432626596\n",
      "Regularized Logistic Regression(646/999): loss=0.5822815500866863, w0=-3.7836548446198587e-06, w1=0.22578549837849085\n",
      "Regularized Logistic Regression(647/999): loss=0.5822722183719544, w0=-3.786772343193076e-06, w1=0.2258568703723202\n",
      "Regularized Logistic Regression(648/999): loss=0.5822629386238835, w0=-3.7898852177893652e-06, w1=0.2259279514749537\n",
      "Regularized Logistic Regression(649/999): loss=0.5822537105147868, w0=-3.7929934776300277e-06, w1=0.22599874284882526\n",
      "Regularized Logistic Regression(650/999): loss=0.582244533719242, w0=-3.796097131911156e-06, w1=0.22606924565162262\n",
      "Regularized Logistic Regression(651/999): loss=0.5822354079140727, w0=-3.7991961898037246e-06, w1=0.2261394610363083\n",
      "Regularized Logistic Regression(652/999): loss=0.5822263327783347, w0=-3.8022906604536776e-06, w1=0.22620939015113886\n",
      "Regularized Logistic Regression(653/999): loss=0.5822173079932967, w0=-3.8053805529820197e-06, w1=0.2262790341396874\n",
      "Regularized Logistic Regression(654/999): loss=0.5822083332424258, w0=-3.8084658764849026e-06, w1=0.22634839414086108\n",
      "Regularized Logistic Regression(655/999): loss=0.5821994082113704, w0=-3.8115466400337145e-06, w1=0.22641747128892292\n",
      "Regularized Logistic Regression(656/999): loss=0.5821905325879438, w0=-3.8146228526751676e-06, w1=0.22648626671351202\n",
      "Regularized Logistic Regression(657/999): loss=0.58218170606211, w0=-3.817694523431385e-06, w1=0.22655478153966274\n",
      "Regularized Logistic Regression(658/999): loss=0.5821729283259645, w0=-3.820761661299987e-06, w1=0.2266230168878241\n",
      "Regularized Logistic Regression(659/999): loss=0.5821641990737223, w0=-3.823824275254181e-06, w1=0.22669097387388112\n",
      "Regularized Logistic Regression(660/999): loss=0.5821555180016995, w0=-3.826882374242842e-06, w1=0.2267586536091732\n",
      "Regularized Logistic Regression(661/999): loss=0.5821468848082989, w0=-3.8299359671906045e-06, w1=0.2268260572005144\n",
      "Regularized Logistic Regression(662/999): loss=0.5821382991939943, w0=-3.8329850629979435e-06, w1=0.22689318575021258\n",
      "Regularized Logistic Regression(663/999): loss=0.5821297608613162, w0=-3.836029670541261e-06, w1=0.2269600403560878\n",
      "Regularized Logistic Regression(664/999): loss=0.5821212695148356, w0=-3.839069798672972e-06, w1=0.22702662211149333\n",
      "Regularized Logistic Regression(665/999): loss=0.5821128248611483, w0=-3.842105456221586e-06, w1=0.22709293210533535\n",
      "Regularized Logistic Regression(666/999): loss=0.5821044266088624, w0=-3.845136651991793e-06, w1=0.22715897142208816\n",
      "Regularized Logistic Regression(667/999): loss=0.5820960744685816, w0=-3.848163394764548e-06, w1=0.22722474114181843\n",
      "Regularized Logistic Regression(668/999): loss=0.5820877681528903, w0=-3.8511856932971495e-06, w1=0.22729024234019918\n",
      "Regularized Logistic Regression(669/999): loss=0.5820795073763404, w0=-3.854203556323329e-06, w1=0.22735547608853418\n",
      "Regularized Logistic Regression(670/999): loss=0.582071291855436, w0=-3.857216992553327e-06, w1=0.22742044345377052\n",
      "Regularized Logistic Regression(671/999): loss=0.5820631213086198, w0=-3.860226010673981e-06, w1=0.22748514549852186\n",
      "Regularized Logistic Regression(672/999): loss=0.582054995456257, w0=-3.863230619348801e-06, w1=0.22754958328108604\n",
      "Regularized Logistic Regression(673/999): loss=0.5820469140206241, w0=-3.866230827218058e-06, w1=0.22761375785546228\n",
      "Regularized Logistic Regression(674/999): loss=0.5820388767258922, w0=-3.869226642898859e-06, w1=0.22767767027137026\n",
      "Regularized Logistic Regression(675/999): loss=0.5820308832981145, w0=-3.872218074985229e-06, w1=0.22774132157426893\n",
      "Regularized Logistic Regression(676/999): loss=0.5820229334652125, w0=-3.875205132048195e-06, w1=0.2278047128053746\n",
      "Regularized Logistic Regression(677/999): loss=0.5820150269569621, w0=-3.8781878226358605e-06, w1=0.22786784500167867\n",
      "Regularized Logistic Regression(678/999): loss=0.5820071635049794, w0=-3.88116615527349e-06, w1=0.2279307191959656\n",
      "Regularized Logistic Regression(679/999): loss=0.581999342842708, w0=-3.884140138463586e-06, w1=0.22799333641683261\n",
      "Regularized Logistic Regression(680/999): loss=0.5819915647054069, w0=-3.887109780685965e-06, w1=0.22805569768870407\n",
      "Regularized Logistic Regression(681/999): loss=0.5819838288301333, w0=-3.890075090397844e-06, w1=0.22811780403185342\n",
      "Regularized Logistic Regression(682/999): loss=0.5819761349557339, w0=-3.89303607603391e-06, w1=0.22817965646241786\n",
      "Regularized Logistic Regression(683/999): loss=0.5819684828228293, w0=-3.895992746006404e-06, w1=0.22824125599241915\n",
      "Regularized Logistic Regression(684/999): loss=0.5819608721738013, w0=-3.898945108705195e-06, w1=0.228302603629777\n",
      "Regularized Logistic Regression(685/999): loss=0.5819533027527813, w0=-3.9018931724978615e-06, w1=0.22836370037833023\n",
      "Regularized Logistic Regression(686/999): loss=0.5819457743056355, w0=-3.904836945729762e-06, w1=0.228424547237852\n",
      "Regularized Logistic Regression(687/999): loss=0.5819382865799548, w0=-3.907776436724118e-06, w1=0.22848514520406807\n",
      "Regularized Logistic Regression(688/999): loss=0.5819308393250393, w0=-3.910711653782086e-06, w1=0.22854549526867454\n",
      "Regularized Logistic Regression(689/999): loss=0.5819234322918887, w0=-3.913642605182836e-06, w1=0.22860559841935335\n",
      "Regularized Logistic Regression(690/999): loss=0.5819160652331877, w0=-3.9165692991836245e-06, w1=0.22866545563979146\n",
      "Regularized Logistic Regression(691/999): loss=0.5819087379032958, w0=-3.919491744019873e-06, w1=0.22872506790969638\n",
      "Regularized Logistic Regression(692/999): loss=0.5819014500582321, w0=-3.92240994790524e-06, w1=0.22878443620481395\n",
      "Regularized Logistic Regression(693/999): loss=0.5818942014556676, w0=-3.925323919031698e-06, w1=0.2288435614969444\n",
      "Regularized Logistic Regression(694/999): loss=0.5818869918549089, w0=-3.928233665569606e-06, w1=0.22890244475396074\n",
      "Regularized Logistic Regression(695/999): loss=0.5818798210168888, w0=-3.931139195667786e-06, w1=0.22896108693982248\n",
      "Regularized Logistic Regression(696/999): loss=0.5818726887041539, w0=-3.9340405174535915e-06, w1=0.22901948901459535\n",
      "Regularized Logistic Regression(697/999): loss=0.5818655946808524, w0=-3.936937639032988e-06, w1=0.2290776519344673\n",
      "Regularized Logistic Regression(698/999): loss=0.5818585387127237, w0=-3.93983056849062e-06, w1=0.22913557665176348\n",
      "Regularized Logistic Regression(699/999): loss=0.5818515205670856, w0=-3.942719313889887e-06, w1=0.22919326411496427\n",
      "Regularized Logistic Regression(700/999): loss=0.5818445400128234, w0=-3.945603883273017e-06, w1=0.22925071526871896\n",
      "Regularized Logistic Regression(701/999): loss=0.5818375968203786, w0=-3.948484284661133e-06, w1=0.22930793105386651\n",
      "Regularized Logistic Regression(702/999): loss=0.5818306907617377, w0=-3.9513605260543305e-06, w1=0.2293649124074469\n",
      "Regularized Logistic Regression(703/999): loss=0.5818238216104205, w0=-3.954232615431748e-06, w1=0.22942166026272162\n",
      "Regularized Logistic Regression(704/999): loss=0.5818169891414698, w0=-3.957100560751638e-06, w1=0.229478175549186\n",
      "Regularized Logistic Regression(705/999): loss=0.5818101931314397, w0=-3.959964369951433e-06, w1=0.2295344591925873\n",
      "Regularized Logistic Regression(706/999): loss=0.581803433358385, w0=-3.962824050947826e-06, w1=0.2295905121149406\n",
      "Regularized Logistic Regression(707/999): loss=0.5817967096018506, w0=-3.9656796116368305e-06, w1=0.22964633523454264\n",
      "Regularized Logistic Regression(708/999): loss=0.5817900216428602, w0=-3.968531059893859e-06, w1=0.22970192946599008\n",
      "Regularized Logistic Regression(709/999): loss=0.5817833692639058, w0=-3.971378403573787e-06, w1=0.2297572957201943\n",
      "Regularized Logistic Regression(710/999): loss=0.5817767522489375, w0=-3.9742216505110255e-06, w1=0.22981243490439665\n",
      "Regularized Logistic Regression(711/999): loss=0.5817701703833528, w0=-3.977060808519588e-06, w1=0.22986734792218352\n",
      "Regularized Logistic Regression(712/999): loss=0.5817636234539856, w0=-3.979895885393163e-06, w1=0.22992203567350328\n",
      "Regularized Logistic Regression(713/999): loss=0.5817571112490963, w0=-3.982726888905177e-06, w1=0.22997649905468046\n",
      "Regularized Logistic Regression(714/999): loss=0.5817506335583624, w0=-3.985553826808868e-06, w1=0.23003073895843118\n",
      "Regularized Logistic Regression(715/999): loss=0.5817441901728664, w0=-3.98837670683735e-06, w1=0.2300847562738785\n",
      "Regularized Logistic Regression(716/999): loss=0.5817377808850877, w0=-3.991195536703685e-06, w1=0.23013855188656815\n",
      "Regularized Logistic Regression(717/999): loss=0.5817314054888907, w0=-3.994010324100944e-06, w1=0.23019212667848435\n",
      "Regularized Logistic Regression(718/999): loss=0.5817250637795168, w0=-3.996821076702281e-06, w1=0.23024548152806087\n",
      "Regularized Logistic Regression(719/999): loss=0.581718755553572, w0=-3.999627802160994e-06, w1=0.23029861731020193\n",
      "Regularized Logistic Regression(720/999): loss=0.5817124806090204, w0=-4.002430508110596e-06, w1=0.23035153489629243\n",
      "Regularized Logistic Regression(721/999): loss=0.5817062387451711, w0=-4.005229202164879e-06, w1=0.23040423515421424\n",
      "Regularized Logistic Regression(722/999): loss=0.5817000297626712, w0=-4.008023891917981e-06, w1=0.23045671894836262\n",
      "Regularized Logistic Regression(723/999): loss=0.5816938534634938, w0=-4.010814584944451e-06, w1=0.23050898713965728\n",
      "Regularized Logistic Regression(724/999): loss=0.581687709650931, w0=-4.013601288799316e-06, w1=0.23056104058556104\n",
      "Regularized Logistic Regression(725/999): loss=0.5816815981295828, w0=-4.016384011018143e-06, w1=0.23061288014009115\n",
      "Regularized Logistic Regression(726/999): loss=0.5816755187053481, w0=-4.01916275911711e-06, w1=0.2306645066538361\n",
      "Regularized Logistic Regression(727/999): loss=0.5816694711854155, w0=-4.021937540593062e-06, w1=0.23071592097396934\n",
      "Regularized Logistic Regression(728/999): loss=0.5816634553782539, w0=-4.024708362923584e-06, w1=0.2307671239442623\n",
      "Regularized Logistic Regression(729/999): loss=0.581657471093604, w0=-4.02747523356706e-06, w1=0.23081811640510078\n",
      "Regularized Logistic Regression(730/999): loss=0.5816515181424678, w0=-4.0302381599627385e-06, w1=0.230868899193498\n",
      "Regularized Logistic Regression(731/999): loss=0.581645596337102, w0=-4.0329971495307965e-06, w1=0.2309194731431089\n",
      "Regularized Logistic Regression(732/999): loss=0.5816397054910055, w0=-4.035752209672402e-06, w1=0.23096983908424512\n",
      "Regularized Logistic Regression(733/999): loss=0.5816338454189149, w0=-4.0385033477697775e-06, w1=0.23101999784388716\n",
      "Regularized Logistic Regression(734/999): loss=0.5816280159367913, w0=-4.0412505711862635e-06, w1=0.23106995024569996\n",
      "Regularized Logistic Regression(735/999): loss=0.5816222168618139, w0=-4.04399388726638e-06, w1=0.23111969711004704\n",
      "Regularized Logistic Regression(736/999): loss=0.581616448012373, w0=-4.046733303335889e-06, w1=0.23116923925400237\n",
      "Regularized Logistic Regression(737/999): loss=0.5816107092080566, w0=-4.049468826701857e-06, w1=0.2312185774913667\n",
      "Regularized Logistic Regression(738/999): loss=0.5816050002696466, w0=-4.052200464652717e-06, w1=0.23126771263267967\n",
      "Regularized Logistic Regression(739/999): loss=0.5815993210191075, w0=-4.05492822445833e-06, w1=0.23131664548523398\n",
      "Regularized Logistic Regression(740/999): loss=0.5815936712795793, w0=-4.057652113370047e-06, w1=0.2313653768530883\n",
      "Regularized Logistic Regression(741/999): loss=0.5815880508753687, w0=-4.060372138620767e-06, w1=0.23141390753708246\n",
      "Regularized Logistic Regression(742/999): loss=0.58158245963194, w0=-4.063088307425002e-06, w1=0.23146223833484883\n",
      "Regularized Logistic Regression(743/999): loss=0.5815768973759091, w0=-4.065800626978936e-06, w1=0.2315103700408277\n",
      "Regularized Logistic Regression(744/999): loss=0.5815713639350325, w0=-4.068509104460483e-06, w1=0.23155830344627928\n",
      "Regularized Logistic Regression(745/999): loss=0.5815658591382011, w0=-4.07121374702935e-06, w1=0.23160603933929788\n",
      "Regularized Logistic Regression(746/999): loss=0.5815603828154319, w0=-4.073914561827097e-06, w1=0.2316535785048245\n",
      "Regularized Logistic Regression(747/999): loss=0.5815549347978596, w0=-4.0766115559771945e-06, w1=0.2317009217246596\n",
      "Regularized Logistic Regression(748/999): loss=0.5815495149177288, w0=-4.079304736585085e-06, w1=0.23174806977747822\n",
      "Regularized Logistic Regression(749/999): loss=0.5815441230083859, w0=-4.08199411073824e-06, w1=0.23179502343884062\n",
      "Regularized Logistic Regression(750/999): loss=0.5815387589042722, w0=-4.08467968550622e-06, w1=0.23184178348120787\n",
      "Regularized Logistic Regression(751/999): loss=0.5815334224409145, w0=-4.0873614679407335e-06, w1=0.23188835067395136\n",
      "Regularized Logistic Regression(752/999): loss=0.58152811345492, w0=-4.090039465075696e-06, w1=0.23193472578336896\n",
      "Regularized Logistic Regression(753/999): loss=0.5815228317839658, w0=-4.0927136839272855e-06, w1=0.2319809095726965\n",
      "Regularized Logistic Regression(754/999): loss=0.5815175772667924, w0=-4.095384131494003e-06, w1=0.23202690280211943\n",
      "Regularized Logistic Regression(755/999): loss=0.5815123497431975, w0=-4.09805081475673e-06, w1=0.2320727062287884\n",
      "Regularized Logistic Regression(756/999): loss=0.5815071490540268, w0=-4.100713740678785e-06, w1=0.23211832060682921\n",
      "Regularized Logistic Regression(757/999): loss=0.5815019750411672, w0=-4.10337291620598e-06, w1=0.2321637466873566\n",
      "Regularized Logistic Regression(758/999): loss=0.58149682754754, w0=-4.106028348266679e-06, w1=0.23220898521848568\n",
      "Regularized Logistic Regression(759/999): loss=0.5814917064170924, w0=-4.108680043771856e-06, w1=0.2322540369453477\n",
      "Regularized Logistic Regression(760/999): loss=0.5814866114947912, w0=-4.1113280096151486e-06, w1=0.23229890261009778\n",
      "Regularized Logistic Regression(761/999): loss=0.5814815426266159, w0=-4.113972252672915e-06, w1=0.23234358295193103\n",
      "Regularized Logistic Regression(762/999): loss=0.5814764996595497, w0=-4.116612779804293e-06, w1=0.23238807870709277\n",
      "Regularized Logistic Regression(763/999): loss=0.5814714824415758, w0=-4.119249597851252e-06, w1=0.23243239060889184\n",
      "Regularized Logistic Regression(764/999): loss=0.5814664908216661, w0=-4.121882713638651e-06, w1=0.23247651938771205\n",
      "Regularized Logistic Regression(765/999): loss=0.5814615246497779, w0=-4.124512133974294e-06, w1=0.23252046577102553\n",
      "Regularized Logistic Regression(766/999): loss=0.5814565837768451, w0=-4.1271378656489825e-06, w1=0.23256423048340327\n",
      "Regularized Logistic Regression(767/999): loss=0.5814516680547718, w0=-4.129759915436575e-06, w1=0.23260781424652965\n",
      "Regularized Logistic Regression(768/999): loss=0.5814467773364256, w0=-4.1323782900940395e-06, w1=0.23265121777921063\n",
      "Regularized Logistic Regression(769/999): loss=0.5814419114756304, w0=-4.134992996361506e-06, w1=0.23269444179738916\n",
      "Regularized Logistic Regression(770/999): loss=0.5814370703271602, w0=-4.1376040409623245e-06, w1=0.23273748701415453\n",
      "Regularized Logistic Regression(771/999): loss=0.5814322537467318, w0=-4.140211430603116e-06, w1=0.23278035413975673\n",
      "Regularized Logistic Regression(772/999): loss=0.5814274615909993, w0=-4.14281517197383e-06, w1=0.2328230438816163\n",
      "Regularized Logistic Regression(773/999): loss=0.581422693717546, w0=-4.145415271747794e-06, w1=0.23286555694433675\n",
      "Regularized Logistic Regression(774/999): loss=0.5814179499848797, w0=-4.1480117365817695e-06, w1=0.23290789402971626\n",
      "Regularized Logistic Regression(775/999): loss=0.5814132302524239, w0=-4.150604573116006e-06, w1=0.23295005583675873\n",
      "Regularized Logistic Regression(776/999): loss=0.5814085343805139, w0=-4.153193787974293e-06, w1=0.23299204306168575\n",
      "Regularized Logistic Regression(777/999): loss=0.5814038622303886, w0=-4.15577938776401e-06, w1=0.23303385639795\n",
      "Regularized Logistic Regression(778/999): loss=0.5813992136641852, w0=-4.158361379076186e-06, w1=0.23307549653624218\n",
      "Regularized Logistic Regression(779/999): loss=0.5813945885449321, w0=-4.160939768485544e-06, w1=0.2331169641645069\n",
      "Regularized Logistic Regression(780/999): loss=0.5813899867365433, w0=-4.163514562550561e-06, w1=0.23315825996795234\n",
      "Regularized Logistic Regression(781/999): loss=0.5813854081038121, w0=-4.166085767813513e-06, w1=0.23319938462906042\n",
      "Regularized Logistic Regression(782/999): loss=0.5813808525124046, w0=-4.16865339080053e-06, w1=0.2332403388276005\n",
      "Regularized Logistic Regression(783/999): loss=0.5813763198288545, w0=-4.17121743802165e-06, w1=0.2332811232406384\n",
      "Regularized Logistic Regression(784/999): loss=0.5813718099205557, w0=-4.173777915970867e-06, w1=0.23332173854254887\n",
      "Regularized Logistic Regression(785/999): loss=0.5813673226557572, w0=-4.176334831126182e-06, w1=0.2333621854050264\n",
      "Regularized Logistic Regression(786/999): loss=0.5813628579035575, w0=-4.178888189949656e-06, w1=0.23340246449709576\n",
      "Regularized Logistic Regression(787/999): loss=0.5813584155338971, w0=-4.181437998887461e-06, w1=0.23344257648512376\n",
      "Regularized Logistic Regression(788/999): loss=0.5813539954175545, w0=-4.183984264369929e-06, w1=0.23348252203282946\n",
      "Regularized Logistic Regression(789/999): loss=0.58134959742614, w0=-4.186526992811602e-06, w1=0.23352230180129585\n",
      "Regularized Logistic Regression(790/999): loss=0.5813452214320879, w0=-4.189066190611284e-06, w1=0.23356191644898064\n",
      "Regularized Logistic Regression(791/999): loss=0.5813408673086535, w0=-4.191601864152092e-06, w1=0.23360136663172648\n",
      "Regularized Logistic Regression(792/999): loss=0.5813365349299062, w0=-4.1941340198015024e-06, w1=0.23364065300277265\n",
      "Regularized Logistic Regression(793/999): loss=0.5813322241707232, w0=-4.196662663911403e-06, w1=0.23367977621276426\n",
      "Regularized Logistic Regression(794/999): loss=0.5813279349067851, w0=-4.1991878028181415e-06, w1=0.23371873690976458\n",
      "Regularized Logistic Regression(795/999): loss=0.58132366701457, w0=-4.201709442842575e-06, w1=0.2337575357392651\n",
      "Regularized Logistic Regression(796/999): loss=0.5813194203713467, w0=-4.20422759029012e-06, w1=0.2337961733441962\n",
      "Regularized Logistic Regression(797/999): loss=0.581315194855171, w0=-4.206742251450799e-06, w1=0.23383465036493747\n",
      "Regularized Logistic Regression(798/999): loss=0.5813109903448797, w0=-4.209253432599294e-06, w1=0.23387296743932737\n",
      "Regularized Logistic Regression(799/999): loss=0.5813068067200843, w0=-4.2117611399949865e-06, w1=0.23391112520267543\n",
      "Regularized Logistic Regression(800/999): loss=0.5813026438611668, w0=-4.214265379882016e-06, w1=0.23394912428777143\n",
      "Regularized Logistic Regression(801/999): loss=0.5812985016492737, w0=-4.216766158489319e-06, w1=0.23398696532489638\n",
      "Regularized Logistic Regression(802/999): loss=0.581294379966311, w0=-4.2192634820306845e-06, w1=0.23402464894183259\n",
      "Regularized Logistic Regression(803/999): loss=0.5812902786949382, w0=-4.221757356704795e-06, w1=0.23406217576387425\n",
      "Regularized Logistic Regression(804/999): loss=0.5812861977185648, w0=-4.224247788695279e-06, w1=0.2340995464138357\n",
      "Regularized Logistic Regression(805/999): loss=0.5812821369213433, w0=-4.226734784170756e-06, w1=0.2341367615120656\n",
      "Regularized Logistic Regression(806/999): loss=0.5812780961881646, w0=-4.229218349284885e-06, w1=0.23417382167645237\n",
      "Regularized Logistic Regression(807/999): loss=0.581274075404654, w0=-4.231698490176411e-06, w1=0.23421072752243777\n",
      "Regularized Logistic Regression(808/999): loss=0.5812700744571646, w0=-4.23417521296921e-06, w1=0.23424747966302678\n",
      "Regularized Logistic Regression(809/999): loss=0.5812660932327722, w0=-4.23664852377234e-06, w1=0.23428407870879403\n",
      "Regularized Logistic Regression(810/999): loss=0.5812621316192729, w0=-4.239118428680083e-06, w1=0.23432052526789868\n",
      "Regularized Logistic Regression(811/999): loss=0.581258189505174, w0=-4.241584933771993e-06, w1=0.23435681994609067\n",
      "Regularized Logistic Regression(812/999): loss=0.5812542667796936, w0=-4.2440480451129434e-06, w1=0.2343929633467224\n",
      "Regularized Logistic Regression(813/999): loss=0.581250363332751, w0=-4.24650776875317e-06, w1=0.2344289560707574\n",
      "Regularized Logistic Regression(814/999): loss=0.5812464790549672, w0=-4.24896411072832e-06, w1=0.23446479871678114\n",
      "Regularized Logistic Regression(815/999): loss=0.5812426138376541, w0=-4.251417077059497e-06, w1=0.23450049188101063\n",
      "Regularized Logistic Regression(816/999): loss=0.5812387675728157, w0=-4.253866673753303e-06, w1=0.23453603615730312\n",
      "Regularized Logistic Regression(817/999): loss=0.5812349401531381, w0=-4.256312906801889e-06, w1=0.23457143213716783\n",
      "Regularized Logistic Regression(818/999): loss=0.5812311314719886, w0=-4.2587557821829935e-06, w1=0.23460668040977165\n",
      "Regularized Logistic Regression(819/999): loss=0.5812273414234095, w0=-4.2611953058599944e-06, w1=0.2346417815619541\n",
      "Regularized Logistic Regression(820/999): loss=0.5812235699021128, w0=-4.263631483781949e-06, w1=0.23467673617823273\n",
      "Regularized Logistic Regression(821/999): loss=0.5812198168034776, w0=-4.26606432188364e-06, w1=0.23471154484081297\n",
      "Regularized Logistic Regression(822/999): loss=0.5812160820235431, w0=-4.268493826085622e-06, w1=0.2347462081296007\n",
      "Regularized Logistic Regression(823/999): loss=0.5812123654590058, w0=-4.27092000229426e-06, w1=0.23478072662220745\n",
      "Regularized Logistic Regression(824/999): loss=0.5812086670072143, w0=-4.273342856401779e-06, w1=0.2348151008939617\n",
      "Regularized Logistic Regression(825/999): loss=0.581204986566166, w0=-4.275762394286307e-06, w1=0.23484933151792065\n",
      "Regularized Logistic Regression(826/999): loss=0.5812013240344999, w0=-4.278178621811915e-06, w1=0.23488341906487437\n",
      "Regularized Logistic Regression(827/999): loss=0.5811976793114949, w0=-4.280591544828666e-06, w1=0.23491736410335892\n",
      "Regularized Logistic Regression(828/999): loss=0.5811940522970649, w0=-4.283001169172652e-06, w1=0.2349511671996642\n",
      "Regularized Logistic Regression(829/999): loss=0.581190442891753, w0=-4.285407500666045e-06, w1=0.23498482891784303\n",
      "Regularized Logistic Regression(830/999): loss=0.5811868509967296, w0=-4.287810545117133e-06, w1=0.2350183498197217\n",
      "Regularized Logistic Regression(831/999): loss=0.5811832765137851, w0=-4.290210308320366e-06, w1=0.2350517304649066\n",
      "Regularized Logistic Regression(832/999): loss=0.5811797193453289, w0=-4.2926067960563996e-06, w1=0.2350849714107951\n",
      "Regularized Logistic Regression(833/999): loss=0.5811761793943825, w0=-4.295000014092135e-06, w1=0.23511807321258396\n",
      "Regularized Logistic Regression(834/999): loss=0.5811726565645771, w0=-4.297389968180763e-06, w1=0.23515103642327742\n",
      "Regularized Logistic Regression(835/999): loss=0.5811691507601477, w0=-4.299776664061809e-06, w1=0.23518386159369822\n",
      "Regularized Logistic Regression(836/999): loss=0.5811656618859314, w0=-4.302160107461167e-06, w1=0.23521654927249563\n",
      "Regularized Logistic Regression(837/999): loss=0.5811621898473611, w0=-4.3045403040911515e-06, w1=0.23524910000615182\n",
      "Regularized Logistic Regression(838/999): loss=0.5811587345504627, w0=-4.306917259650531e-06, w1=0.23528151433899563\n",
      "Regularized Logistic Regression(839/999): loss=0.5811552959018498, w0=-4.309290979824574e-06, w1=0.23531379281320572\n",
      "Regularized Logistic Regression(840/999): loss=0.5811518738087218, w0=-4.31166147028509e-06, w1=0.23534593596882386\n",
      "Regularized Logistic Regression(841/999): loss=0.5811484681788575, w0=-4.314028736690471e-06, w1=0.2353779443437609\n",
      "Regularized Logistic Regression(842/999): loss=0.5811450789206128, w0=-4.3163927846857316e-06, w1=0.23540981847380704\n",
      "Regularized Logistic Regression(843/999): loss=0.5811417059429167, w0=-4.318753619902549e-06, w1=0.23544155889263926\n",
      "Regularized Logistic Regression(844/999): loss=0.5811383491552663, w0=-4.321111247959305e-06, w1=0.23547316613183117\n",
      "Regularized Logistic Regression(845/999): loss=0.5811350084677236, w0=-4.32346567446113e-06, w1=0.2355046407208597\n",
      "Regularized Logistic Regression(846/999): loss=0.5811316837909122, w0=-4.325816904999937e-06, w1=0.2355359831871148\n",
      "Regularized Logistic Regression(847/999): loss=0.581128375036013, w0=-4.328164945154468e-06, w1=0.23556719405590912\n",
      "Regularized Logistic Regression(848/999): loss=0.5811250821147597, w0=-4.330509800490329e-06, w1=0.23559827385048338\n",
      "Regularized Logistic Regression(849/999): loss=0.5811218049394367, w0=-4.332851476560035e-06, w1=0.23562922309201756\n",
      "Regularized Logistic Regression(850/999): loss=0.5811185434228731, w0=-4.335189978903045e-06, w1=0.23566004229963733\n",
      "Regularized Logistic Regression(851/999): loss=0.581115297478442, w0=-4.337525313045808e-06, w1=0.23569073199042387\n",
      "Regularized Logistic Regression(852/999): loss=0.581112067020054, w0=-4.339857484501795e-06, w1=0.2357212926794222\n",
      "Regularized Logistic Regression(853/999): loss=0.5811088519621553, w0=-4.342186498771547e-06, w1=0.23575172487964818\n",
      "Regularized Logistic Regression(854/999): loss=0.5811056522197225, w0=-4.344512361342705e-06, w1=0.2357820291020958\n",
      "Regularized Logistic Regression(855/999): loss=0.5811024677082616, w0=-4.346835077690059e-06, w1=0.23581220585574805\n",
      "Regularized Logistic Regression(856/999): loss=0.5810992983438015, w0=-4.3491546532755775e-06, w1=0.23584225564758485\n",
      "Regularized Logistic Regression(857/999): loss=0.5810961440428919, w0=-4.3514710935484545e-06, w1=0.23587217898258872\n",
      "Regularized Logistic Regression(858/999): loss=0.5810930047226007, w0=-4.353784403945142e-06, w1=0.23590197636375546\n",
      "Regularized Logistic Regression(859/999): loss=0.5810898803005081, w0=-4.356094589889394e-06, w1=0.23593164829209975\n",
      "Regularized Logistic Regression(860/999): loss=0.5810867706947059, w0=-4.358401656792301e-06, w1=0.23596119526666512\n",
      "Regularized Logistic Regression(861/999): loss=0.5810836758237916, w0=-4.360705610052329e-06, w1=0.23599061778453154\n",
      "Regularized Logistic Regression(862/999): loss=0.5810805956068664, w0=-4.363006455055359e-06, w1=0.23601991634082276\n",
      "Regularized Logistic Regression(863/999): loss=0.5810775299635316, w0=-4.365304197174725e-06, w1=0.23604909142871358\n",
      "Regularized Logistic Regression(864/999): loss=0.581074478813885, w0=-4.367598841771251e-06, w1=0.23607814353943982\n",
      "Regularized Logistic Regression(865/999): loss=0.5810714420785176, w0=-4.369890394193289e-06, w1=0.23610707316230525\n",
      "Regularized Logistic Regression(866/999): loss=0.5810684196785105, w0=-4.372178859776755e-06, w1=0.23613588078468817\n",
      "Regularized Logistic Regression(867/999): loss=0.5810654115354311, w0=-4.374464243845172e-06, w1=0.23616456689204962\n",
      "Regularized Logistic Regression(868/999): loss=0.5810624175713313, w0=-4.3767465517097e-06, w1=0.23619313196794342\n",
      "Regularized Logistic Regression(869/999): loss=0.5810594377087409, w0=-4.379025788669178e-06, w1=0.2362215764940205\n",
      "Regularized Logistic Regression(870/999): loss=0.5810564718706688, w0=-4.38130196001016e-06, w1=0.23624990095003917\n",
      "Regularized Logistic Regression(871/999): loss=0.5810535199805963, w0=-4.383575071006952e-06, w1=0.23627810581387138\n",
      "Regularized Logistic Regression(872/999): loss=0.581050581962476, w0=-4.385845126921648e-06, w1=0.2363061915615095\n",
      "Regularized Logistic Regression(873/999): loss=0.5810476577407274, w0=-4.388112133004168e-06, w1=0.23633415866707722\n",
      "Regularized Logistic Regression(874/999): loss=0.5810447472402341, w0=-4.3903760944922916e-06, w1=0.2363620076028332\n",
      "Regularized Logistic Regression(875/999): loss=0.5810418503863415, w0=-4.3926370166117e-06, w1=0.23638973883918013\n",
      "Regularized Logistic Regression(876/999): loss=0.5810389671048526, w0=-4.3948949045760055e-06, w1=0.23641735284467383\n",
      "Regularized Logistic Regression(877/999): loss=0.5810360973220244, w0=-4.397149763586793e-06, w1=0.23644485008602853\n",
      "Regularized Logistic Regression(878/999): loss=0.5810332409645683, w0=-4.399401598833654e-06, w1=0.23647223102812448\n",
      "Regularized Logistic Regression(879/999): loss=0.5810303979596415, w0=-4.401650415494221e-06, w1=0.2364994961340161\n",
      "Regularized Logistic Regression(880/999): loss=0.5810275682348499, w0=-4.4038962187342055e-06, w1=0.2365266458649392\n",
      "Regularized Logistic Regression(881/999): loss=0.5810247517182402, w0=-4.4061390137074335e-06, w1=0.23655368068031832\n",
      "Regularized Logistic Regression(882/999): loss=0.5810219483383003, w0=-4.408378805555879e-06, w1=0.23658060103777287\n",
      "Regularized Logistic Regression(883/999): loss=0.5810191580239548, w0=-4.4106155994097015e-06, w1=0.23660740739312733\n",
      "Regularized Logistic Regression(884/999): loss=0.5810163807045621, w0=-4.4128494003872795e-06, w1=0.236634100200414\n",
      "Regularized Logistic Regression(885/999): loss=0.5810136163099118, w0=-4.415080213595248e-06, w1=0.2366606799118836\n",
      "Regularized Logistic Regression(886/999): loss=0.5810108647702225, w0=-4.41730804412853e-06, w1=0.23668714697801205\n",
      "Regularized Logistic Regression(887/999): loss=0.5810081260161378, w0=-4.419532897070373e-06, w1=0.2367135018475064\n",
      "Regularized Logistic Regression(888/999): loss=0.5810053999787239, w0=-4.4217547774923865e-06, w1=0.2367397449673131\n",
      "Regularized Logistic Regression(889/999): loss=0.5810026865894664, w0=-4.423973690454572e-06, w1=0.2367658767826243\n",
      "Regularized Logistic Regression(890/999): loss=0.580999985780269, w0=-4.426189641005359e-06, w1=0.23679189773688467\n",
      "Regularized Logistic Regression(891/999): loss=0.5809972974834487, w0=-4.428402634181643e-06, w1=0.23681780827179938\n",
      "Regularized Logistic Regression(892/999): loss=0.5809946216317347, w0=-4.430612675008812e-06, w1=0.23684360882734104\n",
      "Regularized Logistic Regression(893/999): loss=0.5809919581582641, w0=-4.432819768500788e-06, w1=0.23686929984175456\n",
      "Regularized Logistic Regression(894/999): loss=0.5809893069965807, w0=-4.435023919660058e-06, w1=0.23689488175156742\n",
      "Regularized Logistic Regression(895/999): loss=0.5809866680806318, w0=-4.437225133477707e-06, w1=0.23692035499159372\n",
      "Regularized Logistic Regression(896/999): loss=0.5809840413447651, w0=-4.439423414933453e-06, w1=0.2369457199949428\n",
      "Regularized Logistic Regression(897/999): loss=0.5809814267237261, w0=-4.44161876899568e-06, w1=0.236970977193025\n",
      "Regularized Logistic Regression(898/999): loss=0.5809788241526557, w0=-4.443811200621472e-06, w1=0.2369961270155589\n",
      "Regularized Logistic Regression(899/999): loss=0.5809762335670877, w0=-4.446000714756645e-06, w1=0.23702116989057734\n",
      "Regularized Logistic Regression(900/999): loss=0.5809736549029464, w0=-4.448187316335784e-06, w1=0.23704610624443676\n",
      "Regularized Logistic Regression(901/999): loss=0.5809710880965427, w0=-4.450371010282271e-06, w1=0.2370709365018207\n",
      "Regularized Logistic Regression(902/999): loss=0.580968533084574, w0=-4.45255180150832e-06, w1=0.23709566108574778\n",
      "Regularized Logistic Regression(903/999): loss=0.5809659898041184, w0=-4.454729694915012e-06, w1=0.2371202804175788\n",
      "Regularized Logistic Regression(904/999): loss=0.5809634581926353, w0=-4.456904695392326e-06, w1=0.23714479491702292\n",
      "Regularized Logistic Regression(905/999): loss=0.5809609381879609, w0=-4.459076807819169e-06, w1=0.23716920500214458\n",
      "Regularized Logistic Regression(906/999): loss=0.5809584297283061, w0=-4.4612460370634154e-06, w1=0.23719351108936937\n",
      "Regularized Logistic Regression(907/999): loss=0.580955932752255, w0=-4.463412387981932e-06, w1=0.23721771359349136\n",
      "Regularized Logistic Regression(908/999): loss=0.5809534471987609, w0=-4.465575865420614e-06, w1=0.23724181292767854\n",
      "Regularized Logistic Regression(909/999): loss=0.5809509730071452, w0=-4.4677364742144175e-06, w1=0.23726580950348047\n",
      "Regularized Logistic Regression(910/999): loss=0.5809485101170936, w0=-4.4698942191873904e-06, w1=0.23728970373083488\n",
      "Regularized Logistic Regression(911/999): loss=0.5809460584686559, w0=-4.472049105152704e-06, w1=0.23731349601807386\n",
      "Regularized Logistic Regression(912/999): loss=0.5809436180022406, w0=-4.4742011369126875e-06, w1=0.23733718677192836\n",
      "Regularized Logistic Regression(913/999): loss=0.5809411886586154, w0=-4.476350319258855e-06, w1=0.23736077639753808\n",
      "Regularized Logistic Regression(914/999): loss=0.5809387703789031, w0=-4.478496656971942e-06, w1=0.23738426529845363\n",
      "Regularized Logistic Regression(915/999): loss=0.5809363631045795, w0=-4.4806401548219325e-06, w1=0.23740765387664764\n",
      "Regularized Logistic Regression(916/999): loss=0.5809339667774719, w0=-4.4827808175680955e-06, w1=0.2374309425325174\n",
      "Regularized Logistic Regression(917/999): loss=0.5809315813397556, w0=-4.484918649959012e-06, w1=0.23745413166489168\n",
      "Regularized Logistic Regression(918/999): loss=0.5809292067339525, w0=-4.487053656732606e-06, w1=0.23747722167103866\n",
      "Regularized Logistic Regression(919/999): loss=0.580926842902929, w0=-4.489185842616179e-06, w1=0.23750021294667098\n",
      "Regularized Logistic Regression(920/999): loss=0.5809244897898923, w0=-4.491315212326438e-06, w1=0.23752310588595169\n",
      "Regularized Logistic Regression(921/999): loss=0.5809221473383896, w0=-4.493441770569529e-06, w1=0.23754590088150124\n",
      "Regularized Logistic Regression(922/999): loss=0.580919815492306, w0=-4.495565522041063e-06, w1=0.23756859832440255\n",
      "Regularized Logistic Regression(923/999): loss=0.5809174941958607, w0=-4.497686471426152e-06, w1=0.2375911986042086\n",
      "Regularized Logistic Regression(924/999): loss=0.5809151833936068, w0=-4.499804623399437e-06, w1=0.23761370210894708\n",
      "Regularized Logistic Regression(925/999): loss=0.5809128830304273, w0=-4.501919982625118e-06, w1=0.23763610922512698\n",
      "Regularized Logistic Regression(926/999): loss=0.5809105930515341, w0=-4.504032553756984e-06, w1=0.23765842033774526\n",
      "Regularized Logistic Regression(927/999): loss=0.5809083134024656, w0=-4.506142341438446e-06, w1=0.2376806358302924\n",
      "Regularized Logistic Regression(928/999): loss=0.5809060440290843, w0=-4.5082493503025635e-06, w1=0.23770275608475808\n",
      "Regularized Logistic Regression(929/999): loss=0.5809037848775751, w0=-4.5103535849720756e-06, w1=0.23772478148163687\n",
      "Regularized Logistic Regression(930/999): loss=0.5809015358944424, w0=-4.512455050059433e-06, w1=0.23774671239993553\n",
      "Regularized Logistic Regression(931/999): loss=0.580899297026509, w0=-4.514553750166825e-06, w1=0.2377685492171775\n",
      "Regularized Logistic Regression(932/999): loss=0.5808970682209137, w0=-4.51664968988621e-06, w1=0.23779029230940976\n",
      "Regularized Logistic Regression(933/999): loss=0.580894849425109, w0=-4.5187428737993454e-06, w1=0.23781194205120776\n",
      "Regularized Logistic Regression(934/999): loss=0.580892640586859, w0=-4.5208333064778175e-06, w1=0.23783349881568264\n",
      "Regularized Logistic Regression(935/999): loss=0.5808904416542372, w0=-4.522920992483069e-06, w1=0.23785496297448647\n",
      "Regularized Logistic Regression(936/999): loss=0.5808882525756262, w0=-4.525005936366431e-06, w1=0.23787633489781643\n",
      "Regularized Logistic Regression(937/999): loss=0.5808860732997124, w0=-4.527088142669148e-06, w1=0.23789761495442313\n",
      "Regularized Logistic Regression(938/999): loss=0.5808839037754875, w0=-4.529167615922413e-06, w1=0.23791880351161482\n",
      "Regularized Logistic Regression(939/999): loss=0.5808817439522438, w0=-4.531244360647389e-06, w1=0.23793990093526304\n",
      "Regularized Logistic Regression(940/999): loss=0.5808795937795745, w0=-4.533318381355244e-06, w1=0.23796090758981\n",
      "Regularized Logistic Regression(941/999): loss=0.5808774532073698, w0=-4.535389682547178e-06, w1=0.23798182383827163\n",
      "Regularized Logistic Regression(942/999): loss=0.5808753221858161, w0=-4.5374582687144486e-06, w1=0.23800265004224583\n",
      "Regularized Logistic Regression(943/999): loss=0.580873200665393, w0=-4.539524144338405e-06, w1=0.23802338656191588\n",
      "Regularized Logistic Regression(944/999): loss=0.5808710885968742, w0=-4.54158731389051e-06, w1=0.23804403375605773\n",
      "Regularized Logistic Regression(945/999): loss=0.5808689859313211, w0=-4.5436477818323755e-06, w1=0.23806459198204466\n",
      "Regularized Logistic Regression(946/999): loss=0.5808668926200853, w0=-4.545705552615784e-06, w1=0.23808506159585321\n",
      "Regularized Logistic Regression(947/999): loss=0.5808648086148036, w0=-4.54776063068272e-06, w1=0.2381054429520684\n",
      "Regularized Logistic Regression(948/999): loss=0.580862733867398, w0=-4.549813020465399e-06, w1=0.2381257364038892\n",
      "Regularized Logistic Regression(949/999): loss=0.5808606683300732, w0=-4.551862726386294e-06, w1=0.23814594230313468\n",
      "Regularized Logistic Regression(950/999): loss=0.5808586119553142, w0=-4.553909752858162e-06, w1=0.23816606100024879\n",
      "Regularized Logistic Regression(951/999): loss=0.5808565646958858, w0=-4.555954104284073e-06, w1=0.23818609284430642\n",
      "Regularized Logistic Regression(952/999): loss=0.5808545265048294, w0=-4.557995785057441e-06, w1=0.23820603818301805\n",
      "Regularized Logistic Regression(953/999): loss=0.5808524973354624, w0=-4.5600347995620446e-06, w1=0.2382258973627357\n",
      "Regularized Logistic Regression(954/999): loss=0.580850477141376, w0=-4.56207115217206e-06, w1=0.23824567072845837\n",
      "Regularized Logistic Regression(955/999): loss=0.5808484658764332, w0=-4.564104847252086e-06, w1=0.2382653586238374\n",
      "Regularized Logistic Regression(956/999): loss=0.5808464634947662, w0=-4.5661358891571715e-06, w1=0.23828496139118058\n",
      "Regularized Logistic Regression(957/999): loss=0.5808444699507764, w0=-4.568164282232844e-06, w1=0.23830447937145888\n",
      "Regularized Logistic Regression(958/999): loss=0.5808424851991327, w0=-4.570190030815135e-06, w1=0.23832391290431218\n",
      "Regularized Logistic Regression(959/999): loss=0.5808405091947678, w0=-4.572213139230608e-06, w1=0.23834326232805267\n",
      "Regularized Logistic Regression(960/999): loss=0.5808385418928774, w0=-4.574233611796383e-06, w1=0.23836252797967147\n",
      "Regularized Logistic Regression(961/999): loss=0.5808365832489206, w0=-4.576251452820168e-06, w1=0.23838171019484336\n",
      "Regularized Logistic Regression(962/999): loss=0.5808346332186143, w0=-4.578266666600281e-06, w1=0.23840080930793228\n",
      "Regularized Logistic Regression(963/999): loss=0.5808326917579347, w0=-4.580279257425679e-06, w1=0.23841982565199715\n",
      "Regularized Logistic Regression(964/999): loss=0.5808307588231142, w0=-4.582289229575985e-06, w1=0.23843875955879348\n",
      "Regularized Logistic Regression(965/999): loss=0.5808288343706407, w0=-4.5842965873215116e-06, w1=0.23845761135878435\n",
      "Regularized Logistic Regression(966/999): loss=0.5808269183572542, w0=-4.586301334923291e-06, w1=0.23847638138114097\n",
      "Regularized Logistic Regression(967/999): loss=0.5808250107399471, w0=-4.588303476633098e-06, w1=0.23849506995374928\n",
      "Regularized Logistic Regression(968/999): loss=0.5808231114759624, w0=-4.590303016693478e-06, w1=0.23851367740321494\n",
      "Regularized Logistic Regression(969/999): loss=0.5808212205227901, w0=-4.5922999593377735e-06, w1=0.23853220405486875\n",
      "Regularized Logistic Regression(970/999): loss=0.5808193378381685, w0=-4.594294308790147e-06, w1=0.2385506502327708\n",
      "Regularized Logistic Regression(971/999): loss=0.5808174633800792, w0=-4.5962860692656114e-06, w1=0.23856901625971622\n",
      "Regularized Logistic Regression(972/999): loss=0.5808155971067499, w0=-4.598275244970052e-06, w1=0.23858730245724005\n",
      "Regularized Logistic Regression(973/999): loss=0.5808137389766483, w0=-4.6002618401002534e-06, w1=0.23860550914562162\n",
      "Regularized Logistic Regression(974/999): loss=0.5808118889484842, w0=-4.602245858843927e-06, w1=0.23862363664389113\n",
      "Regularized Logistic Regression(975/999): loss=0.5808100469812051, w0=-4.604227305379733e-06, w1=0.23864168526983243\n",
      "Regularized Logistic Regression(976/999): loss=0.5808082130339967, w0=-4.606206183877308e-06, w1=0.23865965533998912\n",
      "Regularized Logistic Regression(977/999): loss=0.5808063870662806, w0=-4.6081824984972905e-06, w1=0.23867754716966982\n",
      "Regularized Logistic Regression(978/999): loss=0.5808045690377132, w0=-4.610156253391345e-06, w1=0.23869536107295206\n",
      "Regularized Logistic Regression(979/999): loss=0.580802758908183, w0=-4.61212745270219e-06, w1=0.2387130973626874\n",
      "Regularized Logistic Regression(980/999): loss=0.5808009566378104, w0=-4.614096100563618e-06, w1=0.2387307563505059\n",
      "Regularized Logistic Regression(981/999): loss=0.5807991621869462, w0=-4.6160622011005235e-06, w1=0.23874833834682221\n",
      "Regularized Logistic Regression(982/999): loss=0.5807973755161694, w0=-4.618025758428931e-06, w1=0.23876584366083994\n",
      "Regularized Logistic Regression(983/999): loss=0.5807955965862858, w0=-4.619986776656014e-06, w1=0.23878327260055518\n",
      "Regularized Logistic Regression(984/999): loss=0.5807938253583267, w0=-4.621945259880122e-06, w1=0.23880062547276215\n",
      "Regularized Logistic Regression(985/999): loss=0.5807920617935487, w0=-4.623901212190807e-06, w1=0.23881790258305866\n",
      "Regularized Logistic Regression(986/999): loss=0.58079030585343, w0=-4.6258546376688466e-06, w1=0.23883510423584867\n",
      "Regularized Logistic Regression(987/999): loss=0.58078855749967, w0=-4.6278055403862675e-06, w1=0.23885223073434994\n",
      "Regularized Logistic Regression(988/999): loss=0.5807868166941896, w0=-4.629753924406372e-06, w1=0.23886928238059646\n",
      "Regularized Logistic Regression(989/999): loss=0.5807850833991268, w0=-4.63169979378376e-06, w1=0.23888625947544279\n",
      "Regularized Logistic Regression(990/999): loss=0.5807833575768371, w0=-4.633643152564356e-06, w1=0.23890316231857175\n",
      "Regularized Logistic Regression(991/999): loss=0.5807816391898918, w0=-4.63558400478543e-06, w1=0.23891999120849483\n",
      "Regularized Logistic Regression(992/999): loss=0.5807799282010767, w0=-4.6375223544756256e-06, w1=0.2389367464425611\n",
      "Regularized Logistic Regression(993/999): loss=0.5807782245733905, w0=-4.639458205654979e-06, w1=0.23895342831695754\n",
      "Regularized Logistic Regression(994/999): loss=0.5807765282700433, w0=-4.641391562334948e-06, w1=0.23897003712671638\n",
      "Regularized Logistic Regression(995/999): loss=0.5807748392544564, w0=-4.643322428518431e-06, w1=0.2389865731657198\n",
      "Regularized Logistic Regression(996/999): loss=0.5807731574902596, w0=-4.645250808199797e-06, w1=0.23900303672670217\n",
      "Regularized Logistic Regression(997/999): loss=0.58077148294129, w0=-4.647176705364902e-06, w1=0.23901942810125693\n",
      "Regularized Logistic Regression(998/999): loss=0.5807698155715916, w0=-4.6491001239911164e-06, w1=0.23903574757983978\n",
      "Regularized Logistic Regression(999/999): loss=0.5807681553454137, w0=-4.65102106804735e-06, w1=0.23905199545177327\n",
      "Fold 4 completed.\n",
      "Logistic Regression\n",
      "Train set: Accuracy=0.73, Precision=0.21, Recall=0.75, F1=0.32611\n",
      "Train set fixed: Accuracy=0.74, Precision=0.73, Recall=0.75, F1=0.74156\n",
      "Test set: Accuracy=0.73, Precision=0.21, Recall=0.75, F1=0.32607\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "all_features = features_good_nmi\n",
    "metrics_train_LR, metrics_train_fixed_LR, metrics_test_LR = evaluate_model(x_train, y_train, x_test, all_features, nan_threshold=0.1, dont_balance=False, n_folds=4, model=LogisticRegression(max_iters=500), oneHot=True)\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Train set: Accuracy={metrics_train_LR[0]:.2f}, Precision={metrics_train_LR[1]:.2f}, Recall={metrics_train_LR[2]:.2f}, F1={metrics_train_LR[3]:.5f}\")\n",
    "print(f\"Train set fixed: Accuracy={metrics_train_fixed_LR[0]:.2f}, Precision={metrics_train_fixed_LR[1]:.2f}, Recall={metrics_train_fixed_LR[2]:.2f}, F1={metrics_train_fixed_LR[3]:.5f}\")\n",
    "print(f\"Test set: Accuracy={metrics_test_LR[0]:.2f}, Precision={metrics_test_LR[1]:.2f}, Recall={metrics_test_LR[2]:.2f}, F1={metrics_test_LR[3]:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e592e01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes too much time to run, at least 1h for 1 fold\n",
    "# # Random Forest\n",
    "# all_features = features_good_nmi\n",
    "# metrics_train_LR, metrics_train_fixed_LR, metrics_test_LR = evaluate_model(x_train, y_train, x_test, all_features, nan_threshold=0.1, dont_balance=False, n_folds=4, model=RandomForest(), oneHot=False)\n",
    "# print(\"Logistic Regression\")\n",
    "# print(f\"Train set: Accuracy={metrics_train_LR[0]:.2f}, Precision={metrics_train_LR[1]:.2f}, Recall={metrics_train_LR[2]:.2f}, F1={metrics_train_LR[3]:.5f}\")\n",
    "# print(f\"Train set fixed: Accuracy={metrics_train_fixed_LR[0]:.2f}, Precision={metrics_train_fixed_LR[1]:.2f}, Recall={metrics_train_fixed_LR[2]:.2f}, F1={metrics_train_fixed_LR[3]:.5f}\")\n",
    "# print(f\"Test set: Accuracy={metrics_test_LR[0]:.2f}, Precision={metrics_test_LR[1]:.2f}, Recall={metrics_test_LR[2]:.2f}, F1={metrics_test_LR[3]:.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321ab85b",
   "metadata": {},
   "source": [
    "# 6. Model Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c18f5b",
   "metadata": {},
   "source": [
    "## 6.2 greedy approach to select features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5496bb",
   "metadata": {},
   "source": [
    "Simply choosing the best features according to mutual information can lead to suboptimal performance because the features may be for example strongly correlated. A better approach is to select greedily the best features one by one ideally from all available features until the F1-score decreases. However, this approach is computationally expensive so we can't fully run it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "657ad121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def greedy_feature_selection(x_train, y_train, x_test, features, dont_balance=False):\n",
    "    features = select_features_with_low_nan_ratio(x_train, features, threshold=0.1)\n",
    "    n_features = len(features)\n",
    "    selected_features = []\n",
    "    remaining_features = features.copy()\n",
    "\n",
    "    best_f1 = 0\n",
    "    progress_bar = tqdm(total=n_features, desc=\"Selecting Features\")\n",
    "\n",
    "    while remaining_features:\n",
    "        # Track the best feature and F1 score in the current iteration\n",
    "        best_feature = None\n",
    "        best_f1_iteration = 0\n",
    "        # print(remaining_features)\n",
    "        # Try adding each remaining feature and evaluate F1 score\n",
    "        for feature in remaining_features:\n",
    "            current_features = selected_features + [feature]  # Add feature to the selected set\n",
    "            # print(current_features)\n",
    "\n",
    "            metrics_train, metrics_train_fixed, metrics_test = evaluate_model(x_train, y_train, x_test, current_features, nan_threshold=0.1, dont_balance=dont_balance, n_folds=4, model=CategoricalNaiveBayes(), oneHot=False)\n",
    "            f1_test = metrics_test[3]\n",
    "\n",
    "            # Check if the current F1 score is the best so far\n",
    "            if f1_test > best_f1_iteration:\n",
    "                best_f1_iteration = f1_test\n",
    "                best_feature = feature\n",
    "        # Stop if no improvement is made\n",
    "        if best_f1_iteration <= best_f1:\n",
    "            break\n",
    "        \n",
    "        # Update selected features and remaining features\n",
    "        selected_features.append(best_feature)\n",
    "        remaining_features.remove(best_feature)\n",
    "        best_f1 = best_f1_iteration\n",
    "        print(f\"Best F1 score: {best_f1}\")\n",
    "        print(f\"Selected features: {selected_features}\")\n",
    "        \n",
    "        progress_bar.update(1)  # Update the progress bar\n",
    "        \n",
    "    progress_bar.close()  # Close the progress bar when done\n",
    "    return selected_features, best_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76e4a9c",
   "metadata": {},
   "source": [
    "## 6.1 Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e04936d",
   "metadata": {},
   "source": [
    "To further optimize the model, we will perform hyperparameter tuning using grid search with cross-validation. This technique will help us identify the best combination of hyperparameters for a given model, improving its performance and generalization. This is also computationally expensive so we will only run it for the best model and on two sets of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a62fb8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning(x_train, y_train, x_test, features, model_name, hyperparameters, dont_balance=False, one_hot=False):\n",
    "    best_hyperparameters = None\n",
    "    best_f1 = 0\n",
    "\n",
    "    for hyperparameter in hyperparameters:\n",
    "        model = globals().get(model_name)(**hyperparameter)\n",
    "        print(f\"Hyperparameter: {hyperparameter}\")\n",
    "        metrics_train, metrics_train_fixed, metrics_test = evaluate_model(x_train, y_train, x_test, features, nan_threshold=0.1, dont_balance=dont_balance, n_folds=4, model=model, oneHot=one_hot)\n",
    "        f1 = metrics_test[3]\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_hyperparameters = hyperparameter\n",
    "\n",
    "    return best_hyperparameters, best_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ee427dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter: {'smoothing': 1e-06}\n",
      "Features with low NaN ratio selected. Number of features: 9\n",
      "Features mapped to categorical values.\n",
      "Features label encoded.\n",
      "Data split into k folds.\n",
      "Class imbalance fixed.\n",
      "Fold 1 completed.\n",
      "Fold 2 completed.\n",
      "Fold 3 completed.\n",
      "Fold 4 completed.\n",
      "Hyperparameter: {'smoothing': 1e-05}\n",
      "Features with low NaN ratio selected. Number of features: 9\n",
      "Features mapped to categorical values.\n",
      "Features label encoded.\n",
      "Data split into k folds.\n",
      "Class imbalance fixed.\n",
      "Fold 1 completed.\n",
      "Fold 2 completed.\n",
      "Fold 3 completed.\n",
      "Fold 4 completed.\n",
      "Best hyperparameters: {'smoothing': 1e-06}\n",
      "Best F1 score: 0.3254600992912859\n"
     ]
    }
   ],
   "source": [
    "model_name = \"CategoricalNaiveBayes\"\n",
    "hyperparameters = [{\"smoothing\": 1e-6}, {\"smoothing\": 1e-5}]\n",
    "one_hot = False\n",
    "\n",
    "best_hyperparameters, best_f1 = hyperparameter_tuning(x_train, y_train, x_test, features_good_nmi, model_name, hyperparameters, dont_balance=False, one_hot=one_hot)\n",
    "print(f\"Best hyperparameters: {best_hyperparameters}\")\n",
    "print(f\"Best F1 score: {best_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61e260e",
   "metadata": {},
   "source": [
    "# 7 Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e57c1e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict_model(x_train, y_train, x_test, final_features, dont_balance=False, model=CategoricalNaiveBayes(), oneHot=False):\n",
    "    y_train_mapped = map_y_to_0_1(y_train[:,1])\n",
    "    x_train_encoded, x_train_encoded_fixed, y_train_fixed, x_test_encoded = cleaning_x_pipeline(x_train, y_train_mapped, x_test, final_features, nan_threshold=0.1, n_folds=0, dont_balance=dont_balance, oneHot=oneHot)\n",
    "\n",
    "    model.fit(x_train_encoded_fixed, y_train_fixed)\n",
    "    y_pred = model.predict(x_test_encoded)\n",
    "    y_train_pred = model.predict(x_train_encoded)\n",
    "\n",
    "    # print metrics for the training set\n",
    "    accuracy, precision, recall, f1 = accuracy_precision_recall_f1(y_train_mapped, y_train_pred)\n",
    "    print(f\"Training set: accuracy={accuracy:.2f}, precision={precision:.2f}, recall={recall:.2f}, F1={f1:.5f}\")\n",
    "\n",
    "    return y_pred, y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f231a378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with low NaN ratio selected. Number of features: 9\n",
      "Features mapped to categorical values.\n",
      "Features label encoded.\n",
      "Class imbalance fixed.\n",
      "Training set: accuracy=0.72, precision=0.21, recall=0.76, F1=0.32552\n"
     ]
    }
   ],
   "source": [
    "final_features = features_good_nmi\n",
    "y_pred_test, y_train_pred = fit_predict_model(x_train, y_train, x_test, final_features, dont_balance=False, model=CategoricalNaiveBayes(smoothing= 1e-6), oneHot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f9eda31",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ids = x_test[:,0]\n",
    "y_pred_test_final = map_y_to_minus_1_1(y_pred_test)\n",
    "\n",
    "np.savetxt(\"final_submission.csv\", np.array([Ids, y_pred_test_final]).T, delimiter=\",\", fmt=\"%d\", header=\"Id,Prediction\", comments=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
