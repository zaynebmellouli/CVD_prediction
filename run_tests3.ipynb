{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.genfromtxt(\"data/x_train.csv\", delimiter=\",\", skip_header=1)\n",
    "features = np.genfromtxt(\"data/x_train.csv\", delimiter=\",\", dtype=str, max_rows=1)\n",
    "y_train = np.genfromtxt(\"data/y_train.csv\", delimiter=\",\", skip_header=1, usecols=1)\n",
    "y_features = np.genfromtxt(\"data/y_train.csv\", delimiter=\",\", dtype=str, max_rows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.genfromtxt(\"data/x_test.csv\",delimiter=\",\", skip_header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(array, range_min, range_max, n_bins):\n",
    "    # Filter array to include only values within the specified range\n",
    "    filtered_values = array[(array >= range_min) & (array <= range_max)]\n",
    "    \n",
    "    # Calculate the bin edges using quantiles\n",
    "    bin_edges = np.quantile(filtered_values, np.linspace(0, 1, n_bins + 1))\n",
    "    \n",
    "    def assign_bin(value):\n",
    "        # Check if the value is NaN\n",
    "        if np.isnan(value):\n",
    "            return -1\n",
    "        \n",
    "        # Handle special values like 88 and 999\n",
    "        if value == 88 or value == 999:\n",
    "            return -1\n",
    "        \n",
    "        # If the value is outside the range, return it as is\n",
    "        if value < range_min or value > range_max:\n",
    "            return value\n",
    "        \n",
    "        # Assign bin based on which range the value falls into\n",
    "        # We use right=True to ensure that values exactly equal to range_max are included in the last bin\n",
    "        return np.digitize(value, bin_edges, right=True)\n",
    "    \n",
    "    return assign_bin\n",
    "\n",
    "mapping_dict = {\n",
    "    \"GENHLTH\": lambda value: value if value <= 9 else -1,\n",
    "    \"PHYSHLTH\": to_categorical(array=x_train[:, features==\"PHYSHLTH\"].flatten(), range_min=0, range_max=30, n_bins=4),\n",
    "    \"MENTHLTH\": to_categorical(array=x_train[:, features==\"MENTHLTH\"].flatten(), range_min=0, range_max=30, n_bins=4),\n",
    "    \"POORHLTH\": to_categorical(array=x_train[:, features==\"POORHLTH\"].flatten(), range_min=0, range_max=30, n_bins=4),\n",
    "    \"HLTHPLN1\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"MEDCOST\": lambda value: value if value <= 7 else -1,\n",
    "    \"CHECKUP1\": lambda value: value if value <= 8 else -1,\n",
    "    \"BPHIGH4\": lambda value: value if value <= 7 else -1,\n",
    "    \"BPMEDS\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"BLOODCHO\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"CHOLCHK\": lambda value: value if not np.isnan(value) else -1,\n",
    "    # \"CVDINFR4\": lambda value: 1 if value == 1 else 0,\n",
    "    # \"CVDCRHD4\": lambda value: 1 if value == 1 else 0,\n",
    "    \"TOLDHI2\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"CVDSTRK3\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"ASTHMA3\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"ASTHNOW\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"CHCSCNCR\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"CHCOCNCR\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"CHCCOPD1\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"HAVARTH3\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"ADDEPEV2\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"CHCKIDNY\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"DIABETE3\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"SEX\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"MARITAL\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"EDUCA\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"VETERAN3\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"INCOME2\": lambda value: value if not np.isnan(value) else -1,\n",
    "    \"INTERNET\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"WTKG3\": to_categorical(array=x_train[:, features==\"WTKG3\"].flatten(), range_min=23, range_max=295, n_bins=6),\n",
    "    \"QLACTLM2\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"USEEQUIP\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"BLIND\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"DECIDE\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"DIFFWALK\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"DIFFDRES\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"DIFFALON\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"SMOKE100\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"SMOKDAY2\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"LASTSMK2\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"USENOW3\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"AVEDRNK2\": to_categorical(array=x_train[:, features==\"AVEDRNK2\"].flatten(), range_min=1, range_max=76, n_bins=5),\n",
    "    \"DRNK3GE5\": to_categorical(array=x_train[:, features==\"DRNK3GE5\"].flatten(), range_min=1, range_max=76, n_bins=5),\n",
    "    \"EXERANY2\": lambda value : value if not np.isnan(value) else -1,\n",
    "    # \"EXERHMM1\": lambda value: str(value//200) if value <= 959 and value not in [777,999] else -1,\n",
    "    \"LMTJOIN3\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"FLUSHOT6\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"PDIABTST\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"PREDIAB1\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"INSULIN\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"CIMEMLOS\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_RFHLTH\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_HCVU651\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_RFHYPE5\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_CHOLCHK\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_RFCHOL\": lambda value : value if not np.isnan(value) else -1,\n",
    "    # \"_MICHD\": lambda value: value if value <= 2 else -1,\n",
    "    \"_LTASTH1\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_CASTHM1\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_DRDXAR1\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_AGEG5YR\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_AGE_G\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"HTM4\": to_categorical(array=x_train[:, features==\"HTM4\"].flatten(), range_min=0.91, range_max=2.44, n_bins=6),\n",
    "    \"_RFBMI5\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_EDUCAG\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_SMOKER3\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_RFBING5\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_BMI5CAT\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_RFDRHV5\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"FTJUDA1_\": to_categorical(array=x_train[:, features==\"FTJUDA1_\"].flatten(), range_min=0, range_max=99.99, n_bins=4),\n",
    "    \"MAXVO2_\": to_categorical(array=x_train[:, features==\"MAXVO2_\"].flatten(), range_min=0, range_max=50.1, n_bins=6),\n",
    "    \"ACTIN11_\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"ACTIN21_\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_PACAT1\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_PA150R2\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_PA300R2\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_PASTRNG\":  lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_PASTAE1\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_LMTACT1\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_LMTWRK1\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_LMTSCL1\": lambda value : value if not np.isnan(value) else -1,\n",
    "    \"_INCOMG\": lambda value : value if not np.isnan(value) else -1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def analyze_missing_value_distribution_with_plot(x_train, features):\n",
    "    nan_ratios = []\n",
    "\n",
    "    # Calculate NaN ratio for each feature\n",
    "    for idx in range(len(features)):\n",
    "        nan_count = np.sum(np.isnan(x_train[:, idx]))\n",
    "        nan_ratio = nan_count / x_train.shape[0]\n",
    "        nan_ratios.append(nan_ratio)\n",
    "    \n",
    "    nan_ratios = np.array(nan_ratios)\n",
    "\n",
    "    # Plotting the distribution of NaN ratios\n",
    "    plt.figure(figsize=(8, 5))\n",
    "\n",
    "    # Histogram to visualize the distribution\n",
    "    plt.hist(nan_ratios, bins=20, edgecolor='k', alpha=0.7)\n",
    "    plt.xlabel('NaN Ratio')\n",
    "    plt.ylabel('Number of Features')\n",
    "    plt.title('Distribution of NaN Ratios Across Features')\n",
    "    plt.axvline(x=0.1, color='r', linestyle='--', label='Threshold (10%)')\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "    plt.show()\n",
    "\n",
    "analyze_missing_value_distribution_with_plot(x_train, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 322 features over 322\n",
      "Dropped features with more than 100% NaNs: []\n",
      "['Id', '_STATE', 'FMONTH', 'IDATE', 'IMONTH', 'IDAY', 'IYEAR', 'DISPCODE', 'SEQNO', '_PSU', 'CTELENUM', 'PVTRESD1', 'COLGHOUS', 'STATERES', 'CELLFON3', 'LADULT', 'NUMADULT', 'NUMMEN', 'NUMWOMEN', 'CTELNUM1', 'CELLFON2', 'CADULT', 'PVTRESD2', 'CCLGHOUS', 'CSTATE', 'LANDLINE', 'HHADULT', 'GENHLTH', 'PHYSHLTH', 'MENTHLTH', 'POORHLTH', 'HLTHPLN1', 'PERSDOC2', 'MEDCOST', 'CHECKUP1', 'BPHIGH4', 'BPMEDS', 'BLOODCHO', 'CHOLCHK', 'TOLDHI2', 'CVDSTRK3', 'ASTHMA3', 'ASTHNOW', 'CHCSCNCR', 'CHCOCNCR', 'CHCCOPD1', 'HAVARTH3', 'ADDEPEV2', 'CHCKIDNY', 'DIABETE3', 'DIABAGE2', 'SEX', 'MARITAL', 'EDUCA', 'RENTHOM1', 'NUMHHOL2', 'NUMPHON2', 'CPDEMO1', 'VETERAN3', 'EMPLOY1', 'CHILDREN', 'INCOME2', 'INTERNET', 'WEIGHT2', 'HEIGHT3', 'PREGNANT', 'QLACTLM2', 'USEEQUIP', 'BLIND', 'DECIDE', 'DIFFWALK', 'DIFFDRES', 'DIFFALON', 'SMOKE100', 'SMOKDAY2', 'STOPSMK2', 'LASTSMK2', 'USENOW3', 'ALCDAY5', 'AVEDRNK2', 'DRNK3GE5', 'MAXDRNKS', 'FRUITJU1', 'FRUIT1', 'FVBEANS', 'FVGREEN', 'FVORANG', 'VEGETAB1', 'EXERANY2', 'EXRACT11', 'EXEROFT1', 'EXERHMM1', 'EXRACT21', 'EXEROFT2', 'EXERHMM2', 'STRENGTH', 'LMTJOIN3', 'ARTHDIS2', 'ARTHSOCL', 'JOINPAIN', 'SEATBELT', 'FLUSHOT6', 'FLSHTMY2', 'IMFVPLAC', 'PNEUVAC3', 'HIVTST6', 'HIVTSTD3', 'WHRTST10', 'PDIABTST', 'PREDIAB1', 'INSULIN', 'BLDSUGAR', 'FEETCHK2', 'DOCTDIAB', 'CHKHEMO3', 'FEETCHK', 'EYEEXAM', 'DIABEYE', 'DIABEDU', 'CAREGIV1', 'CRGVREL1', 'CRGVLNG1', 'CRGVHRS1', 'CRGVPRB1', 'CRGVPERS', 'CRGVHOUS', 'CRGVMST2', 'CRGVEXPT', 'VIDFCLT2', 'VIREDIF3', 'VIPRFVS2', 'VINOCRE2', 'VIEYEXM2', 'VIINSUR2', 'VICTRCT4', 'VIGLUMA2', 'VIMACDG2', 'CIMEMLOS', 'CDHOUSE', 'CDASSIST', 'CDHELP', 'CDSOCIAL', 'CDDISCUS', 'WTCHSALT', 'LONGWTCH', 'DRADVISE', 'ASTHMAGE', 'ASATTACK', 'ASERVIST', 'ASDRVIST', 'ASRCHKUP', 'ASACTLIM', 'ASYMPTOM', 'ASNOSLEP', 'ASTHMED3', 'ASINHALR', 'HAREHAB1', 'STREHAB1', 'CVDASPRN', 'ASPUNSAF', 'RLIVPAIN', 'RDUCHART', 'RDUCSTRK', 'ARTTODAY', 'ARTHWGT', 'ARTHEXER', 'ARTHEDU', 'TETANUS', 'HPVADVC2', 'HPVADSHT', 'SHINGLE2', 'HADMAM', 'HOWLONG', 'HADPAP2', 'LASTPAP2', 'HPVTEST', 'HPLSTTST', 'HADHYST2', 'PROFEXAM', 'LENGEXAM', 'BLDSTOOL', 'LSTBLDS3', 'HADSIGM3', 'HADSGCO1', 'LASTSIG3', 'PCPSAAD2', 'PCPSADI1', 'PCPSARE1', 'PSATEST1', 'PSATIME', 'PCPSARS1', 'PCPSADE1', 'PCDMDECN', 'SCNTMNY1', 'SCNTMEL1', 'SCNTPAID', 'SCNTWRK1', 'SCNTLPAD', 'SCNTLWK1', 'SXORIENT', 'TRNSGNDR', 'RCSGENDR', 'RCSRLTN2', 'CASTHDX2', 'CASTHNO2', 'EMTSUPRT', 'LSATISFY', 'ADPLEASR', 'ADDOWN', 'ADSLEEP', 'ADENERGY', 'ADEAT1', 'ADFAIL', 'ADTHINK', 'ADMOVE', 'MISTMNT', 'ADANXEV', 'QSTVER', 'QSTLANG', 'MSCODE', '_STSTR', '_STRWT', '_RAWRAKE', '_WT2RAKE', '_CHISPNC', '_CRACE1', '_CPRACE', '_CLLCPWT', '_DUALUSE', '_DUALCOR', '_LLCPWT', '_RFHLTH', '_HCVU651', '_RFHYPE5', '_CHOLCHK', '_RFCHOL', '_LTASTH1', '_CASTHM1', '_ASTHMS1', '_DRDXAR1', '_PRACE1', '_MRACE1', '_HISPANC', '_RACE', '_RACEG21', '_RACEGR3', '_RACE_G1', '_AGEG5YR', '_AGE65YR', '_AGE80', '_AGE_G', 'HTIN4', 'HTM4', 'WTKG3', '_BMI5', '_BMI5CAT', '_RFBMI5', '_CHLDCNT', '_EDUCAG', '_INCOMG', '_SMOKER3', '_RFSMOK3', 'DRNKANY5', 'DROCDY3_', '_RFBING5', '_DRNKWEK', '_RFDRHV5', 'FTJUDA1_', 'FRUTDA1_', 'BEANDAY_', 'GRENDAY_', 'ORNGDAY_', 'VEGEDA1_', '_MISFRTN', '_MISVEGN', '_FRTRESP', '_VEGRESP', '_FRUTSUM', '_VEGESUM', '_FRTLT1', '_VEGLT1', '_FRT16', '_VEG23', '_FRUITEX', '_VEGETEX', '_TOTINDA', 'METVL11_', 'METVL21_', 'MAXVO2_', 'FC60_', 'ACTIN11_', 'ACTIN21_', 'PADUR1_', 'PADUR2_', 'PAFREQ1_', 'PAFREQ2_', '_MINAC11', '_MINAC21', 'STRFREQ_', 'PAMISS1_', 'PAMIN11_', 'PAMIN21_', 'PA1MIN_', 'PAVIG11_', 'PAVIG21_', 'PA1VIGM_', '_PACAT1', '_PAINDX1', '_PA150R2', '_PA300R2', '_PA30021', '_PASTRNG', '_PAREC1', '_PASTAE1', '_LMTACT1', '_LMTWRK1', '_LMTSCL1', '_RFSEAT2', '_RFSEAT3', '_FLSHOT6', '_PNEUMO2', '_AIDTST3']\n"
     ]
    }
   ],
   "source": [
    "def select_features_with_low_nan_ratio(x_train, features, threshold=1):\n",
    "    nan_ratios = {}\n",
    "    # Loop over each feature and calculate NaN ratio\n",
    "    for idx, feature in enumerate(features):\n",
    "        nan_count = np.sum(np.isnan(x_train[:, idx]))\n",
    "        nan_ratio = nan_count / len(x_train)\n",
    "        nan_ratios[feature] = nan_ratio\n",
    "\n",
    "    # Select features with NaN ratio below the given threshold\n",
    "    selected_features = [feature for feature, nan_ratio in nan_ratios.items() if nan_ratio < threshold]\n",
    "\n",
    "    print(f\"Selected {len(selected_features)} features over {len(features)}\")\n",
    "    dropped_features = [feature for feature, nan_ratio in nan_ratios.items() if nan_ratio >= threshold]\n",
    "    print(f\"Dropped features with more than {threshold * 100}% NaNs: {dropped_features}\")\n",
    "\n",
    "    return selected_features\n",
    "\n",
    "# Assuming x_train and features are already available\n",
    "selected_features = select_features_with_low_nan_ratio(x_train, features)\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Feature 'Id' Skipping.\n",
      "Warning: Feature '_STATE' Skipping.\n",
      "Warning: Feature 'FMONTH' Skipping.\n",
      "Warning: Feature 'IDATE' Skipping.\n",
      "Warning: Feature 'IMONTH' Skipping.\n",
      "Warning: Feature 'IDAY' Skipping.\n",
      "Warning: Feature 'IYEAR' Skipping.\n",
      "Warning: Feature 'DISPCODE' Skipping.\n",
      "Warning: Feature 'SEQNO' Skipping.\n",
      "Warning: Feature '_PSU' Skipping.\n",
      "Warning: Feature 'CTELENUM' Skipping.\n",
      "Warning: Feature 'PVTRESD1' Skipping.\n",
      "Warning: Feature 'COLGHOUS' Skipping.\n",
      "Warning: Feature 'STATERES' Skipping.\n",
      "Warning: Feature 'CELLFON3' Skipping.\n",
      "Warning: Feature 'LADULT' Skipping.\n",
      "Warning: Feature 'NUMADULT' Skipping.\n",
      "Warning: Feature 'NUMMEN' Skipping.\n",
      "Warning: Feature 'NUMWOMEN' Skipping.\n",
      "Warning: Feature 'CTELNUM1' Skipping.\n",
      "Warning: Feature 'CELLFON2' Skipping.\n",
      "Warning: Feature 'CADULT' Skipping.\n",
      "Warning: Feature 'PVTRESD2' Skipping.\n",
      "Warning: Feature 'CCLGHOUS' Skipping.\n",
      "Warning: Feature 'CSTATE' Skipping.\n",
      "Warning: Feature 'LANDLINE' Skipping.\n",
      "Warning: Feature 'HHADULT' Skipping.\n",
      "Warning: Feature 'PERSDOC2' Skipping.\n",
      "Warning: Feature 'DIABAGE2' Skipping.\n",
      "Warning: Feature 'RENTHOM1' Skipping.\n",
      "Warning: Feature 'NUMHHOL2' Skipping.\n",
      "Warning: Feature 'NUMPHON2' Skipping.\n",
      "Warning: Feature 'CPDEMO1' Skipping.\n",
      "Warning: Feature 'EMPLOY1' Skipping.\n",
      "Warning: Feature 'CHILDREN' Skipping.\n",
      "Warning: Feature 'WEIGHT2' Skipping.\n",
      "Warning: Feature 'HEIGHT3' Skipping.\n",
      "Warning: Feature 'PREGNANT' Skipping.\n",
      "Warning: Feature 'STOPSMK2' Skipping.\n",
      "Warning: Feature 'ALCDAY5' Skipping.\n",
      "Warning: Feature 'MAXDRNKS' Skipping.\n",
      "Warning: Feature 'FRUITJU1' Skipping.\n",
      "Warning: Feature 'FRUIT1' Skipping.\n",
      "Warning: Feature 'FVBEANS' Skipping.\n",
      "Warning: Feature 'FVGREEN' Skipping.\n",
      "Warning: Feature 'FVORANG' Skipping.\n",
      "Warning: Feature 'VEGETAB1' Skipping.\n",
      "Warning: Feature 'EXRACT11' Skipping.\n",
      "Warning: Feature 'EXEROFT1' Skipping.\n",
      "Warning: Feature 'EXERHMM1' Skipping.\n",
      "Warning: Feature 'EXRACT21' Skipping.\n",
      "Warning: Feature 'EXEROFT2' Skipping.\n",
      "Warning: Feature 'EXERHMM2' Skipping.\n",
      "Warning: Feature 'STRENGTH' Skipping.\n",
      "Warning: Feature 'ARTHDIS2' Skipping.\n",
      "Warning: Feature 'ARTHSOCL' Skipping.\n",
      "Warning: Feature 'JOINPAIN' Skipping.\n",
      "Warning: Feature 'SEATBELT' Skipping.\n",
      "Warning: Feature 'FLSHTMY2' Skipping.\n",
      "Warning: Feature 'IMFVPLAC' Skipping.\n",
      "Warning: Feature 'PNEUVAC3' Skipping.\n",
      "Warning: Feature 'HIVTST6' Skipping.\n",
      "Warning: Feature 'HIVTSTD3' Skipping.\n",
      "Warning: Feature 'WHRTST10' Skipping.\n",
      "Warning: Feature 'BLDSUGAR' Skipping.\n",
      "Warning: Feature 'FEETCHK2' Skipping.\n",
      "Warning: Feature 'DOCTDIAB' Skipping.\n",
      "Warning: Feature 'CHKHEMO3' Skipping.\n",
      "Warning: Feature 'FEETCHK' Skipping.\n",
      "Warning: Feature 'EYEEXAM' Skipping.\n",
      "Warning: Feature 'DIABEYE' Skipping.\n",
      "Warning: Feature 'DIABEDU' Skipping.\n",
      "Warning: Feature 'CAREGIV1' Skipping.\n",
      "Warning: Feature 'CRGVREL1' Skipping.\n",
      "Warning: Feature 'CRGVLNG1' Skipping.\n",
      "Warning: Feature 'CRGVHRS1' Skipping.\n",
      "Warning: Feature 'CRGVPRB1' Skipping.\n",
      "Warning: Feature 'CRGVPERS' Skipping.\n",
      "Warning: Feature 'CRGVHOUS' Skipping.\n",
      "Warning: Feature 'CRGVMST2' Skipping.\n",
      "Warning: Feature 'CRGVEXPT' Skipping.\n",
      "Warning: Feature 'VIDFCLT2' Skipping.\n",
      "Warning: Feature 'VIREDIF3' Skipping.\n",
      "Warning: Feature 'VIPRFVS2' Skipping.\n",
      "Warning: Feature 'VINOCRE2' Skipping.\n",
      "Warning: Feature 'VIEYEXM2' Skipping.\n",
      "Warning: Feature 'VIINSUR2' Skipping.\n",
      "Warning: Feature 'VICTRCT4' Skipping.\n",
      "Warning: Feature 'VIGLUMA2' Skipping.\n",
      "Warning: Feature 'VIMACDG2' Skipping.\n",
      "Warning: Feature 'CDHOUSE' Skipping.\n",
      "Warning: Feature 'CDASSIST' Skipping.\n",
      "Warning: Feature 'CDHELP' Skipping.\n",
      "Warning: Feature 'CDSOCIAL' Skipping.\n",
      "Warning: Feature 'CDDISCUS' Skipping.\n",
      "Warning: Feature 'WTCHSALT' Skipping.\n",
      "Warning: Feature 'LONGWTCH' Skipping.\n",
      "Warning: Feature 'DRADVISE' Skipping.\n",
      "Warning: Feature 'ASTHMAGE' Skipping.\n",
      "Warning: Feature 'ASATTACK' Skipping.\n",
      "Warning: Feature 'ASERVIST' Skipping.\n",
      "Warning: Feature 'ASDRVIST' Skipping.\n",
      "Warning: Feature 'ASRCHKUP' Skipping.\n",
      "Warning: Feature 'ASACTLIM' Skipping.\n",
      "Warning: Feature 'ASYMPTOM' Skipping.\n",
      "Warning: Feature 'ASNOSLEP' Skipping.\n",
      "Warning: Feature 'ASTHMED3' Skipping.\n",
      "Warning: Feature 'ASINHALR' Skipping.\n",
      "Warning: Feature 'HAREHAB1' Skipping.\n",
      "Warning: Feature 'STREHAB1' Skipping.\n",
      "Warning: Feature 'CVDASPRN' Skipping.\n",
      "Warning: Feature 'ASPUNSAF' Skipping.\n",
      "Warning: Feature 'RLIVPAIN' Skipping.\n",
      "Warning: Feature 'RDUCHART' Skipping.\n",
      "Warning: Feature 'RDUCSTRK' Skipping.\n",
      "Warning: Feature 'ARTTODAY' Skipping.\n",
      "Warning: Feature 'ARTHWGT' Skipping.\n",
      "Warning: Feature 'ARTHEXER' Skipping.\n",
      "Warning: Feature 'ARTHEDU' Skipping.\n",
      "Warning: Feature 'TETANUS' Skipping.\n",
      "Warning: Feature 'HPVADVC2' Skipping.\n",
      "Warning: Feature 'HPVADSHT' Skipping.\n",
      "Warning: Feature 'SHINGLE2' Skipping.\n",
      "Warning: Feature 'HADMAM' Skipping.\n",
      "Warning: Feature 'HOWLONG' Skipping.\n",
      "Warning: Feature 'HADPAP2' Skipping.\n",
      "Warning: Feature 'LASTPAP2' Skipping.\n",
      "Warning: Feature 'HPVTEST' Skipping.\n",
      "Warning: Feature 'HPLSTTST' Skipping.\n",
      "Warning: Feature 'HADHYST2' Skipping.\n",
      "Warning: Feature 'PROFEXAM' Skipping.\n",
      "Warning: Feature 'LENGEXAM' Skipping.\n",
      "Warning: Feature 'BLDSTOOL' Skipping.\n",
      "Warning: Feature 'LSTBLDS3' Skipping.\n",
      "Warning: Feature 'HADSIGM3' Skipping.\n",
      "Warning: Feature 'HADSGCO1' Skipping.\n",
      "Warning: Feature 'LASTSIG3' Skipping.\n",
      "Warning: Feature 'PCPSAAD2' Skipping.\n",
      "Warning: Feature 'PCPSADI1' Skipping.\n",
      "Warning: Feature 'PCPSARE1' Skipping.\n",
      "Warning: Feature 'PSATEST1' Skipping.\n",
      "Warning: Feature 'PSATIME' Skipping.\n",
      "Warning: Feature 'PCPSARS1' Skipping.\n",
      "Warning: Feature 'PCPSADE1' Skipping.\n",
      "Warning: Feature 'PCDMDECN' Skipping.\n",
      "Warning: Feature 'SCNTMNY1' Skipping.\n",
      "Warning: Feature 'SCNTMEL1' Skipping.\n",
      "Warning: Feature 'SCNTPAID' Skipping.\n",
      "Warning: Feature 'SCNTWRK1' Skipping.\n",
      "Warning: Feature 'SCNTLPAD' Skipping.\n",
      "Warning: Feature 'SCNTLWK1' Skipping.\n",
      "Warning: Feature 'SXORIENT' Skipping.\n",
      "Warning: Feature 'TRNSGNDR' Skipping.\n",
      "Warning: Feature 'RCSGENDR' Skipping.\n",
      "Warning: Feature 'RCSRLTN2' Skipping.\n",
      "Warning: Feature 'CASTHDX2' Skipping.\n",
      "Warning: Feature 'CASTHNO2' Skipping.\n",
      "Warning: Feature 'EMTSUPRT' Skipping.\n",
      "Warning: Feature 'LSATISFY' Skipping.\n",
      "Warning: Feature 'ADPLEASR' Skipping.\n",
      "Warning: Feature 'ADDOWN' Skipping.\n",
      "Warning: Feature 'ADSLEEP' Skipping.\n",
      "Warning: Feature 'ADENERGY' Skipping.\n",
      "Warning: Feature 'ADEAT1' Skipping.\n",
      "Warning: Feature 'ADFAIL' Skipping.\n",
      "Warning: Feature 'ADTHINK' Skipping.\n",
      "Warning: Feature 'ADMOVE' Skipping.\n",
      "Warning: Feature 'MISTMNT' Skipping.\n",
      "Warning: Feature 'ADANXEV' Skipping.\n",
      "Warning: Feature 'QSTVER' Skipping.\n",
      "Warning: Feature 'QSTLANG' Skipping.\n",
      "Warning: Feature 'MSCODE' Skipping.\n",
      "Warning: Feature '_STSTR' Skipping.\n",
      "Warning: Feature '_STRWT' Skipping.\n",
      "Warning: Feature '_RAWRAKE' Skipping.\n",
      "Warning: Feature '_WT2RAKE' Skipping.\n",
      "Warning: Feature '_CHISPNC' Skipping.\n",
      "Warning: Feature '_CRACE1' Skipping.\n",
      "Warning: Feature '_CPRACE' Skipping.\n",
      "Warning: Feature '_CLLCPWT' Skipping.\n",
      "Warning: Feature '_DUALUSE' Skipping.\n",
      "Warning: Feature '_DUALCOR' Skipping.\n",
      "Warning: Feature '_LLCPWT' Skipping.\n",
      "Warning: Feature '_ASTHMS1' Skipping.\n",
      "Warning: Feature '_PRACE1' Skipping.\n",
      "Warning: Feature '_MRACE1' Skipping.\n",
      "Warning: Feature '_HISPANC' Skipping.\n",
      "Warning: Feature '_RACE' Skipping.\n",
      "Warning: Feature '_RACEG21' Skipping.\n",
      "Warning: Feature '_RACEGR3' Skipping.\n",
      "Warning: Feature '_RACE_G1' Skipping.\n",
      "Warning: Feature '_AGE65YR' Skipping.\n",
      "Warning: Feature '_AGE80' Skipping.\n",
      "Warning: Feature 'HTIN4' Skipping.\n",
      "Warning: Feature '_BMI5' Skipping.\n",
      "Warning: Feature '_CHLDCNT' Skipping.\n",
      "Warning: Feature '_RFSMOK3' Skipping.\n",
      "Warning: Feature 'DRNKANY5' Skipping.\n",
      "Warning: Feature 'DROCDY3_' Skipping.\n",
      "Warning: Feature '_DRNKWEK' Skipping.\n",
      "Warning: Feature 'FRUTDA1_' Skipping.\n",
      "Warning: Feature 'BEANDAY_' Skipping.\n",
      "Warning: Feature 'GRENDAY_' Skipping.\n",
      "Warning: Feature 'ORNGDAY_' Skipping.\n",
      "Warning: Feature 'VEGEDA1_' Skipping.\n",
      "Warning: Feature '_MISFRTN' Skipping.\n",
      "Warning: Feature '_MISVEGN' Skipping.\n",
      "Warning: Feature '_FRTRESP' Skipping.\n",
      "Warning: Feature '_VEGRESP' Skipping.\n",
      "Warning: Feature '_FRUTSUM' Skipping.\n",
      "Warning: Feature '_VEGESUM' Skipping.\n",
      "Warning: Feature '_FRTLT1' Skipping.\n",
      "Warning: Feature '_VEGLT1' Skipping.\n",
      "Warning: Feature '_FRT16' Skipping.\n",
      "Warning: Feature '_VEG23' Skipping.\n",
      "Warning: Feature '_FRUITEX' Skipping.\n",
      "Warning: Feature '_VEGETEX' Skipping.\n",
      "Warning: Feature '_TOTINDA' Skipping.\n",
      "Warning: Feature 'METVL11_' Skipping.\n",
      "Warning: Feature 'METVL21_' Skipping.\n",
      "Warning: Feature 'FC60_' Skipping.\n",
      "Warning: Feature 'PADUR1_' Skipping.\n",
      "Warning: Feature 'PADUR2_' Skipping.\n",
      "Warning: Feature 'PAFREQ1_' Skipping.\n",
      "Warning: Feature 'PAFREQ2_' Skipping.\n",
      "Warning: Feature '_MINAC11' Skipping.\n",
      "Warning: Feature '_MINAC21' Skipping.\n",
      "Warning: Feature 'STRFREQ_' Skipping.\n",
      "Warning: Feature 'PAMISS1_' Skipping.\n",
      "Warning: Feature 'PAMIN11_' Skipping.\n",
      "Warning: Feature 'PAMIN21_' Skipping.\n",
      "Warning: Feature 'PA1MIN_' Skipping.\n",
      "Warning: Feature 'PAVIG11_' Skipping.\n",
      "Warning: Feature 'PAVIG21_' Skipping.\n",
      "Warning: Feature 'PA1VIGM_' Skipping.\n",
      "Warning: Feature '_PAINDX1' Skipping.\n",
      "Warning: Feature '_PA30021' Skipping.\n",
      "Warning: Feature '_PAREC1' Skipping.\n",
      "Warning: Feature '_RFSEAT2' Skipping.\n",
      "Warning: Feature '_RFSEAT3' Skipping.\n",
      "Warning: Feature '_FLSHOT6' Skipping.\n",
      "Warning: Feature '_PNEUMO2' Skipping.\n",
      "Warning: Feature '_AIDTST3' Skipping.\n"
     ]
    }
   ],
   "source": [
    "def apply_mapping(x_train, selected_features, mapping_dict):\n",
    "    x_train_filtered = np.zeros((x_train.shape[0], len(selected_features)))\n",
    "\n",
    "    # Create a dictionary to map feature names to their column indices in the original dataset\n",
    "    feature_indices = {feature: idx for idx, feature in enumerate(selected_features)}\n",
    "\n",
    "    # Iterate over selected features to apply the mapping\n",
    "    for idx, feature in enumerate(selected_features):\n",
    "        if feature in mapping_dict:\n",
    "            transform_function = mapping_dict[feature]\n",
    "            if callable(transform_function):\n",
    "                # Extract the feature values from the original x_train for the selected feature\n",
    "                feature_index = feature_indices[feature]\n",
    "                feature_values = x_train[:, feature_index]\n",
    "\n",
    "                # Apply the transformation to each value\n",
    "                transformed_values = np.array([transform_function(value) for value in feature_values])\n",
    "                x_train_filtered[:, idx] = transformed_values  \n",
    "        else:\n",
    "            print(f\"Warning: Feature '{feature}' Skipping.\")\n",
    "\n",
    "    return x_train_filtered\n",
    "\n",
    "x_train_filtered = apply_mapping(x_train, selected_features, mapping_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Point-Biserial Correlation:\n",
    "A way to evaluate the correlation between your categorical features (treated as continuous) and a binary target. It provides a linear correlation measure that is appropriate for your binary target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between Id and target: 0.00\n",
      "Correlation between _STATE and target: 0.00\n",
      "Correlation between FMONTH and target: 0.00\n",
      "Correlation between IDATE and target: 0.00\n",
      "Correlation between IMONTH and target: 0.00\n",
      "Correlation between IDAY and target: 0.00\n",
      "Correlation between IYEAR and target: 0.00\n",
      "Correlation between DISPCODE and target: 0.00\n",
      "Correlation between SEQNO and target: 0.00\n",
      "Correlation between _PSU and target: 0.00\n",
      "Correlation between CTELENUM and target: 0.00\n",
      "Correlation between PVTRESD1 and target: 0.00\n",
      "Correlation between COLGHOUS and target: 0.00\n",
      "Correlation between STATERES and target: 0.00\n",
      "Correlation between CELLFON3 and target: 0.00\n",
      "Correlation between LADULT and target: 0.00\n",
      "Correlation between NUMADULT and target: 0.00\n",
      "Correlation between NUMMEN and target: 0.00\n",
      "Correlation between NUMWOMEN and target: 0.00\n",
      "Correlation between CTELNUM1 and target: 0.00\n",
      "Correlation between CELLFON2 and target: 0.00\n",
      "Correlation between CADULT and target: 0.00\n",
      "Correlation between PVTRESD2 and target: 0.00\n",
      "Correlation between CCLGHOUS and target: 0.00\n",
      "Correlation between CSTATE and target: 0.00\n",
      "Correlation between LANDLINE and target: 0.00\n",
      "Correlation between HHADULT and target: 0.00\n",
      "Correlation between GENHLTH and target: 0.24\n",
      "Correlation between PHYSHLTH and target: 0.06\n",
      "Correlation between MENTHLTH and target: 0.03\n",
      "Correlation between POORHLTH and target: 0.06\n",
      "Correlation between HLTHPLN1 and target: -0.03\n",
      "Correlation between PERSDOC2 and target: 0.00\n",
      "Correlation between MEDCOST and target: -0.01\n",
      "Correlation between CHECKUP1 and target: -0.05\n",
      "Correlation between BPHIGH4 and target: -0.21\n",
      "Correlation between BPMEDS and target: 0.19\n",
      "Correlation between BLOODCHO and target: -0.05\n",
      "Correlation between CHOLCHK and target: 0.00\n",
      "Correlation between TOLDHI2 and target: 0.00\n",
      "Correlation between CVDSTRK3 and target: -0.10\n",
      "Correlation between ASTHMA3 and target: -0.03\n",
      "Correlation between ASTHNOW and target: 0.04\n",
      "Correlation between CHCSCNCR and target: -0.06\n",
      "Correlation between CHCOCNCR and target: -0.06\n",
      "Correlation between CHCCOPD1 and target: -0.09\n",
      "Correlation between HAVARTH3 and target: -0.12\n",
      "Correlation between ADDEPEV2 and target: -0.04\n",
      "Correlation between CHCKIDNY and target: -0.05\n",
      "Correlation between DIABETE3 and target: -0.16\n",
      "Correlation between DIABAGE2 and target: 0.00\n",
      "Correlation between SEX and target: -0.07\n",
      "Correlation between MARITAL and target: -0.03\n",
      "Correlation between EDUCA and target: -0.08\n",
      "Correlation between RENTHOM1 and target: 0.00\n",
      "Correlation between NUMHHOL2 and target: 0.00\n",
      "Correlation between NUMPHON2 and target: 0.00\n",
      "Correlation between CPDEMO1 and target: 0.00\n",
      "Correlation between VETERAN3 and target: -0.09\n",
      "Correlation between EMPLOY1 and target: 0.00\n",
      "Correlation between CHILDREN and target: 0.00\n",
      "Correlation between INCOME2 and target: 0.00\n",
      "Correlation between INTERNET and target: 0.10\n",
      "Correlation between WEIGHT2 and target: 0.00\n",
      "Correlation between HEIGHT3 and target: 0.00\n",
      "Correlation between PREGNANT and target: 0.00\n",
      "Correlation between QLACTLM2 and target: -0.09\n",
      "Correlation between USEEQUIP and target: -0.09\n",
      "Correlation between BLIND and target: -0.02\n",
      "Correlation between DECIDE and target: -0.03\n",
      "Correlation between DIFFWALK and target: -0.10\n",
      "Correlation between DIFFDRES and target: -0.03\n",
      "Correlation between DIFFALON and target: -0.04\n",
      "Correlation between SMOKE100 and target: -0.06\n",
      "Correlation between SMOKDAY2 and target: 0.11\n",
      "Correlation between STOPSMK2 and target: 0.00\n",
      "Correlation between LASTSMK2 and target: 0.08\n",
      "Correlation between USENOW3 and target: 0.01\n",
      "Correlation between ALCDAY5 and target: 0.00\n",
      "Correlation between AVEDRNK2 and target: -0.02\n",
      "Correlation between DRNK3GE5 and target: -0.01\n",
      "Correlation between MAXDRNKS and target: 0.00\n",
      "Correlation between FRUITJU1 and target: 0.00\n",
      "Correlation between FRUIT1 and target: 0.00\n",
      "Correlation between FVBEANS and target: 0.00\n",
      "Correlation between FVGREEN and target: 0.00\n",
      "Correlation between FVORANG and target: 0.00\n",
      "Correlation between VEGETAB1 and target: 0.00\n",
      "Correlation between EXERANY2 and target: 0.04\n",
      "Correlation between EXRACT11 and target: 0.00\n",
      "Correlation between EXEROFT1 and target: 0.00\n",
      "Correlation between EXERHMM1 and target: 0.00\n",
      "Correlation between EXRACT21 and target: 0.00\n",
      "Correlation between EXEROFT2 and target: 0.00\n",
      "Correlation between EXERHMM2 and target: 0.00\n",
      "Correlation between STRENGTH and target: 0.00\n",
      "Correlation between LMTJOIN3 and target: 0.13\n",
      "Correlation between ARTHDIS2 and target: 0.00\n",
      "Correlation between ARTHSOCL and target: 0.00\n",
      "Correlation between JOINPAIN and target: 0.00\n",
      "Correlation between SEATBELT and target: 0.00\n",
      "Correlation between FLUSHOT6 and target: -0.03\n",
      "Correlation between FLSHTMY2 and target: 0.00\n",
      "Correlation between IMFVPLAC and target: 0.00\n",
      "Correlation between PNEUVAC3 and target: 0.00\n",
      "Correlation between HIVTST6 and target: 0.00\n",
      "Correlation between HIVTSTD3 and target: 0.00\n",
      "Correlation between WHRTST10 and target: 0.00\n",
      "Correlation between PDIABTST and target: -0.02\n",
      "Correlation between PREDIAB1 and target: -0.02\n",
      "Correlation between INSULIN and target: 0.11\n",
      "Correlation between BLDSUGAR and target: 0.00\n",
      "Correlation between FEETCHK2 and target: 0.00\n",
      "Correlation between DOCTDIAB and target: 0.00\n",
      "Correlation between CHKHEMO3 and target: 0.00\n",
      "Correlation between FEETCHK and target: 0.00\n",
      "Correlation between EYEEXAM and target: 0.00\n",
      "Correlation between DIABEYE and target: 0.00\n",
      "Correlation between DIABEDU and target: 0.00\n",
      "Correlation between CAREGIV1 and target: 0.00\n",
      "Correlation between CRGVREL1 and target: 0.00\n",
      "Correlation between CRGVLNG1 and target: 0.00\n",
      "Correlation between CRGVHRS1 and target: 0.00\n",
      "Correlation between CRGVPRB1 and target: 0.00\n",
      "Correlation between CRGVPERS and target: 0.00\n",
      "Correlation between CRGVHOUS and target: 0.00\n",
      "Correlation between CRGVMST2 and target: 0.00\n",
      "Correlation between CRGVEXPT and target: 0.00\n",
      "Correlation between VIDFCLT2 and target: 0.00\n",
      "Correlation between VIREDIF3 and target: 0.00\n",
      "Correlation between VIPRFVS2 and target: 0.00\n",
      "Correlation between VINOCRE2 and target: 0.00\n",
      "Correlation between VIEYEXM2 and target: 0.00\n",
      "Correlation between VIINSUR2 and target: 0.00\n",
      "Correlation between VICTRCT4 and target: 0.00\n",
      "Correlation between VIGLUMA2 and target: 0.00\n",
      "Correlation between VIMACDG2 and target: 0.00\n",
      "Correlation between CIMEMLOS and target: 0.06\n",
      "Correlation between CDHOUSE and target: 0.00\n",
      "Correlation between CDASSIST and target: 0.00\n",
      "Correlation between CDHELP and target: 0.00\n",
      "Correlation between CDSOCIAL and target: 0.00\n",
      "Correlation between CDDISCUS and target: 0.00\n",
      "Correlation between WTCHSALT and target: 0.00\n",
      "Correlation between LONGWTCH and target: 0.00\n",
      "Correlation between DRADVISE and target: 0.00\n",
      "Correlation between ASTHMAGE and target: 0.00\n",
      "Correlation between ASATTACK and target: 0.00\n",
      "Correlation between ASERVIST and target: 0.00\n",
      "Correlation between ASDRVIST and target: 0.00\n",
      "Correlation between ASRCHKUP and target: 0.00\n",
      "Correlation between ASACTLIM and target: 0.00\n",
      "Correlation between ASYMPTOM and target: 0.00\n",
      "Correlation between ASNOSLEP and target: 0.00\n",
      "Correlation between ASTHMED3 and target: 0.00\n",
      "Correlation between ASINHALR and target: 0.00\n",
      "Correlation between HAREHAB1 and target: 0.00\n",
      "Correlation between STREHAB1 and target: 0.00\n",
      "Correlation between CVDASPRN and target: 0.00\n",
      "Correlation between ASPUNSAF and target: 0.00\n",
      "Correlation between RLIVPAIN and target: 0.00\n",
      "Correlation between RDUCHART and target: 0.00\n",
      "Correlation between RDUCSTRK and target: 0.00\n",
      "Correlation between ARTTODAY and target: 0.00\n",
      "Correlation between ARTHWGT and target: 0.00\n",
      "Correlation between ARTHEXER and target: 0.00\n",
      "Correlation between ARTHEDU and target: 0.00\n",
      "Correlation between TETANUS and target: 0.00\n",
      "Correlation between HPVADVC2 and target: 0.00\n",
      "Correlation between HPVADSHT and target: 0.00\n",
      "Correlation between SHINGLE2 and target: 0.00\n",
      "Correlation between HADMAM and target: 0.00\n",
      "Correlation between HOWLONG and target: 0.00\n",
      "Correlation between HADPAP2 and target: 0.00\n",
      "Correlation between LASTPAP2 and target: 0.00\n",
      "Correlation between HPVTEST and target: 0.00\n",
      "Correlation between HPLSTTST and target: 0.00\n",
      "Correlation between HADHYST2 and target: 0.00\n",
      "Correlation between PROFEXAM and target: 0.00\n",
      "Correlation between LENGEXAM and target: 0.00\n",
      "Correlation between BLDSTOOL and target: 0.00\n",
      "Correlation between LSTBLDS3 and target: 0.00\n",
      "Correlation between HADSIGM3 and target: 0.00\n",
      "Correlation between HADSGCO1 and target: 0.00\n",
      "Correlation between LASTSIG3 and target: 0.00\n",
      "Correlation between PCPSAAD2 and target: 0.00\n",
      "Correlation between PCPSADI1 and target: 0.00\n",
      "Correlation between PCPSARE1 and target: 0.00\n",
      "Correlation between PSATEST1 and target: 0.00\n",
      "Correlation between PSATIME and target: 0.00\n",
      "Correlation between PCPSARS1 and target: 0.00\n",
      "Correlation between PCPSADE1 and target: 0.00\n",
      "Correlation between PCDMDECN and target: 0.00\n",
      "Correlation between SCNTMNY1 and target: 0.00\n",
      "Correlation between SCNTMEL1 and target: 0.00\n",
      "Correlation between SCNTPAID and target: 0.00\n",
      "Correlation between SCNTWRK1 and target: 0.00\n",
      "Correlation between SCNTLPAD and target: 0.00\n",
      "Correlation between SCNTLWK1 and target: 0.00\n",
      "Correlation between SXORIENT and target: 0.00\n",
      "Correlation between TRNSGNDR and target: 0.00\n",
      "Correlation between RCSGENDR and target: 0.00\n",
      "Correlation between RCSRLTN2 and target: 0.00\n",
      "Correlation between CASTHDX2 and target: 0.00\n",
      "Correlation between CASTHNO2 and target: 0.00\n",
      "Correlation between EMTSUPRT and target: 0.00\n",
      "Correlation between LSATISFY and target: 0.00\n",
      "Correlation between ADPLEASR and target: 0.00\n",
      "Correlation between ADDOWN and target: 0.00\n",
      "Correlation between ADSLEEP and target: 0.00\n",
      "Correlation between ADENERGY and target: 0.00\n",
      "Correlation between ADEAT1 and target: 0.00\n",
      "Correlation between ADFAIL and target: 0.00\n",
      "Correlation between ADTHINK and target: 0.00\n",
      "Correlation between ADMOVE and target: 0.00\n",
      "Correlation between MISTMNT and target: 0.00\n",
      "Correlation between ADANXEV and target: 0.00\n",
      "Correlation between QSTVER and target: 0.00\n",
      "Correlation between QSTLANG and target: 0.00\n",
      "Correlation between MSCODE and target: 0.00\n",
      "Correlation between _STSTR and target: 0.00\n",
      "Correlation between _STRWT and target: 0.00\n",
      "Correlation between _RAWRAKE and target: 0.00\n",
      "Correlation between _WT2RAKE and target: 0.00\n",
      "Correlation between _CHISPNC and target: 0.00\n",
      "Correlation between _CRACE1 and target: 0.00\n",
      "Correlation between _CPRACE and target: 0.00\n",
      "Correlation between _CLLCPWT and target: 0.00\n",
      "Correlation between _DUALUSE and target: 0.00\n",
      "Correlation between _DUALCOR and target: 0.00\n",
      "Correlation between _LLCPWT and target: 0.00\n",
      "Correlation between _RFHLTH and target: 0.17\n",
      "Correlation between _HCVU651 and target: 0.20\n",
      "Correlation between _RFHYPE5 and target: 0.17\n",
      "Correlation between _CHOLCHK and target: -0.05\n",
      "Correlation between _RFCHOL and target: 0.14\n",
      "Correlation between _LTASTH1 and target: 0.04\n",
      "Correlation between _CASTHM1 and target: 0.03\n",
      "Correlation between _ASTHMS1 and target: 0.00\n",
      "Correlation between _DRDXAR1 and target: -0.16\n",
      "Correlation between _PRACE1 and target: 0.00\n",
      "Correlation between _MRACE1 and target: 0.00\n",
      "Correlation between _HISPANC and target: 0.00\n",
      "Correlation between _RACE and target: 0.00\n",
      "Correlation between _RACEG21 and target: 0.00\n",
      "Correlation between _RACEGR3 and target: 0.00\n",
      "Correlation between _RACE_G1 and target: 0.00\n",
      "Correlation between _AGEG5YR and target: 0.22\n",
      "Correlation between _AGE65YR and target: 0.00\n",
      "Correlation between _AGE80 and target: 0.00\n",
      "Correlation between _AGE_G and target: 0.21\n",
      "Correlation between HTIN4 and target: 0.00\n",
      "Correlation between HTM4 and target: 0.02\n",
      "Correlation between WTKG3 and target: 0.06\n",
      "Correlation between _BMI5 and target: 0.00\n",
      "Correlation between _BMI5CAT and target: 0.06\n",
      "Correlation between _RFBMI5 and target: -0.02\n",
      "Correlation between _CHLDCNT and target: 0.00\n",
      "Correlation between _EDUCAG and target: -0.08\n",
      "Correlation between _INCOMG and target: -0.05\n",
      "Correlation between _SMOKER3 and target: -0.05\n",
      "Correlation between _RFSMOK3 and target: 0.00\n",
      "Correlation between DRNKANY5 and target: 0.00\n",
      "Correlation between DROCDY3_ and target: 0.00\n",
      "Correlation between _RFBING5 and target: -0.02\n",
      "Correlation between _DRNKWEK and target: 0.00\n",
      "Correlation between _RFDRHV5 and target: -0.01\n",
      "Correlation between FTJUDA1_ and target: 0.01\n",
      "Correlation between FRUTDA1_ and target: 0.00\n",
      "Correlation between BEANDAY_ and target: 0.00\n",
      "Correlation between GRENDAY_ and target: 0.00\n",
      "Correlation between ORNGDAY_ and target: 0.00\n",
      "Correlation between VEGEDA1_ and target: 0.00\n",
      "Correlation between _MISFRTN and target: 0.00\n",
      "Correlation between _MISVEGN and target: 0.00\n",
      "Correlation between _FRTRESP and target: 0.00\n",
      "Correlation between _VEGRESP and target: 0.00\n",
      "Correlation between _FRUTSUM and target: 0.00\n",
      "Correlation between _VEGESUM and target: 0.00\n",
      "Correlation between _FRTLT1 and target: 0.00\n",
      "Correlation between _VEGLT1 and target: 0.00\n",
      "Correlation between _FRT16 and target: 0.00\n",
      "Correlation between _VEG23 and target: 0.00\n",
      "Correlation between _FRUITEX and target: 0.00\n",
      "Correlation between _VEGETEX and target: 0.00\n",
      "Correlation between _TOTINDA and target: 0.00\n",
      "Correlation between METVL11_ and target: 0.00\n",
      "Correlation between METVL21_ and target: 0.00\n",
      "Correlation between MAXVO2_ and target: -0.21\n",
      "Correlation between FC60_ and target: 0.00\n",
      "Correlation between ACTIN11_ and target: -0.03\n",
      "Correlation between ACTIN21_ and target: -0.04\n",
      "Correlation between PADUR1_ and target: 0.00\n",
      "Correlation between PADUR2_ and target: 0.00\n",
      "Correlation between PAFREQ1_ and target: 0.00\n",
      "Correlation between PAFREQ2_ and target: 0.00\n",
      "Correlation between _MINAC11 and target: 0.00\n",
      "Correlation between _MINAC21 and target: 0.00\n",
      "Correlation between STRFREQ_ and target: 0.00\n",
      "Correlation between PAMISS1_ and target: 0.00\n",
      "Correlation between PAMIN11_ and target: 0.00\n",
      "Correlation between PAMIN21_ and target: 0.00\n",
      "Correlation between PA1MIN_ and target: 0.00\n",
      "Correlation between PAVIG11_ and target: 0.00\n",
      "Correlation between PAVIG21_ and target: 0.00\n",
      "Correlation between PA1VIGM_ and target: 0.00\n",
      "Correlation between _PACAT1 and target: 0.02\n",
      "Correlation between _PAINDX1 and target: 0.00\n",
      "Correlation between _PA150R2 and target: 0.02\n",
      "Correlation between _PA300R2 and target: 0.01\n",
      "Correlation between _PA30021 and target: 0.00\n",
      "Correlation between _PASTRNG and target: 0.00\n",
      "Correlation between _PAREC1 and target: 0.00\n",
      "Correlation between _PASTAE1 and target: 0.00\n",
      "Correlation between _LMTACT1 and target: -0.07\n",
      "Correlation between _LMTWRK1 and target: -0.03\n",
      "Correlation between _LMTSCL1 and target: -0.09\n",
      "Correlation between _RFSEAT2 and target: 0.00\n",
      "Correlation between _RFSEAT3 and target: 0.00\n",
      "Correlation between _FLSHOT6 and target: 0.00\n",
      "Correlation between _PNEUMO2 and target: 0.00\n",
      "Correlation between _AIDTST3 and target: 0.00\n"
     ]
    }
   ],
   "source": [
    "def point_biserial_correlation(x, y):\n",
    "    y_mean_1 = np.mean(x[y == 1])\n",
    "    y_mean_0 = np.mean(x[y == -1])\n",
    "    y_std = np.std(x)\n",
    "    \n",
    "    p = np.sum(y == 1) / len(y)\n",
    "    q = 1 - p\n",
    "    \n",
    "    correlation = (y_mean_1 - y_mean_0) * np.sqrt(p * q) / y_std if y_std > 0 else 0\n",
    "    return correlation\n",
    "\n",
    "def calculate_correlations_point_biserial(x_train, y_train, selected_features):\n",
    "    correlations = {}\n",
    "\n",
    "    for idx, feature_name in enumerate(selected_features):\n",
    "        feature_values = x_train[:, idx]\n",
    "        correlation = point_biserial_correlation(feature_values, y_train)\n",
    "        correlations[feature_name] = correlation\n",
    "        print(f\"Correlation between {feature_name} and target: {correlation:.2f}\")\n",
    "\n",
    "    return correlations\n",
    "\n",
    "y_train = y_train.flatten()\n",
    "x_train_filtered = np.array(x_train_filtered)\n",
    "\n",
    "correlations = calculate_correlations_point_biserial(x_train_filtered, y_train, selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filter out features with zero correlation and sort them in decreasing order\n",
    "filtered_correlations = {feature: corr for feature, corr in correlations.items() if corr != 0}\n",
    "sorted_correlations = dict(sorted(filtered_correlations.items(), key=lambda item: abs(item[1]), reverse=True))\n",
    "\n",
    "feature_names = list(sorted_correlations.keys())\n",
    "correlation_values = list(sorted_correlations.values())\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.bar(feature_names, correlation_values, color='green')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Correlation with Target')\n",
    "plt.title('Feature Correlations with Target Variable')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features based on threshold of 0.05: ['GENHLTH', 'PHYSHLTH', 'POORHLTH', 'CHECKUP1', 'BPHIGH4', 'BPMEDS', 'BLOODCHO', 'CVDSTRK3', 'CHCSCNCR', 'CHCOCNCR', 'CHCCOPD1', 'HAVARTH3', 'CHCKIDNY', 'DIABETE3', 'SEX', 'EDUCA', 'VETERAN3', 'INTERNET', 'QLACTLM2', 'USEEQUIP', 'DIFFWALK', 'SMOKE100', 'SMOKDAY2', 'LASTSMK2', 'LMTJOIN3', 'INSULIN', 'CIMEMLOS', '_RFHLTH', '_HCVU651', '_RFHYPE5', '_CHOLCHK', '_RFCHOL', '_DRDXAR1', '_AGEG5YR', '_AGE_G', 'WTKG3', '_BMI5CAT', '_EDUCAG', '_INCOMG', 'MAXVO2_', '_LMTACT1', '_LMTSCL1']\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.05\n",
    "selected_features_final = [feature for feature, corr in correlations.items() if abs(corr) >= threshold]\n",
    "print(f\"Selected features based on threshold of {threshold}: {selected_features_final}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "print(len(selected_features_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalized Mutual Information \n",
    "This is a non-linear measure that provides insight into how much information about the target can be gained from each feature. It is efficient for categorical data and can give valuable insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Calculate Entropy for each feature and the target.\n",
    "2. Calculate Joint Entropy between each feature and the target.\n",
    "3. Use these values to compute Mutual Information (MI).\n",
    "4. Calculate the Normalized Mutual Information (NMI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Id', 0.0), ('_STATE', 0.0), ('FMONTH', 0.0), ('IDATE', 0.0), ('IMONTH', 0.0), ('IDAY', 0.0), ('IYEAR', 0.0), ('DISPCODE', 0.0), ('SEQNO', 0.0), ('_PSU', 0.0), ('CTELENUM', 0.0), ('PVTRESD1', 0.0), ('COLGHOUS', 0.0), ('STATERES', 0.0), ('CELLFON3', 0.0), ('LADULT', 0.0), ('NUMADULT', 0.0), ('NUMMEN', 0.0), ('NUMWOMEN', 0.0), ('CTELNUM1', 0.0), ('CELLFON2', 0.0), ('CADULT', 0.0), ('PVTRESD2', 0.0), ('CCLGHOUS', 0.0), ('CSTATE', 0.0), ('LANDLINE', 0.0), ('HHADULT', 0.0), ('GENHLTH', 0.034118670925499466), ('PHYSHLTH', 0.017770978299167798), ('MENTHLTH', 0.0030221603567335935), ('POORHLTH', 0.015123024707419028), ('HLTHPLN1', 0.003185139870373865), ('PERSDOC2', 0.0), ('MEDCOST', 0.000714273622114409), ('CHECKUP1', 0.006386783429551598), ('BPHIGH4', 0.04351175953907943), ('BPMEDS', 0.04473806511587297), ('BLOODCHO', 0.013912067907761137), ('CHOLCHK', 0.015983857172958037), ('TOLDHI2', 0.028730190193599008), ('CVDSTRK3', 0.056336495445269384), ('ASTHMA3', 0.0032994945344814093), ('ASTHNOW', 0.0036333488366764076), ('CHCSCNCR', 0.009550963579582197), ('CHCOCNCR', 0.009656658701104591), ('CHCCOPD1', 0.04224858298862315), ('HAVARTH3', 0.02729687976267859), ('ADDEPEV2', 0.005488300998081674), ('CHCKIDNY', 0.027001137315382458), ('DIABETE3', 0.03231970963160708), ('DIABAGE2', 0.0), ('SEX', 0.005068909697124736), ('MARITAL', 0.010258158600312291), ('EDUCA', 0.004390236620688676), ('RENTHOM1', 0.0), ('NUMHHOL2', 0.0), ('NUMPHON2', 0.0), ('CPDEMO1', 0.0), ('VETERAN3', 0.016990233920757592), ('EMPLOY1', 0.0), ('CHILDREN', 0.0), ('INCOME2', 0.005199193136567377), ('INTERNET', 0.021494286716599118), ('WEIGHT2', 0.0), ('HEIGHT3', 0.0), ('PREGNANT', 0.0), ('QLACTLM2', 0.030161366795879375), ('USEEQUIP', 0.03525720007215991), ('BLIND', 0.013008324740156844), ('DECIDE', 0.010453345814705844), ('DIFFWALK', 0.04015560810279565), ('DIFFDRES', 0.015934758839003665), ('DIFFALON', 0.020112026926330455), ('SMOKE100', 0.009945591284554212), ('SMOKDAY2', 0.00916999705391319), ('STOPSMK2', 0.0), ('LASTSMK2', 0.00897280466883647), ('USENOW3', 0.00021335715940996216), ('ALCDAY5', 0.0), ('AVEDRNK2', 0.004974626380786782), ('DRNK3GE5', 0.0050165110624339005), ('MAXDRNKS', 0.0), ('FRUITJU1', 0.0), ('FRUIT1', 0.0), ('FVBEANS', 0.0), ('FVGREEN', 0.0), ('FVORANG', 0.0), ('VEGETAB1', 0.0), ('EXERANY2', 0.005059367233083721), ('EXRACT11', 0.0), ('EXEROFT1', 0.0), ('EXERHMM1', 0.0), ('EXRACT21', 0.0), ('EXEROFT2', 0.0), ('EXERHMM2', 0.0), ('STRENGTH', 0.0), ('LMTJOIN3', 0.02214303457647292), ('ARTHDIS2', 0.0), ('ARTHSOCL', 0.0), ('JOINPAIN', 0.0), ('SEATBELT', 0.0), ('FLUSHOT6', 0.0038751837916616663), ('FLSHTMY2', 0.0), ('IMFVPLAC', 0.0), ('PNEUVAC3', 0.0), ('HIVTST6', 0.0), ('HIVTSTD3', 0.0), ('WHRTST10', 0.0), ('PDIABTST', 0.0009259820598659492), ('PREDIAB1', 0.0014772558847965477), ('INSULIN', 0.020695523750434947), ('BLDSUGAR', 0.0), ('FEETCHK2', 0.0), ('DOCTDIAB', 0.0), ('CHKHEMO3', 0.0), ('FEETCHK', 0.0), ('EYEEXAM', 0.0), ('DIABEYE', 0.0), ('DIABEDU', 0.0), ('CAREGIV1', 0.0), ('CRGVREL1', 0.0), ('CRGVLNG1', 0.0), ('CRGVHRS1', 0.0), ('CRGVPRB1', 0.0), ('CRGVPERS', 0.0), ('CRGVHOUS', 0.0), ('CRGVMST2', 0.0), ('CRGVEXPT', 0.0), ('VIDFCLT2', 0.0), ('VIREDIF3', 0.0), ('VIPRFVS2', 0.0), ('VINOCRE2', 0.0), ('VIEYEXM2', 0.0), ('VIINSUR2', 0.0), ('VICTRCT4', 0.0), ('VIGLUMA2', 0.0), ('VIMACDG2', 0.0), ('CIMEMLOS', 0.007725860840259326), ('CDHOUSE', 0.0), ('CDASSIST', 0.0), ('CDHELP', 0.0), ('CDSOCIAL', 0.0), ('CDDISCUS', 0.0), ('WTCHSALT', 0.0), ('LONGWTCH', 0.0), ('DRADVISE', 0.0), ('ASTHMAGE', 0.0), ('ASATTACK', 0.0), ('ASERVIST', 0.0), ('ASDRVIST', 0.0), ('ASRCHKUP', 0.0), ('ASACTLIM', 0.0), ('ASYMPTOM', 0.0), ('ASNOSLEP', 0.0), ('ASTHMED3', 0.0), ('ASINHALR', 0.0), ('HAREHAB1', 0.0), ('STREHAB1', 0.0), ('CVDASPRN', 0.0), ('ASPUNSAF', 0.0), ('RLIVPAIN', 0.0), ('RDUCHART', 0.0), ('RDUCSTRK', 0.0), ('ARTTODAY', 0.0), ('ARTHWGT', 0.0), ('ARTHEXER', 0.0), ('ARTHEDU', 0.0), ('TETANUS', 0.0), ('HPVADVC2', 0.0), ('HPVADSHT', 0.0), ('SHINGLE2', 0.0), ('HADMAM', 0.0), ('HOWLONG', 0.0), ('HADPAP2', 0.0), ('LASTPAP2', 0.0), ('HPVTEST', 0.0), ('HPLSTTST', 0.0), ('HADHYST2', 0.0), ('PROFEXAM', 0.0), ('LENGEXAM', 0.0), ('BLDSTOOL', 0.0), ('LSTBLDS3', 0.0), ('HADSIGM3', 0.0), ('HADSGCO1', 0.0), ('LASTSIG3', 0.0), ('PCPSAAD2', 0.0), ('PCPSADI1', 0.0), ('PCPSARE1', 0.0), ('PSATEST1', 0.0), ('PSATIME', 0.0), ('PCPSARS1', 0.0), ('PCPSADE1', 0.0), ('PCDMDECN', 0.0), ('SCNTMNY1', 0.0), ('SCNTMEL1', 0.0), ('SCNTPAID', 0.0), ('SCNTWRK1', 0.0), ('SCNTLPAD', 0.0), ('SCNTLWK1', 0.0), ('SXORIENT', 0.0), ('TRNSGNDR', 0.0), ('RCSGENDR', 0.0), ('RCSRLTN2', 0.0), ('CASTHDX2', 0.0), ('CASTHNO2', 0.0), ('EMTSUPRT', 0.0), ('LSATISFY', 0.0), ('ADPLEASR', 0.0), ('ADDOWN', 0.0), ('ADSLEEP', 0.0), ('ADENERGY', 0.0), ('ADEAT1', 0.0), ('ADFAIL', 0.0), ('ADTHINK', 0.0), ('ADMOVE', 0.0), ('MISTMNT', 0.0), ('ADANXEV', 0.0), ('QSTVER', 0.0), ('QSTLANG', 0.0), ('MSCODE', 0.0), ('_STSTR', 0.0), ('_STRWT', 0.0), ('_RAWRAKE', 0.0), ('_WT2RAKE', 0.0), ('_CHISPNC', 0.0), ('_CRACE1', 0.0), ('_CPRACE', 0.0), ('_CLLCPWT', 0.0), ('_DUALUSE', 0.0), ('_DUALCOR', 0.0), ('_LLCPWT', 0.0), ('_RFHLTH', 0.055815522592998774), ('_HCVU651', 0.03180730320721715), ('_RFHYPE5', 0.047337925372982516), ('_CHOLCHK', 0.01308566301827103), ('_RFCHOL', 0.02876801360193264), ('_LTASTH1', 0.003281213635665369), ('_CASTHM1', 0.004569851808970841), ('_ASTHMS1', 0.0), ('_DRDXAR1', 0.027320126551703888), ('_PRACE1', 0.0), ('_MRACE1', 0.0), ('_HISPANC', 0.0), ('_RACE', 0.0), ('_RACEG21', 0.0), ('_RACEGR3', 0.0), ('_RACE_G1', 0.0), ('_AGEG5YR', 0.020965588790533816), ('_AGE65YR', 0.0), ('_AGE80', 0.0), ('_AGE_G', 0.028598797445921875), ('HTIN4', 0.0), ('HTM4', 0.0005860307001768757), ('WTKG3', 0.0018843518573529003), ('_BMI5', 0.0), ('_BMI5CAT', 0.0028789664428868966), ('_RFBMI5', 0.003358431847059735), ('_CHLDCNT', 0.0), ('_EDUCAG', 0.004517282407496679), ('_INCOMG', 0.006123691257079705), ('_SMOKER3', 0.00831994450574932), ('_RFSMOK3', 0.0), ('DRNKANY5', 0.0), ('DROCDY3_', 0.0), ('_RFBING5', 0.005032428382185947), ('_DRNKWEK', 0.0), ('_RFDRHV5', 0.0011303891878202512), ('FTJUDA1_', 0.0007840521458618031), ('FRUTDA1_', 0.0), ('BEANDAY_', 0.0), ('GRENDAY_', 0.0), ('ORNGDAY_', 0.0), ('VEGEDA1_', 0.0), ('_MISFRTN', 0.0), ('_MISVEGN', 0.0), ('_FRTRESP', 0.0), ('_VEGRESP', 0.0), ('_FRUTSUM', 0.0), ('_VEGESUM', 0.0), ('_FRTLT1', 0.0), ('_VEGLT1', 0.0), ('_FRT16', 0.0), ('_VEG23', 0.0), ('_FRUITEX', 0.0), ('_VEGETEX', 0.0), ('_TOTINDA', 0.0), ('METVL11_', 0.0), ('METVL21_', 0.0), ('MAXVO2_', 0.02649234323811885), ('FC60_', 0.0), ('ACTIN11_', 0.008129706128193354), ('ACTIN21_', 0.005359191213945339), ('PADUR1_', 0.0), ('PADUR2_', 0.0), ('PAFREQ1_', 0.0), ('PAFREQ2_', 0.0), ('_MINAC11', 0.0), ('_MINAC21', 0.0), ('STRFREQ_', 0.0), ('PAMISS1_', 0.0), ('PAMIN11_', 0.0), ('PAMIN21_', 0.0), ('PA1MIN_', 0.0), ('PAVIG11_', 0.0), ('PAVIG21_', 0.0), ('PA1VIGM_', 0.0), ('_PACAT1', 0.0034523063799927777), ('_PAINDX1', 0.0), ('_PA150R2', 0.0038038239811361697), ('_PA300R2', 0.003922844622894563), ('_PA30021', 0.0), ('_PASTRNG', 0.0016772519797714572), ('_PAREC1', 0.0), ('_PASTAE1', 0.0016593652173361737), ('_LMTACT1', 0.022746501608645043), ('_LMTWRK1', 0.021807893525840748), ('_LMTSCL1', 0.022346178336998652), ('_RFSEAT2', 0.0), ('_RFSEAT3', 0.0), ('_FLSHOT6', 0.0), ('_PNEUMO2', 0.0), ('_AIDTST3', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "def calculate_entropy(values):\n",
    "    unique_values, counts = np.unique(values, return_counts=True)\n",
    "    probabilities = counts / len(values)\n",
    "    entropy = -np.sum([p * np.log2(p) for p in probabilities if p > 0])\n",
    "    return entropy\n",
    "\n",
    "def calculate_joint_entropy(feature_values, target_values):\n",
    "    unique_pairs, counts = np.unique(list(zip(feature_values, target_values)), axis=0, return_counts=True)\n",
    "    probabilities = counts / len(feature_values)\n",
    "    joint_entropy = -np.sum([p * np.log2(p) for p in probabilities if p > 0])\n",
    "    return joint_entropy\n",
    "\n",
    "def calculate_normalized_mutual_information(x_train_filtered, y_train, selected_features):\n",
    "    nmi_values = []\n",
    "\n",
    "    # Calculate the entropy of the target variable\n",
    "    entropy_y = calculate_entropy(y_train)\n",
    "\n",
    "    # Iterate over each feature in the dataset\n",
    "    for idx, feature_name in enumerate(selected_features):\n",
    "        feature_values = x_train_filtered[:, idx]\n",
    "\n",
    "        # Calculate entropy of the feature\n",
    "        entropy_feature = calculate_entropy(feature_values)\n",
    "\n",
    "        # Calculate joint entropy between the feature and the target\n",
    "        joint_entropy = calculate_joint_entropy(feature_values, y_train)\n",
    "\n",
    "        # Mutual Information: MI(feature, target) = H(feature) + H(target) - H(feature, target)\n",
    "        mutual_information = entropy_feature + entropy_y - joint_entropy\n",
    "\n",
    "        # Normalized Mutual Information: NMI = 2 * MI / (H(feature) + H(target))\n",
    "        if (entropy_feature + entropy_y) == 0:\n",
    "            nmi = 0\n",
    "        else:\n",
    "            nmi = 2 * mutual_information / (entropy_feature + entropy_y)\n",
    "\n",
    "        # Append the NMI value to the list\n",
    "        nmi_values.append(nmi)\n",
    "\n",
    "    return nmi_values\n",
    "\n",
    "# Calculate the NMI for all selected features\n",
    "nmi_values = calculate_normalized_mutual_information(x_train_filtered, y_train, selected_features)\n",
    "\n",
    "# Print the NMI values for each feature\n",
    "print(list(zip(selected_features, nmi_values)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filter out non-zero NMI values\n",
    "non_zero_indices = [i for i, nmi in enumerate(nmi_values) if nmi > 0]\n",
    "non_zero_features = [selected_features[i] for i in non_zero_indices]\n",
    "non_zero_nmi_values = [nmi_values[i] for i in non_zero_indices]\n",
    "\n",
    "# Sort the non-zero NMI values in descending order\n",
    "sorted_indices = np.argsort(non_zero_nmi_values)[::-1]\n",
    "sorted_features = [non_zero_features[i] for i in sorted_indices]\n",
    "sorted_nmi_values = [non_zero_nmi_values[i] for i in sorted_indices]\n",
    "\n",
    "# Plot the sorted NMI values\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.bar(sorted_features, sorted_nmi_values, color='green')\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Normalized Mutual Information')\n",
    "plt.title('NMI of Features Given Target')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features with NMI > 0.03:\n",
      "['GENHLTH', 'BPHIGH4', 'BPMEDS', 'CVDSTRK3', 'CHCCOPD1', 'DIABETE3', 'QLACTLM2', 'USEEQUIP', 'DIFFWALK', '_RFHLTH', '_HCVU651', '_RFHYPE5']\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "# Set the threshold value\n",
    "threshold = 0.03\n",
    "\n",
    "# Filter features with NMI values above the threshold\n",
    "selected_indices = [i for i, nmi in enumerate(nmi_values) if nmi > threshold]\n",
    "selected_features_nmi = [selected_features[i] for i in selected_indices]\n",
    "selected_nmi_values = [nmi_values[i] for i in selected_indices]\n",
    "\n",
    "# Print selected features\n",
    "print(\"Selected features with NMI > 0.03:\")\n",
    "print(list(selected_features_nmi))\n",
    "print(len(selected_features_nmi))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Selected Features**\n",
    "\n",
    "USEEQUIP : Do you now have any health problem that requires you to use special equipment, such as a cane, a wheelchair, a \n",
    "special bed, or a special telephone? \n",
    "\n",
    "INTERNET : Have you used the internet in the past 30 days? \n",
    "\n",
    "INCOME2 : Is your annual household income from all sources\n",
    "\n",
    "DIFFALON : Because of a physical, mental, or emotional condition, do you have difficulty doing errands alone such as visiting a \n",
    "doctor’s office or shopping?\n",
    "\n",
    "DIABETE3 : (Ever told) you have diabetes \n",
    "\n",
    "QLACTLM2 : Are you limited in any way in any activities because of physical, mental, or emotional problems? \n",
    "\n",
    "_RFHYPE5 : Adults who have been told they have high blood pressure by a doctor, nurse, or other health professional \n",
    "\n",
    "SMOKE100 : Four-level smoker status:    Everyday smoker, Someday smoker, Former smoker, Non-smoker\n",
    "\n",
    "_CASTHM1 : Adults who have been told they currently have asthma\n",
    "\n",
    "_PASTAE1 : Aerobic and Strengthening (2-level) \n",
    "\n",
    "_AGEG5YR : Fourteen-level age category\n",
    " \n",
    "_LMTSCL1 : Limited social activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the filtered x_train_filtered: (328135, 42)\n"
     ]
    }
   ],
   "source": [
    "# final_selected_features = [\n",
    "#     \"USEEQUIP\", \"INCOME2\", \"DIFFALON\", \"DIABETE3\",\n",
    "#     \"QLACTLM2\", \"_RFHYPE5\", \"SMOKE100\", \"_CASTHM1\", \"_PASTAE1\",\n",
    "#     \"_AGEG5YR\", \"CVDSTRK3\", \"BPHIGH4\"\n",
    "# ]\n",
    "\n",
    "# final_selected_features = [\n",
    "# 'GENHLTH', 'BPHIGH4', 'BPMEDS', 'CVDSTRK3', 'HAVARTH3', 'DIABETE3', 'INTERNET', 'SMOKDAY2', 'LMTJOIN3', 'INSULIN', '_RFHLTH', '_HCVU651', '_RFHYPE5', '_RFCHOL', '_DRDXAR1', '_AGEG5YR', '_AGE_G', 'MAXVO2_'\n",
    "# ]\n",
    "#########final_selected_features= ['GENHLTH', 'BPHIGH4', 'BPMEDS', 'CVDSTRK3', 'CHCCOPD1', 'HAVARTH3', 'DIABETE3', 'SEX', 'EDUCA', 'VETERAN3', 'INTERNET', 'QLACTLM2', 'USEEQUIP', 'DIFFWALK', 'SMOKDAY2', 'LASTSMK2', 'LMTJOIN3', 'INSULIN', '_RFHLTH', '_HCVU651', '_RFHYPE5', '_RFCHOL', '_DRDXAR1', '_AGEG5YR', '_AGE_G', '_EDUCAG', 'MAXVO2_', '_LMTSCL1']\n",
    "final_selected_features= ['GENHLTH', 'PHYSHLTH', 'POORHLTH', 'CHECKUP1', 'BPHIGH4', 'BPMEDS', 'BLOODCHO', 'CVDSTRK3', 'CHCSCNCR', 'CHCOCNCR', 'CHCCOPD1', 'HAVARTH3', 'CHCKIDNY', 'DIABETE3', 'SEX', 'EDUCA', 'VETERAN3', 'INTERNET', 'QLACTLM2', 'USEEQUIP', 'DIFFWALK', 'SMOKE100', 'SMOKDAY2', 'LASTSMK2', 'LMTJOIN3', 'INSULIN', 'CIMEMLOS', '_RFHLTH', '_HCVU651', '_RFHYPE5', '_CHOLCHK', '_RFCHOL', '_DRDXAR1', '_AGEG5YR', '_AGE_G', 'WTKG3', '_BMI5CAT', '_EDUCAG', '_INCOMG', 'MAXVO2_', '_LMTACT1', '_LMTSCL1']\n",
    "\n",
    "\n",
    "# final_selected_features = [\n",
    "# 'GENHLTH', 'BPHIGH4', 'BPMEDS', 'CVDSTRK3', 'CHCCOPD1', 'HAVARTH3', 'DIABETE3', 'EDUCA', 'VETERAN3', 'INTERNET', 'QLACTLM2', 'USEEQUIP', 'DIFFWALK', 'SMOKDAY2', 'LMTJOIN3', 'INSULIN', '_RFHLTH', '_HCVU651', '_RFHYPE5', '_RFCHOL', '_DRDXAR1', '_AGEG5YR', '_AGE_G', 'MAXVO2_', '_LMTSCL1'\n",
    "# ]\n",
    "\n",
    "# final_selected_features = [\n",
    "#     'GENHLTH', 'BPHIGH4', 'BPMEDS', 'CVDSTRK3', 'CHCCOPD1', 'DIABETE3', 'QLACTLM2', 'USEEQUIP', 'DIFFWALK', '_RFHLTH', '_HCVU651', '_RFHYPE5'\n",
    "\n",
    "# ]\n",
    "# final_selected_features= ['GENHLTH', 'PHYSHLTH', 'HLTHPLN1',\n",
    "#                            'CHECKUP1', 'BPHIGH4', 'BLOODCHO', 'CHOLCHK', 'TOLDHI2', 'CVDSTRK3', \n",
    "#                            'ASTHMA3', 'CHCOCNCR', 'CHCCOPD1', 'ADDEPEV2', 'CHCKIDNY', 'VETERAN3', 'INCOME2',\n",
    "#                              'BLIND', 'DECIDE', 'DIFFWALK', 'DIFFDRES', 'SMOKE100', 'USENOW3', 'EXERANY2', 'FLUSHOT6',\n",
    "#                                '_RFHYPE5', '_CHOLCHK', '_LTASTH1', '_BMI5CAT', '_RFBMI5', '_EDUCAG', '_INCOMG', '_RFBING5',\n",
    "#                                  '_PASTAE1', '_LMTACT1', '_LMTWRK1', '_LMTSCL1']\n",
    "\n",
    "selected_indices = [selected_features.index(feature) for feature in final_selected_features]\n",
    "x_train_final_filtered = x_train_filtered[:, selected_indices]\n",
    "print(\"Shape of the filtered x_train_filtered:\", x_train_final_filtered.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x4200 with 42 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_features = len(final_selected_features)\n",
    "features_per_row = 3\n",
    "num_rows = int(np.ceil(num_features / features_per_row))\n",
    "\n",
    "plt.figure(figsize=(10, num_rows * 3))\n",
    "\n",
    "for i, feature_name in enumerate(final_selected_features):\n",
    "    plt.subplot(num_rows, features_per_row, i + 1)\n",
    "    \n",
    "    feature_values = x_train_final_filtered[:, i]\n",
    "    \n",
    "    unique_values, counts = np.unique(feature_values, return_counts=True)\n",
    "    plt.bar(unique_values, counts, color='green', edgecolor='black')\n",
    "    plt.xlabel(f'{feature_name}')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Distribution of {feature_name}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unique_classes, counts = np.unique(y_train, return_counts=True)\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.bar(unique_classes, counts, color='green', edgecolor='black')\n",
    "plt.xticks(unique_classes)\n",
    "plt.xlabel('Class Label')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Class Imbalance in y_train')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversampling and undersampling combined\n",
    "\n",
    "def fix_class_imbalance(X, y, target_value=1, dont_balance=False):\n",
    "\n",
    "    if dont_balance:\n",
    "        return X, y\n",
    "\n",
    "    # Separate samples by class\n",
    "    class_1_indices = np.where(y == target_value)[0]\n",
    "    class_minus_1_indices = np.where(y != target_value)[0]\n",
    "\n",
    "    # Find class counts\n",
    "    class_1_count = len(class_1_indices)\n",
    "    class_minus_1_count = len(class_minus_1_indices)\n",
    "\n",
    "    if class_1_count == class_minus_1_count:\n",
    "        # If classes are already balanced, return the original data\n",
    "        return X, y\n",
    "\n",
    "    # Determine the number of samples to undersample and oversample\n",
    "    if class_1_count < class_minus_1_count:\n",
    "        minority_class_indices = class_1_indices\n",
    "        majority_class_indices = class_minus_1_indices\n",
    "    else:\n",
    "        minority_class_indices = class_minus_1_indices\n",
    "        majority_class_indices = class_1_indices\n",
    "\n",
    "    minority_class_count = len(minority_class_indices)\n",
    "    majority_class_count = len(majority_class_indices)\n",
    "\n",
    "    # Undersample the majority class to half of its original size\n",
    "    undersample_size = majority_class_count // 2\n",
    "    undersampled_indices = np.random.choice(majority_class_indices, undersample_size, replace=False)\n",
    "\n",
    "    # Oversample the minority class to match the undersampled majority class size\n",
    "    oversample_size = undersample_size - minority_class_count\n",
    "    oversampled_indices = np.random.choice(minority_class_indices, oversample_size, replace=True)\n",
    "\n",
    "    # Combine all indices to create the balanced dataset\n",
    "    new_indices = np.concatenate([undersampled_indices, minority_class_indices, oversampled_indices])\n",
    "    X_balanced = X[new_indices]\n",
    "    y_balanced = y[new_indices]\n",
    "\n",
    "    return X_balanced, y_balanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# def fix_class_imbalance(X, y):\n",
    "#     smote = SMOTE(random_state=42)\n",
    "#     X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "#     return X_balanced, y_balanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fix_class_imbalance(X, y, target_value=1, dont_balance=False):\n",
    "#     if dont_balance:\n",
    "#         return X, y\n",
    "\n",
    "#     # Separate samples by class\n",
    "#     class_1_indices = np.where(y == target_value)[0]\n",
    "#     class_minus_1_indices = np.where(y != target_value)[0]\n",
    "\n",
    "#     # Find class counts\n",
    "#     class_1_count = len(class_1_indices)\n",
    "#     class_minus_1_count = len(class_minus_1_indices)\n",
    "\n",
    "#     if class_1_count == class_minus_1_count:\n",
    "#         # If classes are already balanced, return the original data\n",
    "#         return X, y\n",
    "#     elif class_1_count < class_minus_1_count:\n",
    "#         # If class 1 is the minority, oversample class 1\n",
    "#         oversample_size = class_minus_1_count - class_1_count\n",
    "#         oversampled_indices = np.random.choice(class_1_indices, oversample_size, replace=True)\n",
    "#     else:\n",
    "#         # If class -1 is the minority, oversample class -1\n",
    "#         oversample_size = class_1_count - class_minus_1_count\n",
    "#         oversampled_indices = np.random.choice(class_minus_1_indices, oversample_size, replace=True)\n",
    "\n",
    "#     # Create the balanced dataset\n",
    "#     new_indices = np.concatenate([np.arange(len(y)), oversampled_indices])\n",
    "#     X_balanced = X[new_indices]\n",
    "#     y_balanced = y[new_indices]\n",
    "\n",
    "#     return X_balanced, y_balanced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of One-Hot Encoded Data: (328135, 233)\n",
      "[[0. 0. 1. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 1. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 1. ... 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x_train_final_filtered, final_selected_features):\n",
    "    # Initialize an empty array with shape (n_samples, 0)\n",
    "    x_train_filtered_2_OHE = np.zeros((x_train_final_filtered.shape[0], 0))\n",
    "    \n",
    "    # Iterate over the final selected features\n",
    "    for i, feature in enumerate(final_selected_features):\n",
    "        # Extract feature values for the current feature\n",
    "        feature_values = x_train_final_filtered[:, i]\n",
    "        \n",
    "        # Get unique values for this feature\n",
    "        unique_values = np.unique(feature_values)\n",
    "        \n",
    "        # One-hot encode by creating a column for each unique value\n",
    "        for value in unique_values:\n",
    "            one_hot_encoded_column = (feature_values == value).astype(int).reshape(-1, 1)\n",
    "            x_train_filtered_2_OHE = np.hstack((x_train_filtered_2_OHE, one_hot_encoded_column))\n",
    "    \n",
    "    return x_train_filtered_2_OHE\n",
    "\n",
    "# Apply the one-hot encoding to the final filtered data\n",
    "x_train_filtered_2_OHE = one_hot_encode(x_train_final_filtered, final_selected_features)\n",
    "\n",
    "# Print the shape and content of the one-hot encoded data\n",
    "print(\"Shape of One-Hot Encoded Data:\", x_train_filtered_2_OHE.shape)\n",
    "print(x_train_filtered_2_OHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(x_train, selected_features):\n",
    "    # Create an empty array to store the encoded features\n",
    "    x_train_encoded = np.zeros_like(x_train, dtype=int)\n",
    "\n",
    "    # Iterate through each selected feature (column)\n",
    "    for col in range(len(selected_features)):\n",
    "        # Get unique values and assign a unique integer for each value\n",
    "        unique_values, encoded_values = np.unique(x_train[:, col], return_inverse=True)\n",
    "        \n",
    "        # Assign the encoded values back to the training dataset for this column\n",
    "        x_train_encoded[:, col] = encoded_values\n",
    "\n",
    "    return x_train_encoded\n",
    "\n",
    "# Apply the one-hot encoding to the final filtered data\n",
    "x_train_filtered_2_OHE = label_encode(x_train_final_filtered, final_selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(239332, 233)\n"
     ]
    }
   ],
   "source": [
    "def split_data(x, y, ratio=0.8):\n",
    "    indices = np.random.permutation(x.shape[0])\n",
    "    train_indices = indices[:int(ratio * x.shape[0])]\n",
    "    test_indices = indices[int(ratio * x.shape[0]):]\n",
    "    return x[train_indices], y[train_indices], x[test_indices], y[test_indices]\n",
    "\n",
    "y_train_mapped = (1 + y_train) / 2\n",
    "\n",
    "x_train_filtered_2_OHE_train, y_train_train, x_train_filtered_2_OHE_test, y_train_test = split_data(x_train_filtered_2_OHE, y_train_mapped)\n",
    "\n",
    "# Fix class imbalance in the training data\n",
    "x_train_filtered_2_OHE_train_fixed, y_train_train_fixed = fix_class_imbalance(x_train_filtered_2_OHE_train, y_train_train)\n",
    "\n",
    "print(x_train_filtered_2_OHE_train_fixed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularized Logistic Regression(0/299): loss=0.693147180559945, w0=-4.178296257917872e-07, w1=-0.007241405244597463\n",
      "Regularized Logistic Regression(1/299): loss=0.650541341187849, w0=-8.157729900703069e-07, w1=-0.013248944924525743\n",
      "Regularized Logistic Regression(2/299): loss=0.6198589301326739, w0=-1.210582526350341e-06, w1=-0.01864391567243133\n",
      "Regularized Logistic Regression(3/299): loss=0.5970888099869539, w0=-1.6010619897759356e-06, w1=-0.023512523608902006\n",
      "Regularized Logistic Regression(4/299): loss=0.579883151737486, w0=-1.9870939484246363e-06, w1=-0.027947592320783525\n",
      "Regularized Logistic Regression(5/299): loss=0.5666305376140176, w0=-2.3686186839111963e-06, w1=-0.032022883382168695\n",
      "Regularized Logistic Regression(6/299): loss=0.5562276696344409, w0=-2.7456438977438863e-06, w1=-0.0357972631556104\n",
      "Regularized Logistic Regression(7/299): loss=0.5479135568052571, w0=-3.1182236444265733e-06, w1=-0.03931773932711733\n",
      "Regularized Logistic Regression(8/299): loss=0.5411565785619309, w0=-3.4864447119347587e-06, w1=-0.042622060962115384\n",
      "Regularized Logistic Regression(9/299): loss=0.5355797104427276, w0=-3.850416955524884e-06, w1=-0.045740780176149125\n",
      "Regularized Logistic Regression(10/299): loss=0.5309113245950309, w0=-4.210266236505428e-06, w1=-0.04869884078085592\n",
      "Regularized Logistic Regression(11/299): loss=0.5269526600400702, w0=-4.566129160554173e-06, w1=-0.05151678955269242\n",
      "Regularized Logistic Regression(12/299): loss=0.5235560886455461, w0=-4.918149107981405e-06, w1=-0.05421169861013763\n",
      "Regularized Logistic Regression(13/299): loss=0.5206104023550807, w0=-5.266473218097664e-06, w1=-0.05679787042964845\n",
      "Regularized Logistic Regression(14/299): loss=0.5180307112928464, w0=-5.611250092523878e-06, w1=-0.059287380188952396\n",
      "Regularized Logistic Regression(15/299): loss=0.5157514074959796, w0=-5.952628048014511e-06, w1=-0.06169049623480928\n",
      "Regularized Logistic Regression(16/299): loss=0.5137211946260828, w0=-6.2907537935461105e-06, w1=-0.06401600881756572\n",
      "Regularized Logistic Regression(17/299): loss=0.5118995293586089, w0=-6.625771437304076e-06, w1=-0.06627148932435752\n",
      "Regularized Logistic Regression(18/299): loss=0.5102540406247397, w0=-6.957821751447487e-06, w1=-0.06846349645265527\n",
      "Regularized Logistic Regression(19/299): loss=0.5087586352096297, w0=-7.287041638933123e-06, w1=-0.0705977415472754\n",
      "Regularized Logistic Regression(20/299): loss=0.5073920912337502, w0=-7.613563758987528e-06, w1=-0.07267922224829967\n",
      "Regularized Logistic Regression(21/299): loss=0.5061370026254516, w0=-7.937516277179882e-06, w1=-0.07471233134642453\n",
      "Regularized Logistic Regression(22/299): loss=0.5049789789862648, w0=-8.259022713250919e-06, w1=-0.07670094608570158\n",
      "Regularized Logistic Regression(23/299): loss=0.5039060332902872, w0=-8.578201865442661e-06, w1=-0.0786485019263884\n",
      "Regularized Logistic Regression(24/299): loss=0.5029081091324887, w0=-8.895167794443464e-06, w1=-0.08055805386488937\n",
      "Regularized Logistic Regression(25/299): loss=0.5019767126445605, w0=-9.21002985350031e-06, w1=-0.08243232771935771\n",
      "Regularized Logistic Regression(26/299): loss=0.5011046236233788, w0=-9.522892753968497e-06, w1=-0.0842737632681215\n",
      "Regularized Logistic Regression(27/299): loss=0.5002856671174818, w0=-9.833856657728231e-06, w1=-0.0860845507302241\n",
      "Regularized Logistic Regression(28/299): loss=0.4995145315279227, w0=-1.0143017289619619e-05, w1=-0.08786666177154102\n",
      "Regularized Logistic Regression(29/299): loss=0.4987866227676288, w0=-1.0450466064425275e-05, w1=-0.08962187598317367\n",
      "Regularized Logistic Regression(30/299): loss=0.4980979465749691, w0=-1.0756290224035292e-05, w1=-0.091351803594262\n",
      "Regularized Logistic Regression(31/299): loss=0.4974450129601572, w0=-1.106057298131837e-05, w1=-0.09305790503653226\n",
      "Regularized Logistic Regression(32/299): loss=0.4968247581640117, w0=-1.1363393667939321e-05, w1=-0.09474150786351147\n",
      "Regularized Logistic Regression(33/299): loss=0.4962344805590123, w0=-1.1664827883941267e-05, w1=-0.09640382143644127\n",
      "Regularized Logistic Regression(34/299): loss=0.4956717877160553, w0=-1.1964947647377906e-05, w1=-0.098045949716258\n",
      "Regularized Logistic Regression(35/299): loss=0.4951345524638846, w0=-1.2263821542658617e-05, w1=-0.09966890244258036\n",
      "Regularized Logistic Regression(36/299): loss=0.4946208762303824, w0=-1.2561514866574438e-05, w1=-0.10127360493341368\n",
      "Regularized Logistic Regression(37/299): loss=0.4941290583111229, w0=-1.285808977121943e-05, w1=-0.10286090670089261\n",
      "Regularized Logistic Regression(38/299): loss=0.4936575699867715, w0=-1.3153605403221176e-05, w1=-0.10443158904702997\n",
      "Regularized Logistic Regression(39/299): loss=0.4932050326262971, w0=-1.3448118038854533e-05, w1=-0.10598637177770066\n",
      "Regularized Logistic Regression(40/299): loss=0.4927701990818624, w0=-1.374168121474207e-05, w1=-0.10752591915186675\n",
      "Regularized Logistic Regression(41/299): loss=0.49235193781439746, w0=-1.4034345853948e-05, w1=-0.10905084516546598\n",
      "Regularized Logistic Regression(42/299): loss=0.491949219294353, w0=-1.4326160387355288e-05, w1=-0.11056171825475995\n",
      "Regularized Logistic Regression(43/299): loss=0.49156110430612815, w0=-1.4617170870281306e-05, w1=-0.11205906549171724\n",
      "Regularized Logistic Regression(44/299): loss=0.4911867338518443, w0=-1.4907421094339326e-05, w1=-0.1135433763337574\n",
      "Regularized Logistic Regression(45/299): loss=0.49082532040413057, w0=-1.5196952694593784e-05, w1=-0.11501510598155185\n",
      "Regularized Logistic Regression(46/299): loss=0.49047614030115233, w0=-1.5485805252088647e-05, w1=-0.11647467839128472\n",
      "Regularized Logistic Regression(47/299): loss=0.4901385271124195, w0=-1.5774016391852223e-05, w1=-0.11792248898159216\n",
      "Regularized Logistic Regression(48/299): loss=0.48981186583265135, w0=-1.606162187649956e-05, w1=-0.11935890707013441\n",
      "Regularized Logistic Regression(49/299): loss=0.48949558778443486, w0=-1.634865569556665e-05, w1=-0.12078427807026376\n",
      "Regularized Logistic Regression(50/299): loss=0.4891891661296709, w0=-1.6635150150719528e-05, w1=-0.1221989254744014\n",
      "Regularized Logistic Regression(51/299): loss=0.488892111905642, w0=-1.6921135936987164e-05, w1=-0.12360315264743117\n",
      "Regularized Logistic Regression(52/299): loss=0.48860397051462295, w0=-1.7206642220170315e-05, w1=-0.12499724445057205\n",
      "Regularized Logistic Regression(53/299): loss=0.48832431860680653, w0=-1.749169671057956e-05, w1=-0.12638146871372893\n",
      "Regularized Logistic Regression(54/299): loss=0.4880527613053435, w0=-1.7776325733255365e-05, w1=-0.12775607757219187\n",
      "Regularized Logistic Regression(55/299): loss=0.48778892972982457, w0=-1.8060554294821217e-05, w1=-0.12912130868170402\n",
      "Regularized Logistic Regression(56/299): loss=0.48753247878085054, w0=-1.834440614711826e-05, w1=-0.13047738632430597\n",
      "Regularized Logistic Regression(57/299): loss=0.48728308515362256, w0=-1.8627903847766247e-05, w1=-0.13182452241596052\n",
      "Regularized Logistic Regression(58/299): loss=0.48704044555296516, w0=-1.8911068817791766e-05, w1=-0.13316291742573175\n",
      "Regularized Logistic Regression(59/299): loss=0.48680427508595764, w0=-1.919392139646012e-05, w1=-0.1344927612152156\n",
      "Regularized Logistic Regression(60/299): loss=0.4865743058115628, w0=-1.9476480893442663e-05, w1=-0.1358142338059723\n",
      "Regularized Logistic Regression(61/299): loss=0.486350285429349, w0=-1.9758765638446414e-05, w1=-0.13712750608187896\n",
      "Regularized Logistic Regression(62/299): loss=0.48613197609174075, w0=-2.004079302842788e-05, w1=-0.13843274043258763\n",
      "Regularized Logistic Regression(63/299): loss=0.485919153326203, w0=-2.0322579572507953e-05, w1=-0.13973009134362369\n",
      "Regularized Logistic Regression(64/299): loss=0.4857116050554748, w0=-2.0604140934699784e-05, w1=-0.14101970593808727\n",
      "Regularized Logistic Regression(65/299): loss=0.485509130705431, w0=-2.088549197455666e-05, w1=-0.14230172447441258\n",
      "Regularized Logistic Regression(66/299): loss=0.48531154039141416, w0=-2.1166646785842028e-05, w1=-0.14357628080418666\n",
      "Regularized Logistic Regression(67/299): loss=0.4851186541749634, w0=-2.1447618733319113e-05, w1=-0.14484350279363042\n",
      "Regularized Logistic Regression(68/299): loss=0.48493030138382287, w0=-2.1728420487753092e-05, w1=-0.14610351271198502\n",
      "Regularized Logistic Regression(69/299): loss=0.4847463199889226, w0=-2.2009064059214217e-05, w1=-0.14735642758973086\n",
      "Regularized Logistic Regression(70/299): loss=0.48456655603274984, w0=-2.2289560828766158e-05, w1=-0.14860235954927892\n",
      "Regularized Logistic Regression(71/299): loss=0.48439086310414786, w0=-2.2569921578619637e-05, w1=-0.14984141611052298\n",
      "Regularized Logistic Regression(72/299): loss=0.484219101855119, w0=-2.285015652082757e-05, w1=-0.15107370047341195\n",
      "Regularized Logistic Regression(73/299): loss=0.4840511395557049, w0=-2.3130275324594063e-05, w1=-0.1522993117794973\n",
      "Regularized Logistic Regression(74/299): loss=0.4838868496834186, w0=-2.3410287142266132e-05, w1=-0.15351834535422873\n",
      "Regularized Logistic Regression(75/299): loss=0.48372611154408157, w0=-2.369020063407342e-05, w1=-0.15473089293160647\n",
      "Regularized Logistic Regression(76/299): loss=0.4835688099212489, w0=-2.397002399167805e-05, w1=-0.15593704286264973\n",
      "Regularized Logistic Regression(77/299): loss=0.4834148347516857, w0=-2.4249764960593517e-05, w1=-0.1571368803090093\n",
      "Regularized Logistic Regression(78/299): loss=0.4832640808246205, w0=-2.452943086152861e-05, w1=-0.15833048742293127\n",
      "Regularized Logistic Regression(79/299): loss=0.48311644750272364, w0=-2.4809028610709436e-05, w1=-0.15951794351467116\n",
      "Regularized Logistic Regression(80/299): loss=0.48297183846296193, w0=-2.508856473923006e-05, w1=-0.16069932520835986\n",
      "Regularized Logistic Regression(81/299): loss=0.4828301614556612, w0=-2.536804541147957e-05, w1=-0.16187470658723468\n",
      "Regularized Logistic Regression(82/299): loss=0.48269132808026366, w0=-2.5647476442691032e-05, w1=-0.16304415932906902\n",
      "Regularized Logistic Regression(83/299): loss=0.4825552535764163, w0=-2.5926863315655487e-05, w1=-0.16420775283256198\n",
      "Regularized Logistic Regression(84/299): loss=0.4824218566291475, w0=-2.6206211196641924e-05, w1=-0.16536555433538325\n",
      "Regularized Logistic Regression(85/299): loss=0.4822910591870092, w0=-2.6485524950562076e-05, w1=-0.1665176290245102\n",
      "Regularized Logistic Regression(86/299): loss=0.4821627862921588, w0=-2.676480915541698e-05, w1=-0.1676640401394393\n",
      "Regularized Logistic Regression(87/299): loss=0.4820369659214534, w0=-2.70440681160603e-05, w1=-0.1688048490688058\n",
      "Regularized Logistic Regression(88/299): loss=0.48191352883770583, w0=-2.7323305877311658e-05, w1=-0.16994011544090035\n",
      "Regularized Logistic Regression(89/299): loss=0.4817924084503311, w0=-2.7602526236451602e-05, w1=-0.17106989720853208\n",
      "Regularized Logistic Regression(90/299): loss=0.48167354068467605, w0=-2.788173275512814e-05, w1=-0.17219425072864947\n",
      "Regularized Logistic Regression(91/299): loss=0.4815568638593863, w0=-2.816092877070334e-05, w1=-0.1733132308370976\n",
      "Regularized Logistic Regression(92/299): loss=0.48144231857122477, w0=-2.844011740706702e-05, w1=-0.17442689091885963\n",
      "Regularized Logistic Regression(93/299): loss=0.48132984758679376, w0=-2.871930158494326e-05, w1=-0.1755352829741027\n",
      "Regularized Logistic Regression(94/299): loss=0.4812193957406721, w0=-2.8998484031714045e-05, w1=-0.17663845768032263\n",
      "Regularized Logistic Regression(95/299): loss=0.4811109098395082, w0=-2.9277667290783323e-05, w1=-0.1777364644508587\n",
      "Regularized Logistic Regression(96/299): loss=0.48100433857165426, w0=-2.955685373050341e-05, w1=-0.1788293514900288\n",
      "Regularized Logistic Regression(97/299): loss=0.48089963242195405, w0=-2.9836045552684716e-05, w1=-0.179917165845116\n",
      "Regularized Logistic Regression(98/299): loss=0.4807967435913367, w0=-3.0115244800708688e-05, w1=-0.1809999534554184\n",
      "Regularized Logistic Regression(99/299): loss=0.480695625920884, w0=-3.0394453367262803e-05, w1=-0.18207775919856017\n",
      "Regularized Logistic Regression(100/299): loss=0.48059623482007924, w0=-3.067367300171565e-05, w1=-0.18315062693424475\n",
      "Regularized Logistic Regression(101/299): loss=0.48049852719895303, w0=-3.095290531714914e-05, w1=-0.18421859954561853\n",
      "Regularized Logistic Regression(102/299): loss=0.4804024614038771, w0=-3.123215179706409e-05, w1=-0.1852817189784008\n",
      "Regularized Logistic Regression(103/299): loss=0.4803079971567676, w0=-3.151141380177461e-05, w1=-0.18634002627792332\n",
      "Regularized Logistic Regression(104/299): loss=0.48021509549747765, w0=-3.1790692574505984e-05, w1=-0.18739356162421417\n",
      "Regularized Logistic Regression(105/299): loss=0.4801237187291792, w0=-3.206998924721003e-05, w1=-0.18844236436524828\n",
      "Regularized Logistic Regression(106/299): loss=0.48003383036655023, w0=-3.2349304846111135e-05, w1=-0.18948647304848049\n",
      "Regularized Logistic Regression(107/299): loss=0.47994539508658474, w0=-3.262864029699573e-05, w1=-0.19052592545076724\n",
      "Regularized Logistic Regression(108/299): loss=0.479858378681875, w0=-3.290799643025712e-05, w1=-0.19156075860677607\n",
      "Regularized Logistic Regression(109/299): loss=0.47977274801620756, w0=-3.318737398570714e-05, w1=-0.19259100883597505\n",
      "Regularized Logistic Regression(110/299): loss=0.47968847098234285, w0=-3.346677361716553e-05, w1=-0.19361671176828757\n",
      "Regularized Logistic Regression(111/299): loss=0.47960551646183913, w0=-3.374619589683736e-05, w1=-0.19463790236849227\n",
      "Regularized Logistic Regression(112/299): loss=0.4795238542868115, w0=-3.4025641319488434e-05, w1=-0.19565461495944253\n",
      "Regularized Logistic Regression(113/299): loss=0.4794434552035033, w0=-3.430511030642795e-05, w1=-0.19666688324417406\n",
      "Regularized Logistic Regression(114/299): loss=0.4793642908375761, w0=-3.4584603209307434e-05, w1=-0.19767474032696639\n",
      "Regularized Logistic Regression(115/299): loss=0.4792863336610108, w0=-3.486412031374442e-05, w1=-0.19867821873341643\n",
      "Regularized Logistic Regression(116/299): loss=0.4792095569605369, w0=-3.5143661842779e-05, w1=-0.19967735042958243\n",
      "Regularized Logistic Regression(117/299): loss=0.4791339348075019, w0=-3.5423227960170945e-05, w1=-0.20067216684024922\n",
      "Regularized Logistic Regression(118/299): loss=0.4790594420291001, w0=-3.570281877354471e-05, w1=-0.2016626988663644\n",
      "Regularized Logistic Regression(119/299): loss=0.4789860541808912, w0=-3.598243433738937e-05, w1=-0.20264897690169156\n",
      "Regularized Logistic Regression(120/299): loss=0.47891374752053334, w0=-3.62620746559202e-05, w1=-0.20363103084872314\n",
      "Regularized Logistic Regression(121/299): loss=0.4788424989826714, w0=-3.654173968580804e-05, w1=-0.20460889013389313\n",
      "Regularized Logistic Regression(122/299): loss=0.47877228615491585, w0=-3.6821429338782834e-05, w1=-0.20558258372212726\n",
      "Regularized Logistic Regression(123/299): loss=0.47870308725485666, w0=-3.710114348411674e-05, w1=-0.20655214013076645\n",
      "Regularized Logistic Regression(124/299): loss=0.4786348811080581, w0=-3.7380881950992636e-05, w1=-0.20751758744289603\n",
      "Regularized Logistic Regression(125/299): loss=0.47856764712698446, w0=-3.7660644530763e-05, w1=-0.2084789533201119\n",
      "Regularized Logistic Regression(126/299): loss=0.47850136529081033, w0=-3.7940430979104326e-05, w1=-0.20943626501475354\n",
      "Regularized Logistic Regression(127/299): loss=0.47843601612606795, w0=-3.822024101807177e-05, w1=-0.21038954938163026\n",
      "Regularized Logistic Regression(128/299): loss=0.4783715806880948, w0=-3.850007433805853e-05, w1=-0.21133883288926705\n",
      "Regularized Logistic Regression(129/299): loss=0.47830804054323667, w0=-3.877993059966439e-05, w1=-0.2122841416306944\n",
      "Regularized Logistic Regression(130/299): loss=0.47824537775177367, w0=-3.9059809435477405e-05, w1=-0.2132255013338044\n",
      "Regularized Logistic Regression(131/299): loss=0.4781835748515315, w0=-3.933971045177282e-05, w1=-0.2141629373712955\n",
      "Regularized Logistic Regression(132/299): loss=0.4781226148421466, w0=-3.961963323013284e-05, w1=-0.21509647477022467\n",
      "Regularized Logistic Regression(133/299): loss=0.47806248116995226, w0=-3.989957732899088e-05, w1=-0.21602613822118805\n",
      "Regularized Logistic Regression(134/299): loss=0.47800315771346, w0=-4.017954228510377e-05, w1=-0.21695195208714602\n",
      "Regularized Logistic Regression(135/299): loss=0.4779446287694043, w0=-4.045952761495502e-05, w1=-0.2178739404119116\n",
      "Regularized Logistic Regression(136/299): loss=0.47788687903932675, w0=-4.073953281609244e-05, w1=-0.2187921269283165\n",
      "Regularized Logistic Regression(137/299): loss=0.4778298936166759, w0=-4.101955736840287e-05, w1=-0.21970653506607118\n",
      "Regularized Logistic Regression(138/299): loss=0.4777736579743962, w0=-4.1299600735327035e-05, w1=-0.22061718795933263\n",
      "Regularized Logistic Regression(139/299): loss=0.47771815795298755, w0=-4.157966236501711e-05, w1=-0.2215241084539938\n",
      "Regularized Logistic Regression(140/299): loss=0.4776633797490127, w0=-4.1859741691439595e-05, w1=-0.2224273191147068\n",
      "Regularized Logistic Regression(141/299): loss=0.47760930990403083, w0=-4.213983813542595e-05, w1=-0.22332684223165292\n",
      "Regularized Logistic Regression(142/299): loss=0.4775559352939435, w0=-4.2419951105673374e-05, w1=-0.22422269982706997\n",
      "Regularized Logistic Regression(143/299): loss=0.4775032431187292, w0=-4.270007999969792e-05, w1=-0.2251149136615486\n",
      "Regularized Logistic Regression(144/299): loss=0.4774512208925537, w0=-4.2980224204742085e-05, w1=-0.22600350524010726\n",
      "Regularized Logistic Regression(145/299): loss=0.4773998564342397, w0=-4.326038309863895e-05, w1=-0.22688849581805606\n",
      "Regularized Logistic Regression(146/299): loss=0.47734913785807775, w0=-4.354055605063477e-05, w1=-0.2277699064066584\n",
      "Regularized Logistic Regression(147/299): loss=0.4772990535649675, w0=-4.3820742422171914e-05, w1=-0.22864775777859922\n",
      "Regularized Logistic Regression(148/299): loss=0.47724959223387214, w0=-4.41009415676339e-05, w1=-0.2295220704732685\n",
      "Regularized Logistic Regression(149/299): loss=0.4772007428135772, w0=-4.438115283505424e-05, w1=-0.23039286480186752\n",
      "Regularized Logistic Regression(150/299): loss=0.4771524945147356, w0=-4.466137556679069e-05, w1=-0.23126016085234533\n",
      "Regularized Logistic Regression(151/299): loss=0.477104836802193, w0=-4.4941609100166506e-05, w1=-0.23212397849417352\n",
      "Regularized Logistic Regression(152/299): loss=0.4770577593875766, w0=-4.522185276808005e-05, w1=-0.2329843373829643\n",
      "Regularized Logistic Regression(153/299): loss=0.47701125222214186, w0=-4.5502105899584355e-05, w1=-0.23384125696494026\n",
      "Regularized Logistic Regression(154/299): loss=0.4769653054898626, w0=-4.578236782043782e-05, w1=-0.2346947564812605\n",
      "Regularized Logistic Regression(155/299): loss=0.47691990960075714, w0=-4.606263785362741e-05, w1=-0.23554485497221012\n",
      "Regularized Logistic Regression(156/299): loss=0.4768750551844401, w0=-4.634291531986554e-05, w1=-0.2363915712812574\n",
      "Regularized Logistic Regression(157/299): loss=0.4768307330838925, w0=-4.662319953806191e-05, w1=-0.23723492405898514\n",
      "Regularized Logistic Regression(158/299): loss=0.47678693434943814, w0=-4.6903489825771274e-05, w1=-0.23807493176690017\n",
      "Regularized Logistic Regression(159/299): loss=0.4767436502329235, w0=-4.7183785499618324e-05, w1=-0.23891161268112712\n",
      "Regularized Logistic Regression(160/299): loss=0.47670087218208756, w0=-4.746408587570068e-05, w1=-0.2397449848959895\n",
      "Regularized Logistic Regression(161/299): loss=0.47665859183511927, w0=-4.774439026997097e-05, w1=-0.2405750663274836\n",
      "Regularized Logistic Regression(162/299): loss=0.4766168010153925, w0=-4.8024697998598895e-05, w1=-0.2414018747166492\n",
      "Regularized Logistic Regression(163/299): loss=0.47657549172637176, w0=-4.830500837831423e-05, w1=-0.24222542763284033\n",
      "Regularized Logistic Regression(164/299): loss=0.4765346561466845, w0=-4.858532072673159e-05, w1=-0.24304574247690108\n",
      "Regularized Logistic Regression(165/299): loss=0.47649428662535165, w0=-4.886563436265775e-05, w1=-0.243862836484249\n",
      "Regularized Logistic Regression(166/299): loss=0.4764543756771704, w0=-4.914594860638239e-05, w1=-0.2446767267278702\n",
      "Regularized Logistic Regression(167/299): loss=0.4764149159782471, w0=-4.942626277995286e-05, w1=-0.24548743012122923\n",
      "Regularized Logistic Regression(168/299): loss=0.476375900361668, w0=-4.970657620743384e-05, w1=-0.24629496342109677\n",
      "Regularized Logistic Regression(169/299): loss=0.4763373218133095, w0=-4.9986888215152475e-05, w1=-0.24709934323029892\n",
      "Regularized Logistic Regression(170/299): loss=0.4762991734677814, w0=-5.0267198131929644e-05, w1=-0.24790058600038947\n",
      "Regularized Logistic Regression(171/299): loss=0.4762614486044925, w0=-5.0547505289298035e-05, w1=-0.24869870803424973\n",
      "Regularized Logistic Regression(172/299): loss=0.47622414064384416, w0=-5.082780902170758e-05, w1=-0.249493725488617\n",
      "Regularized Logistic Regression(173/299): loss=0.47618724314353755, w0=-5.110810866671883e-05, w1=-0.2502856543765455\n",
      "Regularized Logistic Regression(174/299): loss=0.4761507497949972, w0=-5.1388403565184834e-05, w1=-0.25107451056980085\n",
      "Regularized Logistic Regression(175/299): loss=0.476114654419904, w0=-5.166869306142198e-05, w1=-0.251860309801192\n",
      "Regularized Logistic Regression(176/299): loss=0.4760789509668335, w0=-5.19489765033704e-05, w1=-0.2526430676668415\n",
      "Regularized Logistic Regression(177/299): loss=0.4760436335079969, w0=-5.22292532427443e-05, w1=-0.25342279962839664\n",
      "Regularized Logistic Regression(178/299): loss=0.47600869623608044, w0=-5.250952263517276e-05, w1=-0.2541995210151838\n",
      "Regularized Logistic Regression(179/299): loss=0.4759741334611809, w0=-5.2789784040331396e-05, w1=-0.2549732470263079\n",
      "Regularized Logistic Regression(180/299): loss=0.4759399396078323, w0=-5.307003682206532e-05, w1=-0.25574399273269854\n",
      "Regularized Logistic Regression(181/299): loss=0.4759061092121214, w0=-5.335028034850375e-05, w1=-0.2565117730791048\n",
      "Regularized Logistic Regression(182/299): loss=0.475872636918889, w0=-5.3630513992166727e-05, w1=-0.2572766028860401\n",
      "Regularized Logistic Regression(183/299): loss=0.4758395174790147, w0=-5.391073713006423e-05, w1=-0.25803849685168007\n",
      "Regularized Logistic Regression(184/299): loss=0.47580674574678167, w0=-5.419094914378812e-05, w1=-0.25879746955371297\n",
      "Regularized Logistic Regression(185/299): loss=0.4757743166773172, w0=-5.447114941959712e-05, w1=-0.2595535354511458\n",
      "Regularized Logistic Regression(186/299): loss=0.47574222532411037, w0=-5.4751337348495274e-05, w1=-0.26030670888606705\n",
      "Regularized Logistic Regression(187/299): loss=0.4757104668366006, w0=-5.5031512326304165e-05, w1=-0.2610570040853675\n",
      "Regularized Logistic Regression(188/299): loss=0.47567903645783566, w0=-5.531167375372909e-05, w1=-0.26180443516241997\n",
      "Regularized Logistic Regression(189/299): loss=0.4756479295221991, w0=-5.5591821036419614e-05, w1=-0.26254901611872067\n",
      "Regularized Logistic Regression(190/299): loss=0.4756171414532008, w0=-5.5871953585024645e-05, w1=-0.2632907608454915\n",
      "Regularized Logistic Regression(191/299): loss=0.4755866677613316, w0=-5.615207081524235e-05, w1=-0.26402968312524633\n",
      "Regularized Logistic Regression(192/299): loss=0.4755565040419806, w0=-5.643217214786516e-05, w1=-0.26476579663332084\n",
      "Regularized Logistic Regression(193/299): loss=0.475526645973408, w0=-5.671225700882007e-05, w1=-0.26549911493936845\n",
      "Regularized Logistic Regression(194/299): loss=0.47549708931477946, w0=-5.699232482920446e-05, w1=-0.26622965150882255\n",
      "Regularized Logistic Regression(195/299): loss=0.4754678299042522, w0=-5.7272375045317675e-05, w1=-0.26695741970432596\n",
      "Regularized Logistic Regression(196/299): loss=0.47543886365711635, w0=-5.7552407098688534e-05, w1=-0.2676824327871291\n",
      "Regularized Logistic Regression(197/299): loss=0.4754101865639891, w0=-5.7832420436099006e-05, w1=-0.26840470391845783\n",
      "Regularized Logistic Regression(198/299): loss=0.4753817946890563, w0=-5.811241450960418e-05, w1=-0.2691242461608514\n",
      "Regularized Logistic Regression(199/299): loss=0.47535368416836565, w0=-5.8392388776548795e-05, w1=-0.2698410724794717\n",
      "Regularized Logistic Regression(200/299): loss=0.4753258512081655, w0=-5.86723426995804e-05, w1=-0.27055519574338477\n",
      "Regularized Logistic Regression(201/299): loss=0.47529829208328883, w0=-5.895227574665938e-05, w1=-0.2712666287268152\n",
      "Regularized Logistic Regression(202/299): loss=0.4752710031355828, w0=-5.9232187391066e-05, w1=-0.2719753841103741\n",
      "Regularized Logistic Regression(203/299): loss=0.47524398077238017, w0=-5.951207711140457e-05, w1=-0.272681474482262\n",
      "Regularized Logistic Regression(204/299): loss=0.4752172214650118, w0=-5.979194439160492e-05, w1=-0.27338491233944634\n",
      "Regularized Logistic Regression(205/299): loss=0.4751907217473607, w0=-6.007178872092129e-05, w1=-0.2740857100888156\n",
      "Regularized Logistic Regression(206/299): loss=0.47516447821445396, w0=-6.035160959392883e-05, w1=-0.2747838800483095\n",
      "Regularized Logistic Regression(207/299): loss=0.47513848752109283, w0=-6.06314065105177e-05, w1=-0.27547943444802675\n",
      "Regularized Logistic Regression(208/299): loss=0.47511274638051887, w0=-6.0911178975885104e-05, w1=-0.2761723854313108\n",
      "Regularized Logistic Regression(209/299): loss=0.4750872515631165, w0=-6.11909265005251e-05, w1=-0.2768627450558137\n",
      "Regularized Logistic Regression(210/299): loss=0.4750619998951495, w0=-6.147064860021659e-05, w1=-0.27755052529453983\n",
      "Regularized Logistic Regression(211/299): loss=0.4750369882575302, w0=-6.175034479600931e-05, w1=-0.2782357380368685\n",
      "Regularized Logistic Regression(212/299): loss=0.4750122135846224, w0=-6.203001461420816e-05, w1=-0.2789183950895574\n",
      "Regularized Logistic Regression(213/299): loss=0.47498767286307464, w0=-6.230965758635583e-05, w1=-0.2795985081777265\n",
      "Regularized Logistic Regression(214/299): loss=0.4749633631306834, w0=-6.258927324921384e-05, w1=-0.2802760889458236\n",
      "Regularized Logistic Regression(215/299): loss=0.4749392814752882, w0=-6.286886114474214e-05, w1=-0.2809511489585713\n",
      "Regularized Logistic Regression(216/299): loss=0.4749154250336926, w0=-6.314842082007724e-05, w1=-0.2816236997018965\n",
      "Regularized Logistic Regression(217/299): loss=0.4748917909906141, w0=-6.342795182750908e-05, w1=-0.2822937525838424\n",
      "Regularized Logistic Regression(218/299): loss=0.47486837657766223, w0=-6.370745372445661e-05, w1=-0.2829613189354641\n",
      "Regularized Logistic Regression(219/299): loss=0.47484517907234075, w0=-6.398692607344226e-05, w1=-0.2836264100117076\n",
      "Regularized Logistic Regression(220/299): loss=0.4748221957970764, w0=-6.426636844206521e-05, w1=-0.2842890369922724\n",
      "Regularized Logistic Regression(221/299): loss=0.4747994241182722, w0=-6.454578040297374e-05, w1=-0.28494921098245984\n",
      "Regularized Logistic Regression(222/299): loss=0.47477686144538467, w0=-6.482516153383652e-05, w1=-0.28560694301400463\n",
      "Regularized Logistic Regression(223/299): loss=0.4747545052300248, w0=-6.510451141731306e-05, w1=-0.28626224404589307\n",
      "Regularized Logistic Regression(224/299): loss=0.4747323529650806, w0=-6.53838296410232e-05, w1=-0.28691512496516586\n",
      "Regularized Logistic Regression(225/299): loss=0.47471040218386185, w0=-6.5663115797516e-05, w1=-0.2875655965877075\n",
      "Regularized Logistic Regression(226/299): loss=0.47468865045926756, w0=-6.594236948423767e-05, w1=-0.2882136696590216\n",
      "Regularized Logistic Regression(227/299): loss=0.474667095402972, w0=-6.622159030349894e-05, w1=-0.28885935485499337\n",
      "Regularized Logistic Regression(228/299): loss=0.47464573466463206, w0=-6.650077786244182e-05, w1=-0.28950266278263814\n",
      "Regularized Logistic Regression(229/299): loss=0.4746245659311148, w0=-6.677993177300563e-05, w1=-0.2901436039808382\n",
      "Regularized Logistic Regression(230/299): loss=0.47460358692574217, w0=-6.705905165189258e-05, w1=-0.29078218892106655\n",
      "Regularized Logistic Regression(231/299): loss=0.47458279540755527, w0=-6.733813712053286e-05, w1=-0.29141842800809875\n",
      "Regularized Logistic Regression(232/299): loss=0.4745621891705976, w0=-6.761718780504917e-05, w1=-0.292052331580713\n",
      "Regularized Logistic Regression(233/299): loss=0.47454176604321263, w0=-6.789620333622092e-05, w1=-0.29268390991237847\n",
      "Regularized Logistic Regression(234/299): loss=0.4745215238873625, w0=-6.817518334944799e-05, w1=-0.2933131732119326\n",
      "Regularized Logistic Regression(235/299): loss=0.4745014605979593, w0=-6.845412748471412e-05, w1=-0.2939401316242472\n",
      "Regularized Logistic Regression(236/299): loss=0.4744815741022147, w0=-6.873303538655002e-05, w1=-0.2945647952308837\n",
      "Regularized Logistic Regression(237/299): loss=0.4744618623590049, w0=-6.901190670399619e-05, w1=-0.29518717405073813\n",
      "Regularized Logistic Regression(238/299): loss=0.47444232335824926, w0=-6.929074109056543e-05, w1=-0.2958072780406755\n",
      "Regularized Logistic Regression(239/299): loss=0.47442295512030613, w0=-6.956953820420514e-05, w1=-0.2964251170961546\n",
      "Regularized Logistic Regression(240/299): loss=0.4744037556953807, w0=-6.98482977072595e-05, w1=-0.2970407010518424\n",
      "Regularized Logistic Regression(241/299): loss=0.4743847231629474, w0=-7.012701926643133e-05, w1=-0.297654039682219\n",
      "Regularized Logistic Regression(242/299): loss=0.4743658556311873, w0=-7.040570255274394e-05, w1=-0.29826514270217364\n",
      "Regularized Logistic Regression(243/299): loss=0.4743471512364369, w0=-7.068434724150274e-05, w1=-0.2988740197675908\n",
      "Regularized Logistic Regression(244/299): loss=0.4743286081426502, w0=-7.096295301225684e-05, w1=-0.299480680475928\n",
      "Regularized Logistic Regression(245/299): loss=0.4743102245408741, w0=-7.124151954876052e-05, w1=-0.3000851343667841\n",
      "Regularized Logistic Regression(246/299): loss=0.4742919986487345, w0=-7.152004653893462e-05, w1=-0.3006873909224595\n",
      "Regularized Logistic Regression(247/299): loss=0.4742739287099363, w0=-7.179853367482794e-05, w1=-0.3012874595685078\n",
      "Regularized Logistic Regression(248/299): loss=0.4742560129937721, w0=-7.207698065257861e-05, w1=-0.30188534967427894\n",
      "Regularized Logistic Regression(249/299): loss=0.474238249794645, w0=-7.235538717237543e-05, w1=-0.3024810705534548\n",
      "Regularized Logistic Regression(250/299): loss=0.47422063743160076, w0=-7.263375293841922e-05, w1=-0.30307463146457636\n",
      "Regularized Logistic Regression(251/299): loss=0.4742031742478698, w0=-7.291207765888426e-05, w1=-0.30366604161156324\n",
      "Regularized Logistic Regression(252/299): loss=0.4741858586104221, w0=-7.31903610458797e-05, w1=-0.3042553101442261\n",
      "Regularized Logistic Regression(253/299): loss=0.4741686889095291, w0=-7.346860281541107e-05, w1=-0.30484244615877076\n",
      "Regularized Logistic Regression(254/299): loss=0.474151663558339, w0=-7.374680268734186e-05, w1=-0.30542745869829596\n",
      "Regularized Logistic Regression(255/299): loss=0.47413478099245643, w0=-7.402496038535518e-05, w1=-0.3060103567532833\n",
      "Regularized Logistic Regression(256/299): loss=0.47411803966953797, w0=-7.430307563691554e-05, w1=-0.30659114926208053\n",
      "Regularized Logistic Regression(257/299): loss=0.4741014380688906, w0=-7.458114817323071e-05, w1=-0.30716984511137796\n",
      "Regularized Logistic Regression(258/299): loss=0.4740849746910835, w0=-7.48591777292137e-05, w1=-0.30774645313667803\n",
      "Regularized Logistic Regression(259/299): loss=0.47406864805756577, w0=-7.513716404344492e-05, w1=-0.30832098212275844\n",
      "Regularized Logistic Regression(260/299): loss=0.4740524567102931, w0=-7.541510685813445e-05, w1=-0.3088934408041288\n",
      "Regularized Logistic Regression(261/299): loss=0.474036399211364, w0=-7.569300591908446e-05, w1=-0.30946383786548076\n",
      "Regularized Logistic Regression(262/299): loss=0.4740204741426618, w0=-7.597086097565177e-05, w1=-0.3100321819421323\n",
      "Regularized Logistic Regression(263/299): loss=0.47400468010550617, w0=-7.624867178071064e-05, w1=-0.3105984816204656\n",
      "Regularized Logistic Regression(264/299): loss=0.47398901572031144, w0=-7.65264380906157e-05, w1=-0.31116274543835915\n",
      "Regularized Logistic Regression(265/299): loss=0.47397347962625186, w0=-7.680415966516506e-05, w1=-0.3117249818856139\n",
      "Regularized Logistic Regression(266/299): loss=0.47395807048093563, w0=-7.708183626756364e-05, w1=-0.3122851994043739\n",
      "Regularized Logistic Regression(267/299): loss=0.47394278696008346, w0=-7.735946766438666e-05, w1=-0.3128434063895406\n",
      "Regularized Logistic Regression(268/299): loss=0.4739276277572169, w0=-7.763705362554335e-05, w1=-0.3133996111891828\n",
      "Regularized Logistic Regression(269/299): loss=0.4739125915833505, w0=-7.791459392424092e-05, w1=-0.31395382210494\n",
      "Regularized Logistic Regression(270/299): loss=0.47389767716669223, w0=-7.819208833694865e-05, w1=-0.3145060473924211\n",
      "Regularized Logistic Regression(271/299): loss=0.4738828832523506, w0=-7.84695366433623e-05, w1=-0.3150562952615977\n",
      "Regularized Logistic Regression(272/299): loss=0.47386820860204537, w0=-7.874693862636867e-05, w1=-0.31560457387719215\n",
      "Regularized Logistic Regression(273/299): loss=0.47385365199382745, w0=-7.902429407201043e-05, w1=-0.3161508913590609\n",
      "Regularized Logistic Regression(274/299): loss=0.4738392122218031, w0=-7.930160276945119e-05, w1=-0.3166952557825724\n",
      "Regularized Logistic Regression(275/299): loss=0.4738248880958633, w0=-7.95788645109408e-05, w1=-0.31723767517898077\n",
      "Regularized Logistic Regression(276/299): loss=0.47381067844142, w0=-7.985607909178084e-05, w1=-0.3177781575357939\n",
      "Regularized Logistic Regression(277/299): loss=0.47379658209914755, w0=-8.01332463102905e-05, w1=-0.3183167107971376\n",
      "Regularized Logistic Regression(278/299): loss=0.47378259792472815, w0=-8.041036596777248e-05, w1=-0.3188533428641146\n",
      "Regularized Logistic Regression(279/299): loss=0.4737687247886046, w0=-8.06874378684794e-05, w1=-0.3193880615951596\n",
      "Regularized Logistic Regression(280/299): loss=0.4737549615757362, w0=-8.096446181958025e-05, w1=-0.31992087480638914\n",
      "Regularized Logistic Regression(281/299): loss=0.47374130718536117, w0=-8.124143763112724e-05, w1=-0.3204517902719479\n",
      "Regularized Logistic Regression(282/299): loss=0.4737277605307627, w0=-8.15183651160228e-05, w1=-0.32098081572435\n",
      "Regularized Logistic Regression(283/299): loss=0.47371432053903995, w0=-8.179524408998695e-05, w1=-0.3215079588548164\n",
      "Regularized Logistic Regression(284/299): loss=0.4737009861508848, w0=-8.207207437152483e-05, w1=-0.3220332273136084\n",
      "Regularized Logistic Regression(285/299): loss=0.47368775632036136, w0=-8.234885578189456e-05, w1=-0.32255662871035634\n",
      "Regularized Logistic Regression(286/299): loss=0.47367463001469057, w0=-8.262558814507531e-05, w1=-0.32307817061438515\n",
      "Regularized Logistic Regression(287/299): loss=0.47366160621404, w0=-8.290227128773572e-05, w1=-0.32359786055503537\n",
      "Regularized Logistic Regression(288/299): loss=0.4736486839113166, w0=-8.317890503920249e-05, w1=-0.32411570602198053\n",
      "Regularized Logistic Regression(289/299): loss=0.4736358621119638, w0=-8.345548923142927e-05, w1=-0.32463171446554073\n",
      "Regularized Logistic Regression(290/299): loss=0.47362313983376303, w0=-8.373202369896592e-05, w1=-0.32514589329699217\n",
      "Regularized Logistic Regression(291/299): loss=0.4736105161066392, w0=-8.400850827892783e-05, w1=-0.32565824988887354\n",
      "Regularized Logistic Regression(292/299): loss=0.47359798997246844, w0=-8.42849428109657e-05, w1=-0.32616879157528816\n",
      "Regularized Logistic Regression(293/299): loss=0.4735855604848926, w0=-8.456132713723549e-05, w1=-0.32667752565220315\n",
      "Regularized Logistic Regression(294/299): loss=0.4735732267091341, w0=-8.48376611023687e-05, w1=-0.3271844593777446\n",
      "Regularized Logistic Regression(295/299): loss=0.47356098772181654, w0=-8.511394455344285e-05, w1=-0.32768959997248975\n",
      "Regularized Logistic Regression(296/299): loss=0.47354884261078767, w0=-8.539017733995226e-05, w1=-0.3281929546197552\n",
      "Regularized Logistic Regression(297/299): loss=0.4735367904749466, w0=-8.566635931377916e-05, w1=-0.32869453046588226\n",
      "Regularized Logistic Regression(298/299): loss=0.4735248304240735, w0=-8.594249032916494e-05, w1=-0.3291943346205189\n",
      "Regularized Logistic Regression(299/299): loss=0.4735129615786632, w0=-8.62185702426818e-05, w1=-0.3296923741568984\n"
     ]
    }
   ],
   "source": [
    "from implementations import *\n",
    "\n",
    "# Initialize the weights\n",
    "initial_w = np.zeros(x_train_filtered_2_OHE_train_fixed.shape[1])\n",
    "max_iters = 300\n",
    "gamma = 0.2\n",
    "lambda_ = 0.001\n",
    "\n",
    "# Run the reg logistic regression\n",
    "w, loss = reg_logistic_regression(y_train_train_fixed, x_train_filtered_2_OHE_train_fixed, lambda_,initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ... 0 1 0]\n",
      "Train accuracy: 0.7616682158258034, precision: 0.24188052111457706, recall: 0.7962978943734899, F1: 0.37105173211090337\n",
      "Test accuracy: 0.7631157907568531, precision: 0.24391781829646367, recall: 0.8004828418692878, F1: 0.3739025372533226\n"
     ]
    }
   ],
   "source": [
    "def predict_logistic_regression(x, w):\n",
    "    return np.array([1 if p > 0.5 else 0 for p in sigmoid(x @ w)])\n",
    "\n",
    "# calculate the accuracy, precision, recall and F1 score\n",
    "def accuracy_precision_recall_f1(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    accuracy = (tp + tn) / len(y_true)\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Predict the values\n",
    "y_pred_train = predict_logistic_regression(x_train_filtered_2_OHE_train, w)\n",
    "y_pred_test = predict_logistic_regression(x_train_filtered_2_OHE_test, w)\n",
    "\n",
    "print(y_pred_train)\n",
    "# Calculate the metrics\n",
    "accuracy_train, precision_train, recall_train, f1_train = accuracy_precision_recall_f1(y_train_train, y_pred_train)\n",
    "accuracy_test, precision_test, recall_test, f1_test = accuracy_precision_recall_f1(y_train_test, y_pred_test)\n",
    "\n",
    "print(f\"Train accuracy: {accuracy_train}, precision: {precision_train}, recall: {recall_train}, F1: {f1_train}\")\n",
    "print(f\"Test accuracy: {accuracy_test}, precision: {precision_test}, recall: {recall_test}, F1: {f1_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularized Logistic Regression(0/99): loss=0.693147180559945, w0=-2.0882930292778682e-08, w1=-0.0003658898216597753\n",
      "Regularized Logistic Regression(1/99): loss=0.6915628293745325, w0=-4.175923720895427e-08, w1=-0.000730102395178145\n",
      "Regularized Logistic Regression(2/99): loss=0.6899957981779016, w0=-6.262909288195137e-08, w1=-0.0010926514057654493\n",
      "Regularized Logistic Regression(3/99): loss=0.6884458801425628, w0=-8.349266532654935e-08, w1=-0.0014535504305981206\n",
      "Regularized Logistic Regression(4/99): loss=0.6869128708237278, w0=-1.043501185193014e-07, w1=-0.0018128129384149471\n",
      "Regularized Logistic Regression(5/99): loss=0.6853965681381441, w0=-1.2520161247790592e-07, w1=-0.0021704522891827344\n",
      "Regularized Logistic Regression(6/99): loss=0.6838967723428875, w0=-1.460473033395218e-07, w1=-0.0025264817338291525\n",
      "Regularized Logistic Regression(7/99): loss=0.6824132860141098, w0=-1.668873434380203e-07, w1=-0.002880914414039203\n",
      "Regularized Logistic Regression(8/99): loss=0.6809459140257669, w0=-1.87721881380168e-07, w1=-0.003233763362113153\n",
      "Regularized Logistic Regression(9/99): loss=0.6794944635283339, w0=-2.0855106212073556e-07, w1=-0.003585041500882932\n",
      "Regularized Logistic Regression(10/99): loss=0.6780587439275165, w0=-2.2937502703652887e-07, w1=-0.003934761643684442\n",
      "Regularized Logistic Regression(11/99): loss=0.6766385668629741, w0=-2.5019391399934016e-07, w1=-0.0042829364943833366\n",
      "Regularized Logistic Regression(12/99): loss=0.6752337461870609, w0=-2.710078574478173e-07, w1=-0.0046295786474517065\n",
      "Regularized Logistic Regression(13/99): loss=0.6738440979435945, w0=-2.9181698845825055e-07, w1=-0.004974700588093256\n",
      "Regularized Logistic Regression(14/99): loss=0.6724694403466581, w0=-3.1262143481427735e-07, w1=-0.005318314692414828\n",
      "Regularized Logistic Regression(15/99): loss=0.6711095937594538, w0=-3.334213210755064e-07, w1=-0.00566043322764197\n",
      "Regularized Logistic Regression(16/99): loss=0.669764380673196, w0=-3.5421676864506237e-07, w1=-0.0060010683523760355\n",
      "Regularized Logistic Regression(17/99): loss=0.6684336256860761, w0=-3.7500789583605514e-07, w1=-0.006340232116891306\n",
      "Regularized Logistic Regression(18/99): loss=0.6671171554822791, w0=-3.957948179369759e-07, w1=-0.0066779364634697086\n",
      "Regularized Logistic Regression(19/99): loss=0.6658147988110817, w0=-4.1657764727602536e-07, w1=-0.007014193226771191\n",
      "Regularized Logistic Regression(20/99): loss=0.6645263864660167, w0=-4.373564932843781e-07, w1=-0.007349014134237905\n",
      "Regularized Logistic Regression(21/99): loss=0.663251751264126, w0=-4.5813146255838947e-07, w1=-0.007682410806530339\n",
      "Regularized Logistic Regression(22/99): loss=0.6619907280252963, w0=-4.789026589207504e-07, w1=-0.008014394757993851\n",
      "Regularized Logistic Regression(23/99): loss=0.6607431535516854, w0=-4.996701834805974e-07, w1=-0.008344977397153414\n",
      "Regularized Logistic Regression(24/99): loss=0.6595088666072438, w0=-5.204341346925841e-07, w1=-0.008674170027235365\n",
      "Regularized Logistic Regression(25/99): loss=0.658287707897335, w0=-5.411946084149231e-07, w1=-0.009001983846714249\n",
      "Regularized Logistic Regression(26/99): loss=0.6570795200484574, w0=-5.619516979664059e-07, w1=-0.00932842994988364\n",
      "Regularized Logistic Regression(27/99): loss=0.6558841475880726, w0=-5.827054941824081e-07, w1=-0.00965351932744892\n",
      "Regularized Logistic Regression(28/99): loss=0.6547014369245411, w0=-6.034560854698912e-07, w1=-0.009977262867141044\n",
      "Regularized Logistic Regression(29/99): loss=0.6535312363271729, w0=-6.242035578614087e-07, w1=-0.010299671354349709\n",
      "Regularized Logistic Regression(30/99): loss=0.6523733959063882, w0=-6.449479950681263e-07, w1=-0.010620755472774641\n",
      "Regularized Logistic Regression(31/99): loss=0.6512277675939979, w0=-6.656894785318671e-07, w1=-0.010940525805093889\n",
      "Regularized Logistic Regression(32/99): loss=0.6500942051236037, w0=-6.864280874761903e-07, w1=-0.011258992833647617\n",
      "Regularized Logistic Regression(33/99): loss=0.6489725640111187, w0=-7.07163898956516e-07, w1=-0.01157616694113655\n",
      "Regularized Logistic Regression(34/99): loss=0.6478627015354096, w0=-7.278969879093051e-07, w1=-0.011892058411333766\n",
      "Regularized Logistic Regression(35/99): loss=0.6467644767190711, w0=-7.48627427200307e-07, w1=-0.012206677429808856\n",
      "Regularized Logistic Regression(36/99): loss=0.6456777503093185, w0=-7.693552876718835e-07, w1=-0.01252003408466335\n",
      "Regularized Logistic Regression(37/99): loss=0.6446023847590182, w0=-7.900806381894243e-07, w1=-0.012832138367276535\n",
      "Regularized Logistic Regression(38/99): loss=0.6435382442078402, w0=-8.108035456868615e-07, w1=-0.013143000173060535\n",
      "Regularized Logistic Regression(39/99): loss=0.6424851944635503, w0=-8.31524075211297e-07, w1=-0.01345262930222418\n",
      "Regularized Logistic Regression(40/99): loss=0.6414431029834291, w0=-8.522422899667545e-07, w1=-0.013761035460544076\n",
      "Regularized Logistic Regression(41/99): loss=0.6404118388558305, w0=-8.729582513570674e-07, w1=-0.01406822826014298\n",
      "Regularized Logistic Regression(42/99): loss=0.6393912727818712, w0=-8.936720190279142e-07, w1=-0.01437421722027388\n",
      "Regularized Logistic Regression(43/99): loss=0.6383812770572603, w0=-9.143836509080148e-07, w1=-0.014679011768109467\n",
      "Regularized Logistic Regression(44/99): loss=0.6373817255542643, w0=-9.350932032494978e-07, w1=-0.01498262123953628\n",
      "Regularized Logistic Regression(45/99): loss=0.6363924937038118, w0=-9.558007306674526e-07, w1=-0.01528505487995255\n",
      "Regularized Logistic Regression(46/99): loss=0.6354134584777352, w0=-9.765062861786778e-07, w1=-0.015586321845069537\n",
      "Regularized Logistic Regression(47/99): loss=0.6344444983711555, w0=-9.972099212396382e-07, w1=-0.015886431201715265\n",
      "Regularized Logistic Regression(48/99): loss=0.6334854933850048, w0=-1.0179116857836418e-06, w1=-0.01618539192864062\n",
      "Regularized Logistic Regression(49/99): loss=0.6325363250086907, w0=-1.0386116282572504e-06, w1=-0.016483212917326674\n",
      "Regularized Logistic Regression(50/99): loss=0.6315968762029036, w0=-1.059309795655934e-06, w1=-0.016779902972792977\n",
      "Regularized Logistic Regression(51/99): loss=0.6306670313825643, w0=-1.080006233558984e-06, w1=-0.017075470814406586\n",
      "Regularized Logistic Regression(52/99): loss=0.629746676399915, w0=-1.1007009861636934e-06, w1=-0.017369925076690777\n",
      "Regularized Logistic Regression(53/99): loss=0.6288356985277533, w0=-1.12139409631882e-06, w1=-0.017663274310133464\n",
      "Regularized Logistic Regression(54/99): loss=0.6279339864428096, w0=-1.142085605557341e-06, w1=-0.01795552698199472\n",
      "Regularized Logistic Regression(55/99): loss=0.6270414302092661, w0=-1.1627755541285145e-06, w1=-0.018246691477112893\n",
      "Regularized Logistic Regression(56/99): loss=0.6261579212624242, w0=-1.1834639810292568e-06, w1=-0.018536776098709174\n",
      "Regularized Logistic Regression(57/99): loss=0.6252833523925082, w0=-1.2041509240348473e-06, w1=-0.018825789069190035\n",
      "Regularized Logistic Regression(58/99): loss=0.6244176177286208, w0=-1.2248364197289762e-06, w1=-0.019113738530947277\n",
      "Regularized Logistic Regression(59/99): loss=0.6235606127228384, w0=-1.2455205035331421e-06, w1=-0.019400632547155506\n",
      "Regularized Logistic Regression(60/99): loss=0.6227122341344503, w0=-1.2662032097354143e-06, w1=-0.019686479102566365\n",
      "Regularized Logistic Regression(61/99): loss=0.621872380014344, w0=-1.2868845715185688e-06, w1=-0.019971286104299723\n",
      "Regularized Logistic Regression(62/99): loss=0.6210409496895335, w0=-1.3075646209876116e-06, w1=-0.02025506138263105\n",
      "Regularized Logistic Regression(63/99): loss=0.6202178437478318, w0=-1.3282433891966986e-06, w1=-0.02053781269177518\n",
      "Regularized Logistic Regression(64/99): loss=0.6194029640226671, w0=-1.3489209061754645e-06, w1=-0.02081954771066594\n",
      "Regularized Logistic Regression(65/99): loss=0.6185962135780424, w0=-1.3695972009547707e-06, w1=-0.021100274043731475\n",
      "Regularized Logistic Regression(66/99): loss=0.6177974966936405, w0=-1.3902723015918832e-06, w1=-0.021379999221665117\n",
      "Regularized Logistic Regression(67/99): loss=0.6170067188500702, w0=-1.410946235195093e-06, w1=-0.021658730702191584\n",
      "Regularized Logistic Regression(68/99): loss=0.616223786714256, w0=-1.4316190279477863e-06, w1=-0.021936475870828337\n",
      "Regularized Logistic Regression(69/99): loss=0.615448608124972, w0=-1.4522907051319778e-06, w1=-0.022213242041641906\n",
      "Regularized Logistic Regression(70/99): loss=0.6146810920785174, w0=-1.4729612911513165e-06, w1=-0.022489036457999\n",
      "Regularized Logistic Regression(71/99): loss=0.6139211487145337, w0=-1.4936308095535735e-06, w1=-0.022763866293312382\n",
      "Regularized Logistic Regression(72/99): loss=0.6131686893019641, w0=-1.5142992830526235e-06, w1=-0.023037738651781225\n",
      "Regularized Logistic Regression(73/99): loss=0.6124236262251561, w0=-1.5349667335499286e-06, w1=-0.023310660569125886\n",
      "Regularized Logistic Regression(74/99): loss=0.6116858729701007, w0=-1.5556331821555342e-06, w1=-0.023582639013317017\n",
      "Regularized Logistic Regression(75/99): loss=0.6109553441108174, w0=-1.5762986492085881e-06, w1=-0.023853680885299006\n",
      "Regularized Logistic Regression(76/99): loss=0.6102319552958765, w0=-1.5969631542973906e-06, w1=-0.024123793019707313\n",
      "Regularized Logistic Regression(77/99): loss=0.6095156232350594, w0=-1.617626716278986e-06, w1=-0.024392982185579993\n",
      "Regularized Logistic Regression(78/99): loss=0.608806265686162, w0=-1.6382893532983048e-06, w1=-0.024661255087063128\n",
      "Regularized Logistic Regression(79/99): loss=0.6081038014419338, w0=-1.6589510828068649e-06, w1=-0.024928618364110252\n",
      "Regularized Logistic Regression(80/99): loss=0.6074081503171549, w0=-1.6796119215810425e-06, w1=-0.025195078593175442\n",
      "Regularized Logistic Regression(81/99): loss=0.6067192331358505, w0=-1.7002718857399189e-06, w1=-0.025460642287900303\n",
      "Regularized Logistic Regression(82/99): loss=0.6060369717186451, w0=-1.7209309907627145e-06, w1=-0.025725315899794685\n",
      "Regularized Logistic Regression(83/99): loss=0.6053612888702472, w0=-1.7415892515058176e-06, w1=-0.02598910581891105\n",
      "Regularized Logistic Regression(84/99): loss=0.604692108367074, w0=-1.7622466822194154e-06, w1=-0.026252018374512485\n",
      "Regularized Logistic Regression(85/99): loss=0.6040293549450093, w0=-1.7829032965637373e-06, w1=-0.02651405983573447\n",
      "Regularized Logistic Regression(86/99): loss=0.6033729542872931, w0=-1.8035591076249174e-06, w1=-0.02677523641224003\n",
      "Regularized Logistic Regression(87/99): loss=0.6027228330125504, w0=-1.8242141279304843e-06, w1=-0.02703555425486873\n",
      "Regularized Logistic Regression(88/99): loss=0.602078918662945, w0=-1.8448683694644875e-06, w1=-0.02729501945627908\n",
      "Regularized Logistic Regression(89/99): loss=0.6014411396924713, w0=-1.8655218436822655e-06, w1=-0.027553638051584603\n",
      "Regularized Logistic Regression(90/99): loss=0.6008094254553735, w0=-1.8861745615248659e-06, w1=-0.027811416018983303\n",
      "Regularized Logistic Regression(91/99): loss=0.6001837061946965, w0=-1.906826533433123e-06, w1=-0.028068359280381074\n",
      "Regularized Logistic Regression(92/99): loss=0.5995639130309659, w0=-1.927477769361402e-06, w1=-0.02832447370200822\n",
      "Regularized Logistic Regression(93/99): loss=0.5989499779509967, w0=-1.9481282787910156e-06, w1=-0.02857976509502992\n",
      "Regularized Logistic Regression(94/99): loss=0.5983418337968286, w0=-1.968778070743319e-06, w1=-0.028834239216150054\n",
      "Regularized Logistic Regression(95/99): loss=0.5977394142547902, w0=-1.9894271537924943e-06, w1=-0.02908790176820881\n",
      "Regularized Logistic Regression(96/99): loss=0.5971426538446879, w0=-2.010075536078028e-06, w1=-0.02934075840077383\n",
      "Regularized Logistic Regression(97/99): loss=0.59655148790912, w0=-2.030723225316888e-06, w1=-0.029592814710725047\n",
      "Regularized Logistic Regression(98/99): loss=0.5959658526029161, w0=-2.0513702288154064e-06, w1=-0.029844076242833098\n",
      "Regularized Logistic Regression(99/99): loss=0.5953856848826973, w0=-2.0720165534808797e-06, w1=-0.0300945484903316\n",
      "Regularized Logistic Regression(0/199): loss=0.693147180559945, w0=-2.0882930292778682e-08, w1=-0.0003658898216597753\n",
      "Regularized Logistic Regression(1/199): loss=0.6915628293745325, w0=-4.175923720895427e-08, w1=-0.000730102395178145\n",
      "Regularized Logistic Regression(2/199): loss=0.6899957981779016, w0=-6.262909288195137e-08, w1=-0.0010926514057654493\n",
      "Regularized Logistic Regression(3/199): loss=0.6884458801425628, w0=-8.349266532654935e-08, w1=-0.0014535504305981206\n",
      "Regularized Logistic Regression(4/199): loss=0.6869128708237278, w0=-1.043501185193014e-07, w1=-0.0018128129384149471\n",
      "Regularized Logistic Regression(5/199): loss=0.6853965681381441, w0=-1.2520161247790592e-07, w1=-0.0021704522891827344\n",
      "Regularized Logistic Regression(6/199): loss=0.6838967723428875, w0=-1.460473033395218e-07, w1=-0.0025264817338291525\n",
      "Regularized Logistic Regression(7/199): loss=0.6824132860141098, w0=-1.668873434380203e-07, w1=-0.002880914414039203\n",
      "Regularized Logistic Regression(8/199): loss=0.6809459140257669, w0=-1.87721881380168e-07, w1=-0.003233763362113153\n",
      "Regularized Logistic Regression(9/199): loss=0.6794944635283339, w0=-2.0855106212073556e-07, w1=-0.003585041500882932\n",
      "Regularized Logistic Regression(10/199): loss=0.6780587439275165, w0=-2.2937502703652887e-07, w1=-0.003934761643684442\n",
      "Regularized Logistic Regression(11/199): loss=0.6766385668629741, w0=-2.5019391399934016e-07, w1=-0.0042829364943833366\n",
      "Regularized Logistic Regression(12/199): loss=0.6752337461870609, w0=-2.710078574478173e-07, w1=-0.0046295786474517065\n",
      "Regularized Logistic Regression(13/199): loss=0.6738440979435945, w0=-2.9181698845825055e-07, w1=-0.004974700588093256\n",
      "Regularized Logistic Regression(14/199): loss=0.6724694403466581, w0=-3.1262143481427735e-07, w1=-0.005318314692414828\n",
      "Regularized Logistic Regression(15/199): loss=0.6711095937594538, w0=-3.334213210755064e-07, w1=-0.00566043322764197\n",
      "Regularized Logistic Regression(16/199): loss=0.669764380673196, w0=-3.5421676864506237e-07, w1=-0.0060010683523760355\n",
      "Regularized Logistic Regression(17/199): loss=0.6684336256860761, w0=-3.7500789583605514e-07, w1=-0.006340232116891306\n",
      "Regularized Logistic Regression(18/199): loss=0.6671171554822791, w0=-3.957948179369759e-07, w1=-0.0066779364634697086\n",
      "Regularized Logistic Regression(19/199): loss=0.6658147988110817, w0=-4.1657764727602536e-07, w1=-0.007014193226771191\n",
      "Regularized Logistic Regression(20/199): loss=0.6645263864660167, w0=-4.373564932843781e-07, w1=-0.007349014134237905\n",
      "Regularized Logistic Regression(21/199): loss=0.663251751264126, w0=-4.5813146255838947e-07, w1=-0.007682410806530339\n",
      "Regularized Logistic Regression(22/199): loss=0.6619907280252963, w0=-4.789026589207504e-07, w1=-0.008014394757993851\n",
      "Regularized Logistic Regression(23/199): loss=0.6607431535516854, w0=-4.996701834805974e-07, w1=-0.008344977397153414\n",
      "Regularized Logistic Regression(24/199): loss=0.6595088666072438, w0=-5.204341346925841e-07, w1=-0.008674170027235365\n",
      "Regularized Logistic Regression(25/199): loss=0.658287707897335, w0=-5.411946084149231e-07, w1=-0.009001983846714249\n",
      "Regularized Logistic Regression(26/199): loss=0.6570795200484574, w0=-5.619516979664059e-07, w1=-0.00932842994988364\n",
      "Regularized Logistic Regression(27/199): loss=0.6558841475880726, w0=-5.827054941824081e-07, w1=-0.00965351932744892\n",
      "Regularized Logistic Regression(28/199): loss=0.6547014369245411, w0=-6.034560854698912e-07, w1=-0.009977262867141044\n",
      "Regularized Logistic Regression(29/199): loss=0.6535312363271729, w0=-6.242035578614087e-07, w1=-0.010299671354349709\n",
      "Regularized Logistic Regression(30/199): loss=0.6523733959063882, w0=-6.449479950681263e-07, w1=-0.010620755472774641\n",
      "Regularized Logistic Regression(31/199): loss=0.6512277675939979, w0=-6.656894785318671e-07, w1=-0.010940525805093889\n",
      "Regularized Logistic Regression(32/199): loss=0.6500942051236037, w0=-6.864280874761903e-07, w1=-0.011258992833647617\n",
      "Regularized Logistic Regression(33/199): loss=0.6489725640111187, w0=-7.07163898956516e-07, w1=-0.01157616694113655\n",
      "Regularized Logistic Regression(34/199): loss=0.6478627015354096, w0=-7.278969879093051e-07, w1=-0.011892058411333766\n",
      "Regularized Logistic Regression(35/199): loss=0.6467644767190711, w0=-7.48627427200307e-07, w1=-0.012206677429808856\n",
      "Regularized Logistic Regression(36/199): loss=0.6456777503093185, w0=-7.693552876718835e-07, w1=-0.01252003408466335\n",
      "Regularized Logistic Regression(37/199): loss=0.6446023847590182, w0=-7.900806381894243e-07, w1=-0.012832138367276535\n",
      "Regularized Logistic Regression(38/199): loss=0.6435382442078402, w0=-8.108035456868615e-07, w1=-0.013143000173060535\n",
      "Regularized Logistic Regression(39/199): loss=0.6424851944635503, w0=-8.31524075211297e-07, w1=-0.01345262930222418\n",
      "Regularized Logistic Regression(40/199): loss=0.6414431029834291, w0=-8.522422899667545e-07, w1=-0.013761035460544076\n",
      "Regularized Logistic Regression(41/199): loss=0.6404118388558305, w0=-8.729582513570674e-07, w1=-0.01406822826014298\n",
      "Regularized Logistic Regression(42/199): loss=0.6393912727818712, w0=-8.936720190279142e-07, w1=-0.01437421722027388\n",
      "Regularized Logistic Regression(43/199): loss=0.6383812770572603, w0=-9.143836509080148e-07, w1=-0.014679011768109467\n",
      "Regularized Logistic Regression(44/199): loss=0.6373817255542643, w0=-9.350932032494978e-07, w1=-0.01498262123953628\n",
      "Regularized Logistic Regression(45/199): loss=0.6363924937038118, w0=-9.558007306674526e-07, w1=-0.01528505487995255\n",
      "Regularized Logistic Regression(46/199): loss=0.6354134584777352, w0=-9.765062861786778e-07, w1=-0.015586321845069537\n",
      "Regularized Logistic Regression(47/199): loss=0.6344444983711555, w0=-9.972099212396382e-07, w1=-0.015886431201715265\n",
      "Regularized Logistic Regression(48/199): loss=0.6334854933850048, w0=-1.0179116857836418e-06, w1=-0.01618539192864062\n",
      "Regularized Logistic Regression(49/199): loss=0.6325363250086907, w0=-1.0386116282572504e-06, w1=-0.016483212917326674\n",
      "Regularized Logistic Regression(50/199): loss=0.6315968762029036, w0=-1.059309795655934e-06, w1=-0.016779902972792977\n",
      "Regularized Logistic Regression(51/199): loss=0.6306670313825643, w0=-1.080006233558984e-06, w1=-0.017075470814406586\n",
      "Regularized Logistic Regression(52/199): loss=0.629746676399915, w0=-1.1007009861636934e-06, w1=-0.017369925076690777\n",
      "Regularized Logistic Regression(53/199): loss=0.6288356985277533, w0=-1.12139409631882e-06, w1=-0.017663274310133464\n",
      "Regularized Logistic Regression(54/199): loss=0.6279339864428096, w0=-1.142085605557341e-06, w1=-0.01795552698199472\n",
      "Regularized Logistic Regression(55/199): loss=0.6270414302092661, w0=-1.1627755541285145e-06, w1=-0.018246691477112893\n",
      "Regularized Logistic Regression(56/199): loss=0.6261579212624242, w0=-1.1834639810292568e-06, w1=-0.018536776098709174\n",
      "Regularized Logistic Regression(57/199): loss=0.6252833523925082, w0=-1.2041509240348473e-06, w1=-0.018825789069190035\n",
      "Regularized Logistic Regression(58/199): loss=0.6244176177286208, w0=-1.2248364197289762e-06, w1=-0.019113738530947277\n",
      "Regularized Logistic Regression(59/199): loss=0.6235606127228384, w0=-1.2455205035331421e-06, w1=-0.019400632547155506\n",
      "Regularized Logistic Regression(60/199): loss=0.6227122341344503, w0=-1.2662032097354143e-06, w1=-0.019686479102566365\n",
      "Regularized Logistic Regression(61/199): loss=0.621872380014344, w0=-1.2868845715185688e-06, w1=-0.019971286104299723\n",
      "Regularized Logistic Regression(62/199): loss=0.6210409496895335, w0=-1.3075646209876116e-06, w1=-0.02025506138263105\n",
      "Regularized Logistic Regression(63/199): loss=0.6202178437478318, w0=-1.3282433891966986e-06, w1=-0.02053781269177518\n",
      "Regularized Logistic Regression(64/199): loss=0.6194029640226671, w0=-1.3489209061754645e-06, w1=-0.02081954771066594\n",
      "Regularized Logistic Regression(65/199): loss=0.6185962135780424, w0=-1.3695972009547707e-06, w1=-0.021100274043731475\n",
      "Regularized Logistic Regression(66/199): loss=0.6177974966936405, w0=-1.3902723015918832e-06, w1=-0.021379999221665117\n",
      "Regularized Logistic Regression(67/199): loss=0.6170067188500702, w0=-1.410946235195093e-06, w1=-0.021658730702191584\n",
      "Regularized Logistic Regression(68/199): loss=0.616223786714256, w0=-1.4316190279477863e-06, w1=-0.021936475870828337\n",
      "Regularized Logistic Regression(69/199): loss=0.615448608124972, w0=-1.4522907051319778e-06, w1=-0.022213242041641906\n",
      "Regularized Logistic Regression(70/199): loss=0.6146810920785174, w0=-1.4729612911513165e-06, w1=-0.022489036457999\n",
      "Regularized Logistic Regression(71/199): loss=0.6139211487145337, w0=-1.4936308095535735e-06, w1=-0.022763866293312382\n",
      "Regularized Logistic Regression(72/199): loss=0.6131686893019641, w0=-1.5142992830526235e-06, w1=-0.023037738651781225\n",
      "Regularized Logistic Regression(73/199): loss=0.6124236262251561, w0=-1.5349667335499286e-06, w1=-0.023310660569125886\n",
      "Regularized Logistic Regression(74/199): loss=0.6116858729701007, w0=-1.5556331821555342e-06, w1=-0.023582639013317017\n",
      "Regularized Logistic Regression(75/199): loss=0.6109553441108174, w0=-1.5762986492085881e-06, w1=-0.023853680885299006\n",
      "Regularized Logistic Regression(76/199): loss=0.6102319552958765, w0=-1.5969631542973906e-06, w1=-0.024123793019707313\n",
      "Regularized Logistic Regression(77/199): loss=0.6095156232350594, w0=-1.617626716278986e-06, w1=-0.024392982185579993\n",
      "Regularized Logistic Regression(78/199): loss=0.608806265686162, w0=-1.6382893532983048e-06, w1=-0.024661255087063128\n",
      "Regularized Logistic Regression(79/199): loss=0.6081038014419338, w0=-1.6589510828068649e-06, w1=-0.024928618364110252\n",
      "Regularized Logistic Regression(80/199): loss=0.6074081503171549, w0=-1.6796119215810425e-06, w1=-0.025195078593175442\n",
      "Regularized Logistic Regression(81/199): loss=0.6067192331358505, w0=-1.7002718857399189e-06, w1=-0.025460642287900303\n",
      "Regularized Logistic Regression(82/199): loss=0.6060369717186451, w0=-1.7209309907627145e-06, w1=-0.025725315899794685\n",
      "Regularized Logistic Regression(83/199): loss=0.6053612888702472, w0=-1.7415892515058176e-06, w1=-0.02598910581891105\n",
      "Regularized Logistic Regression(84/199): loss=0.604692108367074, w0=-1.7622466822194154e-06, w1=-0.026252018374512485\n",
      "Regularized Logistic Regression(85/199): loss=0.6040293549450093, w0=-1.7829032965637373e-06, w1=-0.02651405983573447\n",
      "Regularized Logistic Regression(86/199): loss=0.6033729542872931, w0=-1.8035591076249174e-06, w1=-0.02677523641224003\n",
      "Regularized Logistic Regression(87/199): loss=0.6027228330125504, w0=-1.8242141279304843e-06, w1=-0.02703555425486873\n",
      "Regularized Logistic Regression(88/199): loss=0.602078918662945, w0=-1.8448683694644875e-06, w1=-0.02729501945627908\n",
      "Regularized Logistic Regression(89/199): loss=0.6014411396924713, w0=-1.8655218436822655e-06, w1=-0.027553638051584603\n",
      "Regularized Logistic Regression(90/199): loss=0.6008094254553735, w0=-1.8861745615248659e-06, w1=-0.027811416018983303\n",
      "Regularized Logistic Regression(91/199): loss=0.6001837061946965, w0=-1.906826533433123e-06, w1=-0.028068359280381074\n",
      "Regularized Logistic Regression(92/199): loss=0.5995639130309659, w0=-1.927477769361402e-06, w1=-0.02832447370200822\n",
      "Regularized Logistic Regression(93/199): loss=0.5989499779509967, w0=-1.9481282787910156e-06, w1=-0.02857976509502992\n",
      "Regularized Logistic Regression(94/199): loss=0.5983418337968286, w0=-1.968778070743319e-06, w1=-0.028834239216150054\n",
      "Regularized Logistic Regression(95/199): loss=0.5977394142547902, w0=-1.9894271537924943e-06, w1=-0.02908790176820881\n",
      "Regularized Logistic Regression(96/199): loss=0.5971426538446879, w0=-2.010075536078028e-06, w1=-0.02934075840077383\n",
      "Regularized Logistic Regression(97/199): loss=0.59655148790912, w0=-2.030723225316888e-06, w1=-0.029592814710725047\n",
      "Regularized Logistic Regression(98/199): loss=0.5959658526029161, w0=-2.0513702288154064e-06, w1=-0.029844076242833098\n",
      "Regularized Logistic Regression(99/199): loss=0.5953856848826973, w0=-2.0720165534808797e-06, w1=-0.0300945484903316\n",
      "Regularized Logistic Regression(100/199): loss=0.5948109224965623, w0=-2.0926622058328853e-06, w1=-0.030344236895483063\n",
      "Regularized Logistic Regression(101/199): loss=0.5942415039738927, w0=-2.113307192014324e-06, w1=-0.03059314685013855\n",
      "Regularized Logistic Regression(102/199): loss=0.5936773686152799, w0=-2.1339515178021974e-06, w1=-0.030841283696291223\n",
      "Regularized Logistic Regression(103/199): loss=0.5931184564825703, w0=-2.154595188618119e-06, w1=-0.031088652726623586\n",
      "Regularized Logistic Regression(104/199): loss=0.5925647083890315, w0=-2.175238209538574e-06, w1=-0.0313352591850487\n",
      "Regularized Logistic Regression(105/199): loss=0.5920160658896342, w0=-2.195880585304924e-06, w1=-0.031581108267245354\n",
      "Regularized Logistic Regression(106/199): loss=0.5914724712714514, w0=-2.21652232033317e-06, w1=-0.03182620512118702\n",
      "Regularized Logistic Regression(107/199): loss=0.5909338675441745, w0=-2.2371634187234754e-06, w1=-0.03207055484766495\n",
      "Regularized Logistic Regression(108/199): loss=0.5904001984307434, w0=-2.2578038842694534e-06, w1=-0.032314162500805266\n",
      "Regularized Logistic Regression(109/199): loss=0.5898714083580895, w0=-2.278443720467228e-06, w1=-0.03255703308858002\n",
      "Regularized Logistic Regression(110/199): loss=0.5893474424479935, w0=-2.299082930524271e-06, w1=-0.032799171573312656\n",
      "Regularized Logistic Regression(111/199): loss=0.5888282465080539, w0=-2.3197215173680193e-06, w1=-0.033040582872177315\n",
      "Regularized Logistic Regression(112/199): loss=0.5883137670227668, w0=-2.34035948365428e-06, w1=-0.03328127185769266\n",
      "Regularized Logistic Regression(113/199): loss=0.5878039511447156, w0=-2.3609968317754277e-06, w1=-0.03352124335820961\n",
      "Regularized Logistic Regression(114/199): loss=0.5872987466858685, w0=-2.3816335638683944e-06, w1=-0.03376050215839385\n",
      "Regularized Logistic Regression(115/199): loss=0.5867981021089859, w0=-2.402269681822466e-06, w1=-0.033999052999702344\n",
      "Regularized Logistic Regression(116/199): loss=0.5863019665191332, w0=-2.4229051872868786e-06, w1=-0.034236900580854476\n",
      "Regularized Logistic Regression(117/199): loss=0.5858102896552998, w0=-2.443540081678229e-06, w1=-0.03447404955829749\n",
      "Regularized Logistic Regression(118/199): loss=0.5853230218821232, w0=-2.464174366187696e-06, w1=-0.03471050454666665\n",
      "Regularized Logistic Regression(119/199): loss=0.5848401141817163, w0=-2.4848080417880843e-06, w1=-0.034946270119239856\n",
      "Regularized Logistic Regression(120/199): loss=0.5843615181455981, w0=-2.505441109240686e-06, w1=-0.0351813508083869\n",
      "Regularized Logistic Regression(121/199): loss=0.583887185966727, w0=-2.5260735691019743e-06, w1=-0.035415751106013395\n",
      "Regularized Logistic Regression(122/199): loss=0.5834170704316316, w0=-2.5467054217301252e-06, w1=-0.035649475463999554\n",
      "Regularized Logistic Regression(123/199): loss=0.5829511249126473, w0=-2.567336667291374e-06, w1=-0.03588252829463361\n",
      "Regularized Logistic Regression(124/199): loss=0.5824893033602438, w0=-2.5879673057662107e-06, w1=-0.03611491397104017\n",
      "Regularized Logistic Regression(125/199): loss=0.5820315602954574, w0=-2.6085973369554205e-06, w1=-0.0363466368276034\n",
      "Regularized Logistic Regression(126/199): loss=0.5815778508024164, w0=-2.6292267604859655e-06, w1=-0.03657770116038533\n",
      "Regularized Logistic Regression(127/199): loss=0.5811281305209626, w0=-2.6498555758167207e-06, w1=-0.0368081112275389\n",
      "Regularized Logistic Regression(128/199): loss=0.5806823556393681, w0=-2.670483782244061e-06, w1=-0.037037871249716275\n",
      "Regularized Logistic Regression(129/199): loss=0.5802404828871458, w0=-2.691111378907305e-06, w1=-0.037266985410472166\n",
      "Regularized Logistic Regression(130/199): loss=0.5798024695279517, w0=-2.71173836479402e-06, w1=-0.03749545785666243\n",
      "Regularized Logistic Regression(131/199): loss=0.5793682733525812, w0=-2.7323647387451887e-06, w1=-0.037723292698837785\n",
      "Regularized Logistic Regression(132/199): loss=0.5789378526720529, w0=-2.752990499460245e-06, w1=-0.037950494011632856\n",
      "Regularized Logistic Regression(133/199): loss=0.5785111663107846, w0=-2.7736156455019775e-06, w1=-0.03817706583415047\n",
      "Regularized Logistic Regression(134/199): loss=0.5780881735998562, w0=-2.7942401753013057e-06, w1=-0.03840301217034152\n",
      "Regularized Logistic Regression(135/199): loss=0.5776688343703631, w0=-2.814864087161935e-06, w1=-0.038628336989380035\n",
      "Regularized Logistic Regression(136/199): loss=0.5772531089468529, w0=-2.835487379264886e-06, w1=-0.038853044226033924\n",
      "Regularized Logistic Regression(137/199): loss=0.5768409581408503, w0=-2.85611004967291e-06, w1=-0.03907713778103112\n",
      "Regularized Logistic Regression(138/199): loss=0.5764323432444661, w0=-2.8767320963347863e-06, w1=-0.0393006215214214\n",
      "Regularized Logistic Regression(139/199): loss=0.5760272260240897, w0=-2.897353517089507e-06, w1=-0.0395234992809339\n",
      "Regularized Logistic Regression(140/199): loss=0.5756255687141654, w0=-2.9179743096703543e-06, w1=-0.039745774860330166\n",
      "Regularized Logistic Regression(141/199): loss=0.575227334011049, w0=-2.938594471708866e-06, w1=-0.0399674520277531\n",
      "Regularized Logistic Regression(142/199): loss=0.5748324850669475, w0=-2.9592140007386995e-06, w1=-0.040188534519071624\n",
      "Regularized Logistic Regression(143/199): loss=0.5744409854839374, w0=-2.979832894199395e-06, w1=-0.04040902603822129\n",
      "Regularized Logistic Regression(144/199): loss=0.5740527993080616, w0=-3.000451149440033e-06, w1=-0.04062893025754063\n",
      "Regularized Logistic Regression(145/199): loss=0.5736678910235077, w0=-3.0210687637228004e-06, w1=-0.04084825081810361\n",
      "Regularized Logistic Regression(146/199): loss=0.5732862255468569, w0=-3.041685734226459e-06, w1=-0.04106699133004808\n",
      "Regularized Logistic Regression(147/199): loss=0.5729077682214176, w0=-3.0623020580497204e-06, w1=-0.04128515537290015\n",
      "Regularized Logistic Regression(148/199): loss=0.5725324848116288, w0=-3.0829177322145315e-06, w1=-0.04150274649589466\n",
      "Regularized Logistic Regression(149/199): loss=0.5721603414975379, w0=-3.1035327536692718e-06, w1=-0.041719768218292\n",
      "Regularized Logistic Regression(150/199): loss=0.5717913048693553, w0=-3.1241471192918645e-06, w1=-0.04193622402969102\n",
      "Regularized Logistic Regression(151/199): loss=0.5714253419220798, w0=-3.144760825892803e-06, w1=-0.042152117390338015\n",
      "Regularized Logistic Regression(152/199): loss=0.5710624200501951, w0=-3.165373870218094e-06, w1=-0.042367451731432165\n",
      "Regularized Logistic Regression(153/199): loss=0.5707025070424392, w0=-3.1859862489521253e-06, w1=-0.04258223045542739\n",
      "Regularized Logistic Regression(154/199): loss=0.5703455710766409, w0=-3.206597958720449e-06, w1=-0.04279645693633038\n",
      "Regularized Logistic Regression(155/199): loss=0.5699915807146296, w0=-3.2272089960924924e-06, w1=-0.04301013451999521\n",
      "Regularized Logistic Regression(156/199): loss=0.5696405048972091, w0=-3.247819357584196e-06, w1=-0.0432232665244143\n",
      "Regularized Logistic Regression(157/199): loss=0.569292312939202, w0=-3.268429039660574e-06, w1=-0.043435856240006045\n",
      "Regularized Logistic Regression(158/199): loss=0.5689469745245597, w0=-3.289038038738208e-06, w1=-0.04364790692989882\n",
      "Regularized Logistic Regression(159/199): loss=0.5686044597015368, w0=-3.3096463511876717e-06, w1=-0.04385942183021174\n",
      "Regularized Logistic Regression(160/199): loss=0.5682647388779326, w0=-3.3302539733358845e-06, w1=-0.044070404150332046\n",
      "Regularized Logistic Regression(161/199): loss=0.5679277828163961, w0=-3.3508609014684045e-06, w1=-0.044280857073189045\n",
      "Regularized Logistic Regression(162/199): loss=0.5675935626297928, w0=-3.371467131831653e-06, w1=-0.044490783755524975\n",
      "Regularized Logistic Regression(163/199): loss=0.5672620497766371, w0=-3.3920726606350803e-06, w1=-0.044700187328162506\n",
      "Regularized Logistic Regression(164/199): loss=0.5669332160565819, w0=-3.412677484053267e-06, w1=-0.04490907089626915\n",
      "Regularized Logistic Regression(165/199): loss=0.566607033605975, w0=-3.4332815982279685e-06, w1=-0.045117437539618443\n",
      "Regularized Logistic Regression(166/199): loss=0.5662834748934692, w0=-3.4538849992701006e-06, w1=-0.04532529031284804\n",
      "Regularized Logistic Regression(167/199): loss=0.5659625127156966, w0=-3.4744876832616674e-06, w1=-0.04553263224571481\n",
      "Regularized Logistic Regression(168/199): loss=0.5656441201929999, w0=-3.4950896462576375e-06, w1=-0.04573946634334685\n",
      "Regularized Logistic Regression(169/199): loss=0.56532827076522, w0=-3.5156908842877617e-06, w1=-0.04594579558649251\n",
      "Regularized Logistic Regression(170/199): loss=0.5650149381875438, w0=-3.5362913933583423e-06, w1=-0.046151622931766514\n",
      "Regularized Logistic Regression(171/199): loss=0.564704096526405, w0=-3.5568911694539493e-06, w1=-0.046356951311893116\n",
      "Regularized Logistic Regression(172/199): loss=0.5643957201554433, w0=-3.577490208539087e-06, w1=-0.04656178363594646\n",
      "Regularized Logistic Regression(173/199): loss=0.5640897837515159, w0=-3.598088506559812e-06, w1=-0.046766122789588\n",
      "Regularized Logistic Regression(174/199): loss=0.5637862622907652, w0=-3.6186860594453044e-06, w1=-0.04696997163530123\n",
      "Regularized Logistic Regression(175/199): loss=0.5634851310447396, w0=-3.639282863109392e-06, w1=-0.047173333012623546\n",
      "Regularized Logistic Regression(176/199): loss=0.5631863655765652, w0=-3.6598789134520292e-06, w1=-0.047376209738375516\n",
      "Regularized Logistic Regression(177/199): loss=0.5628899417371714, w0=-3.6804742063607344e-06, w1=-0.04757860460688738\n",
      "Regularized Logistic Regression(178/199): loss=0.5625958356615678, w0=-3.7010687377119803e-06, w1=-0.04778052039022277\n",
      "Regularized Logistic Regression(179/199): loss=0.5623040237651699, w0=-3.7216625033725466e-06, w1=-0.04798195983840012\n",
      "Regularized Logistic Regression(180/199): loss=0.5620144827401772, w0=-3.74225549920083e-06, w1=-0.048182925679611216\n",
      "Regularized Logistic Regression(181/199): loss=0.5617271895519981, w0=-3.762847721048114e-06, w1=-0.04838342062043726\n",
      "Regularized Logistic Regression(182/199): loss=0.5614421214357264, w0=-3.7834391647598015e-06, w1=-0.04858344734606247\n",
      "Regularized Logistic Regression(183/199): loss=0.5611592558926619, w0=-3.8040298261766097e-06, w1=-0.04878300852048513\n",
      "Regularized Logistic Regression(184/199): loss=0.5608785706868826, w0=-3.824619701135726e-06, w1=-0.048982106786726265\n",
      "Regularized Logistic Regression(185/199): loss=0.5606000438418598, w0=-3.845208785471931e-06, w1=-0.0491807447670358\n",
      "Regularized Logistic Regression(186/199): loss=0.5603236536371211, w0=-3.865797075018686e-06, w1=-0.04937892506309639\n",
      "Regularized Logistic Regression(187/199): loss=0.5600493786049597, w0=-3.886384565609184e-06, w1=-0.04957665025622478\n",
      "Regularized Logistic Regression(188/199): loss=0.5597771975271865, w0=-3.906971253077369e-06, w1=-0.049773922907571044\n",
      "Regularized Logistic Regression(189/199): loss=0.5595070894319274, w0=-3.927557133258925e-06, w1=-0.049970745558315315\n",
      "Regularized Logistic Regression(190/199): loss=0.5592390335904642, w0=-3.948142201992233e-06, w1=-0.05016712072986235\n",
      "Regularized Logistic Regression(191/199): loss=0.5589730095141184, w0=-3.968726455119293e-06, w1=-0.0503630509240338\n",
      "Regularized Logistic Regression(192/199): loss=0.5587089969511776, w0=-3.989309888486623e-06, w1=-0.050558538623258355\n",
      "Regularized Logistic Regression(193/199): loss=0.5584469758838629, w0=-4.009892497946122e-06, w1=-0.050753586290759614\n",
      "Regularized Logistic Regression(194/199): loss=0.5581869265253396, w0=-4.030474279355908e-06, w1=-0.05094819637074177\n",
      "Regularized Logistic Regression(195/199): loss=0.5579288293167659, w0=-4.051055228581132e-06, w1=-0.051142371288573366\n",
      "Regularized Logistic Regression(196/199): loss=0.5576726649243833, w0=-4.071635341494757e-06, w1=-0.05133611345096867\n",
      "Regularized Logistic Regression(197/199): loss=0.5574184142366468, w0=-4.092214613978318e-06, w1=-0.05152942524616721\n",
      "Regularized Logistic Regression(198/199): loss=0.5571660583613939, w0=-4.1127930419226504e-06, w1=-0.051722309044111105\n",
      "Regularized Logistic Regression(199/199): loss=0.5569155786230502, w0=-4.133370621228601e-06, w1=-0.05191476719662055\n",
      "Regularized Logistic Regression(0/299): loss=0.693147180559945, w0=-2.0882930292778682e-08, w1=-0.0003658898216597753\n",
      "Regularized Logistic Regression(1/299): loss=0.6915628293745325, w0=-4.175923720895427e-08, w1=-0.000730102395178145\n",
      "Regularized Logistic Regression(2/299): loss=0.6899957981779016, w0=-6.262909288195137e-08, w1=-0.0010926514057654493\n",
      "Regularized Logistic Regression(3/299): loss=0.6884458801425628, w0=-8.349266532654935e-08, w1=-0.0014535504305981206\n",
      "Regularized Logistic Regression(4/299): loss=0.6869128708237278, w0=-1.043501185193014e-07, w1=-0.0018128129384149471\n",
      "Regularized Logistic Regression(5/299): loss=0.6853965681381441, w0=-1.2520161247790592e-07, w1=-0.0021704522891827344\n",
      "Regularized Logistic Regression(6/299): loss=0.6838967723428875, w0=-1.460473033395218e-07, w1=-0.0025264817338291525\n",
      "Regularized Logistic Regression(7/299): loss=0.6824132860141098, w0=-1.668873434380203e-07, w1=-0.002880914414039203\n",
      "Regularized Logistic Regression(8/299): loss=0.6809459140257669, w0=-1.87721881380168e-07, w1=-0.003233763362113153\n",
      "Regularized Logistic Regression(9/299): loss=0.6794944635283339, w0=-2.0855106212073556e-07, w1=-0.003585041500882932\n",
      "Regularized Logistic Regression(10/299): loss=0.6780587439275165, w0=-2.2937502703652887e-07, w1=-0.003934761643684442\n",
      "Regularized Logistic Regression(11/299): loss=0.6766385668629741, w0=-2.5019391399934016e-07, w1=-0.0042829364943833366\n",
      "Regularized Logistic Regression(12/299): loss=0.6752337461870609, w0=-2.710078574478173e-07, w1=-0.0046295786474517065\n",
      "Regularized Logistic Regression(13/299): loss=0.6738440979435945, w0=-2.9181698845825055e-07, w1=-0.004974700588093256\n",
      "Regularized Logistic Regression(14/299): loss=0.6724694403466581, w0=-3.1262143481427735e-07, w1=-0.005318314692414828\n",
      "Regularized Logistic Regression(15/299): loss=0.6711095937594538, w0=-3.334213210755064e-07, w1=-0.00566043322764197\n",
      "Regularized Logistic Regression(16/299): loss=0.669764380673196, w0=-3.5421676864506237e-07, w1=-0.0060010683523760355\n",
      "Regularized Logistic Regression(17/299): loss=0.6684336256860761, w0=-3.7500789583605514e-07, w1=-0.006340232116891306\n",
      "Regularized Logistic Regression(18/299): loss=0.6671171554822791, w0=-3.957948179369759e-07, w1=-0.0066779364634697086\n",
      "Regularized Logistic Regression(19/299): loss=0.6658147988110817, w0=-4.1657764727602536e-07, w1=-0.007014193226771191\n",
      "Regularized Logistic Regression(20/299): loss=0.6645263864660167, w0=-4.373564932843781e-07, w1=-0.007349014134237905\n",
      "Regularized Logistic Regression(21/299): loss=0.663251751264126, w0=-4.5813146255838947e-07, w1=-0.007682410806530339\n",
      "Regularized Logistic Regression(22/299): loss=0.6619907280252963, w0=-4.789026589207504e-07, w1=-0.008014394757993851\n",
      "Regularized Logistic Regression(23/299): loss=0.6607431535516854, w0=-4.996701834805974e-07, w1=-0.008344977397153414\n",
      "Regularized Logistic Regression(24/299): loss=0.6595088666072438, w0=-5.204341346925841e-07, w1=-0.008674170027235365\n",
      "Regularized Logistic Regression(25/299): loss=0.658287707897335, w0=-5.411946084149231e-07, w1=-0.009001983846714249\n",
      "Regularized Logistic Regression(26/299): loss=0.6570795200484574, w0=-5.619516979664059e-07, w1=-0.00932842994988364\n",
      "Regularized Logistic Regression(27/299): loss=0.6558841475880726, w0=-5.827054941824081e-07, w1=-0.00965351932744892\n",
      "Regularized Logistic Regression(28/299): loss=0.6547014369245411, w0=-6.034560854698912e-07, w1=-0.009977262867141044\n",
      "Regularized Logistic Regression(29/299): loss=0.6535312363271729, w0=-6.242035578614087e-07, w1=-0.010299671354349709\n",
      "Regularized Logistic Regression(30/299): loss=0.6523733959063882, w0=-6.449479950681263e-07, w1=-0.010620755472774641\n",
      "Regularized Logistic Regression(31/299): loss=0.6512277675939979, w0=-6.656894785318671e-07, w1=-0.010940525805093889\n",
      "Regularized Logistic Regression(32/299): loss=0.6500942051236037, w0=-6.864280874761903e-07, w1=-0.011258992833647617\n",
      "Regularized Logistic Regression(33/299): loss=0.6489725640111187, w0=-7.07163898956516e-07, w1=-0.01157616694113655\n",
      "Regularized Logistic Regression(34/299): loss=0.6478627015354096, w0=-7.278969879093051e-07, w1=-0.011892058411333766\n",
      "Regularized Logistic Regression(35/299): loss=0.6467644767190711, w0=-7.48627427200307e-07, w1=-0.012206677429808856\n",
      "Regularized Logistic Regression(36/299): loss=0.6456777503093185, w0=-7.693552876718835e-07, w1=-0.01252003408466335\n",
      "Regularized Logistic Regression(37/299): loss=0.6446023847590182, w0=-7.900806381894243e-07, w1=-0.012832138367276535\n",
      "Regularized Logistic Regression(38/299): loss=0.6435382442078402, w0=-8.108035456868615e-07, w1=-0.013143000173060535\n",
      "Regularized Logistic Regression(39/299): loss=0.6424851944635503, w0=-8.31524075211297e-07, w1=-0.01345262930222418\n",
      "Regularized Logistic Regression(40/299): loss=0.6414431029834291, w0=-8.522422899667545e-07, w1=-0.013761035460544076\n",
      "Regularized Logistic Regression(41/299): loss=0.6404118388558305, w0=-8.729582513570674e-07, w1=-0.01406822826014298\n",
      "Regularized Logistic Regression(42/299): loss=0.6393912727818712, w0=-8.936720190279142e-07, w1=-0.01437421722027388\n",
      "Regularized Logistic Regression(43/299): loss=0.6383812770572603, w0=-9.143836509080148e-07, w1=-0.014679011768109467\n",
      "Regularized Logistic Regression(44/299): loss=0.6373817255542643, w0=-9.350932032494978e-07, w1=-0.01498262123953628\n",
      "Regularized Logistic Regression(45/299): loss=0.6363924937038118, w0=-9.558007306674526e-07, w1=-0.01528505487995255\n",
      "Regularized Logistic Regression(46/299): loss=0.6354134584777352, w0=-9.765062861786778e-07, w1=-0.015586321845069537\n",
      "Regularized Logistic Regression(47/299): loss=0.6344444983711555, w0=-9.972099212396382e-07, w1=-0.015886431201715265\n",
      "Regularized Logistic Regression(48/299): loss=0.6334854933850048, w0=-1.0179116857836418e-06, w1=-0.01618539192864062\n",
      "Regularized Logistic Regression(49/299): loss=0.6325363250086907, w0=-1.0386116282572504e-06, w1=-0.016483212917326674\n",
      "Regularized Logistic Regression(50/299): loss=0.6315968762029036, w0=-1.059309795655934e-06, w1=-0.016779902972792977\n",
      "Regularized Logistic Regression(51/299): loss=0.6306670313825643, w0=-1.080006233558984e-06, w1=-0.017075470814406586\n",
      "Regularized Logistic Regression(52/299): loss=0.629746676399915, w0=-1.1007009861636934e-06, w1=-0.017369925076690777\n",
      "Regularized Logistic Regression(53/299): loss=0.6288356985277533, w0=-1.12139409631882e-06, w1=-0.017663274310133464\n",
      "Regularized Logistic Regression(54/299): loss=0.6279339864428096, w0=-1.142085605557341e-06, w1=-0.01795552698199472\n",
      "Regularized Logistic Regression(55/299): loss=0.6270414302092661, w0=-1.1627755541285145e-06, w1=-0.018246691477112893\n",
      "Regularized Logistic Regression(56/299): loss=0.6261579212624242, w0=-1.1834639810292568e-06, w1=-0.018536776098709174\n",
      "Regularized Logistic Regression(57/299): loss=0.6252833523925082, w0=-1.2041509240348473e-06, w1=-0.018825789069190035\n",
      "Regularized Logistic Regression(58/299): loss=0.6244176177286208, w0=-1.2248364197289762e-06, w1=-0.019113738530947277\n",
      "Regularized Logistic Regression(59/299): loss=0.6235606127228384, w0=-1.2455205035331421e-06, w1=-0.019400632547155506\n",
      "Regularized Logistic Regression(60/299): loss=0.6227122341344503, w0=-1.2662032097354143e-06, w1=-0.019686479102566365\n",
      "Regularized Logistic Regression(61/299): loss=0.621872380014344, w0=-1.2868845715185688e-06, w1=-0.019971286104299723\n",
      "Regularized Logistic Regression(62/299): loss=0.6210409496895335, w0=-1.3075646209876116e-06, w1=-0.02025506138263105\n",
      "Regularized Logistic Regression(63/299): loss=0.6202178437478318, w0=-1.3282433891966986e-06, w1=-0.02053781269177518\n",
      "Regularized Logistic Regression(64/299): loss=0.6194029640226671, w0=-1.3489209061754645e-06, w1=-0.02081954771066594\n",
      "Regularized Logistic Regression(65/299): loss=0.6185962135780424, w0=-1.3695972009547707e-06, w1=-0.021100274043731475\n",
      "Regularized Logistic Regression(66/299): loss=0.6177974966936405, w0=-1.3902723015918832e-06, w1=-0.021379999221665117\n",
      "Regularized Logistic Regression(67/299): loss=0.6170067188500702, w0=-1.410946235195093e-06, w1=-0.021658730702191584\n",
      "Regularized Logistic Regression(68/299): loss=0.616223786714256, w0=-1.4316190279477863e-06, w1=-0.021936475870828337\n",
      "Regularized Logistic Regression(69/299): loss=0.615448608124972, w0=-1.4522907051319778e-06, w1=-0.022213242041641906\n",
      "Regularized Logistic Regression(70/299): loss=0.6146810920785174, w0=-1.4729612911513165e-06, w1=-0.022489036457999\n",
      "Regularized Logistic Regression(71/299): loss=0.6139211487145337, w0=-1.4936308095535735e-06, w1=-0.022763866293312382\n",
      "Regularized Logistic Regression(72/299): loss=0.6131686893019641, w0=-1.5142992830526235e-06, w1=-0.023037738651781225\n",
      "Regularized Logistic Regression(73/299): loss=0.6124236262251561, w0=-1.5349667335499286e-06, w1=-0.023310660569125886\n",
      "Regularized Logistic Regression(74/299): loss=0.6116858729701007, w0=-1.5556331821555342e-06, w1=-0.023582639013317017\n",
      "Regularized Logistic Regression(75/299): loss=0.6109553441108174, w0=-1.5762986492085881e-06, w1=-0.023853680885299006\n",
      "Regularized Logistic Regression(76/299): loss=0.6102319552958765, w0=-1.5969631542973906e-06, w1=-0.024123793019707313\n",
      "Regularized Logistic Regression(77/299): loss=0.6095156232350594, w0=-1.617626716278986e-06, w1=-0.024392982185579993\n",
      "Regularized Logistic Regression(78/299): loss=0.608806265686162, w0=-1.6382893532983048e-06, w1=-0.024661255087063128\n",
      "Regularized Logistic Regression(79/299): loss=0.6081038014419338, w0=-1.6589510828068649e-06, w1=-0.024928618364110252\n",
      "Regularized Logistic Regression(80/299): loss=0.6074081503171549, w0=-1.6796119215810425e-06, w1=-0.025195078593175442\n",
      "Regularized Logistic Regression(81/299): loss=0.6067192331358505, w0=-1.7002718857399189e-06, w1=-0.025460642287900303\n",
      "Regularized Logistic Regression(82/299): loss=0.6060369717186451, w0=-1.7209309907627145e-06, w1=-0.025725315899794685\n",
      "Regularized Logistic Regression(83/299): loss=0.6053612888702472, w0=-1.7415892515058176e-06, w1=-0.02598910581891105\n",
      "Regularized Logistic Regression(84/299): loss=0.604692108367074, w0=-1.7622466822194154e-06, w1=-0.026252018374512485\n",
      "Regularized Logistic Regression(85/299): loss=0.6040293549450093, w0=-1.7829032965637373e-06, w1=-0.02651405983573447\n",
      "Regularized Logistic Regression(86/299): loss=0.6033729542872931, w0=-1.8035591076249174e-06, w1=-0.02677523641224003\n",
      "Regularized Logistic Regression(87/299): loss=0.6027228330125504, w0=-1.8242141279304843e-06, w1=-0.02703555425486873\n",
      "Regularized Logistic Regression(88/299): loss=0.602078918662945, w0=-1.8448683694644875e-06, w1=-0.02729501945627908\n",
      "Regularized Logistic Regression(89/299): loss=0.6014411396924713, w0=-1.8655218436822655e-06, w1=-0.027553638051584603\n",
      "Regularized Logistic Regression(90/299): loss=0.6008094254553735, w0=-1.8861745615248659e-06, w1=-0.027811416018983303\n",
      "Regularized Logistic Regression(91/299): loss=0.6001837061946965, w0=-1.906826533433123e-06, w1=-0.028068359280381074\n",
      "Regularized Logistic Regression(92/299): loss=0.5995639130309659, w0=-1.927477769361402e-06, w1=-0.02832447370200822\n",
      "Regularized Logistic Regression(93/299): loss=0.5989499779509967, w0=-1.9481282787910156e-06, w1=-0.02857976509502992\n",
      "Regularized Logistic Regression(94/299): loss=0.5983418337968286, w0=-1.968778070743319e-06, w1=-0.028834239216150054\n",
      "Regularized Logistic Regression(95/299): loss=0.5977394142547902, w0=-1.9894271537924943e-06, w1=-0.02908790176820881\n",
      "Regularized Logistic Regression(96/299): loss=0.5971426538446879, w0=-2.010075536078028e-06, w1=-0.02934075840077383\n",
      "Regularized Logistic Regression(97/299): loss=0.59655148790912, w0=-2.030723225316888e-06, w1=-0.029592814710725047\n",
      "Regularized Logistic Regression(98/299): loss=0.5959658526029161, w0=-2.0513702288154064e-06, w1=-0.029844076242833098\n",
      "Regularized Logistic Regression(99/299): loss=0.5953856848826973, w0=-2.0720165534808797e-06, w1=-0.0300945484903316\n",
      "Regularized Logistic Regression(100/299): loss=0.5948109224965623, w0=-2.0926622058328853e-06, w1=-0.030344236895483063\n",
      "Regularized Logistic Regression(101/299): loss=0.5942415039738927, w0=-2.113307192014324e-06, w1=-0.03059314685013855\n",
      "Regularized Logistic Regression(102/299): loss=0.5936773686152799, w0=-2.1339515178021974e-06, w1=-0.030841283696291223\n",
      "Regularized Logistic Regression(103/299): loss=0.5931184564825703, w0=-2.154595188618119e-06, w1=-0.031088652726623586\n",
      "Regularized Logistic Regression(104/299): loss=0.5925647083890315, w0=-2.175238209538574e-06, w1=-0.0313352591850487\n",
      "Regularized Logistic Regression(105/299): loss=0.5920160658896342, w0=-2.195880585304924e-06, w1=-0.031581108267245354\n",
      "Regularized Logistic Regression(106/299): loss=0.5914724712714514, w0=-2.21652232033317e-06, w1=-0.03182620512118702\n",
      "Regularized Logistic Regression(107/299): loss=0.5909338675441745, w0=-2.2371634187234754e-06, w1=-0.03207055484766495\n",
      "Regularized Logistic Regression(108/299): loss=0.5904001984307434, w0=-2.2578038842694534e-06, w1=-0.032314162500805266\n",
      "Regularized Logistic Regression(109/299): loss=0.5898714083580895, w0=-2.278443720467228e-06, w1=-0.03255703308858002\n",
      "Regularized Logistic Regression(110/299): loss=0.5893474424479935, w0=-2.299082930524271e-06, w1=-0.032799171573312656\n",
      "Regularized Logistic Regression(111/299): loss=0.5888282465080539, w0=-2.3197215173680193e-06, w1=-0.033040582872177315\n",
      "Regularized Logistic Regression(112/299): loss=0.5883137670227668, w0=-2.34035948365428e-06, w1=-0.03328127185769266\n",
      "Regularized Logistic Regression(113/299): loss=0.5878039511447156, w0=-2.3609968317754277e-06, w1=-0.03352124335820961\n",
      "Regularized Logistic Regression(114/299): loss=0.5872987466858685, w0=-2.3816335638683944e-06, w1=-0.03376050215839385\n",
      "Regularized Logistic Regression(115/299): loss=0.5867981021089859, w0=-2.402269681822466e-06, w1=-0.033999052999702344\n",
      "Regularized Logistic Regression(116/299): loss=0.5863019665191332, w0=-2.4229051872868786e-06, w1=-0.034236900580854476\n",
      "Regularized Logistic Regression(117/299): loss=0.5858102896552998, w0=-2.443540081678229e-06, w1=-0.03447404955829749\n",
      "Regularized Logistic Regression(118/299): loss=0.5853230218821232, w0=-2.464174366187696e-06, w1=-0.03471050454666665\n",
      "Regularized Logistic Regression(119/299): loss=0.5848401141817163, w0=-2.4848080417880843e-06, w1=-0.034946270119239856\n",
      "Regularized Logistic Regression(120/299): loss=0.5843615181455981, w0=-2.505441109240686e-06, w1=-0.0351813508083869\n",
      "Regularized Logistic Regression(121/299): loss=0.583887185966727, w0=-2.5260735691019743e-06, w1=-0.035415751106013395\n",
      "Regularized Logistic Regression(122/299): loss=0.5834170704316316, w0=-2.5467054217301252e-06, w1=-0.035649475463999554\n",
      "Regularized Logistic Regression(123/299): loss=0.5829511249126473, w0=-2.567336667291374e-06, w1=-0.03588252829463361\n",
      "Regularized Logistic Regression(124/299): loss=0.5824893033602438, w0=-2.5879673057662107e-06, w1=-0.03611491397104017\n",
      "Regularized Logistic Regression(125/299): loss=0.5820315602954574, w0=-2.6085973369554205e-06, w1=-0.0363466368276034\n",
      "Regularized Logistic Regression(126/299): loss=0.5815778508024164, w0=-2.6292267604859655e-06, w1=-0.03657770116038533\n",
      "Regularized Logistic Regression(127/299): loss=0.5811281305209626, w0=-2.6498555758167207e-06, w1=-0.0368081112275389\n",
      "Regularized Logistic Regression(128/299): loss=0.5806823556393681, w0=-2.670483782244061e-06, w1=-0.037037871249716275\n",
      "Regularized Logistic Regression(129/299): loss=0.5802404828871458, w0=-2.691111378907305e-06, w1=-0.037266985410472166\n",
      "Regularized Logistic Regression(130/299): loss=0.5798024695279517, w0=-2.71173836479402e-06, w1=-0.03749545785666243\n",
      "Regularized Logistic Regression(131/299): loss=0.5793682733525812, w0=-2.7323647387451887e-06, w1=-0.037723292698837785\n",
      "Regularized Logistic Regression(132/299): loss=0.5789378526720529, w0=-2.752990499460245e-06, w1=-0.037950494011632856\n",
      "Regularized Logistic Regression(133/299): loss=0.5785111663107846, w0=-2.7736156455019775e-06, w1=-0.03817706583415047\n",
      "Regularized Logistic Regression(134/299): loss=0.5780881735998562, w0=-2.7942401753013057e-06, w1=-0.03840301217034152\n",
      "Regularized Logistic Regression(135/299): loss=0.5776688343703631, w0=-2.814864087161935e-06, w1=-0.038628336989380035\n",
      "Regularized Logistic Regression(136/299): loss=0.5772531089468529, w0=-2.835487379264886e-06, w1=-0.038853044226033924\n",
      "Regularized Logistic Regression(137/299): loss=0.5768409581408503, w0=-2.85611004967291e-06, w1=-0.03907713778103112\n",
      "Regularized Logistic Regression(138/299): loss=0.5764323432444661, w0=-2.8767320963347863e-06, w1=-0.0393006215214214\n",
      "Regularized Logistic Regression(139/299): loss=0.5760272260240897, w0=-2.897353517089507e-06, w1=-0.0395234992809339\n",
      "Regularized Logistic Regression(140/299): loss=0.5756255687141654, w0=-2.9179743096703543e-06, w1=-0.039745774860330166\n",
      "Regularized Logistic Regression(141/299): loss=0.575227334011049, w0=-2.938594471708866e-06, w1=-0.0399674520277531\n",
      "Regularized Logistic Regression(142/299): loss=0.5748324850669475, w0=-2.9592140007386995e-06, w1=-0.040188534519071624\n",
      "Regularized Logistic Regression(143/299): loss=0.5744409854839374, w0=-2.979832894199395e-06, w1=-0.04040902603822129\n",
      "Regularized Logistic Regression(144/299): loss=0.5740527993080616, w0=-3.000451149440033e-06, w1=-0.04062893025754063\n",
      "Regularized Logistic Regression(145/299): loss=0.5736678910235077, w0=-3.0210687637228004e-06, w1=-0.04084825081810361\n",
      "Regularized Logistic Regression(146/299): loss=0.5732862255468569, w0=-3.041685734226459e-06, w1=-0.04106699133004808\n",
      "Regularized Logistic Regression(147/299): loss=0.5729077682214176, w0=-3.0623020580497204e-06, w1=-0.04128515537290015\n",
      "Regularized Logistic Regression(148/299): loss=0.5725324848116288, w0=-3.0829177322145315e-06, w1=-0.04150274649589466\n",
      "Regularized Logistic Regression(149/299): loss=0.5721603414975379, w0=-3.1035327536692718e-06, w1=-0.041719768218292\n",
      "Regularized Logistic Regression(150/299): loss=0.5717913048693553, w0=-3.1241471192918645e-06, w1=-0.04193622402969102\n",
      "Regularized Logistic Regression(151/299): loss=0.5714253419220798, w0=-3.144760825892803e-06, w1=-0.042152117390338015\n",
      "Regularized Logistic Regression(152/299): loss=0.5710624200501951, w0=-3.165373870218094e-06, w1=-0.042367451731432165\n",
      "Regularized Logistic Regression(153/299): loss=0.5707025070424392, w0=-3.1859862489521253e-06, w1=-0.04258223045542739\n",
      "Regularized Logistic Regression(154/299): loss=0.5703455710766409, w0=-3.206597958720449e-06, w1=-0.04279645693633038\n",
      "Regularized Logistic Regression(155/299): loss=0.5699915807146296, w0=-3.2272089960924924e-06, w1=-0.04301013451999521\n",
      "Regularized Logistic Regression(156/299): loss=0.5696405048972091, w0=-3.247819357584196e-06, w1=-0.0432232665244143\n",
      "Regularized Logistic Regression(157/299): loss=0.569292312939202, w0=-3.268429039660574e-06, w1=-0.043435856240006045\n",
      "Regularized Logistic Regression(158/299): loss=0.5689469745245597, w0=-3.289038038738208e-06, w1=-0.04364790692989882\n",
      "Regularized Logistic Regression(159/299): loss=0.5686044597015368, w0=-3.3096463511876717e-06, w1=-0.04385942183021174\n",
      "Regularized Logistic Regression(160/299): loss=0.5682647388779326, w0=-3.3302539733358845e-06, w1=-0.044070404150332046\n",
      "Regularized Logistic Regression(161/299): loss=0.5679277828163961, w0=-3.3508609014684045e-06, w1=-0.044280857073189045\n",
      "Regularized Logistic Regression(162/299): loss=0.5675935626297928, w0=-3.371467131831653e-06, w1=-0.044490783755524975\n",
      "Regularized Logistic Regression(163/299): loss=0.5672620497766371, w0=-3.3920726606350803e-06, w1=-0.044700187328162506\n",
      "Regularized Logistic Regression(164/299): loss=0.5669332160565819, w0=-3.412677484053267e-06, w1=-0.04490907089626915\n",
      "Regularized Logistic Regression(165/299): loss=0.566607033605975, w0=-3.4332815982279685e-06, w1=-0.045117437539618443\n",
      "Regularized Logistic Regression(166/299): loss=0.5662834748934692, w0=-3.4538849992701006e-06, w1=-0.04532529031284804\n",
      "Regularized Logistic Regression(167/299): loss=0.5659625127156966, w0=-3.4744876832616674e-06, w1=-0.04553263224571481\n",
      "Regularized Logistic Regression(168/299): loss=0.5656441201929999, w0=-3.4950896462576375e-06, w1=-0.04573946634334685\n",
      "Regularized Logistic Regression(169/299): loss=0.56532827076522, w0=-3.5156908842877617e-06, w1=-0.04594579558649251\n",
      "Regularized Logistic Regression(170/299): loss=0.5650149381875438, w0=-3.5362913933583423e-06, w1=-0.046151622931766514\n",
      "Regularized Logistic Regression(171/299): loss=0.564704096526405, w0=-3.5568911694539493e-06, w1=-0.046356951311893116\n",
      "Regularized Logistic Regression(172/299): loss=0.5643957201554433, w0=-3.577490208539087e-06, w1=-0.04656178363594646\n",
      "Regularized Logistic Regression(173/299): loss=0.5640897837515159, w0=-3.598088506559812e-06, w1=-0.046766122789588\n",
      "Regularized Logistic Regression(174/299): loss=0.5637862622907652, w0=-3.6186860594453044e-06, w1=-0.04696997163530123\n",
      "Regularized Logistic Regression(175/299): loss=0.5634851310447396, w0=-3.639282863109392e-06, w1=-0.047173333012623546\n",
      "Regularized Logistic Regression(176/299): loss=0.5631863655765652, w0=-3.6598789134520292e-06, w1=-0.047376209738375516\n",
      "Regularized Logistic Regression(177/299): loss=0.5628899417371714, w0=-3.6804742063607344e-06, w1=-0.04757860460688738\n",
      "Regularized Logistic Regression(178/299): loss=0.5625958356615678, w0=-3.7010687377119803e-06, w1=-0.04778052039022277\n",
      "Regularized Logistic Regression(179/299): loss=0.5623040237651699, w0=-3.7216625033725466e-06, w1=-0.04798195983840012\n",
      "Regularized Logistic Regression(180/299): loss=0.5620144827401772, w0=-3.74225549920083e-06, w1=-0.048182925679611216\n",
      "Regularized Logistic Regression(181/299): loss=0.5617271895519981, w0=-3.762847721048114e-06, w1=-0.04838342062043726\n",
      "Regularized Logistic Regression(182/299): loss=0.5614421214357264, w0=-3.7834391647598015e-06, w1=-0.04858344734606247\n",
      "Regularized Logistic Regression(183/299): loss=0.5611592558926619, w0=-3.8040298261766097e-06, w1=-0.04878300852048513\n",
      "Regularized Logistic Regression(184/299): loss=0.5608785706868826, w0=-3.824619701135726e-06, w1=-0.048982106786726265\n",
      "Regularized Logistic Regression(185/299): loss=0.5606000438418598, w0=-3.845208785471931e-06, w1=-0.0491807447670358\n",
      "Regularized Logistic Regression(186/299): loss=0.5603236536371211, w0=-3.865797075018686e-06, w1=-0.04937892506309639\n",
      "Regularized Logistic Regression(187/299): loss=0.5600493786049597, w0=-3.886384565609184e-06, w1=-0.04957665025622478\n",
      "Regularized Logistic Regression(188/299): loss=0.5597771975271865, w0=-3.906971253077369e-06, w1=-0.049773922907571044\n",
      "Regularized Logistic Regression(189/299): loss=0.5595070894319274, w0=-3.927557133258925e-06, w1=-0.049970745558315315\n",
      "Regularized Logistic Regression(190/299): loss=0.5592390335904642, w0=-3.948142201992233e-06, w1=-0.05016712072986235\n",
      "Regularized Logistic Regression(191/299): loss=0.5589730095141184, w0=-3.968726455119293e-06, w1=-0.0503630509240338\n",
      "Regularized Logistic Regression(192/299): loss=0.5587089969511776, w0=-3.989309888486623e-06, w1=-0.050558538623258355\n",
      "Regularized Logistic Regression(193/299): loss=0.5584469758838629, w0=-4.009892497946122e-06, w1=-0.050753586290759614\n",
      "Regularized Logistic Regression(194/299): loss=0.5581869265253396, w0=-4.030474279355908e-06, w1=-0.05094819637074177\n",
      "Regularized Logistic Regression(195/299): loss=0.5579288293167659, w0=-4.051055228581132e-06, w1=-0.051142371288573366\n",
      "Regularized Logistic Regression(196/299): loss=0.5576726649243833, w0=-4.071635341494757e-06, w1=-0.05133611345096867\n",
      "Regularized Logistic Regression(197/299): loss=0.5574184142366468, w0=-4.092214613978318e-06, w1=-0.05152942524616721\n",
      "Regularized Logistic Regression(198/299): loss=0.5571660583613939, w0=-4.1127930419226504e-06, w1=-0.051722309044111105\n",
      "Regularized Logistic Regression(199/299): loss=0.5569155786230502, w0=-4.133370621228601e-06, w1=-0.05191476719662055\n",
      "Regularized Logistic Regression(200/299): loss=0.5566669565598773, w0=-4.153947347807706e-06, w1=-0.052106802037567054\n",
      "Regularized Logistic Regression(201/299): loss=0.5564201739212539, w0=-4.174523217582855e-06, w1=-0.052298415883044916\n",
      "Regularized Logistic Regression(202/299): loss=0.5561752126649951, w0=-4.195098226488919e-06, w1=-0.052489611031540775\n",
      "Regularized Logistic Regression(203/299): loss=0.5559320549547095, w0=-4.215672370473373e-06, w1=-0.052680389764101034\n",
      "Regularized Logistic Regression(204/299): loss=0.5556906831571907, w0=-4.236245645496881e-06, w1=-0.052870754344497554\n",
      "Regularized Logistic Regression(205/299): loss=0.5554510798398448, w0=-4.256818047533868e-06, w1=-0.05306070701939146\n",
      "Regularized Logistic Regression(206/299): loss=0.555213227768153, w0=-4.277389572573072e-06, w1=-0.053250250018495135\n",
      "Regularized Logistic Regression(207/299): loss=0.5549771099031675, w0=-4.297960216618069e-06, w1=-0.053439385554732245\n",
      "Regularized Logistic Regression(208/299): loss=0.5547427093990431, w0=-4.318529975687788e-06, w1=-0.053628115824396076\n",
      "Regularized Logistic Regression(209/299): loss=0.5545100096006021, w0=-4.339098845816998e-06, w1=-0.053816443007306085\n",
      "Regularized Logistic Regression(210/299): loss=0.5542789940409285, w0=-4.359666823056783e-06, w1=-0.05400436926696274\n",
      "Regularized Logistic Regression(211/299): loss=0.5540496464390018, w0=-4.380233903474994e-06, w1=-0.05419189675070059\n",
      "Regularized Logistic Regression(212/299): loss=0.5538219506973563, w0=-4.4008000831566865e-06, w1=-0.054379027589839424\n",
      "Regularized Logistic Regression(213/299): loss=0.5535958908997755, w0=-4.42136535820454e-06, w1=-0.054565763899834224\n",
      "Regularized Logistic Regression(214/299): loss=0.5533714513090168, w0=-4.441929724739262e-06, w1=-0.05475210778042296\n",
      "Regularized Logistic Regression(215/299): loss=0.5531486163645672, w0=-4.4624931788999705e-06, w1=-0.05493806131577295\n",
      "Regularized Logistic Regression(216/299): loss=0.5529273706804291, w0=-4.48305571684457e-06, w1=-0.05512362657462575\n",
      "Regularized Logistic Regression(217/299): loss=0.552707699042937, w0=-4.503617334750103e-06, w1=-0.0553088056104401\n",
      "Regularized Logistic Regression(218/299): loss=0.5524895864086004, w0=-4.524178028813092e-06, w1=-0.05549360046153352\n",
      "Regularized Logistic Regression(219/299): loss=0.552273017901982, w0=-4.5447377952498636e-06, w1=-0.05567801315122242\n",
      "Regularized Logistic Regression(220/299): loss=0.5520579788135976, w0=-4.565296630296864e-06, w1=-0.055862045687960334\n",
      "Regularized Logistic Regression(221/299): loss=0.5518444545978491, w0=-4.585854530210952e-06, w1=-0.05604570006547496\n",
      "Regularized Logistic Regression(222/299): loss=0.5516324308709841, w0=-4.6064114912696884e-06, w1=-0.05622897826290357\n",
      "Regularized Logistic Regression(223/299): loss=0.5514218934090815, w0=-4.626967509771602e-06, w1=-0.056411882244926895\n",
      "Regularized Logistic Regression(224/299): loss=0.551212828146067, w0=-4.647522582036453e-06, w1=-0.05659441396190154\n",
      "Regularized Logistic Regression(225/299): loss=0.5510052211717522, w0=-4.668076704405477e-06, w1=-0.056776575349991136\n",
      "Regularized Logistic Regression(226/299): loss=0.5507990587299033, w0=-4.68862987324162e-06, w1=-0.05695836833129579\n",
      "Regularized Logistic Regression(227/299): loss=0.5505943272163327, w0=-4.709182084929763e-06, w1=-0.05713979481398029\n",
      "Regularized Logistic Regression(228/299): loss=0.5503910131770186, w0=-4.729733335876929e-06, w1=-0.057320856692400904\n",
      "Regularized Logistic Regression(229/299): loss=0.5501891033062483, w0=-4.750283622512488e-06, w1=-0.05750155584723071\n",
      "Regularized Logistic Regression(230/299): loss=0.549988584444788, w0=-4.770832941288345e-06, w1=-0.05768189414558371\n",
      "Regularized Logistic Regression(231/299): loss=0.5497894435780751, w0=-4.791381288679118e-06, w1=-0.0578618734411375\n",
      "Regularized Logistic Regression(232/299): loss=0.5495916678344367, w0=-4.811928661182309e-06, w1=-0.05804149557425461\n",
      "Regularized Logistic Regression(233/299): loss=0.5493952444833302, w0=-4.83247505531846e-06, w1=-0.05822076237210255\n",
      "Regularized Logistic Regression(234/299): loss=0.5492001609336088, w0=-4.85302046763131e-06, w1=-0.05839967564877261\n",
      "Regularized Logistic Regression(235/299): loss=0.5490064047318108, w0=-4.873564894687929e-06, w1=-0.058578237205397315\n",
      "Regularized Logistic Regression(236/299): loss=0.5488139635604684, w0=-4.8941083330788485e-06, w1=-0.05875644883026669\n",
      "Regularized Logistic Regression(237/299): loss=0.5486228252364436, w0=-4.91465077941819e-06, w1=-0.05893431229894317\n",
      "Regularized Logistic Regression(238/299): loss=0.5484329777092826, w0=-4.9351922303437765e-06, w1=-0.05911182937437538\n",
      "Regularized Logistic Regression(239/299): loss=0.548244409059595, w0=-4.955732682517235e-06, w1=-0.059289001807010695\n",
      "Regularized Logistic Regression(240/299): loss=0.5480571074974515, w0=-4.976272132624099e-06, w1=-0.059465831334906516\n",
      "Regularized Logistic Regression(241/299): loss=0.5478710613608072, w0=-4.996810577373898e-06, w1=-0.059642319683840435\n",
      "Regularized Logistic Regression(242/299): loss=0.5476862591139405, w0=-5.017348013500236e-06, w1=-0.059818468567419195\n",
      "Regularized Logistic Regression(243/299): loss=0.5475026893459177, w0=-5.037884437760873e-06, w1=-0.05999427968718656\n",
      "Regularized Logistic Regression(244/299): loss=0.547320340769076, w0=-5.058419846937786e-06, w1=-0.06016975473272989\n",
      "Regularized Logistic Regression(245/299): loss=0.5471392022175245, w0=-5.078954237837235e-06, w1=-0.06034489538178577\n",
      "Regularized Logistic Regression(246/299): loss=0.5469592626456707, w0=-5.099487607289817e-06, w1=-0.06051970330034436\n",
      "Regularized Logistic Regression(247/299): loss=0.5467805111267584, w0=-5.120019952150511e-06, w1=-0.06069418014275281\n",
      "Regularized Logistic Regression(248/299): loss=0.5466029368514348, w0=-5.140551269298721e-06, w1=-0.060868327551817404\n",
      "Regularized Logistic Regression(249/299): loss=0.5464265291263272, w0=-5.161081555638313e-06, w1=-0.0610421471589048\n",
      "Regularized Logistic Regression(250/299): loss=0.5462512773726446, w0=-5.181610808097642e-06, w1=-0.0612156405840421\n",
      "Regularized Logistic Regression(251/299): loss=0.546077171124796, w0=-5.202139023629575e-06, w1=-0.06138880943601587\n",
      "Regularized Logistic Regression(252/299): loss=0.5459042000290256, w0=-5.222666199211514e-06, w1=-0.061561655312470216\n",
      "Regularized Logistic Regression(253/299): loss=0.5457323538420685, w0=-5.243192331845402e-06, w1=-0.061734179800003695\n",
      "Regularized Logistic Regression(254/299): loss=0.5455616224298224, w0=-5.263717418557735e-06, w1=-0.06190638447426541\n",
      "Regularized Logistic Regression(255/299): loss=0.5453919957660368, w0=-5.284241456399563e-06, w1=-0.06207827090004987\n",
      "Regularized Logistic Regression(256/299): loss=0.5452234639310203, w0=-5.3047644424464855e-06, w1=-0.06224984063139105\n",
      "Regularized Logistic Regression(257/299): loss=0.545056017110365, w0=-5.325286373798648e-06, w1=-0.06242109521165542\n",
      "Regularized Logistic Regression(258/299): loss=0.5448896455936857, w0=-5.345807247580725e-06, w1=-0.06259203617363394\n",
      "Regularized Logistic Regression(259/299): loss=0.5447243397733785, w0=-5.366327060941909e-06, w1=-0.06276266503963322\n",
      "Regularized Logistic Regression(260/299): loss=0.5445600901433945, w0=-5.386845811055884e-06, w1=-0.06293298332156567\n",
      "Regularized Logistic Regression(261/299): loss=0.5443968872980268, w0=-5.407363495120804e-06, w1=-0.06310299252103868\n",
      "Regularized Logistic Regression(262/299): loss=0.5442347219307191, w0=-5.427880110359264e-06, w1=-0.06327269412944299\n",
      "Regularized Logistic Regression(263/299): loss=0.5440735848328854, w0=-5.448395654018264e-06, w1=-0.0634420896280401\n",
      "Regularized Logistic Regression(264/299): loss=0.5439134668927454, w0=-5.468910123369174e-06, w1=-0.06361118048804876\n",
      "Regularized Logistic Regression(265/299): loss=0.5437543590941781, w0=-5.4894235157076945e-06, w1=-0.06377996817073056\n",
      "Regularized Logistic Regression(266/299): loss=0.5435962525155866, w0=-5.50993582835381e-06, w1=-0.06394845412747473\n",
      "Regularized Logistic Regression(267/299): loss=0.5434391383287805, w0=-5.530447058651744e-06, w1=-0.064116639799882\n",
      "Regularized Logistic Regression(268/299): loss=0.5432830077978715, w0=-5.550957203969906e-06, w1=-0.06428452661984761\n",
      "Regularized Logistic Regression(269/299): loss=0.5431278522781839, w0=-5.571466261700836e-06, w1=-0.06445211600964357\n",
      "Regularized Logistic Regression(270/299): loss=0.5429736632151788, w0=-5.591974229261153e-06, w1=-0.06461940938199989\n",
      "Regularized Logistic Regression(271/299): loss=0.5428204321433925, w0=-5.612481104091491e-06, w1=-0.06478640814018519\n",
      "Regularized Logistic Regression(272/299): loss=0.5426681506853891, w0=-5.632986883656433e-06, w1=-0.06495311367808634\n",
      "Regularized Logistic Regression(273/299): loss=0.5425168105507253, w0=-5.653491565444452e-06, w1=-0.06511952738028749\n",
      "Regularized Logistic Regression(274/299): loss=0.5423664035349308, w0=-5.673995146967836e-06, w1=-0.06528565062214799\n",
      "Regularized Logistic Regression(275/299): loss=0.5422169215184995, w0=-5.694497625762623e-06, w1=-0.06545148476987987\n",
      "Regularized Logistic Regression(276/299): loss=0.5420683564658956, w0=-5.714998999388522e-06, w1=-0.06561703118062427\n",
      "Regularized Logistic Regression(277/299): loss=0.5419207004245717, w0=-5.73549926542884e-06, w1=-0.06578229120252731\n",
      "Regularized Logistic Regression(278/299): loss=0.5417739455239994, w0=-5.755998421490402e-06, w1=-0.06594726617481494\n",
      "Regularized Logistic Regression(279/299): loss=0.5416280839747124, w0=-5.776496465203474e-06, w1=-0.06611195742786742\n",
      "Regularized Logistic Regression(280/299): loss=0.5414831080673632, w0=-5.796993394221675e-06, w1=-0.06627636628329267\n",
      "Regularized Logistic Regression(281/299): loss=0.5413390101717896, w0=-5.817489206221898e-06, w1=-0.06644049405399917\n",
      "Regularized Logistic Regression(282/299): loss=0.5411957827360955, w0=-5.8379838989042175e-06, w1=-0.06660434204426785\n",
      "Regularized Logistic Regression(283/299): loss=0.5410534182857414, w0=-5.858477469991806e-06, w1=-0.06676791154982362\n",
      "Regularized Logistic Regression(284/299): loss=0.540911909422649, w0=-5.878969917230841e-06, w1=-0.0669312038579059\n",
      "Regularized Logistic Regression(285/299): loss=0.5407712488243143, w0=-5.899461238390413e-06, w1=-0.06709422024733862\n",
      "Regularized Logistic Regression(286/299): loss=0.5406314292429354, w0=-5.919951431262432e-06, w1=-0.06725696198859936\n",
      "Regularized Logistic Regression(287/299): loss=0.5404924435045482, w0=-5.940440493661532e-06, w1=-0.06741943034388799\n",
      "Regularized Logistic Regression(288/299): loss=0.5403542845081749, w0=-5.960928423424976e-06, w1=-0.06758162656719452\n",
      "Regularized Logistic Regression(289/299): loss=0.5402169452249838, w0=-5.981415218412553e-06, w1=-0.06774355190436634\n",
      "Regularized Logistic Regression(290/299): loss=0.5400804186974582, w0=-6.001900876506484e-06, w1=-0.06790520759317473\n",
      "Regularized Logistic Regression(291/299): loss=0.5399446980385773, w0=-6.022385395611314e-06, w1=-0.06806659486338076\n",
      "Regularized Logistic Regression(292/299): loss=0.5398097764310067, w0=-6.042868773653816e-06, w1=-0.06822771493680058\n",
      "Regularized Logistic Regression(293/299): loss=0.5396756471262996, w0=-6.063351008582884e-06, w1=-0.06838856902736999\n",
      "Regularized Logistic Regression(294/299): loss=0.5395423034441074, w0=-6.083832098369428e-06, w1=-0.06854915834120846\n",
      "Regularized Logistic Regression(295/299): loss=0.5394097387714017, w0=-6.10431204100627e-06, w1=-0.06870948407668245\n",
      "Regularized Logistic Regression(296/299): loss=0.539277946561705, w0=-6.124790834508034e-06, w1=-0.06886954742446816\n",
      "Regularized Logistic Regression(297/299): loss=0.539146920334331, w0=-6.145268476911042e-06, w1=-0.06902934956761365\n",
      "Regularized Logistic Regression(298/299): loss=0.5390166536736356, w0=-6.165744966273202e-06, w1=-0.06918889168160039\n",
      "Regularized Logistic Regression(299/299): loss=0.538887140228277, w0=-6.186220300673897e-06, w1=-0.06934817493440414\n",
      "Regularized Logistic Regression(0/99): loss=0.693147180559945, w0=-2.0882930292778682e-08, w1=-0.0003658898216597753\n",
      "Regularized Logistic Regression(1/99): loss=0.6915629008663575, w0=-4.175735774522792e-08, w1=-0.0007300694650941958\n",
      "Regularized Logistic Regression(2/99): loss=0.6899962239739409, w0=-6.262345585213166e-08, w1=-0.0010925529203710397\n",
      "Regularized Logistic Regression(3/99): loss=0.6884469368166584, w0=-8.34813939429775e-08, w1=-0.0014533540657563984\n",
      "Regularized Logistic Regression(4/99): loss=0.6869148288240694, w0=-1.0433133726387137e-07, w1=-0.0018124866673348806\n",
      "Regularized Logistic Regression(5/99): loss=0.6853996918985489, w0=-1.251734470583701e-07, w1=-0.002169964378701043\n",
      "Regularized Logistic Regression(6/99): loss=0.6839013203924579, w0=-1.4600788064722386e-07, w1=-0.002525800740718776\n",
      "Regularized Logistic Regression(7/99): loss=0.6824195110852875, w0=-1.668347915070213e-07, w1=-0.002880009181345803\n",
      "Regularized Logistic Regression(8/99): loss=0.6809540631607883, w0=-1.8765432934773238e-07, w1=-0.00323260301552063\n",
      "Regularized Logistic Regression(9/99): loss=0.6795047781840962, w0=-2.0846664018914391e-07, w1=-0.003583595445108936\n",
      "Regularized Logistic Regression(10/99): loss=0.6780714600788672, w0=-2.2927186643618492e-07, w1=-0.003932999558906913\n",
      "Regularized Logistic Regression(11/99): loss=0.6766539151044312, w0=-2.500701469531395e-07, w1=-0.0042808283326987535\n",
      "Regularized Logistic Regression(12/99): loss=0.6752519518329783, w0=-2.708616171367459e-07, w1=-0.004627094629365947\n",
      "Regularized Logistic Regression(13/99): loss=0.6738653811267825, w0=-2.9164640898818203e-07, w1=-0.0049718111990458115\n",
      "Regularized Logistic Regression(14/99): loss=0.6724940161154747, w0=-3.124246511839375e-07, w1=-0.00531499067933688\n",
      "Regularized Logistic Regression(15/99): loss=0.6711376721733718, w0=-3.3319646914557473e-07, w1=-0.00565664559554893\n",
      "Regularized Logistic Regression(16/99): loss=0.6697961668968722, w0=-3.539619851083806e-07, w1=-0.00599678836099527\n",
      "Regularized Logistic Regression(17/99): loss=0.6684693200819196, w0=-3.7472131818891267e-07, w1=-0.006335431277325152\n",
      "Regularized Logistic Regression(18/99): loss=0.6671569537015488, w0=-3.954745844514439e-07, w1=-0.006672586534894472\n",
      "Regularized Logistic Regression(19/99): loss=0.6658588918835152, w0=-4.1622189697330984e-07, w1=-0.007008266213172274\n",
      "Regularized Logistic Regression(20/99): loss=0.6645749608880123, w0=-4.369633659091651e-07, w1=-0.0073424822811816014\n",
      "Regularized Logistic Regression(21/99): loss=0.6633049890854883, w0=-4.5769909855415356e-07, w1=-0.007675246597972331\n",
      "Regularized Logistic Regression(22/99): loss=0.6620488069345623, w0=-4.784291994060003e-07, w1=-0.008006570913124619\n",
      "Regularized Logistic Regression(23/99): loss=0.660806246960047, w0=-4.991537702260314e-07, w1=-0.008336466867280944\n",
      "Regularized Logistic Regression(24/99): loss=0.6595771437310793, w0=-5.1987291009913e-07, w1=-0.008664945992704984\n",
      "Regularized Logistic Regression(25/99): loss=0.6583613338393685, w0=-5.405867154926368e-07, w1=-0.008992019713865968\n",
      "Regularized Logistic Regression(26/99): loss=0.6571586558775592, w0=-5.612952803142026e-07, w1=-0.009317699348046755\n",
      "Regularized Logistic Regression(27/99): loss=0.6559689504177167, w0=-5.819986959686037e-07, w1=-0.00964199610597415\n",
      "Regularized Logistic Regression(28/99): loss=0.6547920599899387, w0=-6.026970514135283e-07, w1=-0.009964921092470068\n",
      "Regularized Logistic Regression(29/99): loss=0.6536278290610911, w0=-6.233904332143441e-07, w1=-0.010286485307122008\n",
      "Regularized Logistic Regression(30/99): loss=0.6524761040136793, w0=-6.440789255978569e-07, w1=-0.010606699644971712\n",
      "Regularized Logistic Regression(31/99): loss=0.6513367331248471, w0=-6.647626105050724e-07, w1=-0.010925574897220573\n",
      "Regularized Logistic Regression(32/99): loss=0.6502095665455144, w0=-6.854415676429686e-07, w1=-0.011243121751950592\n",
      "Regularized Logistic Regression(33/99): loss=0.6490944562796549, w0=-7.061158745352938e-07, w1=-0.011559350794859652\n",
      "Regularized Logistic Regression(34/99): loss=0.647991256163705, w0=-7.267856065723982e-07, w1=-0.011874272510010128\n",
      "Regularized Logistic Regression(35/99): loss=0.6468998218461269, w0=-7.474508370601125e-07, w1=-0.012187897280589565\n",
      "Regularized Logistic Regression(36/99): loss=0.6458200107671035, w0=-7.681116372676838e-07, w1=-0.012500235389682458\n",
      "Regularized Logistic Regression(37/99): loss=0.6447516821383859, w0=-7.887680764747823e-07, w1=-0.012811297021052152\n",
      "Regularized Logistic Regression(38/99): loss=0.6436946969232835, w0=-8.094202220175891e-07, w1=-0.013121092259932025\n",
      "Regularized Logistic Regression(39/99): loss=0.6426489178168056, w0=-8.30068139333978e-07, w1=-0.013429631093824609\n",
      "Regularized Logistic Regression(40/99): loss=0.641614209225949, w0=-8.507118920078039e-07, w1=-0.013736923413308304\n",
      "Regularized Logistic Regression(41/99): loss=0.6405904372501373, w0=-8.713515418123085e-07, w1=-0.014042979012850745\n",
      "Regularized Logistic Regression(42/99): loss=0.639577469661812, w0=-8.919871487526582e-07, w1=-0.014347807591627609\n",
      "Regularized Logistic Regression(43/99): loss=0.638575175887176, w0=-9.126187711076244e-07, w1=-0.0146514187543466\n",
      "Regularized Logistic Regression(44/99): loss=0.6375834269870898, w0=-9.3324646547042e-07, w1=-0.0149538220120756\n",
      "Regularized Logistic Regression(45/99): loss=0.6366020956381226, w0=-9.538702867887038e-07, w1=-0.015255026783074586\n",
      "Regularized Logistic Regression(46/99): loss=0.6356310561137566, w0=-9.744902884037663e-07, w1=-0.015555042393630177\n",
      "Regularized Logistic Regression(47/99): loss=0.6346701842657487, w0=-9.951065220889086e-07, w1=-0.015853878078892804\n",
      "Regularized Logistic Regression(48/99): loss=0.6337193575056472, w0=-1.0157190380870274e-06, w1=-0.016151542983715284\n",
      "Regularized Logistic Regression(49/99): loss=0.632778454786465, w0=-1.0363278851474189e-06, w1=-0.016448046163492734\n",
      "Regularized Logistic Regression(50/99): loss=0.6318473565845094, w0=-1.056933110561813e-06, w1=-0.016743396585003102\n",
      "Regularized Logistic Regression(51/99): loss=0.6309259448813699, w0=-1.0775347601996526e-06, w1=-0.017037603127247565\n",
      "Regularized Logistic Regression(52/99): loss=0.6300141031460632, w0=-1.098132878542627e-06, w1=-0.017330674582290893\n",
      "Regularized Logistic Regression(53/99): loss=0.6291117163173341, w0=-1.118727508718476e-06, w1=-0.017622619656100732\n",
      "Regularized Logistic Regression(54/99): loss=0.6282186707861191, w0=-1.1393186925340724e-06, w1=-0.017913446969385644\n",
      "Regularized Logistic Regression(55/99): loss=0.627334854378162, w0=-1.1599064705077995e-06, w1=-0.0182031650584316\n",
      "Regularized Logistic Regression(56/99): loss=0.6264601563367932, w0=-1.1804908819012333e-06, w1=-0.018491782375936325\n",
      "Regularized Logistic Regression(57/99): loss=0.6255944673058632, w0=-1.2010719647501417e-06, w1=-0.018779307291841237\n",
      "Regularized Logistic Regression(58/99): loss=0.6247376793128362, w0=-1.2216497558948137e-06, w1=-0.019065748094160586\n",
      "Regularized Logistic Regression(59/99): loss=0.6238896857520421, w0=-1.242224291009729e-06, w1=-0.019351112989807517\n",
      "Regularized Logistic Regression(60/99): loss=0.6230503813680845, w0=-1.2627956046325826e-06, w1=-0.019635410105416865\n",
      "Regularized Logistic Regression(61/99): loss=0.6222196622394093, w0=-1.2833637301926716e-06, w1=-0.019918647488164038\n",
      "Regularized Logistic Regression(62/99): loss=0.6213974257620292, w0=-1.3039287000386612e-06, w1=-0.020200833106580236\n",
      "Regularized Logistic Regression(63/99): loss=0.6205835706334057, w0=-1.324490545465737e-06, w1=-0.020481974851363174\n",
      "Regularized Logistic Regression(64/99): loss=0.6197779968364896, w0=-1.345049296742157e-06, w1=-0.020762080536183673\n",
      "Regularized Logistic Regression(65/99): loss=0.6189806056239169, w0=-1.3656049831352151e-06, w1=-0.021041157898487324\n",
      "Regularized Logistic Regression(66/99): loss=0.6181912995023612, w0=-1.3861576329366245e-06, w1=-0.021319214600291514\n",
      "Regularized Logistic Regression(67/99): loss=0.6174099822170453, w0=-1.406707273487336e-06, w1=-0.021596258228977275\n",
      "Regularized Logistic Regression(68/99): loss=0.6166365587364037, w0=-1.4272539312017985e-06, w1=-0.021872296298075928\n",
      "Regularized Logistic Regression(69/99): loss=0.6158709352369057, w0=-1.4477976315916749e-06, w1=-0.02214733624805045\n",
      "Regularized Logistic Regression(70/99): loss=0.6151130190880282, w0=-1.468338399289021e-06, w1=-0.022421385447071084\n",
      "Regularized Logistic Regression(71/99): loss=0.6143627188373872, w0=-1.4888762580689427e-06, w1=-0.02269445119178551\n",
      "Regularized Logistic Regression(72/99): loss=0.613619944196022, w0=-1.5094112308717364e-06, w1=-0.02296654070808302\n",
      "Regularized Logistic Regression(73/99): loss=0.6128846060238294, w0=-1.5299433398245274e-06, w1=-0.023237661151852867\n",
      "Regularized Logistic Regression(74/99): loss=0.6121566163151573, w0=-1.550472606262412e-06, w1=-0.023507819609736517\n",
      "Regularized Logistic Regression(75/99): loss=0.6114358881845431, w0=-1.570999050749118e-06, w1=-0.023777023099873788\n",
      "Regularized Logistic Regression(76/99): loss=0.6107223358526107, w0=-1.5915226930971887e-06, w1=-0.02404527857264277\n",
      "Regularized Logistic Regression(77/99): loss=0.6100158746321103, w0=-1.6120435523877033e-06, w1=-0.02431259291139348\n",
      "Regularized Logistic Regression(78/99): loss=0.6093164209141163, w0=-1.6325616469895416e-06, w1=-0.024578972933175045\n",
      "Regularized Logistic Regression(79/99): loss=0.60862389215437, w0=-1.6530769945782034e-06, w1=-0.024844425389456446\n",
      "Regularized Logistic Regression(80/99): loss=0.6079382068597721, w0=-1.67358961215419e-06, w1=-0.025108956966840915\n",
      "Regularized Logistic Regression(81/99): loss=0.6072592845750224, w0=-1.6940995160609586e-06, w1=-0.025372574287773553\n",
      "Regularized Logistic Regression(82/99): loss=0.6065870458694079, w0=-1.714606722002458e-06, w1=-0.02563528391124259\n",
      "Regularized Logistic Regression(83/99): loss=0.605921412323738, w0=-1.7351112450602534e-06, w1=-0.025897092333473815\n",
      "Regularized Logistic Regression(84/99): loss=0.6052623065174214, w0=-1.755613099710251e-06, w1=-0.026158005988618505\n",
      "Regularized Logistic Regression(85/99): loss=0.6046096520156926, w0=-1.776112299839028e-06, w1=-0.026418031249434534\n",
      "Regularized Logistic Regression(86/99): loss=0.6039633733569781, w0=-1.796608858759778e-06, w1=-0.026677174427960918\n",
      "Regularized Logistic Regression(87/99): loss=0.6033233960404101, w0=-1.8171027892278821e-06, w1=-0.026935441776185582\n",
      "Regularized Logistic Regression(88/99): loss=0.6026896465134756, w0=-1.8375941034561079e-06, w1=-0.027192839486706352\n",
      "Regularized Logistic Regression(89/99): loss=0.6020620521598148, w0=-1.85808281312945e-06, w1=-0.027449373693385293\n",
      "Regularized Logistic Regression(90/99): loss=0.6014405412871526, w0=-1.8785689294196177e-06, w1=-0.027705050471996354\n",
      "Regularized Logistic Regression(91/99): loss=0.6008250431153742, w0=-1.8990524629991765e-06, w1=-0.027959875840866198\n",
      "Regularized Logistic Regression(92/99): loss=0.6002154877647363, w0=-1.9195334240553516e-06, w1=-0.02821385576150837\n",
      "Regularized Logistic Regression(93/99): loss=0.5996118062442173, w0=-1.940011822303502e-06, w1=-0.02846699613925078\n",
      "Regularized Logistic Regression(94/99): loss=0.5990139304400028, w0=-1.9604876670002697e-06, w1=-0.028719302823856464\n",
      "Regularized Logistic Regression(95/99): loss=0.5984217931041075, w0=-1.9809609669564152e-06, w1=-0.028970781610137802\n",
      "Regularized Logistic Regression(96/99): loss=0.59783532784313, w0=-2.0014317305493395e-06, w1=-0.02922143823856383\n",
      "Regularized Logistic Regression(97/99): loss=0.5972544691071436, w0=-2.021899965735309e-06, w1=-0.02947127839586129\n",
      "Regularized Logistic Regression(98/99): loss=0.5966791521787167, w0=-2.0423656800613795e-06, w1=-0.029720307715608834\n",
      "Regularized Logistic Regression(99/99): loss=0.5961093131620653, w0=-2.0628288806770344e-06, w1=-0.02996853177882475\n",
      "Regularized Logistic Regression(0/199): loss=0.693147180559945, w0=-2.0882930292778682e-08, w1=-0.0003658898216597753\n",
      "Regularized Logistic Regression(1/199): loss=0.6915629008663575, w0=-4.175735774522792e-08, w1=-0.0007300694650941958\n",
      "Regularized Logistic Regression(2/199): loss=0.6899962239739409, w0=-6.262345585213166e-08, w1=-0.0010925529203710397\n",
      "Regularized Logistic Regression(3/199): loss=0.6884469368166584, w0=-8.34813939429775e-08, w1=-0.0014533540657563984\n",
      "Regularized Logistic Regression(4/199): loss=0.6869148288240694, w0=-1.0433133726387137e-07, w1=-0.0018124866673348806\n",
      "Regularized Logistic Regression(5/199): loss=0.6853996918985489, w0=-1.251734470583701e-07, w1=-0.002169964378701043\n",
      "Regularized Logistic Regression(6/199): loss=0.6839013203924579, w0=-1.4600788064722386e-07, w1=-0.002525800740718776\n",
      "Regularized Logistic Regression(7/199): loss=0.6824195110852875, w0=-1.668347915070213e-07, w1=-0.002880009181345803\n",
      "Regularized Logistic Regression(8/199): loss=0.6809540631607883, w0=-1.8765432934773238e-07, w1=-0.00323260301552063\n",
      "Regularized Logistic Regression(9/199): loss=0.6795047781840962, w0=-2.0846664018914391e-07, w1=-0.003583595445108936\n",
      "Regularized Logistic Regression(10/199): loss=0.6780714600788672, w0=-2.2927186643618492e-07, w1=-0.003932999558906913\n",
      "Regularized Logistic Regression(11/199): loss=0.6766539151044312, w0=-2.500701469531395e-07, w1=-0.0042808283326987535\n",
      "Regularized Logistic Regression(12/199): loss=0.6752519518329783, w0=-2.708616171367459e-07, w1=-0.004627094629365947\n",
      "Regularized Logistic Regression(13/199): loss=0.6738653811267825, w0=-2.9164640898818203e-07, w1=-0.0049718111990458115\n",
      "Regularized Logistic Regression(14/199): loss=0.6724940161154747, w0=-3.124246511839375e-07, w1=-0.00531499067933688\n",
      "Regularized Logistic Regression(15/199): loss=0.6711376721733718, w0=-3.3319646914557473e-07, w1=-0.00565664559554893\n",
      "Regularized Logistic Regression(16/199): loss=0.6697961668968722, w0=-3.539619851083806e-07, w1=-0.00599678836099527\n",
      "Regularized Logistic Regression(17/199): loss=0.6684693200819196, w0=-3.7472131818891267e-07, w1=-0.006335431277325152\n",
      "Regularized Logistic Regression(18/199): loss=0.6671569537015488, w0=-3.954745844514439e-07, w1=-0.006672586534894472\n",
      "Regularized Logistic Regression(19/199): loss=0.6658588918835152, w0=-4.1622189697330984e-07, w1=-0.007008266213172274\n",
      "Regularized Logistic Regression(20/199): loss=0.6645749608880123, w0=-4.369633659091651e-07, w1=-0.0073424822811816014\n",
      "Regularized Logistic Regression(21/199): loss=0.6633049890854883, w0=-4.5769909855415356e-07, w1=-0.007675246597972331\n",
      "Regularized Logistic Regression(22/199): loss=0.6620488069345623, w0=-4.784291994060003e-07, w1=-0.008006570913124619\n",
      "Regularized Logistic Regression(23/199): loss=0.660806246960047, w0=-4.991537702260314e-07, w1=-0.008336466867280944\n",
      "Regularized Logistic Regression(24/199): loss=0.6595771437310793, w0=-5.1987291009913e-07, w1=-0.008664945992704984\n",
      "Regularized Logistic Regression(25/199): loss=0.6583613338393685, w0=-5.405867154926368e-07, w1=-0.008992019713865968\n",
      "Regularized Logistic Regression(26/199): loss=0.6571586558775592, w0=-5.612952803142026e-07, w1=-0.009317699348046755\n",
      "Regularized Logistic Regression(27/199): loss=0.6559689504177167, w0=-5.819986959686037e-07, w1=-0.00964199610597415\n",
      "Regularized Logistic Regression(28/199): loss=0.6547920599899387, w0=-6.026970514135283e-07, w1=-0.009964921092470068\n",
      "Regularized Logistic Regression(29/199): loss=0.6536278290610911, w0=-6.233904332143441e-07, w1=-0.010286485307122008\n",
      "Regularized Logistic Regression(30/199): loss=0.6524761040136793, w0=-6.440789255978569e-07, w1=-0.010606699644971712\n",
      "Regularized Logistic Regression(31/199): loss=0.6513367331248471, w0=-6.647626105050724e-07, w1=-0.010925574897220573\n",
      "Regularized Logistic Regression(32/199): loss=0.6502095665455144, w0=-6.854415676429686e-07, w1=-0.011243121751950592\n",
      "Regularized Logistic Regression(33/199): loss=0.6490944562796549, w0=-7.061158745352938e-07, w1=-0.011559350794859652\n",
      "Regularized Logistic Regression(34/199): loss=0.647991256163705, w0=-7.267856065723982e-07, w1=-0.011874272510010128\n",
      "Regularized Logistic Regression(35/199): loss=0.6468998218461269, w0=-7.474508370601125e-07, w1=-0.012187897280589565\n",
      "Regularized Logistic Regression(36/199): loss=0.6458200107671035, w0=-7.681116372676838e-07, w1=-0.012500235389682458\n",
      "Regularized Logistic Regression(37/199): loss=0.6447516821383859, w0=-7.887680764747823e-07, w1=-0.012811297021052152\n",
      "Regularized Logistic Regression(38/199): loss=0.6436946969232835, w0=-8.094202220175891e-07, w1=-0.013121092259932025\n",
      "Regularized Logistic Regression(39/199): loss=0.6426489178168056, w0=-8.30068139333978e-07, w1=-0.013429631093824609\n",
      "Regularized Logistic Regression(40/199): loss=0.641614209225949, w0=-8.507118920078039e-07, w1=-0.013736923413308304\n",
      "Regularized Logistic Regression(41/199): loss=0.6405904372501373, w0=-8.713515418123085e-07, w1=-0.014042979012850745\n",
      "Regularized Logistic Regression(42/199): loss=0.639577469661812, w0=-8.919871487526582e-07, w1=-0.014347807591627609\n",
      "Regularized Logistic Regression(43/199): loss=0.638575175887176, w0=-9.126187711076244e-07, w1=-0.0146514187543466\n",
      "Regularized Logistic Regression(44/199): loss=0.6375834269870898, w0=-9.3324646547042e-07, w1=-0.0149538220120756\n",
      "Regularized Logistic Regression(45/199): loss=0.6366020956381226, w0=-9.538702867887038e-07, w1=-0.015255026783074586\n",
      "Regularized Logistic Regression(46/199): loss=0.6356310561137566, w0=-9.744902884037663e-07, w1=-0.015555042393630177\n",
      "Regularized Logistic Regression(47/199): loss=0.6346701842657487, w0=-9.951065220889086e-07, w1=-0.015853878078892804\n",
      "Regularized Logistic Regression(48/199): loss=0.6337193575056472, w0=-1.0157190380870274e-06, w1=-0.016151542983715284\n",
      "Regularized Logistic Regression(49/199): loss=0.632778454786465, w0=-1.0363278851474189e-06, w1=-0.016448046163492734\n",
      "Regularized Logistic Regression(50/199): loss=0.6318473565845094, w0=-1.056933110561813e-06, w1=-0.016743396585003102\n",
      "Regularized Logistic Regression(51/199): loss=0.6309259448813699, w0=-1.0775347601996526e-06, w1=-0.017037603127247565\n",
      "Regularized Logistic Regression(52/199): loss=0.6300141031460632, w0=-1.098132878542627e-06, w1=-0.017330674582290893\n",
      "Regularized Logistic Regression(53/199): loss=0.6291117163173341, w0=-1.118727508718476e-06, w1=-0.017622619656100732\n",
      "Regularized Logistic Regression(54/199): loss=0.6282186707861191, w0=-1.1393186925340724e-06, w1=-0.017913446969385644\n",
      "Regularized Logistic Regression(55/199): loss=0.627334854378162, w0=-1.1599064705077995e-06, w1=-0.0182031650584316\n",
      "Regularized Logistic Regression(56/199): loss=0.6264601563367932, w0=-1.1804908819012333e-06, w1=-0.018491782375936325\n",
      "Regularized Logistic Regression(57/199): loss=0.6255944673058632, w0=-1.2010719647501417e-06, w1=-0.018779307291841237\n",
      "Regularized Logistic Regression(58/199): loss=0.6247376793128362, w0=-1.2216497558948137e-06, w1=-0.019065748094160586\n",
      "Regularized Logistic Regression(59/199): loss=0.6238896857520421, w0=-1.242224291009729e-06, w1=-0.019351112989807517\n",
      "Regularized Logistic Regression(60/199): loss=0.6230503813680845, w0=-1.2627956046325826e-06, w1=-0.019635410105416865\n",
      "Regularized Logistic Regression(61/199): loss=0.6222196622394093, w0=-1.2833637301926716e-06, w1=-0.019918647488164038\n",
      "Regularized Logistic Regression(62/199): loss=0.6213974257620292, w0=-1.3039287000386612e-06, w1=-0.020200833106580236\n",
      "Regularized Logistic Regression(63/199): loss=0.6205835706334057, w0=-1.324490545465737e-06, w1=-0.020481974851363174\n",
      "Regularized Logistic Regression(64/199): loss=0.6197779968364896, w0=-1.345049296742157e-06, w1=-0.020762080536183673\n",
      "Regularized Logistic Regression(65/199): loss=0.6189806056239169, w0=-1.3656049831352151e-06, w1=-0.021041157898487324\n",
      "Regularized Logistic Regression(66/199): loss=0.6181912995023612, w0=-1.3861576329366245e-06, w1=-0.021319214600291514\n",
      "Regularized Logistic Regression(67/199): loss=0.6174099822170453, w0=-1.406707273487336e-06, w1=-0.021596258228977275\n",
      "Regularized Logistic Regression(68/199): loss=0.6166365587364037, w0=-1.4272539312017985e-06, w1=-0.021872296298075928\n",
      "Regularized Logistic Regression(69/199): loss=0.6158709352369057, w0=-1.4477976315916749e-06, w1=-0.02214733624805045\n",
      "Regularized Logistic Regression(70/199): loss=0.6151130190880282, w0=-1.468338399289021e-06, w1=-0.022421385447071084\n",
      "Regularized Logistic Regression(71/199): loss=0.6143627188373872, w0=-1.4888762580689427e-06, w1=-0.02269445119178551\n",
      "Regularized Logistic Regression(72/199): loss=0.613619944196022, w0=-1.5094112308717364e-06, w1=-0.02296654070808302\n",
      "Regularized Logistic Regression(73/199): loss=0.6128846060238294, w0=-1.5299433398245274e-06, w1=-0.023237661151852867\n",
      "Regularized Logistic Regression(74/199): loss=0.6121566163151573, w0=-1.550472606262412e-06, w1=-0.023507819609736517\n",
      "Regularized Logistic Regression(75/199): loss=0.6114358881845431, w0=-1.570999050749118e-06, w1=-0.023777023099873788\n",
      "Regularized Logistic Regression(76/199): loss=0.6107223358526107, w0=-1.5915226930971887e-06, w1=-0.02404527857264277\n",
      "Regularized Logistic Regression(77/199): loss=0.6100158746321103, w0=-1.6120435523877033e-06, w1=-0.02431259291139348\n",
      "Regularized Logistic Regression(78/199): loss=0.6093164209141163, w0=-1.6325616469895416e-06, w1=-0.024578972933175045\n",
      "Regularized Logistic Regression(79/199): loss=0.60862389215437, w0=-1.6530769945782034e-06, w1=-0.024844425389456446\n",
      "Regularized Logistic Regression(80/199): loss=0.6079382068597721, w0=-1.67358961215419e-06, w1=-0.025108956966840915\n",
      "Regularized Logistic Regression(81/199): loss=0.6072592845750224, w0=-1.6940995160609586e-06, w1=-0.025372574287773553\n",
      "Regularized Logistic Regression(82/199): loss=0.6065870458694079, w0=-1.714606722002458e-06, w1=-0.02563528391124259\n",
      "Regularized Logistic Regression(83/199): loss=0.605921412323738, w0=-1.7351112450602534e-06, w1=-0.025897092333473815\n",
      "Regularized Logistic Regression(84/199): loss=0.6052623065174214, w0=-1.755613099710251e-06, w1=-0.026158005988618505\n",
      "Regularized Logistic Regression(85/199): loss=0.6046096520156926, w0=-1.776112299839028e-06, w1=-0.026418031249434534\n",
      "Regularized Logistic Regression(86/199): loss=0.6039633733569781, w0=-1.796608858759778e-06, w1=-0.026677174427960918\n",
      "Regularized Logistic Regression(87/199): loss=0.6033233960404101, w0=-1.8171027892278821e-06, w1=-0.026935441776185582\n",
      "Regularized Logistic Regression(88/199): loss=0.6026896465134756, w0=-1.8375941034561079e-06, w1=-0.027192839486706352\n",
      "Regularized Logistic Regression(89/199): loss=0.6020620521598148, w0=-1.85808281312945e-06, w1=-0.027449373693385293\n",
      "Regularized Logistic Regression(90/199): loss=0.6014405412871526, w0=-1.8785689294196177e-06, w1=-0.027705050471996354\n",
      "Regularized Logistic Regression(91/199): loss=0.6008250431153742, w0=-1.8990524629991765e-06, w1=-0.027959875840866198\n",
      "Regularized Logistic Regression(92/199): loss=0.6002154877647363, w0=-1.9195334240553516e-06, w1=-0.02821385576150837\n",
      "Regularized Logistic Regression(93/199): loss=0.5996118062442173, w0=-1.940011822303502e-06, w1=-0.02846699613925078\n",
      "Regularized Logistic Regression(94/199): loss=0.5990139304400028, w0=-1.9604876670002697e-06, w1=-0.028719302823856464\n",
      "Regularized Logistic Regression(95/199): loss=0.5984217931041075, w0=-1.9809609669564152e-06, w1=-0.028970781610137802\n",
      "Regularized Logistic Regression(96/199): loss=0.59783532784313, w0=-2.0014317305493395e-06, w1=-0.02922143823856383\n",
      "Regularized Logistic Regression(97/199): loss=0.5972544691071436, w0=-2.021899965735309e-06, w1=-0.02947127839586129\n",
      "Regularized Logistic Regression(98/199): loss=0.5966791521787167, w0=-2.0423656800613795e-06, w1=-0.029720307715608834\n",
      "Regularized Logistic Regression(99/199): loss=0.5961093131620653, w0=-2.0628288806770344e-06, w1=-0.02996853177882475\n",
      "Regularized Logistic Regression(100/199): loss=0.5955448889723379, w0=-2.083289574345538e-06, w1=-0.030215956114548168\n",
      "Regularized Logistic Regression(101/199): loss=0.5949858173250265, w0=-2.1037477674550134e-06, w1=-0.030462586200413835\n",
      "Regularized Logistic Regression(102/199): loss=0.5944320367255078, w0=-2.1242034660292506e-06, w1=-0.030708427463220337\n",
      "Regularized Logistic Regression(103/199): loss=0.5938834864587099, w0=-2.144656675738247e-06, w1=-0.030953485279491995\n",
      "Regularized Logistic Regression(104/199): loss=0.5933401065789077, w0=-2.1651074019084935e-06, w1=-0.031197764976034353\n",
      "Regularized Logistic Regression(105/199): loss=0.5928018378996399, w0=-2.1855556495330067e-06, w1=-0.031441271830483335\n",
      "Regularized Logistic Regression(106/199): loss=0.5922686219837527, w0=-2.2060014232811128e-06, w1=-0.03168401107184815\n",
      "Regularized Logistic Regression(107/199): loss=0.5917404011335646, w0=-2.2264447275079916e-06, w1=-0.031925987881047946\n",
      "Regularized Logistic Regression(108/199): loss=0.5912171183811529, w0=-2.2468855662639846e-06, w1=-0.0321672073914423\n",
      "Regularized Logistic Regression(109/199): loss=0.5906987174787619, w0=-2.2673239433036723e-06, w1=-0.03240767468935549\n",
      "Regularized Logistic Regression(110/199): loss=0.5901851428893294, w0=-2.2877598620947273e-06, w1=-0.03264739481459484\n",
      "Regularized Logistic Regression(111/199): loss=0.5896763397771323, w0=-2.3081933258265445e-06, w1=-0.03288637276096278\n",
      "Regularized Logistic Regression(112/199): loss=0.5891722539985477, w0=-2.3286243374186597e-06, w1=-0.033124613476763175\n",
      "Regularized Logistic Regression(113/199): loss=0.5886728320929342, w0=-2.349052899528957e-06, w1=-0.03336212186530153\n",
      "Regularized Logistic Regression(114/199): loss=0.5881780212736226, w0=-2.3694790145616686e-06, w1=-0.03359890278537945\n",
      "Regularized Logistic Regression(115/199): loss=0.5876877694190249, w0=-2.3899026846751795e-06, w1=-0.03383496105178314\n",
      "Regularized Logistic Regression(116/199): loss=0.5872020250638548, w0=-2.4103239117896296e-06, w1=-0.03407030143576621\n",
      "Regularized Logistic Regression(117/199): loss=0.586720737390458, w0=-2.430742697594329e-06, w1=-0.034304928665526827\n",
      "Regularized Logistic Regression(118/199): loss=0.5862438562202564, w0=-2.4511590435549863e-06, w1=-0.034538847426679\n",
      "Regularized Logistic Regression(119/199): loss=0.5857713320052984, w0=-2.4715729509207503e-06, w1=-0.03477206236271844\n",
      "Regularized Logistic Regression(120/199): loss=0.5853031158199197, w0=-2.491984420731079e-06, w1=-0.03500457807548281\n",
      "Regularized Logistic Regression(121/199): loss=0.58483915935251, w0=-2.51239345382243e-06, w1=-0.03523639912560646\n",
      "Regularized Logistic Regression(122/199): loss=0.5843794148973874, w0=-2.5328000508347837e-06, w1=-0.03546753003296975\n",
      "Regularized Logistic Regression(123/199): loss=0.5839238353467747, w0=-2.553204212217997e-06, w1=-0.03569797527714296\n",
      "Regularized Logistic Regression(124/199): loss=0.5834723741828836, w0=-2.5736059382379993e-06, w1=-0.03592773929782486\n",
      "Regularized Logistic Regression(125/199): loss=0.5830249854700967, w0=-2.5940052289828254e-06, w1=-0.036156826495276066\n",
      "Regularized Logistic Regression(126/199): loss=0.5825816238472563, w0=-2.614402084368498e-06, w1=-0.03638524123074718\n",
      "Regularized Logistic Regression(127/199): loss=0.5821422445200508, w0=-2.6347965041447556e-06, w1=-0.03661298782690171\n",
      "Regularized Logistic Regression(128/199): loss=0.5817068032534992, w0=-2.6551884879006343e-06, w1=-0.03684007056823395\n",
      "Regularized Logistic Regression(129/199): loss=0.5812752563645378, w0=-2.6755780350699072e-06, w1=-0.03706649370148178\n",
      "Regularized Logistic Regression(130/199): loss=0.5808475607147007, w0=-2.69596514493638e-06, w1=-0.03729226143603451\n",
      "Regularized Logistic Regression(131/199): loss=0.5804236737028984, w0=-2.716349816639052e-06, w1=-0.03751737794433578\n",
      "Regularized Logistic Regression(132/199): loss=0.5800035532582897, w0=-2.7367320491771422e-06, w1=-0.037741847362281596\n",
      "Regularized Logistic Regression(133/199): loss=0.5795871578332498, w0=-2.7571118414149838e-06, w1=-0.03796567378961348\n",
      "Regularized Logistic Regression(134/199): loss=0.5791744463964296, w0=-2.7774891920867923e-06, w1=-0.03818886129030696\n",
      "Regularized Logistic Regression(135/199): loss=0.5787653784259077, w0=-2.7978640998013087e-06, w1=-0.03841141389295532\n",
      "Regularized Logistic Regression(136/199): loss=0.5783599139024321, w0=-2.8182365630463197e-06, w1=-0.038633335591148725\n",
      "Regularized Logistic Regression(137/199): loss=0.5779580133027531, w0=-2.8386065801930616e-06, w1=-0.03885463034384863\n",
      "Regularized Logistic Regression(138/199): loss=0.5775596375930442, w0=-2.8589741495005056e-06, w1=-0.039075302075757816\n",
      "Regularized Logistic Regression(139/199): loss=0.5771647482224096, w0=-2.879339269119532e-06, w1=-0.039295354677685745\n",
      "Regularized Logistic Regression(140/199): loss=0.5767733071164804, w0=-2.8997019370969934e-06, w1=-0.039514792006909744\n",
      "Regularized Logistic Regression(141/199): loss=0.5763852766710947, w0=-2.9200621513796688e-06, w1=-0.03973361788753159\n",
      "Regularized Logistic Regression(142/199): loss=0.5760006197460626, w0=-2.940419909818115e-06, w1=-0.039951836110829766\n",
      "Regularized Logistic Regression(143/199): loss=0.575619299659016, w0=-2.9607752101704143e-06, w1=-0.04016945043560768\n",
      "Regularized Logistic Regression(144/199): loss=0.5752412801793378, w0=-2.9811280501058203e-06, w1=-0.04038646458853729\n",
      "Regularized Logistic Regression(145/199): loss=0.5748665255221762, w0=-3.0014784272083097e-06, w1=-0.04060288226449907\n",
      "Regularized Logistic Regression(146/199): loss=0.5744950003425381, w0=-3.021826338980035e-06, w1=-0.040818707126917195\n",
      "Regularized Logistic Regression(147/199): loss=0.5741266697294606, w0=-3.0421717828446864e-06, w1=-0.041033942808091306\n",
      "Regularized Logistic Regression(148/199): loss=0.5737614992002631, w0=-3.062514756150762e-06, w1=-0.041248592909523915\n",
      "Regularized Logistic Regression(149/199): loss=0.573399454694877, w0=-3.0828552561747514e-06, w1=-0.0414626610022438\n",
      "Regularized Logistic Regression(150/199): loss=0.5730405025702497, w0=-3.1031932801242284e-06, w1=-0.041676150627125735\n",
      "Regularized Logistic Regression(151/199): loss=0.5726846095948268, w0=-3.1235288251408646e-06, w1=-0.04188906529520614\n",
      "Regularized Logistic Regression(152/199): loss=0.5723317429431081, w0=-3.143861888303358e-06, w1=-0.04210140848799498\n",
      "Regularized Logistic Regression(153/199): loss=0.5719818701902761, w0=-3.164192466630281e-06, w1=-0.04231318365778393\n",
      "Regularized Logistic Regression(154/199): loss=0.5716349593069007, w0=-3.184520557082852e-06, w1=-0.04252439422795075\n",
      "Regularized Logistic Regression(155/199): loss=0.5712909786537116, w0=-3.2048461565676293e-06, w1=-0.04273504359326014\n",
      "Regularized Logistic Regression(156/199): loss=0.5709498969764446, w0=-3.2251692619391294e-06, w1=-0.04294513512016071\n",
      "Regularized Logistic Regression(157/199): loss=0.5706116834007576, w0=-3.245489870002376e-06, w1=-0.04315467214707856\n",
      "Regularized Logistic Regression(158/199): loss=0.5702763074272151, w0=-3.2658079775153757e-06, w1=-0.0433636579847073\n",
      "Regularized Logistic Regression(159/199): loss=0.5699437389263405, w0=-3.286123581191525e-06, w1=-0.04357209591629461\n",
      "Regularized Logistic Regression(160/199): loss=0.5696139481337379, w0=-3.3064366777019495e-06, w1=-0.04377998919792514\n",
      "Regularized Logistic Regression(161/199): loss=0.5692869056452785, w0=-3.326747263677779e-06, w1=-0.04398734105880035\n",
      "Regularized Logistic Regression(162/199): loss=0.5689625824123531, w0=-3.3470553357123564e-06, w1=-0.044194154701514714\n",
      "Regularized Logistic Regression(163/199): loss=0.5686409497371901, w0=-3.367360890363385e-06, w1=-0.044400433302328755\n",
      "Regularized Logistic Regression(164/199): loss=0.5683219792682379, w0=-3.387663924155016e-06, w1=-0.044606180011438776\n",
      "Regularized Logistic Regression(165/199): loss=0.5680056429956077, w0=-3.4079644335798744e-06, w1=-0.04481139795324329\n",
      "Regularized Logistic Regression(166/199): loss=0.5676919132465826, w0=-3.428262415101028e-06, w1=-0.045016090226606374\n",
      "Regularized Logistic Regression(167/199): loss=0.5673807626811849, w0=-3.4485578651539002e-06, w1=-0.04522025990511786\n",
      "Regularized Logistic Regression(168/199): loss=0.5670721642878055, w0=-3.4688507801481267e-06, w1=-0.04542391003735038\n",
      "Regularized Logistic Regression(169/199): loss=0.5667660913788937, w0=-3.4891411564693596e-06, w1=-0.045627043647113265\n",
      "Regularized Logistic Regression(170/199): loss=0.5664625175867047, w0=-3.5094289904810183e-06, w1=-0.04582966373370371\n",
      "Regularized Logistic Regression(171/199): loss=0.5661614168591071, w0=-3.529714278525989e-06, w1=-0.04603177327215452\n",
      "Regularized Logistic Regression(172/199): loss=0.5658627634554451, w0=-3.5499970169282753e-06, w1=-0.04623337521347939\n",
      "Regularized Logistic Regression(173/199): loss=0.5655665319424614, w0=-3.5702772019945998e-06, w1=-0.04643447248491481\n",
      "Regularized Logistic Regression(174/199): loss=0.565272697190273, w0=-3.590554830015958e-06, w1=-0.04663506799015954\n",
      "Regularized Logistic Regression(175/199): loss=0.5649812343684031, w0=-3.6108298972691267e-06, w1=-0.04683516460961089\n",
      "Regularized Logistic Regression(176/199): loss=0.5646921189418677, w0=-3.631102400018127e-06, w1=-0.047034765200598426\n",
      "Regularized Logistic Regression(177/199): loss=0.5644053266673162, w0=-3.6513723345156437e-06, w1=-0.047233872597614815\n",
      "Regularized Logistic Regression(178/199): loss=0.5641208335892246, w0=-3.671639697004401e-06, w1=-0.047432489612543996\n",
      "Regularized Logistic Regression(179/199): loss=0.5638386160361406, w0=-3.6919044837184974e-06, w1=-0.04763061903488666\n",
      "Regularized Logistic Regression(180/199): loss=0.5635586506169817, w0=-3.7121666908847017e-06, w1=-0.04782826363198304\n",
      "Regularized Logistic Regression(181/199): loss=0.5632809142173824, w0=-3.732426314723704e-06, w1=-0.04802542614923309\n",
      "Regularized Logistic Regression(182/199): loss=0.563005383996093, w0=-3.7526833514513355e-06, w1=-0.04822210931031419\n",
      "Regularized Logistic Regression(183/199): loss=0.562732037381428, w0=-3.7729377972797433e-06, w1=-0.04841831581739606\n",
      "Regularized Logistic Regression(184/199): loss=0.5624608520677606, w0=-3.793189648418535e-06, w1=-0.048614048351353446\n",
      "Regularized Logistic Regression(185/199): loss=0.5621918060120712, w0=-3.8134389010758837e-06, w1=-0.048809309571976114\n",
      "Regularized Logistic Regression(186/199): loss=0.5619248774305355, w0=-3.8336855514595976e-06, w1=-0.04900410211817649\n",
      "Regularized Logistic Regression(187/199): loss=0.561660044795167, w0=-3.85392959577816e-06, w1=-0.04919842860819482\n",
      "Regularized Logistic Regression(188/199): loss=0.5613972868305012, w0=-3.8741710302417325e-06, w1=-0.04939229163980207\n",
      "Regularized Logistic Regression(189/199): loss=0.5611365825103257, w0=-3.894409851063125e-06, w1=-0.04958569379050024\n",
      "Regularized Logistic Regression(190/199): loss=0.5608779110544581, w0=-3.914646054458739e-06, w1=-0.049778637617720535\n",
      "Regularized Logistic Regression(191/199): loss=0.5606212519255644, w0=-3.9348796366494765e-06, w1=-0.04997112565901929\n",
      "Regularized Logistic Regression(192/199): loss=0.5603665848260236, w0=-3.955110593861622e-06, w1=-0.05016316043227141\n",
      "Regularized Logistic Regression(193/199): loss=0.5601138896948352, w0=-3.97533892232769e-06, w1=-0.050354744435861784\n",
      "Regularized Logistic Regression(194/199): loss=0.5598631467045682, w0=-3.995564618287254e-06, w1=-0.05054588014887432\n",
      "Regularized Logistic Regression(195/199): loss=0.5596143362583531, w0=-4.015787677987735e-06, w1=-0.05073657003127896\n",
      "Regularized Logistic Regression(196/199): loss=0.5593674389869138, w0=-4.036008097685176e-06, w1=-0.05092681652411638\n",
      "Regularized Logistic Regression(197/199): loss=0.5591224357456416, w0=-4.056225873644981e-06, w1=-0.05111662204968086\n",
      "Regularized Logistic Regression(198/199): loss=0.5588793076117092, w0=-4.076441002142635e-06, w1=-0.05130598901170058\n",
      "Regularized Logistic Regression(199/199): loss=0.5586380358812242, w0=-4.096653479464394e-06, w1=-0.051494919795516375\n",
      "Regularized Logistic Regression(0/299): loss=0.693147180559945, w0=-2.0882930292778682e-08, w1=-0.0003658898216597753\n",
      "Regularized Logistic Regression(1/299): loss=0.6915629008663575, w0=-4.175735774522792e-08, w1=-0.0007300694650941958\n",
      "Regularized Logistic Regression(2/299): loss=0.6899962239739409, w0=-6.262345585213166e-08, w1=-0.0010925529203710397\n",
      "Regularized Logistic Regression(3/299): loss=0.6884469368166584, w0=-8.34813939429775e-08, w1=-0.0014533540657563984\n",
      "Regularized Logistic Regression(4/299): loss=0.6869148288240694, w0=-1.0433133726387137e-07, w1=-0.0018124866673348806\n",
      "Regularized Logistic Regression(5/299): loss=0.6853996918985489, w0=-1.251734470583701e-07, w1=-0.002169964378701043\n",
      "Regularized Logistic Regression(6/299): loss=0.6839013203924579, w0=-1.4600788064722386e-07, w1=-0.002525800740718776\n",
      "Regularized Logistic Regression(7/299): loss=0.6824195110852875, w0=-1.668347915070213e-07, w1=-0.002880009181345803\n",
      "Regularized Logistic Regression(8/299): loss=0.6809540631607883, w0=-1.8765432934773238e-07, w1=-0.00323260301552063\n",
      "Regularized Logistic Regression(9/299): loss=0.6795047781840962, w0=-2.0846664018914391e-07, w1=-0.003583595445108936\n",
      "Regularized Logistic Regression(10/299): loss=0.6780714600788672, w0=-2.2927186643618492e-07, w1=-0.003932999558906913\n",
      "Regularized Logistic Regression(11/299): loss=0.6766539151044312, w0=-2.500701469531395e-07, w1=-0.0042808283326987535\n",
      "Regularized Logistic Regression(12/299): loss=0.6752519518329783, w0=-2.708616171367459e-07, w1=-0.004627094629365947\n",
      "Regularized Logistic Regression(13/299): loss=0.6738653811267825, w0=-2.9164640898818203e-07, w1=-0.0049718111990458115\n",
      "Regularized Logistic Regression(14/299): loss=0.6724940161154747, w0=-3.124246511839375e-07, w1=-0.00531499067933688\n",
      "Regularized Logistic Regression(15/299): loss=0.6711376721733718, w0=-3.3319646914557473e-07, w1=-0.00565664559554893\n",
      "Regularized Logistic Regression(16/299): loss=0.6697961668968722, w0=-3.539619851083806e-07, w1=-0.00599678836099527\n",
      "Regularized Logistic Regression(17/299): loss=0.6684693200819196, w0=-3.7472131818891267e-07, w1=-0.006335431277325152\n",
      "Regularized Logistic Regression(18/299): loss=0.6671569537015488, w0=-3.954745844514439e-07, w1=-0.006672586534894472\n",
      "Regularized Logistic Regression(19/299): loss=0.6658588918835152, w0=-4.1622189697330984e-07, w1=-0.007008266213172274\n",
      "Regularized Logistic Regression(20/299): loss=0.6645749608880123, w0=-4.369633659091651e-07, w1=-0.0073424822811816014\n",
      "Regularized Logistic Regression(21/299): loss=0.6633049890854883, w0=-4.5769909855415356e-07, w1=-0.007675246597972331\n",
      "Regularized Logistic Regression(22/299): loss=0.6620488069345623, w0=-4.784291994060003e-07, w1=-0.008006570913124619\n",
      "Regularized Logistic Regression(23/299): loss=0.660806246960047, w0=-4.991537702260314e-07, w1=-0.008336466867280944\n",
      "Regularized Logistic Regression(24/299): loss=0.6595771437310793, w0=-5.1987291009913e-07, w1=-0.008664945992704984\n",
      "Regularized Logistic Regression(25/299): loss=0.6583613338393685, w0=-5.405867154926368e-07, w1=-0.008992019713865968\n",
      "Regularized Logistic Regression(26/299): loss=0.6571586558775592, w0=-5.612952803142026e-07, w1=-0.009317699348046755\n",
      "Regularized Logistic Regression(27/299): loss=0.6559689504177167, w0=-5.819986959686037e-07, w1=-0.00964199610597415\n",
      "Regularized Logistic Regression(28/299): loss=0.6547920599899387, w0=-6.026970514135283e-07, w1=-0.009964921092470068\n",
      "Regularized Logistic Regression(29/299): loss=0.6536278290610911, w0=-6.233904332143441e-07, w1=-0.010286485307122008\n",
      "Regularized Logistic Regression(30/299): loss=0.6524761040136793, w0=-6.440789255978569e-07, w1=-0.010606699644971712\n",
      "Regularized Logistic Regression(31/299): loss=0.6513367331248471, w0=-6.647626105050724e-07, w1=-0.010925574897220573\n",
      "Regularized Logistic Regression(32/299): loss=0.6502095665455144, w0=-6.854415676429686e-07, w1=-0.011243121751950592\n",
      "Regularized Logistic Regression(33/299): loss=0.6490944562796549, w0=-7.061158745352938e-07, w1=-0.011559350794859652\n",
      "Regularized Logistic Regression(34/299): loss=0.647991256163705, w0=-7.267856065723982e-07, w1=-0.011874272510010128\n",
      "Regularized Logistic Regression(35/299): loss=0.6468998218461269, w0=-7.474508370601125e-07, w1=-0.012187897280589565\n",
      "Regularized Logistic Regression(36/299): loss=0.6458200107671035, w0=-7.681116372676838e-07, w1=-0.012500235389682458\n",
      "Regularized Logistic Regression(37/299): loss=0.6447516821383859, w0=-7.887680764747823e-07, w1=-0.012811297021052152\n",
      "Regularized Logistic Regression(38/299): loss=0.6436946969232835, w0=-8.094202220175891e-07, w1=-0.013121092259932025\n",
      "Regularized Logistic Regression(39/299): loss=0.6426489178168056, w0=-8.30068139333978e-07, w1=-0.013429631093824609\n",
      "Regularized Logistic Regression(40/299): loss=0.641614209225949, w0=-8.507118920078039e-07, w1=-0.013736923413308304\n",
      "Regularized Logistic Regression(41/299): loss=0.6405904372501373, w0=-8.713515418123085e-07, w1=-0.014042979012850745\n",
      "Regularized Logistic Regression(42/299): loss=0.639577469661812, w0=-8.919871487526582e-07, w1=-0.014347807591627609\n",
      "Regularized Logistic Regression(43/299): loss=0.638575175887176, w0=-9.126187711076244e-07, w1=-0.0146514187543466\n",
      "Regularized Logistic Regression(44/299): loss=0.6375834269870898, w0=-9.3324646547042e-07, w1=-0.0149538220120756\n",
      "Regularized Logistic Regression(45/299): loss=0.6366020956381226, w0=-9.538702867887038e-07, w1=-0.015255026783074586\n",
      "Regularized Logistic Regression(46/299): loss=0.6356310561137566, w0=-9.744902884037663e-07, w1=-0.015555042393630177\n",
      "Regularized Logistic Regression(47/299): loss=0.6346701842657487, w0=-9.951065220889086e-07, w1=-0.015853878078892804\n",
      "Regularized Logistic Regression(48/299): loss=0.6337193575056472, w0=-1.0157190380870274e-06, w1=-0.016151542983715284\n",
      "Regularized Logistic Regression(49/299): loss=0.632778454786465, w0=-1.0363278851474189e-06, w1=-0.016448046163492734\n",
      "Regularized Logistic Regression(50/299): loss=0.6318473565845094, w0=-1.056933110561813e-06, w1=-0.016743396585003102\n",
      "Regularized Logistic Regression(51/299): loss=0.6309259448813699, w0=-1.0775347601996526e-06, w1=-0.017037603127247565\n",
      "Regularized Logistic Regression(52/299): loss=0.6300141031460632, w0=-1.098132878542627e-06, w1=-0.017330674582290893\n",
      "Regularized Logistic Regression(53/299): loss=0.6291117163173341, w0=-1.118727508718476e-06, w1=-0.017622619656100732\n",
      "Regularized Logistic Regression(54/299): loss=0.6282186707861191, w0=-1.1393186925340724e-06, w1=-0.017913446969385644\n",
      "Regularized Logistic Regression(55/299): loss=0.627334854378162, w0=-1.1599064705077995e-06, w1=-0.0182031650584316\n",
      "Regularized Logistic Regression(56/299): loss=0.6264601563367932, w0=-1.1804908819012333e-06, w1=-0.018491782375936325\n",
      "Regularized Logistic Regression(57/299): loss=0.6255944673058632, w0=-1.2010719647501417e-06, w1=-0.018779307291841237\n",
      "Regularized Logistic Regression(58/299): loss=0.6247376793128362, w0=-1.2216497558948137e-06, w1=-0.019065748094160586\n",
      "Regularized Logistic Regression(59/299): loss=0.6238896857520421, w0=-1.242224291009729e-06, w1=-0.019351112989807517\n",
      "Regularized Logistic Regression(60/299): loss=0.6230503813680845, w0=-1.2627956046325826e-06, w1=-0.019635410105416865\n",
      "Regularized Logistic Regression(61/299): loss=0.6222196622394093, w0=-1.2833637301926716e-06, w1=-0.019918647488164038\n",
      "Regularized Logistic Regression(62/299): loss=0.6213974257620292, w0=-1.3039287000386612e-06, w1=-0.020200833106580236\n",
      "Regularized Logistic Regression(63/299): loss=0.6205835706334057, w0=-1.324490545465737e-06, w1=-0.020481974851363174\n",
      "Regularized Logistic Regression(64/299): loss=0.6197779968364896, w0=-1.345049296742157e-06, w1=-0.020762080536183673\n",
      "Regularized Logistic Regression(65/299): loss=0.6189806056239169, w0=-1.3656049831352151e-06, w1=-0.021041157898487324\n",
      "Regularized Logistic Regression(66/299): loss=0.6181912995023612, w0=-1.3861576329366245e-06, w1=-0.021319214600291514\n",
      "Regularized Logistic Regression(67/299): loss=0.6174099822170453, w0=-1.406707273487336e-06, w1=-0.021596258228977275\n",
      "Regularized Logistic Regression(68/299): loss=0.6166365587364037, w0=-1.4272539312017985e-06, w1=-0.021872296298075928\n",
      "Regularized Logistic Regression(69/299): loss=0.6158709352369057, w0=-1.4477976315916749e-06, w1=-0.02214733624805045\n",
      "Regularized Logistic Regression(70/299): loss=0.6151130190880282, w0=-1.468338399289021e-06, w1=-0.022421385447071084\n",
      "Regularized Logistic Regression(71/299): loss=0.6143627188373872, w0=-1.4888762580689427e-06, w1=-0.02269445119178551\n",
      "Regularized Logistic Regression(72/299): loss=0.613619944196022, w0=-1.5094112308717364e-06, w1=-0.02296654070808302\n",
      "Regularized Logistic Regression(73/299): loss=0.6128846060238294, w0=-1.5299433398245274e-06, w1=-0.023237661151852867\n",
      "Regularized Logistic Regression(74/299): loss=0.6121566163151573, w0=-1.550472606262412e-06, w1=-0.023507819609736517\n",
      "Regularized Logistic Regression(75/299): loss=0.6114358881845431, w0=-1.570999050749118e-06, w1=-0.023777023099873788\n",
      "Regularized Logistic Regression(76/299): loss=0.6107223358526107, w0=-1.5915226930971887e-06, w1=-0.02404527857264277\n",
      "Regularized Logistic Regression(77/299): loss=0.6100158746321103, w0=-1.6120435523877033e-06, w1=-0.02431259291139348\n",
      "Regularized Logistic Regression(78/299): loss=0.6093164209141163, w0=-1.6325616469895416e-06, w1=-0.024578972933175045\n",
      "Regularized Logistic Regression(79/299): loss=0.60862389215437, w0=-1.6530769945782034e-06, w1=-0.024844425389456446\n",
      "Regularized Logistic Regression(80/299): loss=0.6079382068597721, w0=-1.67358961215419e-06, w1=-0.025108956966840915\n",
      "Regularized Logistic Regression(81/299): loss=0.6072592845750224, w0=-1.6940995160609586e-06, w1=-0.025372574287773553\n",
      "Regularized Logistic Regression(82/299): loss=0.6065870458694079, w0=-1.714606722002458e-06, w1=-0.02563528391124259\n",
      "Regularized Logistic Regression(83/299): loss=0.605921412323738, w0=-1.7351112450602534e-06, w1=-0.025897092333473815\n",
      "Regularized Logistic Regression(84/299): loss=0.6052623065174214, w0=-1.755613099710251e-06, w1=-0.026158005988618505\n",
      "Regularized Logistic Regression(85/299): loss=0.6046096520156926, w0=-1.776112299839028e-06, w1=-0.026418031249434534\n",
      "Regularized Logistic Regression(86/299): loss=0.6039633733569781, w0=-1.796608858759778e-06, w1=-0.026677174427960918\n",
      "Regularized Logistic Regression(87/299): loss=0.6033233960404101, w0=-1.8171027892278821e-06, w1=-0.026935441776185582\n",
      "Regularized Logistic Regression(88/299): loss=0.6026896465134756, w0=-1.8375941034561079e-06, w1=-0.027192839486706352\n",
      "Regularized Logistic Regression(89/299): loss=0.6020620521598148, w0=-1.85808281312945e-06, w1=-0.027449373693385293\n",
      "Regularized Logistic Regression(90/299): loss=0.6014405412871526, w0=-1.8785689294196177e-06, w1=-0.027705050471996354\n",
      "Regularized Logistic Regression(91/299): loss=0.6008250431153742, w0=-1.8990524629991765e-06, w1=-0.027959875840866198\n",
      "Regularized Logistic Regression(92/299): loss=0.6002154877647363, w0=-1.9195334240553516e-06, w1=-0.02821385576150837\n",
      "Regularized Logistic Regression(93/299): loss=0.5996118062442173, w0=-1.940011822303502e-06, w1=-0.02846699613925078\n",
      "Regularized Logistic Regression(94/299): loss=0.5990139304400028, w0=-1.9604876670002697e-06, w1=-0.028719302823856464\n",
      "Regularized Logistic Regression(95/299): loss=0.5984217931041075, w0=-1.9809609669564152e-06, w1=-0.028970781610137802\n",
      "Regularized Logistic Regression(96/299): loss=0.59783532784313, w0=-2.0014317305493395e-06, w1=-0.02922143823856383\n",
      "Regularized Logistic Regression(97/299): loss=0.5972544691071436, w0=-2.021899965735309e-06, w1=-0.02947127839586129\n",
      "Regularized Logistic Regression(98/299): loss=0.5966791521787167, w0=-2.0423656800613795e-06, w1=-0.029720307715608834\n",
      "Regularized Logistic Regression(99/299): loss=0.5961093131620653, w0=-2.0628288806770344e-06, w1=-0.02996853177882475\n",
      "Regularized Logistic Regression(100/299): loss=0.5955448889723379, w0=-2.083289574345538e-06, w1=-0.030215956114548168\n",
      "Regularized Logistic Regression(101/299): loss=0.5949858173250265, w0=-2.1037477674550134e-06, w1=-0.030462586200413835\n",
      "Regularized Logistic Regression(102/299): loss=0.5944320367255078, w0=-2.1242034660292506e-06, w1=-0.030708427463220337\n",
      "Regularized Logistic Regression(103/299): loss=0.5938834864587099, w0=-2.144656675738247e-06, w1=-0.030953485279491995\n",
      "Regularized Logistic Regression(104/299): loss=0.5933401065789077, w0=-2.1651074019084935e-06, w1=-0.031197764976034353\n",
      "Regularized Logistic Regression(105/299): loss=0.5928018378996399, w0=-2.1855556495330067e-06, w1=-0.031441271830483335\n",
      "Regularized Logistic Regression(106/299): loss=0.5922686219837527, w0=-2.2060014232811128e-06, w1=-0.03168401107184815\n",
      "Regularized Logistic Regression(107/299): loss=0.5917404011335646, w0=-2.2264447275079916e-06, w1=-0.031925987881047946\n",
      "Regularized Logistic Regression(108/299): loss=0.5912171183811529, w0=-2.2468855662639846e-06, w1=-0.0321672073914423\n",
      "Regularized Logistic Regression(109/299): loss=0.5906987174787619, w0=-2.2673239433036723e-06, w1=-0.03240767468935549\n",
      "Regularized Logistic Regression(110/299): loss=0.5901851428893294, w0=-2.2877598620947273e-06, w1=-0.03264739481459484\n",
      "Regularized Logistic Regression(111/299): loss=0.5896763397771323, w0=-2.3081933258265445e-06, w1=-0.03288637276096278\n",
      "Regularized Logistic Regression(112/299): loss=0.5891722539985477, w0=-2.3286243374186597e-06, w1=-0.033124613476763175\n",
      "Regularized Logistic Regression(113/299): loss=0.5886728320929342, w0=-2.349052899528957e-06, w1=-0.03336212186530153\n",
      "Regularized Logistic Regression(114/299): loss=0.5881780212736226, w0=-2.3694790145616686e-06, w1=-0.03359890278537945\n",
      "Regularized Logistic Regression(115/299): loss=0.5876877694190249, w0=-2.3899026846751795e-06, w1=-0.03383496105178314\n",
      "Regularized Logistic Regression(116/299): loss=0.5872020250638548, w0=-2.4103239117896296e-06, w1=-0.03407030143576621\n",
      "Regularized Logistic Regression(117/299): loss=0.586720737390458, w0=-2.430742697594329e-06, w1=-0.034304928665526827\n",
      "Regularized Logistic Regression(118/299): loss=0.5862438562202564, w0=-2.4511590435549863e-06, w1=-0.034538847426679\n",
      "Regularized Logistic Regression(119/299): loss=0.5857713320052984, w0=-2.4715729509207503e-06, w1=-0.03477206236271844\n",
      "Regularized Logistic Regression(120/299): loss=0.5853031158199197, w0=-2.491984420731079e-06, w1=-0.03500457807548281\n",
      "Regularized Logistic Regression(121/299): loss=0.58483915935251, w0=-2.51239345382243e-06, w1=-0.03523639912560646\n",
      "Regularized Logistic Regression(122/299): loss=0.5843794148973874, w0=-2.5328000508347837e-06, w1=-0.03546753003296975\n",
      "Regularized Logistic Regression(123/299): loss=0.5839238353467747, w0=-2.553204212217997e-06, w1=-0.03569797527714296\n",
      "Regularized Logistic Regression(124/299): loss=0.5834723741828836, w0=-2.5736059382379993e-06, w1=-0.03592773929782486\n",
      "Regularized Logistic Regression(125/299): loss=0.5830249854700967, w0=-2.5940052289828254e-06, w1=-0.036156826495276066\n",
      "Regularized Logistic Regression(126/299): loss=0.5825816238472563, w0=-2.614402084368498e-06, w1=-0.03638524123074718\n",
      "Regularized Logistic Regression(127/299): loss=0.5821422445200508, w0=-2.6347965041447556e-06, w1=-0.03661298782690171\n",
      "Regularized Logistic Regression(128/299): loss=0.5817068032534992, w0=-2.6551884879006343e-06, w1=-0.03684007056823395\n",
      "Regularized Logistic Regression(129/299): loss=0.5812752563645378, w0=-2.6755780350699072e-06, w1=-0.03706649370148178\n",
      "Regularized Logistic Regression(130/299): loss=0.5808475607147007, w0=-2.69596514493638e-06, w1=-0.03729226143603451\n",
      "Regularized Logistic Regression(131/299): loss=0.5804236737028984, w0=-2.716349816639052e-06, w1=-0.03751737794433578\n",
      "Regularized Logistic Regression(132/299): loss=0.5800035532582897, w0=-2.7367320491771422e-06, w1=-0.037741847362281596\n",
      "Regularized Logistic Regression(133/299): loss=0.5795871578332498, w0=-2.7571118414149838e-06, w1=-0.03796567378961348\n",
      "Regularized Logistic Regression(134/299): loss=0.5791744463964296, w0=-2.7774891920867923e-06, w1=-0.03818886129030696\n",
      "Regularized Logistic Regression(135/299): loss=0.5787653784259077, w0=-2.7978640998013087e-06, w1=-0.03841141389295532\n",
      "Regularized Logistic Regression(136/299): loss=0.5783599139024321, w0=-2.8182365630463197e-06, w1=-0.038633335591148725\n",
      "Regularized Logistic Regression(137/299): loss=0.5779580133027531, w0=-2.8386065801930616e-06, w1=-0.03885463034384863\n",
      "Regularized Logistic Regression(138/299): loss=0.5775596375930442, w0=-2.8589741495005056e-06, w1=-0.039075302075757816\n",
      "Regularized Logistic Regression(139/299): loss=0.5771647482224096, w0=-2.879339269119532e-06, w1=-0.039295354677685745\n",
      "Regularized Logistic Regression(140/299): loss=0.5767733071164804, w0=-2.8997019370969934e-06, w1=-0.039514792006909744\n",
      "Regularized Logistic Regression(141/299): loss=0.5763852766710947, w0=-2.9200621513796688e-06, w1=-0.03973361788753159\n",
      "Regularized Logistic Regression(142/299): loss=0.5760006197460626, w0=-2.940419909818115e-06, w1=-0.039951836110829766\n",
      "Regularized Logistic Regression(143/299): loss=0.575619299659016, w0=-2.9607752101704143e-06, w1=-0.04016945043560768\n",
      "Regularized Logistic Regression(144/299): loss=0.5752412801793378, w0=-2.9811280501058203e-06, w1=-0.04038646458853729\n",
      "Regularized Logistic Regression(145/299): loss=0.5748665255221762, w0=-3.0014784272083097e-06, w1=-0.04060288226449907\n",
      "Regularized Logistic Regression(146/299): loss=0.5744950003425381, w0=-3.021826338980035e-06, w1=-0.040818707126917195\n",
      "Regularized Logistic Regression(147/299): loss=0.5741266697294606, w0=-3.0421717828446864e-06, w1=-0.041033942808091306\n",
      "Regularized Logistic Regression(148/299): loss=0.5737614992002631, w0=-3.062514756150762e-06, w1=-0.041248592909523915\n",
      "Regularized Logistic Regression(149/299): loss=0.573399454694877, w0=-3.0828552561747514e-06, w1=-0.0414626610022438\n",
      "Regularized Logistic Regression(150/299): loss=0.5730405025702497, w0=-3.1031932801242284e-06, w1=-0.041676150627125735\n",
      "Regularized Logistic Regression(151/299): loss=0.5726846095948268, w0=-3.1235288251408646e-06, w1=-0.04188906529520614\n",
      "Regularized Logistic Regression(152/299): loss=0.5723317429431081, w0=-3.143861888303358e-06, w1=-0.04210140848799498\n",
      "Regularized Logistic Regression(153/299): loss=0.5719818701902761, w0=-3.164192466630281e-06, w1=-0.04231318365778393\n",
      "Regularized Logistic Regression(154/299): loss=0.5716349593069007, w0=-3.184520557082852e-06, w1=-0.04252439422795075\n",
      "Regularized Logistic Regression(155/299): loss=0.5712909786537116, w0=-3.2048461565676293e-06, w1=-0.04273504359326014\n",
      "Regularized Logistic Regression(156/299): loss=0.5709498969764446, w0=-3.2251692619391294e-06, w1=-0.04294513512016071\n",
      "Regularized Logistic Regression(157/299): loss=0.5706116834007576, w0=-3.245489870002376e-06, w1=-0.04315467214707856\n",
      "Regularized Logistic Regression(158/299): loss=0.5702763074272151, w0=-3.2658079775153757e-06, w1=-0.0433636579847073\n",
      "Regularized Logistic Regression(159/299): loss=0.5699437389263405, w0=-3.286123581191525e-06, w1=-0.04357209591629461\n",
      "Regularized Logistic Regression(160/299): loss=0.5696139481337379, w0=-3.3064366777019495e-06, w1=-0.04377998919792514\n",
      "Regularized Logistic Regression(161/299): loss=0.5692869056452785, w0=-3.326747263677779e-06, w1=-0.04398734105880035\n",
      "Regularized Logistic Regression(162/299): loss=0.5689625824123531, w0=-3.3470553357123564e-06, w1=-0.044194154701514714\n",
      "Regularized Logistic Regression(163/299): loss=0.5686409497371901, w0=-3.367360890363385e-06, w1=-0.044400433302328755\n",
      "Regularized Logistic Regression(164/299): loss=0.5683219792682379, w0=-3.387663924155016e-06, w1=-0.044606180011438776\n",
      "Regularized Logistic Regression(165/299): loss=0.5680056429956077, w0=-3.4079644335798744e-06, w1=-0.04481139795324329\n",
      "Regularized Logistic Regression(166/299): loss=0.5676919132465826, w0=-3.428262415101028e-06, w1=-0.045016090226606374\n",
      "Regularized Logistic Regression(167/299): loss=0.5673807626811849, w0=-3.4485578651539002e-06, w1=-0.04522025990511786\n",
      "Regularized Logistic Regression(168/299): loss=0.5670721642878055, w0=-3.4688507801481267e-06, w1=-0.04542391003735038\n",
      "Regularized Logistic Regression(169/299): loss=0.5667660913788937, w0=-3.4891411564693596e-06, w1=-0.045627043647113265\n",
      "Regularized Logistic Regression(170/299): loss=0.5664625175867047, w0=-3.5094289904810183e-06, w1=-0.04582966373370371\n",
      "Regularized Logistic Regression(171/299): loss=0.5661614168591071, w0=-3.529714278525989e-06, w1=-0.04603177327215452\n",
      "Regularized Logistic Regression(172/299): loss=0.5658627634554451, w0=-3.5499970169282753e-06, w1=-0.04623337521347939\n",
      "Regularized Logistic Regression(173/299): loss=0.5655665319424614, w0=-3.5702772019945998e-06, w1=-0.04643447248491481\n",
      "Regularized Logistic Regression(174/299): loss=0.565272697190273, w0=-3.590554830015958e-06, w1=-0.04663506799015954\n",
      "Regularized Logistic Regression(175/299): loss=0.5649812343684031, w0=-3.6108298972691267e-06, w1=-0.04683516460961089\n",
      "Regularized Logistic Regression(176/299): loss=0.5646921189418677, w0=-3.631102400018127e-06, w1=-0.047034765200598426\n",
      "Regularized Logistic Regression(177/299): loss=0.5644053266673162, w0=-3.6513723345156437e-06, w1=-0.047233872597614815\n",
      "Regularized Logistic Regression(178/299): loss=0.5641208335892246, w0=-3.671639697004401e-06, w1=-0.047432489612543996\n",
      "Regularized Logistic Regression(179/299): loss=0.5638386160361406, w0=-3.6919044837184974e-06, w1=-0.04763061903488666\n",
      "Regularized Logistic Regression(180/299): loss=0.5635586506169817, w0=-3.7121666908847017e-06, w1=-0.04782826363198304\n",
      "Regularized Logistic Regression(181/299): loss=0.5632809142173824, w0=-3.732426314723704e-06, w1=-0.04802542614923309\n",
      "Regularized Logistic Regression(182/299): loss=0.563005383996093, w0=-3.7526833514513355e-06, w1=-0.04822210931031419\n",
      "Regularized Logistic Regression(183/299): loss=0.562732037381428, w0=-3.7729377972797433e-06, w1=-0.04841831581739606\n",
      "Regularized Logistic Regression(184/299): loss=0.5624608520677606, w0=-3.793189648418535e-06, w1=-0.048614048351353446\n",
      "Regularized Logistic Regression(185/299): loss=0.5621918060120712, w0=-3.8134389010758837e-06, w1=-0.048809309571976114\n",
      "Regularized Logistic Regression(186/299): loss=0.5619248774305355, w0=-3.8336855514595976e-06, w1=-0.04900410211817649\n",
      "Regularized Logistic Regression(187/299): loss=0.561660044795167, w0=-3.85392959577816e-06, w1=-0.04919842860819482\n",
      "Regularized Logistic Regression(188/299): loss=0.5613972868305012, w0=-3.8741710302417325e-06, w1=-0.04939229163980207\n",
      "Regularized Logistic Regression(189/299): loss=0.5611365825103257, w0=-3.894409851063125e-06, w1=-0.04958569379050024\n",
      "Regularized Logistic Regression(190/299): loss=0.5608779110544581, w0=-3.914646054458739e-06, w1=-0.049778637617720535\n",
      "Regularized Logistic Regression(191/299): loss=0.5606212519255644, w0=-3.9348796366494765e-06, w1=-0.04997112565901929\n",
      "Regularized Logistic Regression(192/299): loss=0.5603665848260236, w0=-3.955110593861622e-06, w1=-0.05016316043227141\n",
      "Regularized Logistic Regression(193/299): loss=0.5601138896948352, w0=-3.97533892232769e-06, w1=-0.050354744435861784\n",
      "Regularized Logistic Regression(194/299): loss=0.5598631467045682, w0=-3.995564618287254e-06, w1=-0.05054588014887432\n",
      "Regularized Logistic Regression(195/299): loss=0.5596143362583531, w0=-4.015787677987735e-06, w1=-0.05073657003127896\n",
      "Regularized Logistic Regression(196/299): loss=0.5593674389869138, w0=-4.036008097685176e-06, w1=-0.05092681652411638\n",
      "Regularized Logistic Regression(197/299): loss=0.5591224357456416, w0=-4.056225873644981e-06, w1=-0.05111662204968086\n",
      "Regularized Logistic Regression(198/299): loss=0.5588793076117092, w0=-4.076441002142635e-06, w1=-0.05130598901170058\n",
      "Regularized Logistic Regression(199/299): loss=0.5586380358812242, w0=-4.096653479464394e-06, w1=-0.051494919795516375\n",
      "Regularized Logistic Regression(200/299): loss=0.5583986020664206, w0=-4.116863301907954e-06, w1=-0.05168341676825807\n",
      "Regularized Logistic Regression(201/299): loss=0.5581609878928901, w0=-4.137070465783098e-06, w1=-0.05187148227901893\n",
      "Regularized Logistic Regression(202/299): loss=0.5579251752968519, w0=-4.157274967412313e-06, w1=-0.052059118659028174\n",
      "Regularized Logistic Regression(203/299): loss=0.5576911464224577, w0=-4.177476803131397e-06, w1=-0.052246328221821486\n",
      "Regularized Logistic Regression(204/299): loss=0.5574588836191362, w0=-4.197675969290028e-06, w1=-0.05243311326340954\n",
      "Regularized Logistic Regression(205/299): loss=0.5572283694389718, w0=-4.21787246225233e-06, w1=-0.05261947606244464\n",
      "Regularized Logistic Regression(206/299): loss=0.5569995866341209, w0=-4.238066278397402e-06, w1=-0.05280541888038556\n",
      "Regularized Logistic Regression(207/299): loss=0.5567725181542622, w0=-4.258257414119837e-06, w1=-0.05299094396166038\n",
      "Regularized Logistic Regression(208/299): loss=0.5565471471440838, w0=-4.2784458658302204e-06, w1=-0.053176053533827566\n",
      "Regularized Logistic Regression(209/299): loss=0.5563234569408015, w0=-4.2986316299556066e-06, w1=-0.05336074980773521\n",
      "Regularized Logistic Regression(210/299): loss=0.5561014310717156, w0=-4.318814702939976e-06, w1=-0.05354503497767856\n",
      "Regularized Logistic Regression(211/299): loss=0.5558810532517959, w0=-4.338995081244682e-06, w1=-0.053728911221555584\n",
      "Regularized Logistic Regression(212/299): loss=0.5556623073813037, w0=-4.359172761348868e-06, w1=-0.05391238070102095\n",
      "Regularized Logistic Regression(213/299): loss=0.5554451775434455, w0=-4.37934773974988e-06, w1=-0.05409544556163837\n",
      "Regularized Logistic Regression(214/299): loss=0.5552296480020561, w0=-4.399520012963655e-06, w1=-0.05427810793303089\n",
      "Regularized Logistic Regression(215/299): loss=0.5550157031993177, w0=-4.4196895775250935e-06, w1=-0.0544603699290299\n",
      "Regularized Logistic Regression(216/299): loss=0.5548033277535053, w0=-4.439856429988421e-06, w1=-0.05464223364782224\n",
      "Regularized Logistic Regression(217/299): loss=0.5545925064567677, w0=-4.460020566927531e-06, w1=-0.054823701172095764\n",
      "Regularized Logistic Regression(218/299): loss=0.5543832242729351, w0=-4.48018198493631e-06, w1=-0.0550047745691832\n",
      "Regularized Logistic Regression(219/299): loss=0.5541754663353574, w0=-4.500340680628956e-06, w1=-0.055185455891204556\n",
      "Regularized Logistic Regression(220/299): loss=0.553969217944773, w0=-4.520496650640276e-06, w1=-0.055365747175207786\n",
      "Regularized Logistic Regression(221/299): loss=0.5537644645672055, w0=-4.5406498916259735e-06, w1=-0.05554565044330803\n",
      "Regularized Logistic Regression(222/299): loss=0.5535611918318898, w0=-4.56080040026292e-06, w1=-0.055725167702825325\n",
      "Regularized Logistic Regression(223/299): loss=0.5533593855292257, w0=-4.580948173249418e-06, w1=-0.055904300946420536\n",
      "Regularized Logistic Regression(224/299): loss=0.5531590316087595, w0=-4.6010932073054455e-06, w1=-0.05608305215223029\n",
      "Regularized Logistic Regression(225/299): loss=0.5529601161771932, w0=-4.621235499172893e-06, w1=-0.0562614232839999\n",
      "Regularized Logistic Regression(226/299): loss=0.5527626254964202, w0=-4.641375045615788e-06, w1=-0.05643941629121518\n",
      "Regularized Logistic Regression(227/299): loss=0.5525665459815885, w0=-4.6615118434205045e-06, w1=-0.05661703310923267\n",
      "Regularized Logistic Regression(228/299): loss=0.5523718641991889, w0=-4.681645889395966e-06, w1=-0.05679427565940847\n",
      "Regularized Logistic Regression(229/299): loss=0.5521785668651711, w0=-4.701777180373835e-06, w1=-0.056971145849225704\n",
      "Regularized Logistic Regression(230/299): loss=0.5519866408430818, w0=-4.7219057132086915e-06, w1=-0.057147645572420476\n",
      "Regularized Logistic Regression(231/299): loss=0.5517960731422324, w0=-4.742031484778202e-06, w1=-0.05732377670910665\n",
      "Regularized Logistic Regression(232/299): loss=0.5516068509158879, w0=-4.762154491983279e-06, w1=-0.05749954112589911\n",
      "Regularized Logistic Regression(233/299): loss=0.5514189614594815, w0=-4.7822747317482315e-06, w1=-0.057674940676035735\n",
      "Regularized Logistic Regression(234/299): loss=0.5512323922088537, w0=-4.802392201020901e-06, w1=-0.057849977199498095\n",
      "Regularized Logistic Regression(235/299): loss=0.5510471307385151, w0=-4.822506896772798e-06, w1=-0.05802465252313077\n",
      "Regularized Logistic Regression(236/299): loss=0.5508631647599312, w0=-4.842618815999217e-06, w1=-0.058198968460759434\n",
      "Regularized Logistic Regression(237/299): loss=0.5506804821198329, w0=-4.862727955719355e-06, w1=-0.05837292681330765\n",
      "Regularized Logistic Regression(238/299): loss=0.5504990707985463, w0=-4.882834312976414e-06, w1=-0.05854652936891242\n",
      "Regularized Logistic Regression(239/299): loss=0.550318918908348, w0=-4.902937884837697e-06, w1=-0.0587197779030385\n",
      "Regularized Logistic Regression(240/299): loss=0.5501400146918418, w0=-4.923038668394698e-06, w1=-0.05889267417859145\n",
      "Regularized Logistic Regression(241/299): loss=0.5499623465203552, w0=-4.943136660763182e-06, w1=-0.059065219946029555\n",
      "Regularized Logistic Regression(242/299): loss=0.5497859028923598, w0=-4.963231859083261e-06, w1=-0.0592374169434744\n",
      "Regularized Logistic Regression(243/299): loss=0.5496106724319122, w0=-4.983324260519453e-06, w1=-0.05940926689682046\n",
      "Regularized Logistic Regression(244/299): loss=0.549436643887115, w0=-5.00341386226075e-06, w1=-0.059580771519843395\n",
      "Regularized Logistic Regression(245/299): loss=0.5492638061285989, w0=-5.023500661520665e-06, w1=-0.05975193251430722\n",
      "Regularized Logistic Regression(246/299): loss=0.5490921481480259, w0=-5.043584655537282e-06, w1=-0.05992275157007035\n",
      "Regularized Logistic Regression(247/299): loss=0.5489216590566103, w0=-5.0636658415732875e-06, w1=-0.06009323036519053\n",
      "Regularized Logistic Regression(248/299): loss=0.5487523280836628, w0=-5.083744216916013e-06, w1=-0.060263370566028644\n",
      "Regularized Logistic Regression(249/299): loss=0.5485841445751498, w0=-5.103819778877457e-06, w1=-0.06043317382735141\n",
      "Regularized Logistic Regression(250/299): loss=0.5484170979922764, w0=-5.123892524794307e-06, w1=-0.060602641792433035\n",
      "Regularized Logistic Regression(251/299): loss=0.5482511779100843, w0=-5.143962452027953e-06, w1=-0.06077177609315575\n",
      "Regularized Logistic Regression(252/299): loss=0.5480863740160706, w0=-5.164029557964503e-06, w1=-0.06094057835010934\n",
      "Regularized Logistic Regression(253/299): loss=0.5479226761088251, w0=-5.184093840014784e-06, w1=-0.061109050172689576\n",
      "Regularized Logistic Regression(254/299): loss=0.547760074096684, w0=-5.20415529561434e-06, w1=-0.061277193159195685\n",
      "Regularized Logistic Regression(255/299): loss=0.5475985579964034, w0=-5.2242139222234306e-06, w1=-0.06144500889692675\n",
      "Regularized Logistic Regression(256/299): loss=0.5474381179318486, w0=-5.244269717327018e-06, w1=-0.06161249896227713\n",
      "Regularized Logistic Regression(257/299): loss=0.5472787441327017, w0=-5.264322678434755e-06, w1=-0.06177966492083082\n",
      "Regularized Logistic Regression(258/299): loss=0.5471204269331873, w0=-5.284372803080961e-06, w1=-0.061946508327454936\n",
      "Regularized Logistic Regression(259/299): loss=0.5469631567708131, w0=-5.304420088824604e-06, w1=-0.062113030726392175\n",
      "Regularized Logistic Regression(260/299): loss=0.5468069241851272, w0=-5.324464533249265e-06, w1=-0.06227923365135232\n",
      "Regularized Logistic Regression(261/299): loss=0.5466517198164946, w0=-5.344506133963115e-06, w1=-0.06244511862560276\n",
      "Regularized Logistic Regression(262/299): loss=0.5464975344048862, w0=-5.364544888598871e-06, w1=-0.06261068716205817\n",
      "Regularized Logistic Regression(263/299): loss=0.5463443587886858, w0=-5.38458079481376e-06, w1=-0.06277594076336912\n",
      "Regularized Logistic Regression(264/299): loss=0.5461921839035127, w0=-5.404613850289475e-06, w1=-0.06294088092201003\n",
      "Regularized Logistic Regression(265/299): loss=0.5460410007810589, w0=-5.424644052732126e-06, w1=-0.0631055091203658\n",
      "Regularized Logistic Regression(266/299): loss=0.5458908005479423, w0=-5.444671399872191e-06, w1=-0.06326982683081808\n",
      "Regularized Logistic Regression(267/299): loss=0.5457415744245757, w0=-5.464695889464463e-06, w1=-0.0634338355158302\n",
      "Regularized Logistic Regression(268/299): loss=0.5455933137240478, w0=-5.484717519287989e-06, w1=-0.06359753662803143\n",
      "Regularized Logistic Regression(269/299): loss=0.5454460098510225, w0=-5.504736287146012e-06, w1=-0.06376093161030043\n",
      "Regularized Logistic Regression(270/299): loss=0.54529965430065, w0=-5.524752190865909e-06, w1=-0.0639240218958477\n",
      "Regularized Logistic Regression(271/299): loss=0.545154238657493, w0=-5.54476522829912e-06, w1=-0.06408680890829731\n",
      "Regularized Logistic Regression(272/299): loss=0.5450097545944674, w0=-5.564775397321085e-06, w1=-0.06424929406176778\n",
      "Regularized Logistic Regression(273/299): loss=0.5448661938717952, w0=-5.584782695831167e-06, w1=-0.06441147876095213\n",
      "Regularized Logistic Regression(274/299): loss=0.544723548335974, w0=-5.60478712175258e-06, w1=-0.06457336440119703\n",
      "Regularized Logistic Regression(275/299): loss=0.5445818099187562, w0=-5.6247886730323116e-06, w1=-0.06473495236858126\n",
      "Regularized Logistic Regression(276/299): loss=0.5444409706361446, w0=-5.644787347641044e-06, w1=-0.06489624403999349\n",
      "Regularized Logistic Regression(277/299): loss=0.5443010225873988, w0=-5.664783143573074e-06, w1=-0.06505724078320889\n",
      "Regularized Logistic Regression(278/299): loss=0.5441619579540571, w0=-5.684776058846223e-06, w1=-0.06521794395696537\n",
      "Regularized Logistic Regression(279/299): loss=0.5440237689989676, w0=-5.704766091501758e-06, w1=-0.0653783549110389\n",
      "Regularized Logistic Regression(280/299): loss=0.5438864480653354, w0=-5.7247532396043005e-06, w1=-0.065538474986318\n",
      "Regularized Logistic Regression(281/299): loss=0.5437499875757786, w0=-5.744737501241738e-06, w1=-0.06569830551487763\n",
      "Regularized Logistic Regression(282/299): loss=0.543614380031401, w0=-5.764718874525128e-06, w1=-0.06585784782005219\n",
      "Regularized Logistic Regression(283/299): loss=0.5434796180108713, w0=-5.784697357588608e-06, w1=-0.06601710321650793\n",
      "Regularized Logistic Regression(284/299): loss=0.5433456941695194, w0=-5.8046729485893e-06, w1=-0.06617607301031458\n",
      "Regularized Logistic Regression(285/299): loss=0.5432126012384404, w0=-5.824645645707211e-06, w1=-0.06633475849901625\n",
      "Regularized Logistic Regression(286/299): loss=0.5430803320236125, w0=-5.844615447145136e-06, w1=-0.06649316097170166\n",
      "Regularized Logistic Regression(287/299): loss=0.5429488794050256, w0=-5.864582351128558e-06, w1=-0.06665128170907358\n",
      "Regularized Logistic Regression(288/299): loss=0.5428182363358199, w0=-5.884546355905543e-06, w1=-0.06680912198351777\n",
      "Regularized Logistic Regression(289/299): loss=0.542688395841438, w0=-5.904507459746643e-06, w1=-0.06696668305917104\n",
      "Regularized Logistic Regression(290/299): loss=0.5425593510187859, w0=-5.924465660944782e-06, w1=-0.06712396619198881\n",
      "Regularized Logistic Regression(291/299): loss=0.5424310950354048, w0=-5.9444209578151615e-06, w1=-0.06728097262981185\n",
      "Regularized Logistic Regression(292/299): loss=0.5423036211286548, w0=-5.964373348695144e-06, w1=-0.06743770361243244\n",
      "Regularized Logistic Regression(293/299): loss=0.5421769226049085, w0=-5.984322831944151e-06, w1=-0.06759416037165995\n",
      "Regularized Logistic Regression(294/299): loss=0.5420509928387531, w0=-6.00426940594355e-06, w1=-0.06775034413138563\n",
      "Regularized Logistic Regression(295/299): loss=0.5419258252722063, w0=-6.024213069096546e-06, w1=-0.06790625610764682\n",
      "Regularized Logistic Regression(296/299): loss=0.5418014134139388, w0=-6.0441538198280695e-06, w1=-0.06806189750869064\n",
      "Regularized Logistic Regression(297/299): loss=0.5416777508385082, w0=-6.064091656584665e-06, w1=-0.06821726953503693\n",
      "Regularized Logistic Regression(298/299): loss=0.5415548311856032, w0=-6.0840265778343764e-06, w1=-0.06837237337954057\n",
      "Regularized Logistic Regression(299/299): loss=0.5414326481592954, w0=-6.1039585820666355e-06, w1=-0.0685272102274533\n",
      "Regularized Logistic Regression(0/99): loss=0.693147180559945, w0=-2.0882930292778682e-08, w1=-0.0003658898216597753\n",
      "Regularized Logistic Regression(1/99): loss=0.6915636157846066, w0=-4.173856310796442e-08, w1=-0.0007297401642547018\n",
      "Regularized Logistic Regression(2/99): loss=0.6900004791156684, w0=-6.256710416062533e-08, w1=-0.0010915683924349279\n",
      "Regularized Logistic Regression(3/99): loss=0.6884574895874042, w0=-8.336875449622289e-08, w1=-0.001451391716536063\n",
      "Regularized Logistic Regression(4/99): loss=0.6869343700521863, w0=-1.0414371058844798e-07, w1=-0.001809227192523169\n",
      "Regularized Logistic Regression(5/99): loss=0.6854308471365446, w0=-1.2489216443559245e-07, w1=-0.002165091722019075\n",
      "Regularized Logistic Regression(6/99): loss=0.6839466511973373, w0=-1.4561430365545155e-07, w1=-0.0025190020524135034\n",
      "Regularized Logistic Regression(7/99): loss=0.6824815162780676, w0=-1.6631031157875474e-07, w1=-0.0028709747770488208\n",
      "Regularized Logistic Regression(8/99): loss=0.6810351800653592, w0=-1.8698036734112384e-07, w1=-0.003221026335478526\n",
      "Regularized Logistic Regression(9/99): loss=0.6796073838456137, w0=-2.0762464597355822e-07, w1=-0.003569173013794989\n",
      "Regularized Logistic Regression(10/99): loss=0.678197872461869, w0=-2.2824331849144931e-07, w1=-0.003915430945022837\n",
      "Regularized Logistic Regression(11/99): loss=0.6768063942708706, w0=-2.4883655198212663e-07, w1=-0.004259816109574463\n",
      "Regularized Logistic Regression(12/99): loss=0.6754327011003756, w0=-2.6940450969093956e-07, w1=-0.004602344335764269\n",
      "Regularized Logistic Regression(13/99): loss=0.6740765482067022, w0=-2.899473511058801e-07, w1=-0.004943031300378625\n",
      "Regularized Logistic Regression(14/99): loss=0.6727376942325368, w0=-3.104652320407528e-07, w1=-0.005281892529298227\n",
      "Regularized Logistic Regression(15/99): loss=0.6714159011650122, w0=-3.3095830471689855e-07, w1=-0.005618943398169977\n",
      "Regularized Logistic Regression(16/99): loss=0.6701109342940662, w0=-3.514267178434814e-07, w1=-0.005954199133125437\n",
      "Regularized Logistic Regression(17/99): loss=0.6688225621710955, w0=-3.7187061669634623e-07, w1=-0.00628767481154319\n",
      "Regularized Logistic Regression(18/99): loss=0.6675505565679035, w0=-3.922901431954579e-07, w1=-0.006619385362852056\n",
      "Regularized Logistic Regression(19/99): loss=0.6662946924359657, w0=-4.1268543598093227e-07, w1=-0.006949345569373161\n",
      "Regularized Logistic Regression(20/99): loss=0.6650547478660078, w0=-4.3305663048767016e-07, w1=-0.0072775700671979865\n",
      "Regularized Logistic Regression(21/99): loss=0.6638305040479098, w0=-4.534038590186067e-07, w1=-0.007604073347099937\n",
      "Regularized Logistic Regression(22/99): loss=0.6626217452309376, w0=-4.7372725081658807e-07, w1=-0.00792886975547745\n",
      "Regularized Logistic Regression(23/99): loss=0.6614282586843176, w0=-4.940269321348893e-07, w1=-0.00825197349532621\n",
      "Regularized Logistic Regression(24/99): loss=0.6602498346581466, w0=-5.143030263063865e-07, w1=-0.008573398627238524\n",
      "Regularized Logistic Regression(25/99): loss=0.6590862663446516, w0=-5.345556538113981e-07, w1=-0.008893159070427776\n",
      "Regularized Logistic Regression(26/99): loss=0.6579373498397996, w0=-5.547849323442085e-07, w1=-0.00921126860377576\n",
      "Regularized Logistic Regression(27/99): loss=0.6568028841052603, w0=-5.749909768782916e-07, w1=-0.009527740866901538\n",
      "Regularized Logistic Regression(28/99): loss=0.6556826709307261, w0=-5.951738997302463e-07, w1=-0.009842589361249762\n",
      "Regularized Logistic Regression(29/99): loss=0.6545765148965962, w0=-6.153338106224623e-07, w1=-0.010155827451196519\n",
      "Regularized Logistic Regression(30/99): loss=0.6534842233370166, w0=-6.354708167445316e-07, w1=-0.01046746836517173\n",
      "Regularized Logistic Regression(31/99): loss=0.6524056063032901, w0=-6.555850228134207e-07, w1=-0.010777525196795883\n",
      "Regularized Logistic Regression(32/99): loss=0.6513404765276504, w0=-6.756765311324217e-07, w1=-0.011086010906029925\n",
      "Regularized Logistic Regression(33/99): loss=0.6502886493874023, w0=-6.957454416488976e-07, w1=-0.01139293832033699\n",
      "Regularized Logistic Regression(34/99): loss=0.6492499428694343, w0=-7.157918520108401e-07, w1=-0.01169832013585443\n",
      "Regularized Logistic Regression(35/99): loss=0.6482241775350969, w0=-7.358158576222551e-07, w1=-0.01200216891857487\n",
      "Regularized Logistic Regression(36/99): loss=0.647211176485453, w0=-7.558175516973947e-07, w1=-0.012304497105535245\n",
      "Regularized Logistic Regression(37/99): loss=0.6462107653268965, w0=-7.757970253138517e-07, w1=-0.012605317006012536\n",
      "Regularized Logistic Regression(38/99): loss=0.6452227721371454, w0=-7.957543674645353e-07, w1=-0.012904640802724944\n",
      "Regularized Logistic Regression(39/99): loss=0.6442470274315985, w0=-8.156896651085434e-07, w1=-0.0132024805530378\n",
      "Regularized Logistic Regression(40/99): loss=0.6432833641300671, w0=-8.356030032209507e-07, w1=-0.013498848190172889\n",
      "Regularized Logistic Regression(41/99): loss=0.6423316175238737, w0=-8.554944648415292e-07, w1=-0.013793755524420387\n",
      "Regularized Logistic Regression(42/99): loss=0.6413916252433196, w0=-8.753641311224179e-07, w1=-0.01408721424435259\n",
      "Regularized Logistic Regression(43/99): loss=0.6404632272255185, w0=-8.952120813747607e-07, w1=-0.014379235918038477\n",
      "Regularized Logistic Regression(44/99): loss=0.639546265682598, w0=-9.150383931143286e-07, w1=-0.014669831994258106\n",
      "Regularized Logistic Regression(45/99): loss=0.638640585070268, w0=-9.348431421061427e-07, w1=-0.014959013803716585\n",
      "Regularized Logistic Regression(46/99): loss=0.6377460320567473, w0=-9.54626402408118e-07, w1=-0.015246792560256359\n",
      "Regularized Logistic Regression(47/99): loss=0.6368624554920586, w0=-9.74388246413742e-07, w1=-0.015533179362067611\n",
      "Regularized Logistic Regression(48/99): loss=0.6359897063776816, w0=-9.941287448938072e-07, w1=-0.015818185192895712\n",
      "Regularized Logistic Regression(49/99): loss=0.6351276378365646, w0=-1.0138479670372135e-06, w1=-0.016101820923245292\n",
      "Regularized Logistic Regression(50/99): loss=0.6342761050834951, w0=-1.0335459804908576e-06, w1=-0.0163840973115806\n",
      "Regularized Logistic Regression(51/99): loss=0.6334349653958252, w0=-1.0532228513986263e-06, w1=-0.016665025005521087\n",
      "Regularized Logistic Regression(52/99): loss=0.6326040780845494, w0=-1.07287864443951e-06, w1=-0.01694461454303234\n",
      "Regularized Logistic Regression(53/99): loss=0.6317833044657353, w0=-1.092513422864852e-06, w1=-0.017222876353611253\n",
      "Regularized Logistic Regression(54/99): loss=0.630972507832303, w0=-1.1121272485347527e-06, w1=-0.017499820759465563\n",
      "Regularized Logistic Regression(55/99): loss=0.6301715534261508, w0=-1.1317201819536405e-06, w1=-0.017775457976686955\n",
      "Regularized Logistic Regression(56/99): loss=0.6293803084106289, w0=-1.1512922823050291e-06, w1=-0.018049798116417538\n",
      "Regularized Logistic Regression(57/99): loss=0.6285986418433487, w0=-1.1708436074854756e-06, w1=-0.01832285118600926\n",
      "Regularized Logistic Regression(58/99): loss=0.6278264246493399, w0=-1.1903742141377542e-06, w1=-0.018594627090175918\n",
      "Regularized Logistic Regression(59/99): loss=0.6270635295945414, w0=-1.2098841576832617e-06, w1=-0.018865135632137594\n",
      "Regularized Logistic Regression(60/99): loss=0.6263098312596257, w0=-1.229373492353671e-06, w1=-0.01913438651475697\n",
      "Regularized Logistic Regression(61/99): loss=0.6255652060141598, w0=-1.2488422712218457e-06, w1=-0.019402389341667478\n",
      "Regularized Logistic Regression(62/99): loss=0.6248295319910918, w0=-1.2682905462320318e-06, w1=-0.01966915361839297\n",
      "Regularized Logistic Regression(63/99): loss=0.624102689061569, w0=-1.2877183682293404e-06, w1=-0.019934688753458676\n",
      "Regularized Logistic Regression(64/99): loss=0.6233845588100775, w0=-1.3071257869885382e-06, w1=-0.02019900405949321\n",
      "Regularized Logistic Regression(65/99): loss=0.6226750245099041, w0=-1.3265128512421558e-06, w1=-0.020462108754321542\n",
      "Regularized Logistic Regression(66/99): loss=0.6219739710989193, w0=-1.3458796087079318e-06, w1=-0.02072401196204861\n",
      "Regularized Logistic Regression(67/99): loss=0.6212812851556739, w0=-1.3652261061156047e-06, w1=-0.020984722714133507\n",
      "Regularized Logistic Regression(68/99): loss=0.6205968548758122, w0=-1.3845523892330667e-06, w1=-0.021244249950454208\n",
      "Regularized Logistic Regression(69/99): loss=0.6199205700487935, w0=-1.4038585028918916e-06, w1=-0.02150260252036241\n",
      "Regularized Logistic Regression(70/99): loss=0.6192523220349225, w0=-1.4231444910122529e-06, w1=-0.021759789183728753\n",
      "Regularized Logistic Regression(71/99): loss=0.618592003742685, w0=-1.4424103966272414e-06, w1=-0.022015818611977978\n",
      "Regularized Logistic Regression(72/99): loss=0.6179395096063864, w0=-1.4616562619065988e-06, w1=-0.02227069938911419\n",
      "Regularized Logistic Regression(73/99): loss=0.6172947355640862, w0=-1.480882128179876e-06, w1=-0.022524440012736017\n",
      "Regularized Logistic Regression(74/99): loss=0.6166575790358355, w0=-1.5000880359590325e-06, w1=-0.022777048895041643\n",
      "Regularized Logistic Regression(75/99): loss=0.6160279389021999, w0=-1.5192740249604858e-06, w1=-0.023028534363823582\n",
      "Regularized Logistic Regression(76/99): loss=0.6154057154830803, w0=-1.5384401341266247e-06, w1=-0.023278904663453406\n",
      "Regularized Logistic Regression(77/99): loss=0.6147908105168165, w0=-1.5575864016467974e-06, w1=-0.023528167955856022\n",
      "Regularized Logistic Regression(78/99): loss=0.6141831271395779, w0=-1.5767128649777858e-06, w1=-0.023776332321473772\n",
      "Regularized Logistic Regression(79/99): loss=0.6135825698650336, w0=-1.5958195608637786e-06, w1=-0.02402340576022025\n",
      "Regularized Logistic Regression(80/99): loss=0.6129890445643065, w0=-1.6149065253558526e-06, w1=-0.02426939619242373\n",
      "Regularized Logistic Regression(81/99): loss=0.6124024584461967, w0=-1.6339737938309741e-06, w1=-0.024514311459760324\n",
      "Regularized Logistic Regression(82/99): loss=0.6118227200376815, w0=-1.6530214010105313e-06, w1=-0.024758159326176886\n",
      "Regularized Logistic Regression(83/99): loss=0.6112497391646871, w0=-1.6720493809784074e-06, w1=-0.02500094747880369\n",
      "Regularized Logistic Regression(84/99): loss=0.6106834269331226, w0=-1.691057767198606e-06, w1=-0.025242683528856575\n",
      "Regularized Logistic Regression(85/99): loss=0.6101236957101821, w0=-1.7100465925324375e-06, w1=-0.025483375012529223\n",
      "Regularized Logistic Regression(86/99): loss=0.6095704591059059, w0=-1.7290158892552771e-06, w1=-0.02572302939187494\n",
      "Regularized Logistic Regression(87/99): loss=0.6090236319550011, w0=-1.7479656890729052e-06, w1=-0.02596165405567848\n",
      "Regularized Logistic Regression(88/99): loss=0.6084831302989169, w0=-1.7668960231374369e-06, w1=-0.026199256320317537\n",
      "Regularized Logistic Regression(89/99): loss=0.6079488713681732, w0=-1.7858069220628535e-06, w1=-0.02643584343061446\n",
      "Regularized Logistic Regression(90/99): loss=0.6074207735649367, w0=-1.804698415940143e-06, w1=-0.026671422560677564\n",
      "Regularized Logistic Regression(91/99): loss=0.6068987564458457, w0=-1.8235705343520579e-06, w1=-0.026906000814732798\n",
      "Regularized Logistic Regression(92/99): loss=0.606382740705078, w0=-1.8424233063875025e-06, w1=-0.02713958522794532\n",
      "Regularized Logistic Regression(93/99): loss=0.6058726481576595, w0=-1.8612567606555543e-06, w1=-0.02737218276723134\n",
      "Regularized Logistic Regression(94/99): loss=0.6053684017230103, w0=-1.8800709252991306e-06, w1=-0.027603800332060023\n",
      "Regularized Logistic Regression(95/99): loss=0.604869925408726, w0=-1.8988658280083073e-06, w1=-0.027834444755245907\n",
      "Regularized Logistic Regression(96/99): loss=0.604377144294594, w0=-1.9176414960332997e-06, w1=-0.028064122803731398\n",
      "Regularized Logistic Regression(97/99): loss=0.6038899845168337, w0=-1.93639795619711e-06, w1=-0.02829284117935993\n",
      "Regularized Logistic Regression(98/99): loss=0.6034083732525691, w0=-1.955135234907854e-06, w1=-0.02852060651963952\n",
      "Regularized Logistic Regression(99/99): loss=0.602932238704522, w0=-1.9738533581707697e-06, w1=-0.02874742539849696\n",
      "Regularized Logistic Regression(0/199): loss=0.693147180559945, w0=-2.0882930292778682e-08, w1=-0.0003658898216597753\n",
      "Regularized Logistic Regression(1/199): loss=0.6915636157846066, w0=-4.173856310796442e-08, w1=-0.0007297401642547018\n",
      "Regularized Logistic Regression(2/199): loss=0.6900004791156684, w0=-6.256710416062533e-08, w1=-0.0010915683924349279\n",
      "Regularized Logistic Regression(3/199): loss=0.6884574895874042, w0=-8.336875449622289e-08, w1=-0.001451391716536063\n",
      "Regularized Logistic Regression(4/199): loss=0.6869343700521863, w0=-1.0414371058844798e-07, w1=-0.001809227192523169\n",
      "Regularized Logistic Regression(5/199): loss=0.6854308471365446, w0=-1.2489216443559245e-07, w1=-0.002165091722019075\n",
      "Regularized Logistic Regression(6/199): loss=0.6839466511973373, w0=-1.4561430365545155e-07, w1=-0.0025190020524135034\n",
      "Regularized Logistic Regression(7/199): loss=0.6824815162780676, w0=-1.6631031157875474e-07, w1=-0.0028709747770488208\n",
      "Regularized Logistic Regression(8/199): loss=0.6810351800653592, w0=-1.8698036734112384e-07, w1=-0.003221026335478526\n",
      "Regularized Logistic Regression(9/199): loss=0.6796073838456137, w0=-2.0762464597355822e-07, w1=-0.003569173013794989\n",
      "Regularized Logistic Regression(10/199): loss=0.678197872461869, w0=-2.2824331849144931e-07, w1=-0.003915430945022837\n",
      "Regularized Logistic Regression(11/199): loss=0.6768063942708706, w0=-2.4883655198212663e-07, w1=-0.004259816109574463\n",
      "Regularized Logistic Regression(12/199): loss=0.6754327011003756, w0=-2.6940450969093956e-07, w1=-0.004602344335764269\n",
      "Regularized Logistic Regression(13/199): loss=0.6740765482067022, w0=-2.899473511058801e-07, w1=-0.004943031300378625\n",
      "Regularized Logistic Regression(14/199): loss=0.6727376942325368, w0=-3.104652320407528e-07, w1=-0.005281892529298227\n",
      "Regularized Logistic Regression(15/199): loss=0.6714159011650122, w0=-3.3095830471689855e-07, w1=-0.005618943398169977\n",
      "Regularized Logistic Regression(16/199): loss=0.6701109342940662, w0=-3.514267178434814e-07, w1=-0.005954199133125437\n",
      "Regularized Logistic Regression(17/199): loss=0.6688225621710955, w0=-3.7187061669634623e-07, w1=-0.00628767481154319\n",
      "Regularized Logistic Regression(18/199): loss=0.6675505565679035, w0=-3.922901431954579e-07, w1=-0.006619385362852056\n",
      "Regularized Logistic Regression(19/199): loss=0.6662946924359657, w0=-4.1268543598093227e-07, w1=-0.006949345569373161\n",
      "Regularized Logistic Regression(20/199): loss=0.6650547478660078, w0=-4.3305663048767016e-07, w1=-0.0072775700671979865\n",
      "Regularized Logistic Regression(21/199): loss=0.6638305040479098, w0=-4.534038590186067e-07, w1=-0.007604073347099937\n",
      "Regularized Logistic Regression(22/199): loss=0.6626217452309376, w0=-4.7372725081658807e-07, w1=-0.00792886975547745\n",
      "Regularized Logistic Regression(23/199): loss=0.6614282586843176, w0=-4.940269321348893e-07, w1=-0.00825197349532621\n",
      "Regularized Logistic Regression(24/199): loss=0.6602498346581466, w0=-5.143030263063865e-07, w1=-0.008573398627238524\n",
      "Regularized Logistic Regression(25/199): loss=0.6590862663446516, w0=-5.345556538113981e-07, w1=-0.008893159070427776\n",
      "Regularized Logistic Regression(26/199): loss=0.6579373498397996, w0=-5.547849323442085e-07, w1=-0.00921126860377576\n",
      "Regularized Logistic Regression(27/199): loss=0.6568028841052603, w0=-5.749909768782916e-07, w1=-0.009527740866901538\n",
      "Regularized Logistic Regression(28/199): loss=0.6556826709307261, w0=-5.951738997302463e-07, w1=-0.009842589361249762\n",
      "Regularized Logistic Regression(29/199): loss=0.6545765148965962, w0=-6.153338106224623e-07, w1=-0.010155827451196519\n",
      "Regularized Logistic Regression(30/199): loss=0.6534842233370166, w0=-6.354708167445316e-07, w1=-0.01046746836517173\n",
      "Regularized Logistic Regression(31/199): loss=0.6524056063032901, w0=-6.555850228134207e-07, w1=-0.010777525196795883\n",
      "Regularized Logistic Regression(32/199): loss=0.6513404765276504, w0=-6.756765311324217e-07, w1=-0.011086010906029925\n",
      "Regularized Logistic Regression(33/199): loss=0.6502886493874023, w0=-6.957454416488976e-07, w1=-0.01139293832033699\n",
      "Regularized Logistic Regression(34/199): loss=0.6492499428694343, w0=-7.157918520108401e-07, w1=-0.01169832013585443\n",
      "Regularized Logistic Regression(35/199): loss=0.6482241775350969, w0=-7.358158576222551e-07, w1=-0.01200216891857487\n",
      "Regularized Logistic Regression(36/199): loss=0.647211176485453, w0=-7.558175516973947e-07, w1=-0.012304497105535245\n",
      "Regularized Logistic Regression(37/199): loss=0.6462107653268965, w0=-7.757970253138517e-07, w1=-0.012605317006012536\n",
      "Regularized Logistic Regression(38/199): loss=0.6452227721371454, w0=-7.957543674645353e-07, w1=-0.012904640802724944\n",
      "Regularized Logistic Regression(39/199): loss=0.6442470274315985, w0=-8.156896651085434e-07, w1=-0.0132024805530378\n",
      "Regularized Logistic Regression(40/199): loss=0.6432833641300671, w0=-8.356030032209507e-07, w1=-0.013498848190172889\n",
      "Regularized Logistic Regression(41/199): loss=0.6423316175238737, w0=-8.554944648415292e-07, w1=-0.013793755524420387\n",
      "Regularized Logistic Regression(42/199): loss=0.6413916252433196, w0=-8.753641311224179e-07, w1=-0.01408721424435259\n",
      "Regularized Logistic Regression(43/199): loss=0.6404632272255185, w0=-8.952120813747607e-07, w1=-0.014379235918038477\n",
      "Regularized Logistic Regression(44/199): loss=0.639546265682598, w0=-9.150383931143286e-07, w1=-0.014669831994258106\n",
      "Regularized Logistic Regression(45/199): loss=0.638640585070268, w0=-9.348431421061427e-07, w1=-0.014959013803716585\n",
      "Regularized Logistic Regression(46/199): loss=0.6377460320567473, w0=-9.54626402408118e-07, w1=-0.015246792560256359\n",
      "Regularized Logistic Regression(47/199): loss=0.6368624554920586, w0=-9.74388246413742e-07, w1=-0.015533179362067611\n",
      "Regularized Logistic Regression(48/199): loss=0.6359897063776816, w0=-9.941287448938072e-07, w1=-0.015818185192895712\n",
      "Regularized Logistic Regression(49/199): loss=0.6351276378365646, w0=-1.0138479670372135e-06, w1=-0.016101820923245292\n",
      "Regularized Logistic Regression(50/199): loss=0.6342761050834951, w0=-1.0335459804908576e-06, w1=-0.0163840973115806\n",
      "Regularized Logistic Regression(51/199): loss=0.6334349653958252, w0=-1.0532228513986263e-06, w1=-0.016665025005521087\n",
      "Regularized Logistic Regression(52/199): loss=0.6326040780845494, w0=-1.07287864443951e-06, w1=-0.01694461454303234\n",
      "Regularized Logistic Regression(53/199): loss=0.6317833044657353, w0=-1.092513422864852e-06, w1=-0.017222876353611253\n",
      "Regularized Logistic Regression(54/199): loss=0.630972507832303, w0=-1.1121272485347527e-06, w1=-0.017499820759465563\n",
      "Regularized Logistic Regression(55/199): loss=0.6301715534261508, w0=-1.1317201819536405e-06, w1=-0.017775457976686955\n",
      "Regularized Logistic Regression(56/199): loss=0.6293803084106289, w0=-1.1512922823050291e-06, w1=-0.018049798116417538\n",
      "Regularized Logistic Regression(57/199): loss=0.6285986418433487, w0=-1.1708436074854756e-06, w1=-0.01832285118600926\n",
      "Regularized Logistic Regression(58/199): loss=0.6278264246493399, w0=-1.1903742141377542e-06, w1=-0.018594627090175918\n",
      "Regularized Logistic Regression(59/199): loss=0.6270635295945414, w0=-1.2098841576832617e-06, w1=-0.018865135632137594\n",
      "Regularized Logistic Regression(60/199): loss=0.6263098312596257, w0=-1.229373492353671e-06, w1=-0.01913438651475697\n",
      "Regularized Logistic Regression(61/199): loss=0.6255652060141598, w0=-1.2488422712218457e-06, w1=-0.019402389341667478\n",
      "Regularized Logistic Regression(62/199): loss=0.6248295319910918, w0=-1.2682905462320318e-06, w1=-0.01966915361839297\n",
      "Regularized Logistic Regression(63/199): loss=0.624102689061569, w0=-1.2877183682293404e-06, w1=-0.019934688753458676\n",
      "Regularized Logistic Regression(64/199): loss=0.6233845588100775, w0=-1.3071257869885382e-06, w1=-0.02019900405949321\n",
      "Regularized Logistic Regression(65/199): loss=0.6226750245099041, w0=-1.3265128512421558e-06, w1=-0.020462108754321542\n",
      "Regularized Logistic Regression(66/199): loss=0.6219739710989193, w0=-1.3458796087079318e-06, w1=-0.02072401196204861\n",
      "Regularized Logistic Regression(67/199): loss=0.6212812851556739, w0=-1.3652261061156047e-06, w1=-0.020984722714133507\n",
      "Regularized Logistic Regression(68/199): loss=0.6205968548758122, w0=-1.3845523892330667e-06, w1=-0.021244249950454208\n",
      "Regularized Logistic Regression(69/199): loss=0.6199205700487935, w0=-1.4038585028918916e-06, w1=-0.02150260252036241\n",
      "Regularized Logistic Regression(70/199): loss=0.6192523220349225, w0=-1.4231444910122529e-06, w1=-0.021759789183728753\n",
      "Regularized Logistic Regression(71/199): loss=0.618592003742685, w0=-1.4424103966272414e-06, w1=-0.022015818611977978\n",
      "Regularized Logistic Regression(72/199): loss=0.6179395096063864, w0=-1.4616562619065988e-06, w1=-0.02227069938911419\n",
      "Regularized Logistic Regression(73/199): loss=0.6172947355640862, w0=-1.480882128179876e-06, w1=-0.022524440012736017\n",
      "Regularized Logistic Regression(74/199): loss=0.6166575790358355, w0=-1.5000880359590325e-06, w1=-0.022777048895041643\n",
      "Regularized Logistic Regression(75/199): loss=0.6160279389021999, w0=-1.5192740249604858e-06, w1=-0.023028534363823582\n",
      "Regularized Logistic Regression(76/199): loss=0.6154057154830803, w0=-1.5384401341266247e-06, w1=-0.023278904663453406\n",
      "Regularized Logistic Regression(77/199): loss=0.6147908105168165, w0=-1.5575864016467974e-06, w1=-0.023528167955856022\n",
      "Regularized Logistic Regression(78/199): loss=0.6141831271395779, w0=-1.5767128649777858e-06, w1=-0.023776332321473772\n",
      "Regularized Logistic Regression(79/199): loss=0.6135825698650336, w0=-1.5958195608637786e-06, w1=-0.02402340576022025\n",
      "Regularized Logistic Regression(80/199): loss=0.6129890445643065, w0=-1.6149065253558526e-06, w1=-0.02426939619242373\n",
      "Regularized Logistic Regression(81/199): loss=0.6124024584461967, w0=-1.6339737938309741e-06, w1=-0.024514311459760324\n",
      "Regularized Logistic Regression(82/199): loss=0.6118227200376815, w0=-1.6530214010105313e-06, w1=-0.024758159326176886\n",
      "Regularized Logistic Regression(83/199): loss=0.6112497391646871, w0=-1.6720493809784074e-06, w1=-0.02500094747880369\n",
      "Regularized Logistic Regression(84/199): loss=0.6106834269331226, w0=-1.691057767198606e-06, w1=-0.025242683528856575\n",
      "Regularized Logistic Regression(85/199): loss=0.6101236957101821, w0=-1.7100465925324375e-06, w1=-0.025483375012529223\n",
      "Regularized Logistic Regression(86/199): loss=0.6095704591059059, w0=-1.7290158892552771e-06, w1=-0.02572302939187494\n",
      "Regularized Logistic Regression(87/199): loss=0.6090236319550011, w0=-1.7479656890729052e-06, w1=-0.02596165405567848\n",
      "Regularized Logistic Regression(88/199): loss=0.6084831302989169, w0=-1.7668960231374369e-06, w1=-0.026199256320317537\n",
      "Regularized Logistic Regression(89/199): loss=0.6079488713681732, w0=-1.7858069220628535e-06, w1=-0.02643584343061446\n",
      "Regularized Logistic Regression(90/199): loss=0.6074207735649367, w0=-1.804698415940143e-06, w1=-0.026671422560677564\n",
      "Regularized Logistic Regression(91/199): loss=0.6068987564458457, w0=-1.8235705343520579e-06, w1=-0.026906000814732798\n",
      "Regularized Logistic Regression(92/199): loss=0.606382740705078, w0=-1.8424233063875025e-06, w1=-0.02713958522794532\n",
      "Regularized Logistic Regression(93/199): loss=0.6058726481576595, w0=-1.8612567606555543e-06, w1=-0.02737218276723134\n",
      "Regularized Logistic Regression(94/199): loss=0.6053684017230103, w0=-1.8800709252991306e-06, w1=-0.027603800332060023\n",
      "Regularized Logistic Regression(95/199): loss=0.604869925408726, w0=-1.8988658280083073e-06, w1=-0.027834444755245907\n",
      "Regularized Logistic Regression(96/199): loss=0.604377144294594, w0=-1.9176414960332997e-06, w1=-0.028064122803731398\n",
      "Regularized Logistic Regression(97/199): loss=0.6038899845168337, w0=-1.93639795619711e-06, w1=-0.02829284117935993\n",
      "Regularized Logistic Regression(98/199): loss=0.6034083732525691, w0=-1.955135234907854e-06, w1=-0.02852060651963952\n",
      "Regularized Logistic Regression(99/199): loss=0.602932238704522, w0=-1.9738533581707697e-06, w1=-0.02874742539849696\n",
      "Regularized Logistic Regression(100/199): loss=0.6024615100859262, w0=-1.992552351599919e-06, w1=-0.028973304327022515\n",
      "Regularized Logistic Regression(101/199): loss=0.6019961176056622, w0=-2.0112322404295867e-06, w1=-0.029198249754205714\n",
      "Regularized Logistic Regression(102/199): loss=0.6015359924536057, w0=-2.0298930495253878e-06, w1=-0.02942226806766152\n",
      "Regularized Logistic Regression(103/199): loss=0.6010810667861879, w0=-2.0485348033950863e-06, w1=-0.02964536559434779\n",
      "Regularized Logistic Regression(104/199): loss=0.6006312737121705, w0=-2.0671575261991347e-06, w1=-0.029867548601273466\n",
      "Regularized Logistic Regression(105/199): loss=0.6001865472786223, w0=-2.0857612417609387e-06, w1=-0.030088823296198105\n",
      "Regularized Logistic Regression(106/199): loss=0.5997468224571038, w0=-2.104345973576858e-06, w1=-0.030309195828322394\n",
      "Regularized Logistic Regression(107/199): loss=0.5993120351300548, w0=-2.1229117448259425e-06, w1=-0.03052867228896992\n",
      "Regularized Logistic Regression(108/199): loss=0.5988821220773786, w0=-2.1414585783794187e-06, w1=-0.03074725871226042\n",
      "Regularized Logistic Regression(109/199): loss=0.5984570209632261, w0=-2.1599864968099246e-06, w1=-0.030964961075774428\n",
      "Regularized Logistic Regression(110/199): loss=0.5980366703229707, w0=-2.178495522400505e-06, w1=-0.031181785301209446\n",
      "Regularized Logistic Regression(111/199): loss=0.5976210095503786, w0=-2.1969856771533688e-06, w1=-0.03139773725502761\n",
      "Regularized Logistic Regression(112/199): loss=0.5972099788849664, w0=-2.215456982798416e-06, w1=-0.03161282274909539\n",
      "Regularized Logistic Regression(113/199): loss=0.5968035193995446, w0=-2.2339094608015425e-06, w1=-0.031827047541314775\n",
      "Regularized Logistic Regression(114/199): loss=0.5964015729879468, w0=-2.2523431323727214e-06, w1=-0.0320404173362465\n",
      "Regularized Logistic Regression(115/199): loss=0.5960040823529404, w0=-2.2707580184738725e-06, w1=-0.032252937785725355\n",
      "Regularized Logistic Regression(116/199): loss=0.5956109909943151, w0=-2.289154139826524e-06, w1=-0.032464614489467414\n",
      "Regularized Logistic Regression(117/199): loss=0.5952222431971493, w0=-2.3075315169192674e-06, w1=-0.032675452995669506\n",
      "Regularized Logistic Regression(118/199): loss=0.5948377840202509, w0=-2.325890170015017e-06, w1=-0.032885458801601035\n",
      "Regularized Logistic Regression(119/199): loss=0.5944575592847701, w0=-2.3442301191580725e-06, w1=-0.03309463735418801\n",
      "Regularized Logistic Regression(120/199): loss=0.5940815155629801, w0=-2.3625513841809956e-06, w1=-0.033302994050589646\n",
      "Regularized Logistic Regression(121/199): loss=0.5937096001672287, w0=-2.3808539847112996e-06, w1=-0.03351053423876757\n",
      "Regularized Logistic Regression(122/199): loss=0.5933417611390499, w0=-2.399137940177961e-06, w1=-0.03371726321804745\n",
      "Regularized Logistic Regression(123/199): loss=0.5929779472384431, w0=-2.417403269817752e-06, w1=-0.03392318623967357\n",
      "Regularized Logistic Regression(124/199): loss=0.5926181079333084, w0=-2.4356499926814097e-06, w1=-0.03412830850735598\n",
      "Regularized Logistic Regression(125/199): loss=0.5922621933890431, w0=-2.4538781276396288e-06, w1=-0.03433263517781094\n",
      "Regularized Logistic Regression(126/199): loss=0.5919101544582929, w0=-2.472087693388898e-06, w1=-0.03453617136129393\n",
      "Regularized Logistic Regression(127/199): loss=0.5915619426708558, w0=-2.4902787084571762e-06, w1=-0.03473892212212608\n",
      "Regularized Logistic Regression(128/199): loss=0.5912175102237394, w0=-2.5084511912094144e-06, w1=-0.034940892479213455\n",
      "Regularized Logistic Regression(129/199): loss=0.5908768099713653, w0=-2.5266051598529235e-06, w1=-0.035142087406559996\n",
      "Regularized Logistic Regression(130/199): loss=0.5905397954159237, w0=-2.544740632442601e-06, w1=-0.035342511833773516\n",
      "Regularized Logistic Regression(131/199): loss=0.5902064206978705, w0=-2.5628576268860078e-06, w1=-0.03554217064656514\n",
      "Regularized Logistic Regression(132/199): loss=0.5898766405865671, w0=-2.5809561609483103e-06, w1=-0.035741068687242465\n",
      "Regularized Logistic Regression(133/199): loss=0.5895504104710646, w0=-2.5990362522570836e-06, w1=-0.035939210755196065\n",
      "Regularized Logistic Regression(134/199): loss=0.589227686351023, w0=-2.6170979183069823e-06, w1=-0.036136601607379984\n",
      "Regularized Logistic Regression(135/199): loss=0.5889084248277688, w0=-2.635141176464281e-06, w1=-0.03633324595878566\n",
      "Regularized Logistic Regression(136/199): loss=0.5885925830954887, w0=-2.653166043971291e-06, w1=-0.036529148482910064\n",
      "Regularized Logistic Regression(137/199): loss=0.5882801189325552, w0=-2.671172537950651e-06, w1=-0.03672431381221742\n",
      "Regularized Logistic Regression(138/199): loss=0.5879709906929816, w0=-2.6891606754094995e-06, w1=-0.03691874653859529\n",
      "Regularized Logistic Regression(139/199): loss=0.5876651572980095, w0=-2.707130473243529e-06, w1=-0.03711245121380447\n",
      "Regularized Logistic Regression(140/199): loss=0.5873625782278205, w0=-2.725081948240928e-06, w1=-0.037305432349923215\n",
      "Regularized Logistic Regression(141/199): loss=0.5870632135133756, w0=-2.743015117086211e-06, w1=-0.037497694419785704\n",
      "Regularized Logistic Regression(142/199): loss=0.5867670237283749, w0=-2.760929996363939e-06, w1=-0.03768924185741485\n",
      "Regularized Logistic Regression(143/199): loss=0.5864739699813433, w0=-2.7788266025623356e-06, w1=-0.03788007905844946\n",
      "Regularized Logistic Regression(144/199): loss=0.5861840139078323, w0=-2.7967049520768018e-06, w1=-0.03807021038056587\n",
      "Regularized Logistic Regression(145/199): loss=0.5858971176627411, w0=-2.8145650612133273e-06, w1=-0.03825964014389429\n",
      "Regularized Logistic Regression(146/199): loss=0.585613243912755, w0=-2.832406946191806e-06, w1=-0.038448372631429646\n",
      "Regularized Logistic Regression(147/199): loss=0.5853323558288971, w0=-2.8502306231492574e-06, w1=-0.03863641208943719\n",
      "Regularized Logistic Regression(148/199): loss=0.585054417079193, w0=-2.868036108142952e-06, w1=-0.038823762727852866\n",
      "Regularized Logistic Regression(149/199): loss=0.5847793918214486, w0=-2.8858234171534503e-06, w1=-0.03901042872067851\n",
      "Regularized Logistic Regression(150/199): loss=0.5845072446961331, w0=-2.903592566087551e-06, w1=-0.03919641420637207\n",
      "Regularized Logistic Regression(151/199): loss=0.5842379408193759, w0=-2.9213435707811534e-06, w1=-0.03938172328823271\n",
      "Regularized Logistic Regression(152/199): loss=0.5839714457760631, w0=-2.939076447002039e-06, w1=-0.0395663600347811\n",
      "Regularized Logistic Regression(153/199): loss=0.5837077256130448, w0=-2.9567912104525675e-06, w1=-0.039750328480134746\n",
      "Regularized Logistic Regression(154/199): loss=0.5834467468324416, w0=-2.9744878767722977e-06, w1=-0.03993363262437847\n",
      "Regularized Logistic Regression(155/199): loss=0.5831884763850534, w0=-2.9921664615405274e-06, w1=-0.040116276433930335\n",
      "Regularized Logistic Regression(156/199): loss=0.5829328816638687, w0=-3.009826980278761e-06, w1=-0.040298263841902805\n",
      "Regularized Logistic Regression(157/199): loss=0.5826799304976724, w0=-3.027469448453103e-06, w1=-0.0404795987484592\n",
      "Regularized Logistic Regression(158/199): loss=0.5824295911447506, w0=-3.045093881476577e-06, w1=-0.040660285021165816\n",
      "Regularized Logistic Regression(159/199): loss=0.58218183228669, w0=-3.062700294711382e-06, w1=-0.040840326495339425\n",
      "Regularized Logistic Regression(160/199): loss=0.5819366230222716, w0=-3.080288703471072e-06, w1=-0.04101972697439045\n",
      "Regularized Logistic Regression(161/199): loss=0.5816939328614593, w0=-3.0978591230226785e-06, w1=-0.04119849023016171\n",
      "Regularized Logistic Regression(162/199): loss=0.5814537317194751, w0=-3.1154115685887615e-06, w1=-0.04137662000326292\n",
      "Regularized Logistic Regression(163/199): loss=0.5812159899109691, w0=-3.132946055349403e-06, w1=-0.04155412000340096\n",
      "Regularized Logistic Regression(164/199): loss=0.5809806781442747, w0=-3.1504625984441364e-06, w1=-0.04173099390970602\n",
      "Regularized Logistic Regression(165/199): loss=0.5807477675157502, w0=-3.1679612129738184e-06, w1=-0.04190724537105345\n",
      "Regularized Logistic Regression(166/199): loss=0.5805172295042119, w0=-3.1854419140024414e-06, w1=-0.04208287800638185\n",
      "Regularized Logistic Regression(167/199): loss=0.5802890359654427, w0=-3.2029047165588914e-06, w1=-0.0422578954050069\n",
      "Regularized Logistic Regression(168/199): loss=0.5800631591267912, w0=-3.22034963563865e-06, w1=-0.04243230112693142\n",
      "Regularized Logistic Regression(169/199): loss=0.5798395715818498, w0=-3.237776686205444e-06, w1=-0.04260609870315139\n",
      "Regularized Logistic Regression(170/199): loss=0.5796182462852133, w0=-3.2551858831928414e-06, w1=-0.04277929163595828\n",
      "Regularized Logistic Regression(171/199): loss=0.579399156547317, w0=-3.2725772415057974e-06, w1=-0.042951883399237535\n",
      "Regularized Logistic Regression(172/199): loss=0.5791822760293538, w0=-3.2899507760221533e-06, w1=-0.04312387743876327\n",
      "Regularized Logistic Regression(173/199): loss=0.5789675787382665, w0=-3.307306501594085e-06, w1=-0.04329527717248953\n",
      "Regularized Logistic Regression(174/199): loss=0.5787550390218175, w0=-3.3246444330495045e-06, w1=-0.04346608599083758\n",
      "Regularized Logistic Regression(175/199): loss=0.5785446315637323, w0=-3.3419645851934184e-06, w1=-0.04363630725697995\n",
      "Regularized Logistic Regression(176/199): loss=0.5783363313789168, w0=-3.3592669728092403e-06, w1=-0.043805944307120696\n",
      "Regularized Logistic Regression(177/199): loss=0.5781301138087456, w0=-3.3765516106600606e-06, w1=-0.04397500045077241\n",
      "Regularized Logistic Regression(178/199): loss=0.5779259545164241, w0=-3.3938185134898753e-06, w1=-0.04414347897102962\n",
      "Regularized Logistic Regression(179/199): loss=0.5777238294824176, w0=-3.4110676960247723e-06, w1=-0.04431138312483891\n",
      "Regularized Logistic Regression(180/199): loss=0.5775237149999509, w0=-3.4282991729740797e-06, w1=-0.04447871614326577\n",
      "Regularized Logistic Regression(181/199): loss=0.5773255876705766, w0=-3.4455129590314745e-06, w1=-0.04464548123175802\n",
      "Regularized Logistic Regression(182/199): loss=0.5771294243998085, w0=-3.4627090688760537e-06, w1=-0.04481168157040608\n",
      "Regularized Logistic Regression(183/199): loss=0.5769352023928213, w0=-3.4798875171733706e-06, w1=-0.044977320314200084\n",
      "Regularized Logistic Regression(184/199): loss=0.5767428991502171, w0=-3.4970483185764325e-06, w1=-0.045142400593283706\n",
      "Regularized Logistic Regression(185/199): loss=0.5765524924638519, w0=-3.5141914877266665e-06, w1=-0.045306925513205074\n",
      "Regularized Logistic Regression(186/199): loss=0.5763639604127281, w0=-3.5313170392548502e-06, w1=-0.045470898155164516\n",
      "Regularized Logistic Regression(187/199): loss=0.576177281358948, w0=-3.5484249877820103e-06, w1=-0.04563432157625924\n",
      "Regularized Logistic Regression(188/199): loss=0.5759924339437261, w0=-3.5655153479202885e-06, w1=-0.045797198809725166\n",
      "Regularized Logistic Regression(189/199): loss=0.5758093970834657, w0=-3.5825881342737777e-06, w1=-0.04595953286517571\n",
      "Regularized Logistic Regression(190/199): loss=0.5756281499658897, w0=-3.599643361439328e-06, w1=-0.04612132672883775\n",
      "Regularized Logistic Regression(191/199): loss=0.5754486720462313, w0=-3.6166810440073204e-06, w1=-0.046282583363784685\n",
      "Regularized Logistic Regression(192/199): loss=0.5752709430434845, w0=-3.633701196562417e-06, w1=-0.046443305710166735\n",
      "Regularized Logistic Regression(193/199): loss=0.5750949429367057, w0=-3.650703833684279e-06, w1=-0.04660349668543845\n",
      "Regularized Logistic Regression(194/199): loss=0.5749206519613753, w0=-3.6676889699482603e-06, w1=-0.04676315918458339\n",
      "Regularized Logistic Regression(195/199): loss=0.5747480506058112, w0=-3.684656619926075e-06, w1=-0.046922296080336254\n",
      "Regularized Logistic Regression(196/199): loss=0.5745771196076369, w0=-3.7016067981864368e-06, w1=-0.04708091022340231\n",
      "Regularized Logistic Regression(197/199): loss=0.5744078399503014, w0=-3.7185395192956767e-06, w1=-0.04723900444267404\n",
      "Regularized Logistic Regression(198/199): loss=0.5742401928596537, w0=-3.7354547978183345e-06, w1=-0.04739658154544544\n",
      "Regularized Logistic Regression(199/199): loss=0.5740741598005653, w0=-3.752352648317729e-06, w1=-0.04755364431762363\n",
      "Regularized Logistic Regression(0/299): loss=0.693147180559945, w0=-2.0882930292778682e-08, w1=-0.0003658898216597753\n",
      "Regularized Logistic Regression(1/299): loss=0.6915636157846066, w0=-4.173856310796442e-08, w1=-0.0007297401642547018\n",
      "Regularized Logistic Regression(2/299): loss=0.6900004791156684, w0=-6.256710416062533e-08, w1=-0.0010915683924349279\n",
      "Regularized Logistic Regression(3/299): loss=0.6884574895874042, w0=-8.336875449622289e-08, w1=-0.001451391716536063\n",
      "Regularized Logistic Regression(4/299): loss=0.6869343700521863, w0=-1.0414371058844798e-07, w1=-0.001809227192523169\n",
      "Regularized Logistic Regression(5/299): loss=0.6854308471365446, w0=-1.2489216443559245e-07, w1=-0.002165091722019075\n",
      "Regularized Logistic Regression(6/299): loss=0.6839466511973373, w0=-1.4561430365545155e-07, w1=-0.0025190020524135034\n",
      "Regularized Logistic Regression(7/299): loss=0.6824815162780676, w0=-1.6631031157875474e-07, w1=-0.0028709747770488208\n",
      "Regularized Logistic Regression(8/299): loss=0.6810351800653592, w0=-1.8698036734112384e-07, w1=-0.003221026335478526\n",
      "Regularized Logistic Regression(9/299): loss=0.6796073838456137, w0=-2.0762464597355822e-07, w1=-0.003569173013794989\n",
      "Regularized Logistic Regression(10/299): loss=0.678197872461869, w0=-2.2824331849144931e-07, w1=-0.003915430945022837\n",
      "Regularized Logistic Regression(11/299): loss=0.6768063942708706, w0=-2.4883655198212663e-07, w1=-0.004259816109574463\n",
      "Regularized Logistic Regression(12/299): loss=0.6754327011003756, w0=-2.6940450969093956e-07, w1=-0.004602344335764269\n",
      "Regularized Logistic Regression(13/299): loss=0.6740765482067022, w0=-2.899473511058801e-07, w1=-0.004943031300378625\n",
      "Regularized Logistic Regression(14/299): loss=0.6727376942325368, w0=-3.104652320407528e-07, w1=-0.005281892529298227\n",
      "Regularized Logistic Regression(15/299): loss=0.6714159011650122, w0=-3.3095830471689855e-07, w1=-0.005618943398169977\n",
      "Regularized Logistic Regression(16/299): loss=0.6701109342940662, w0=-3.514267178434814e-07, w1=-0.005954199133125437\n",
      "Regularized Logistic Regression(17/299): loss=0.6688225621710955, w0=-3.7187061669634623e-07, w1=-0.00628767481154319\n",
      "Regularized Logistic Regression(18/299): loss=0.6675505565679035, w0=-3.922901431954579e-07, w1=-0.006619385362852056\n",
      "Regularized Logistic Regression(19/299): loss=0.6662946924359657, w0=-4.1268543598093227e-07, w1=-0.006949345569373161\n",
      "Regularized Logistic Regression(20/299): loss=0.6650547478660078, w0=-4.3305663048767016e-07, w1=-0.0072775700671979865\n",
      "Regularized Logistic Regression(21/299): loss=0.6638305040479098, w0=-4.534038590186067e-07, w1=-0.007604073347099937\n",
      "Regularized Logistic Regression(22/299): loss=0.6626217452309376, w0=-4.7372725081658807e-07, w1=-0.00792886975547745\n",
      "Regularized Logistic Regression(23/299): loss=0.6614282586843176, w0=-4.940269321348893e-07, w1=-0.00825197349532621\n",
      "Regularized Logistic Regression(24/299): loss=0.6602498346581466, w0=-5.143030263063865e-07, w1=-0.008573398627238524\n",
      "Regularized Logistic Regression(25/299): loss=0.6590862663446516, w0=-5.345556538113981e-07, w1=-0.008893159070427776\n",
      "Regularized Logistic Regression(26/299): loss=0.6579373498397996, w0=-5.547849323442085e-07, w1=-0.00921126860377576\n",
      "Regularized Logistic Regression(27/299): loss=0.6568028841052603, w0=-5.749909768782916e-07, w1=-0.009527740866901538\n",
      "Regularized Logistic Regression(28/299): loss=0.6556826709307261, w0=-5.951738997302463e-07, w1=-0.009842589361249762\n",
      "Regularized Logistic Regression(29/299): loss=0.6545765148965962, w0=-6.153338106224623e-07, w1=-0.010155827451196519\n",
      "Regularized Logistic Regression(30/299): loss=0.6534842233370166, w0=-6.354708167445316e-07, w1=-0.01046746836517173\n",
      "Regularized Logistic Regression(31/299): loss=0.6524056063032901, w0=-6.555850228134207e-07, w1=-0.010777525196795883\n",
      "Regularized Logistic Regression(32/299): loss=0.6513404765276504, w0=-6.756765311324217e-07, w1=-0.011086010906029925\n",
      "Regularized Logistic Regression(33/299): loss=0.6502886493874023, w0=-6.957454416488976e-07, w1=-0.01139293832033699\n",
      "Regularized Logistic Regression(34/299): loss=0.6492499428694343, w0=-7.157918520108401e-07, w1=-0.01169832013585443\n",
      "Regularized Logistic Regression(35/299): loss=0.6482241775350969, w0=-7.358158576222551e-07, w1=-0.01200216891857487\n",
      "Regularized Logistic Regression(36/299): loss=0.647211176485453, w0=-7.558175516973947e-07, w1=-0.012304497105535245\n",
      "Regularized Logistic Regression(37/299): loss=0.6462107653268965, w0=-7.757970253138517e-07, w1=-0.012605317006012536\n",
      "Regularized Logistic Regression(38/299): loss=0.6452227721371454, w0=-7.957543674645353e-07, w1=-0.012904640802724944\n",
      "Regularized Logistic Regression(39/299): loss=0.6442470274315985, w0=-8.156896651085434e-07, w1=-0.0132024805530378\n",
      "Regularized Logistic Regression(40/299): loss=0.6432833641300671, w0=-8.356030032209507e-07, w1=-0.013498848190172889\n",
      "Regularized Logistic Regression(41/299): loss=0.6423316175238737, w0=-8.554944648415292e-07, w1=-0.013793755524420387\n",
      "Regularized Logistic Regression(42/299): loss=0.6413916252433196, w0=-8.753641311224179e-07, w1=-0.01408721424435259\n",
      "Regularized Logistic Regression(43/299): loss=0.6404632272255185, w0=-8.952120813747607e-07, w1=-0.014379235918038477\n",
      "Regularized Logistic Regression(44/299): loss=0.639546265682598, w0=-9.150383931143286e-07, w1=-0.014669831994258106\n",
      "Regularized Logistic Regression(45/299): loss=0.638640585070268, w0=-9.348431421061427e-07, w1=-0.014959013803716585\n",
      "Regularized Logistic Regression(46/299): loss=0.6377460320567473, w0=-9.54626402408118e-07, w1=-0.015246792560256359\n",
      "Regularized Logistic Regression(47/299): loss=0.6368624554920586, w0=-9.74388246413742e-07, w1=-0.015533179362067611\n",
      "Regularized Logistic Regression(48/299): loss=0.6359897063776816, w0=-9.941287448938072e-07, w1=-0.015818185192895712\n",
      "Regularized Logistic Regression(49/299): loss=0.6351276378365646, w0=-1.0138479670372135e-06, w1=-0.016101820923245292\n",
      "Regularized Logistic Regression(50/299): loss=0.6342761050834951, w0=-1.0335459804908576e-06, w1=-0.0163840973115806\n",
      "Regularized Logistic Regression(51/299): loss=0.6334349653958252, w0=-1.0532228513986263e-06, w1=-0.016665025005521087\n",
      "Regularized Logistic Regression(52/299): loss=0.6326040780845494, w0=-1.07287864443951e-06, w1=-0.01694461454303234\n",
      "Regularized Logistic Regression(53/299): loss=0.6317833044657353, w0=-1.092513422864852e-06, w1=-0.017222876353611253\n",
      "Regularized Logistic Regression(54/299): loss=0.630972507832303, w0=-1.1121272485347527e-06, w1=-0.017499820759465563\n",
      "Regularized Logistic Regression(55/299): loss=0.6301715534261508, w0=-1.1317201819536405e-06, w1=-0.017775457976686955\n",
      "Regularized Logistic Regression(56/299): loss=0.6293803084106289, w0=-1.1512922823050291e-06, w1=-0.018049798116417538\n",
      "Regularized Logistic Regression(57/299): loss=0.6285986418433487, w0=-1.1708436074854756e-06, w1=-0.01832285118600926\n",
      "Regularized Logistic Regression(58/299): loss=0.6278264246493399, w0=-1.1903742141377542e-06, w1=-0.018594627090175918\n",
      "Regularized Logistic Regression(59/299): loss=0.6270635295945414, w0=-1.2098841576832617e-06, w1=-0.018865135632137594\n",
      "Regularized Logistic Regression(60/299): loss=0.6263098312596257, w0=-1.229373492353671e-06, w1=-0.01913438651475697\n",
      "Regularized Logistic Regression(61/299): loss=0.6255652060141598, w0=-1.2488422712218457e-06, w1=-0.019402389341667478\n",
      "Regularized Logistic Regression(62/299): loss=0.6248295319910918, w0=-1.2682905462320318e-06, w1=-0.01966915361839297\n",
      "Regularized Logistic Regression(63/299): loss=0.624102689061569, w0=-1.2877183682293404e-06, w1=-0.019934688753458676\n",
      "Regularized Logistic Regression(64/299): loss=0.6233845588100775, w0=-1.3071257869885382e-06, w1=-0.02019900405949321\n",
      "Regularized Logistic Regression(65/299): loss=0.6226750245099041, w0=-1.3265128512421558e-06, w1=-0.020462108754321542\n",
      "Regularized Logistic Regression(66/299): loss=0.6219739710989193, w0=-1.3458796087079318e-06, w1=-0.02072401196204861\n",
      "Regularized Logistic Regression(67/299): loss=0.6212812851556739, w0=-1.3652261061156047e-06, w1=-0.020984722714133507\n",
      "Regularized Logistic Regression(68/299): loss=0.6205968548758122, w0=-1.3845523892330667e-06, w1=-0.021244249950454208\n",
      "Regularized Logistic Regression(69/299): loss=0.6199205700487935, w0=-1.4038585028918916e-06, w1=-0.02150260252036241\n",
      "Regularized Logistic Regression(70/299): loss=0.6192523220349225, w0=-1.4231444910122529e-06, w1=-0.021759789183728753\n",
      "Regularized Logistic Regression(71/299): loss=0.618592003742685, w0=-1.4424103966272414e-06, w1=-0.022015818611977978\n",
      "Regularized Logistic Regression(72/299): loss=0.6179395096063864, w0=-1.4616562619065988e-06, w1=-0.02227069938911419\n",
      "Regularized Logistic Regression(73/299): loss=0.6172947355640862, w0=-1.480882128179876e-06, w1=-0.022524440012736017\n",
      "Regularized Logistic Regression(74/299): loss=0.6166575790358355, w0=-1.5000880359590325e-06, w1=-0.022777048895041643\n",
      "Regularized Logistic Regression(75/299): loss=0.6160279389021999, w0=-1.5192740249604858e-06, w1=-0.023028534363823582\n",
      "Regularized Logistic Regression(76/299): loss=0.6154057154830803, w0=-1.5384401341266247e-06, w1=-0.023278904663453406\n",
      "Regularized Logistic Regression(77/299): loss=0.6147908105168165, w0=-1.5575864016467974e-06, w1=-0.023528167955856022\n",
      "Regularized Logistic Regression(78/299): loss=0.6141831271395779, w0=-1.5767128649777858e-06, w1=-0.023776332321473772\n",
      "Regularized Logistic Regression(79/299): loss=0.6135825698650336, w0=-1.5958195608637786e-06, w1=-0.02402340576022025\n",
      "Regularized Logistic Regression(80/299): loss=0.6129890445643065, w0=-1.6149065253558526e-06, w1=-0.02426939619242373\n",
      "Regularized Logistic Regression(81/299): loss=0.6124024584461967, w0=-1.6339737938309741e-06, w1=-0.024514311459760324\n",
      "Regularized Logistic Regression(82/299): loss=0.6118227200376815, w0=-1.6530214010105313e-06, w1=-0.024758159326176886\n",
      "Regularized Logistic Regression(83/299): loss=0.6112497391646871, w0=-1.6720493809784074e-06, w1=-0.02500094747880369\n",
      "Regularized Logistic Regression(84/299): loss=0.6106834269331226, w0=-1.691057767198606e-06, w1=-0.025242683528856575\n",
      "Regularized Logistic Regression(85/299): loss=0.6101236957101821, w0=-1.7100465925324375e-06, w1=-0.025483375012529223\n",
      "Regularized Logistic Regression(86/299): loss=0.6095704591059059, w0=-1.7290158892552771e-06, w1=-0.02572302939187494\n",
      "Regularized Logistic Regression(87/299): loss=0.6090236319550011, w0=-1.7479656890729052e-06, w1=-0.02596165405567848\n",
      "Regularized Logistic Regression(88/299): loss=0.6084831302989169, w0=-1.7668960231374369e-06, w1=-0.026199256320317537\n",
      "Regularized Logistic Regression(89/299): loss=0.6079488713681732, w0=-1.7858069220628535e-06, w1=-0.02643584343061446\n",
      "Regularized Logistic Regression(90/299): loss=0.6074207735649367, w0=-1.804698415940143e-06, w1=-0.026671422560677564\n",
      "Regularized Logistic Regression(91/299): loss=0.6068987564458457, w0=-1.8235705343520579e-06, w1=-0.026906000814732798\n",
      "Regularized Logistic Regression(92/299): loss=0.606382740705078, w0=-1.8424233063875025e-06, w1=-0.02713958522794532\n",
      "Regularized Logistic Regression(93/299): loss=0.6058726481576595, w0=-1.8612567606555543e-06, w1=-0.02737218276723134\n",
      "Regularized Logistic Regression(94/299): loss=0.6053684017230103, w0=-1.8800709252991306e-06, w1=-0.027603800332060023\n",
      "Regularized Logistic Regression(95/299): loss=0.604869925408726, w0=-1.8988658280083073e-06, w1=-0.027834444755245907\n",
      "Regularized Logistic Regression(96/299): loss=0.604377144294594, w0=-1.9176414960332997e-06, w1=-0.028064122803731398\n",
      "Regularized Logistic Regression(97/299): loss=0.6038899845168337, w0=-1.93639795619711e-06, w1=-0.02829284117935993\n",
      "Regularized Logistic Regression(98/299): loss=0.6034083732525691, w0=-1.955135234907854e-06, w1=-0.02852060651963952\n",
      "Regularized Logistic Regression(99/299): loss=0.602932238704522, w0=-1.9738533581707697e-06, w1=-0.02874742539849696\n",
      "Regularized Logistic Regression(100/299): loss=0.6024615100859262, w0=-1.992552351599919e-06, w1=-0.028973304327022515\n",
      "Regularized Logistic Regression(101/299): loss=0.6019961176056622, w0=-2.0112322404295867e-06, w1=-0.029198249754205714\n",
      "Regularized Logistic Regression(102/299): loss=0.6015359924536057, w0=-2.0298930495253878e-06, w1=-0.02942226806766152\n",
      "Regularized Logistic Regression(103/299): loss=0.6010810667861879, w0=-2.0485348033950863e-06, w1=-0.02964536559434779\n",
      "Regularized Logistic Regression(104/299): loss=0.6006312737121705, w0=-2.0671575261991347e-06, w1=-0.029867548601273466\n",
      "Regularized Logistic Regression(105/299): loss=0.6001865472786223, w0=-2.0857612417609387e-06, w1=-0.030088823296198105\n",
      "Regularized Logistic Regression(106/299): loss=0.5997468224571038, w0=-2.104345973576858e-06, w1=-0.030309195828322394\n",
      "Regularized Logistic Regression(107/299): loss=0.5993120351300548, w0=-2.1229117448259425e-06, w1=-0.03052867228896992\n",
      "Regularized Logistic Regression(108/299): loss=0.5988821220773786, w0=-2.1414585783794187e-06, w1=-0.03074725871226042\n",
      "Regularized Logistic Regression(109/299): loss=0.5984570209632261, w0=-2.1599864968099246e-06, w1=-0.030964961075774428\n",
      "Regularized Logistic Regression(110/299): loss=0.5980366703229707, w0=-2.178495522400505e-06, w1=-0.031181785301209446\n",
      "Regularized Logistic Regression(111/299): loss=0.5976210095503786, w0=-2.1969856771533688e-06, w1=-0.03139773725502761\n",
      "Regularized Logistic Regression(112/299): loss=0.5972099788849664, w0=-2.215456982798416e-06, w1=-0.03161282274909539\n",
      "Regularized Logistic Regression(113/299): loss=0.5968035193995446, w0=-2.2339094608015425e-06, w1=-0.031827047541314775\n",
      "Regularized Logistic Regression(114/299): loss=0.5964015729879468, w0=-2.2523431323727214e-06, w1=-0.0320404173362465\n",
      "Regularized Logistic Regression(115/299): loss=0.5960040823529404, w0=-2.2707580184738725e-06, w1=-0.032252937785725355\n",
      "Regularized Logistic Regression(116/299): loss=0.5956109909943151, w0=-2.289154139826524e-06, w1=-0.032464614489467414\n",
      "Regularized Logistic Regression(117/299): loss=0.5952222431971493, w0=-2.3075315169192674e-06, w1=-0.032675452995669506\n",
      "Regularized Logistic Regression(118/299): loss=0.5948377840202509, w0=-2.325890170015017e-06, w1=-0.032885458801601035\n",
      "Regularized Logistic Regression(119/299): loss=0.5944575592847701, w0=-2.3442301191580725e-06, w1=-0.03309463735418801\n",
      "Regularized Logistic Regression(120/299): loss=0.5940815155629801, w0=-2.3625513841809956e-06, w1=-0.033302994050589646\n",
      "Regularized Logistic Regression(121/299): loss=0.5937096001672287, w0=-2.3808539847112996e-06, w1=-0.03351053423876757\n",
      "Regularized Logistic Regression(122/299): loss=0.5933417611390499, w0=-2.399137940177961e-06, w1=-0.03371726321804745\n",
      "Regularized Logistic Regression(123/299): loss=0.5929779472384431, w0=-2.417403269817752e-06, w1=-0.03392318623967357\n",
      "Regularized Logistic Regression(124/299): loss=0.5926181079333084, w0=-2.4356499926814097e-06, w1=-0.03412830850735598\n",
      "Regularized Logistic Regression(125/299): loss=0.5922621933890431, w0=-2.4538781276396288e-06, w1=-0.03433263517781094\n",
      "Regularized Logistic Regression(126/299): loss=0.5919101544582929, w0=-2.472087693388898e-06, w1=-0.03453617136129393\n",
      "Regularized Logistic Regression(127/299): loss=0.5915619426708558, w0=-2.4902787084571762e-06, w1=-0.03473892212212608\n",
      "Regularized Logistic Regression(128/299): loss=0.5912175102237394, w0=-2.5084511912094144e-06, w1=-0.034940892479213455\n",
      "Regularized Logistic Regression(129/299): loss=0.5908768099713653, w0=-2.5266051598529235e-06, w1=-0.035142087406559996\n",
      "Regularized Logistic Regression(130/299): loss=0.5905397954159237, w0=-2.544740632442601e-06, w1=-0.035342511833773516\n",
      "Regularized Logistic Regression(131/299): loss=0.5902064206978705, w0=-2.5628576268860078e-06, w1=-0.03554217064656514\n",
      "Regularized Logistic Regression(132/299): loss=0.5898766405865671, w0=-2.5809561609483103e-06, w1=-0.035741068687242465\n",
      "Regularized Logistic Regression(133/299): loss=0.5895504104710646, w0=-2.5990362522570836e-06, w1=-0.035939210755196065\n",
      "Regularized Logistic Regression(134/299): loss=0.589227686351023, w0=-2.6170979183069823e-06, w1=-0.036136601607379984\n",
      "Regularized Logistic Regression(135/299): loss=0.5889084248277688, w0=-2.635141176464281e-06, w1=-0.03633324595878566\n",
      "Regularized Logistic Regression(136/299): loss=0.5885925830954887, w0=-2.653166043971291e-06, w1=-0.036529148482910064\n",
      "Regularized Logistic Regression(137/299): loss=0.5882801189325552, w0=-2.671172537950651e-06, w1=-0.03672431381221742\n",
      "Regularized Logistic Regression(138/299): loss=0.5879709906929816, w0=-2.6891606754094995e-06, w1=-0.03691874653859529\n",
      "Regularized Logistic Regression(139/299): loss=0.5876651572980095, w0=-2.707130473243529e-06, w1=-0.03711245121380447\n",
      "Regularized Logistic Regression(140/299): loss=0.5873625782278205, w0=-2.725081948240928e-06, w1=-0.037305432349923215\n",
      "Regularized Logistic Regression(141/299): loss=0.5870632135133756, w0=-2.743015117086211e-06, w1=-0.037497694419785704\n",
      "Regularized Logistic Regression(142/299): loss=0.5867670237283749, w0=-2.760929996363939e-06, w1=-0.03768924185741485\n",
      "Regularized Logistic Regression(143/299): loss=0.5864739699813433, w0=-2.7788266025623356e-06, w1=-0.03788007905844946\n",
      "Regularized Logistic Regression(144/299): loss=0.5861840139078323, w0=-2.7967049520768018e-06, w1=-0.03807021038056587\n",
      "Regularized Logistic Regression(145/299): loss=0.5858971176627411, w0=-2.8145650612133273e-06, w1=-0.03825964014389429\n",
      "Regularized Logistic Regression(146/299): loss=0.585613243912755, w0=-2.832406946191806e-06, w1=-0.038448372631429646\n",
      "Regularized Logistic Regression(147/299): loss=0.5853323558288971, w0=-2.8502306231492574e-06, w1=-0.03863641208943719\n",
      "Regularized Logistic Regression(148/299): loss=0.585054417079193, w0=-2.868036108142952e-06, w1=-0.038823762727852866\n",
      "Regularized Logistic Regression(149/299): loss=0.5847793918214486, w0=-2.8858234171534503e-06, w1=-0.03901042872067851\n",
      "Regularized Logistic Regression(150/299): loss=0.5845072446961331, w0=-2.903592566087551e-06, w1=-0.03919641420637207\n",
      "Regularized Logistic Regression(151/299): loss=0.5842379408193759, w0=-2.9213435707811534e-06, w1=-0.03938172328823271\n",
      "Regularized Logistic Regression(152/299): loss=0.5839714457760631, w0=-2.939076447002039e-06, w1=-0.0395663600347811\n",
      "Regularized Logistic Regression(153/299): loss=0.5837077256130448, w0=-2.9567912104525675e-06, w1=-0.039750328480134746\n",
      "Regularized Logistic Regression(154/299): loss=0.5834467468324416, w0=-2.9744878767722977e-06, w1=-0.03993363262437847\n",
      "Regularized Logistic Regression(155/299): loss=0.5831884763850534, w0=-2.9921664615405274e-06, w1=-0.040116276433930335\n",
      "Regularized Logistic Regression(156/299): loss=0.5829328816638687, w0=-3.009826980278761e-06, w1=-0.040298263841902805\n",
      "Regularized Logistic Regression(157/299): loss=0.5826799304976724, w0=-3.027469448453103e-06, w1=-0.0404795987484592\n",
      "Regularized Logistic Regression(158/299): loss=0.5824295911447506, w0=-3.045093881476577e-06, w1=-0.040660285021165816\n",
      "Regularized Logistic Regression(159/299): loss=0.58218183228669, w0=-3.062700294711382e-06, w1=-0.040840326495339425\n",
      "Regularized Logistic Regression(160/299): loss=0.5819366230222716, w0=-3.080288703471072e-06, w1=-0.04101972697439045\n",
      "Regularized Logistic Regression(161/299): loss=0.5816939328614593, w0=-3.0978591230226785e-06, w1=-0.04119849023016171\n",
      "Regularized Logistic Regression(162/299): loss=0.5814537317194751, w0=-3.1154115685887615e-06, w1=-0.04137662000326292\n",
      "Regularized Logistic Regression(163/299): loss=0.5812159899109691, w0=-3.132946055349403e-06, w1=-0.04155412000340096\n",
      "Regularized Logistic Regression(164/299): loss=0.5809806781442747, w0=-3.1504625984441364e-06, w1=-0.04173099390970602\n",
      "Regularized Logistic Regression(165/299): loss=0.5807477675157502, w0=-3.1679612129738184e-06, w1=-0.04190724537105345\n",
      "Regularized Logistic Regression(166/299): loss=0.5805172295042119, w0=-3.1854419140024414e-06, w1=-0.04208287800638185\n",
      "Regularized Logistic Regression(167/299): loss=0.5802890359654427, w0=-3.2029047165588914e-06, w1=-0.0422578954050069\n",
      "Regularized Logistic Regression(168/299): loss=0.5800631591267912, w0=-3.22034963563865e-06, w1=-0.04243230112693142\n",
      "Regularized Logistic Regression(169/299): loss=0.5798395715818498, w0=-3.237776686205444e-06, w1=-0.04260609870315139\n",
      "Regularized Logistic Regression(170/299): loss=0.5796182462852133, w0=-3.2551858831928414e-06, w1=-0.04277929163595828\n",
      "Regularized Logistic Regression(171/299): loss=0.579399156547317, w0=-3.2725772415057974e-06, w1=-0.042951883399237535\n",
      "Regularized Logistic Regression(172/299): loss=0.5791822760293538, w0=-3.2899507760221533e-06, w1=-0.04312387743876327\n",
      "Regularized Logistic Regression(173/299): loss=0.5789675787382665, w0=-3.307306501594085e-06, w1=-0.04329527717248953\n",
      "Regularized Logistic Regression(174/299): loss=0.5787550390218175, w0=-3.3246444330495045e-06, w1=-0.04346608599083758\n",
      "Regularized Logistic Regression(175/299): loss=0.5785446315637323, w0=-3.3419645851934184e-06, w1=-0.04363630725697995\n",
      "Regularized Logistic Regression(176/299): loss=0.5783363313789168, w0=-3.3592669728092403e-06, w1=-0.043805944307120696\n",
      "Regularized Logistic Regression(177/299): loss=0.5781301138087456, w0=-3.3765516106600606e-06, w1=-0.04397500045077241\n",
      "Regularized Logistic Regression(178/299): loss=0.5779259545164241, w0=-3.3938185134898753e-06, w1=-0.04414347897102962\n",
      "Regularized Logistic Regression(179/299): loss=0.5777238294824176, w0=-3.4110676960247723e-06, w1=-0.04431138312483891\n",
      "Regularized Logistic Regression(180/299): loss=0.5775237149999509, w0=-3.4282991729740797e-06, w1=-0.04447871614326577\n",
      "Regularized Logistic Regression(181/299): loss=0.5773255876705766, w0=-3.4455129590314745e-06, w1=-0.04464548123175802\n",
      "Regularized Logistic Regression(182/299): loss=0.5771294243998085, w0=-3.4627090688760537e-06, w1=-0.04481168157040608\n",
      "Regularized Logistic Regression(183/299): loss=0.5769352023928213, w0=-3.4798875171733706e-06, w1=-0.044977320314200084\n",
      "Regularized Logistic Regression(184/299): loss=0.5767428991502171, w0=-3.4970483185764325e-06, w1=-0.045142400593283706\n",
      "Regularized Logistic Regression(185/299): loss=0.5765524924638519, w0=-3.5141914877266665e-06, w1=-0.045306925513205074\n",
      "Regularized Logistic Regression(186/299): loss=0.5763639604127281, w0=-3.5313170392548502e-06, w1=-0.045470898155164516\n",
      "Regularized Logistic Regression(187/299): loss=0.576177281358948, w0=-3.5484249877820103e-06, w1=-0.04563432157625924\n",
      "Regularized Logistic Regression(188/299): loss=0.5759924339437261, w0=-3.5655153479202885e-06, w1=-0.045797198809725166\n",
      "Regularized Logistic Regression(189/299): loss=0.5758093970834657, w0=-3.5825881342737777e-06, w1=-0.04595953286517571\n",
      "Regularized Logistic Regression(190/299): loss=0.5756281499658897, w0=-3.599643361439328e-06, w1=-0.04612132672883775\n",
      "Regularized Logistic Regression(191/299): loss=0.5754486720462313, w0=-3.6166810440073204e-06, w1=-0.046282583363784685\n",
      "Regularized Logistic Regression(192/299): loss=0.5752709430434845, w0=-3.633701196562417e-06, w1=-0.046443305710166735\n",
      "Regularized Logistic Regression(193/299): loss=0.5750949429367057, w0=-3.650703833684279e-06, w1=-0.04660349668543845\n",
      "Regularized Logistic Regression(194/299): loss=0.5749206519613753, w0=-3.6676889699482603e-06, w1=-0.04676315918458339\n",
      "Regularized Logistic Regression(195/299): loss=0.5747480506058112, w0=-3.684656619926075e-06, w1=-0.046922296080336254\n",
      "Regularized Logistic Regression(196/299): loss=0.5745771196076369, w0=-3.7016067981864368e-06, w1=-0.04708091022340231\n",
      "Regularized Logistic Regression(197/299): loss=0.5744078399503014, w0=-3.7185395192956767e-06, w1=-0.04723900444267404\n",
      "Regularized Logistic Regression(198/299): loss=0.5742401928596537, w0=-3.7354547978183345e-06, w1=-0.04739658154544544\n",
      "Regularized Logistic Regression(199/299): loss=0.5740741598005653, w0=-3.752352648317729e-06, w1=-0.04755364431762363\n",
      "Regularized Logistic Regression(200/299): loss=0.5739097224736052, w0=-3.7692330853565012e-06, w1=-0.047710195523937925\n",
      "Regularized Logistic Regression(201/299): loss=0.5737468628117635, w0=-3.7860961234971423e-06, w1=-0.04786623790814651\n",
      "Regularized Logistic Regression(202/299): loss=0.5735855629772242, w0=-3.802941777302493e-06, w1=-0.048021774193240545\n",
      "Regularized Logistic Regression(203/299): loss=0.573425805358187, w0=-3.819770061336227e-06, w1=-0.04817680708164606\n",
      "Regularized Logistic Regression(204/299): loss=0.573267572565734, w0=-3.83658099016331e-06, w1=-0.04833133925542324\n",
      "Regularized Logistic Regression(205/299): loss=0.5731108474307464, w0=-3.853374578350444e-06, w1=-0.04848537337646341\n",
      "Regularized Logistic Regression(206/299): loss=0.5729556130008625, w0=-3.870150840466487e-06, w1=-0.04863891208668388\n",
      "Regularized Logistic Regression(207/299): loss=0.5728018525374863, w0=-3.886909791082859e-06, w1=-0.04879195800822026\n",
      "Regularized Logistic Regression(208/299): loss=0.5726495495128362, w0=-3.903651444773926e-06, w1=-0.04894451374361665\n",
      "Regularized Logistic Regression(209/299): loss=0.5724986876070396, w0=-3.92037581611737e-06, w1=-0.04909658187601362\n",
      "Regularized Logistic Regression(210/299): loss=0.5723492507052693, w0=-3.937082919694537e-06, w1=-0.04924816496933396\n",
      "Regularized Logistic Regression(211/299): loss=0.5722012228949241, w0=-3.953772770090773e-06, w1=-0.049399265568466154\n",
      "Regularized Logistic Regression(212/299): loss=0.5720545884628494, w0=-3.9704453818957444e-06, w1=-0.049549886199445996\n",
      "Regularized Logistic Regression(213/299): loss=0.5719093318925988, w0=-3.987100769703735e-06, w1=-0.04970002936963575\n",
      "Regularized Logistic Regression(214/299): loss=0.5717654378617388, w0=-4.003738948113941e-06, w1=-0.04984969756790157\n",
      "Regularized Logistic Regression(215/299): loss=0.5716228912391896, w0=-4.020359931730736e-06, w1=-0.04999889326478861\n",
      "Regularized Logistic Regression(216/299): loss=0.5714816770826078, w0=-4.036963735163937e-06, w1=-0.05014761891269422\n",
      "Regularized Logistic Regression(217/299): loss=0.5713417806358065, w0=-4.053550373029042e-06, w1=-0.0502958769460392\n",
      "Regularized Logistic Regression(218/299): loss=0.5712031873262139, w0=-4.070119859947464e-06, w1=-0.050443669781436905\n",
      "Regularized Logistic Regression(219/299): loss=0.5710658827623694, w0=-4.086672210546752e-06, w1=-0.05059099981786066\n",
      "Regularized Logistic Regression(220/299): loss=0.5709298527314551, w0=-4.103207439460791e-06, w1=-0.050737869436809\n",
      "Regularized Logistic Regression(221/299): loss=0.5707950831968662, w0=-4.119725561329999e-06, w1=-0.050884281002469174\n",
      "Regularized Logistic Regression(222/299): loss=0.5706615602958144, w0=-4.136226590801505e-06, w1=-0.05103023686187884\n",
      "Regularized Logistic Regression(223/299): loss=0.5705292703369682, w0=-4.1527105425293215e-06, w1=-0.051175739345085666\n",
      "Regularized Logistic Regression(224/299): loss=0.5703981997981271, w0=-4.1691774311745e-06, w1=-0.05132079076530542\n",
      "Regularized Logistic Regression(225/299): loss=0.5702683353239296, w0=-4.1856272714052796e-06, w1=-0.051465393419077986\n",
      "Regularized Logistic Regression(226/299): loss=0.5701396637235959, w0=-4.202060077897225e-06, w1=-0.051609549586421895\n",
      "Regularized Logistic Regression(227/299): loss=0.5700121719687014, w0=-4.21847586533335e-06, w1=-0.051753261530986874\n",
      "Regularized Logistic Regression(228/299): loss=0.5698858471909851, w0=-4.234874648404238e-06, w1=-0.05189653150020474\n",
      "Regularized Logistic Regression(229/299): loss=0.5697606766801896, w0=-4.251256441808146e-06, w1=-0.052039361725438674\n",
      "Regularized Logistic Regression(230/299): loss=0.5696366478819301, w0=-4.267621260251102e-06, w1=-0.052181754422130716\n",
      "Regularized Logistic Regression(231/299): loss=0.569513748395598, w0=-4.283969118446997e-06, w1=-0.052323711789947686\n",
      "Regularized Logistic Regression(232/299): loss=0.569391965972292, w0=-4.300300031117661e-06, w1=-0.05246523601292539\n",
      "Regularized Logistic Regression(233/299): loss=0.569271288512781, w0=-4.316614012992939e-06, w1=-0.05260632925961131\n",
      "Regularized Logistic Regression(234/299): loss=0.5691517040654971, w0=-4.33291107881075e-06, w1=-0.05274699368320561\n",
      "Regularized Logistic Regression(235/299): loss=0.5690332008245549, w0=-4.349191243317142e-06, w1=-0.052887231421700584\n",
      "Regularized Logistic Regression(236/299): loss=0.5689157671278037, w0=-4.365454521266346e-06, w1=-0.05302704459801868\n",
      "Regularized Logistic Regression(237/299): loss=0.5687993914549031, w0=-4.381700927420808e-06, w1=-0.053166435320148774\n",
      "Regularized Logistic Regression(238/299): loss=0.5686840624254305, w0=-4.397930476551229e-06, w1=-0.05330540568128118\n",
      "Regularized Logistic Regression(239/299): loss=0.5685697687970147, w0=-4.414143183436584e-06, w1=-0.053443957759940916\n",
      "Regularized Logistic Regression(240/299): loss=0.5684564994634944, w0=-4.430339062864149e-06, w1=-0.05358209362011975\n",
      "Regularized Logistic Regression(241/299): loss=0.5683442434531075, w0=-4.446518129629508e-06, w1=-0.05371981531140658\n",
      "Regularized Logistic Regression(242/299): loss=0.5682329899267009, w0=-4.462680398536562e-06, w1=-0.053857124869116535\n",
      "Regularized Logistic Regression(243/299): loss=0.5681227281759729, w0=-4.478825884397529e-06, w1=-0.05399402431441858\n",
      "Regularized Logistic Regression(244/299): loss=0.5680134476217337, w0=-4.494954602032939e-06, w1=-0.05413051565446174\n",
      "Regularized Logistic Regression(245/299): loss=0.5679051378121971, w0=-4.511066566271621e-06, w1=-0.054266600882499946\n",
      "Regularized Logistic Regression(246/299): loss=0.5677977884212921, w0=-4.527161791950691e-06, w1=-0.054402281978015575\n",
      "Regularized Logistic Regression(247/299): loss=0.5676913892470019, w0=-4.543240293915523e-06, w1=-0.0545375609068415\n",
      "Regularized Logistic Regression(248/299): loss=0.5675859302097249, w0=-4.5593020870197276e-06, w1=-0.05467243962128204\n",
      "Regularized Logistic Regression(249/299): loss=0.5674814013506597, w0=-4.5753471861251185e-06, w1=-0.05480692006023234\n",
      "Regularized Logistic Regression(250/299): loss=0.5673777928302133, w0=-4.591375606101676e-06, w1=-0.0549410041492967\n",
      "Regularized Logistic Regression(251/299): loss=0.567275094926432, w0=-4.607387361827504e-06, w1=-0.055074693800905426\n",
      "Regularized Logistic Regression(252/299): loss=0.5671732980334551, w0=-4.623382468188787e-06, w1=-0.05520799091443055\n",
      "Regularized Logistic Regression(253/299): loss=0.5670723926599893, w0=-4.639360940079737e-06, w1=-0.05534089737630035\n",
      "Regularized Logistic Regression(254/299): loss=0.5669723694278063, w0=-4.655322792402544e-06, w1=-0.05547341506011247\n",
      "Regularized Logistic Regression(255/299): loss=0.5668732190702609, w0=-4.671268040067311e-06, w1=-0.05560554582674588\n",
      "Regularized Logistic Regression(256/299): loss=0.5667749324308312, w0=-4.687196697991999e-06, w1=-0.05573729152447178\n",
      "Regularized Logistic Regression(257/299): loss=0.5666775004616779, w0=-4.703108781102358e-06, w1=-0.055868653989063176\n",
      "Regularized Logistic Regression(258/299): loss=0.5665809142222262, w0=-4.719004304331859e-06, w1=-0.055999635043903324\n",
      "Regularized Logistic Regression(259/299): loss=0.5664851648777642, w0=-4.734883282621619e-06, w1=-0.05613023650009307\n",
      "Regularized Logistic Regression(260/299): loss=0.5663902436980671, w0=-4.75074573092033e-06, w1=-0.056260460156556925\n",
      "Regularized Logistic Regression(261/299): loss=0.5662961420560338, w0=-4.766591664184174e-06, w1=-0.05639030780014819\n",
      "Regularized Logistic Regression(262/299): loss=0.5662028514263485, w0=-4.782421097376748e-06, w1=-0.05651978120575288\n",
      "Regularized Logistic Regression(263/299): loss=0.5661103633841573, w0=-4.798234045468975e-06, w1=-0.05664888213639243\n",
      "Regularized Logistic Regression(264/299): loss=0.5660186696037671, w0=-4.814030523439015e-06, w1=-0.056777612343325644\n",
      "Regularized Logistic Regression(265/299): loss=0.5659277618573585, w0=-4.82981054627218e-06, w1=-0.056905973566149216\n",
      "Regularized Logistic Regression(266/299): loss=0.5658376320137213, w0=-4.845574128960837e-06, w1=-0.05703396753289742\n",
      "Regularized Logistic Regression(267/299): loss=0.5657482720370035, w0=-4.861321286504314e-06, w1=-0.0571615959601407\n",
      "Regularized Logistic Regression(268/299): loss=0.5656596739854823, w0=-4.877052033908803e-06, w1=-0.05728886055308317\n",
      "Regularized Logistic Regression(269/299): loss=0.565571830010348, w0=-4.892766386187261e-06, w1=-0.05741576300565916\n",
      "Regularized Logistic Regression(270/299): loss=0.5654847323545076, w0=-4.908464358359306e-06, w1=-0.057542305000628846\n",
      "Regularized Logistic Regression(271/299): loss=0.5653983733514057, w0=-4.924145965451113e-06, w1=-0.05766848820967266\n",
      "Regularized Logistic Regression(272/299): loss=0.5653127454238582, w0=-4.939811222495313e-06, w1=-0.05779431429348481\n",
      "Regularized Logistic Regression(273/299): loss=0.5652278410829075, w0=-4.955460144530878e-06, w1=-0.0579197849018661\n",
      "Regularized Logistic Regression(274/299): loss=0.5651436529266887, w0=-4.971092746603019e-06, w1=-0.058044901673815305\n",
      "Regularized Logistic Regression(275/299): loss=0.5650601736393159, w0=-4.986709043763069e-06, w1=-0.05816966623762001\n",
      "Regularized Logistic Regression(276/299): loss=0.5649773959897799, w0=-5.002309051068373e-06, w1=-0.058294080210946314\n",
      "Regularized Logistic Regression(277/299): loss=0.5648953128308656, w0=-5.017892783582173e-06, w1=-0.058418145200927696\n",
      "Regularized Logistic Regression(278/299): loss=0.5648139170980795, w0=-5.033460256373495e-06, w1=-0.05854186280425288\n",
      "Regularized Logistic Regression(279/299): loss=0.5647332018085968, w0=-5.049011484517027e-06, w1=-0.058665234607252904\n",
      "Regularized Logistic Regression(280/299): loss=0.5646531600602213, w0=-5.0645464830930065e-06, w1=-0.05878826218598725\n",
      "Regularized Logistic Regression(281/299): loss=0.5645737850303569, w0=-5.080065267187095e-06, w1=-0.05891094710632908\n",
      "Regularized Logistic Regression(282/299): loss=0.5644950699749984, w0=-5.095567851890264e-06, w1=-0.05903329092404965\n",
      "Regularized Logistic Regression(283/299): loss=0.5644170082277333, w0=-5.111054252298668e-06, w1=-0.05915529518490185\n",
      "Regularized Logistic Regression(284/299): loss=0.5643395931987563, w0=-5.126524483513523e-06, w1=-0.05927696142470286\n",
      "Regularized Logistic Regression(285/299): loss=0.5642628183739005, w0=-5.141978560640986e-06, w1=-0.05939829116941598\n",
      "Regularized Logistic Regression(286/299): loss=0.5641866773136799, w0=-5.157416498792026e-06, w1=-0.05951928593523173\n",
      "Regularized Logistic Regression(287/299): loss=0.5641111636523457, w0=-5.172838313082304e-06, w1=-0.05963994722864798\n",
      "Regularized Logistic Regression(288/299): loss=0.5640362710969545, w0=-5.188244018632041e-06, w1=-0.05976027654654938\n",
      "Regularized Logistic Regression(289/299): loss=0.5639619934264515, w0=-5.203633630565895e-06, w1=-0.059880275376286\n",
      "Regularized Logistic Regression(290/299): loss=0.5638883244907635, w0=-5.219007164012834e-06, w1=-0.05999994519575108\n",
      "Regularized Logistic Regression(291/299): loss=0.5638152582099075, w0=-5.234364634106006e-06, w1=-0.060119287473458106\n",
      "Regularized Logistic Regression(292/299): loss=0.5637427885731084, w0=-5.249706055982612e-06, w1=-0.06023830366861707\n",
      "Regularized Logistic Regression(293/299): loss=0.563670909637931, w0=-5.265031444783776e-06, w1=-0.06035699523120997\n",
      "Regularized Logistic Regression(294/299): loss=0.5635996155294228, w0=-5.280340815654416e-06, w1=-0.060475363602065575\n",
      "Regularized Logistic Regression(295/299): loss=0.5635289004392682, w0=-5.2956341837431145e-06, w1=-0.0605934102129334\n",
      "Regularized Logistic Regression(296/299): loss=0.5634587586249558, w0=-5.310911564201986e-06, w1=-0.06071113648655709\n",
      "Regularized Logistic Regression(297/299): loss=0.563389184408955, w0=-5.326172972186552e-06, w1=-0.06082854383674686\n",
      "Regularized Logistic Regression(298/299): loss=0.5633201721779053, w0=-5.3414184228556036e-06, w1=-0.06094563366845139\n",
      "Regularized Logistic Regression(299/299): loss=0.5632517163818164, w0=-5.356647931371076e-06, w1=-0.06106240737782892\n",
      "Regularized Logistic Regression(0/99): loss=0.693147180559945, w0=-2.0882930292778682e-08, w1=-0.0003658898216597753\n",
      "Regularized Logistic Regression(1/99): loss=0.691570764967098, w0=-4.155061673532941e-08, w1=-0.000726447155859764\n",
      "Regularized Logistic Regression(2/99): loss=0.6900427493731593, w0=-6.200544791465143e-08, w1=-0.0010817557138626016\n",
      "Regularized Logistic Regression(3/99): loss=0.6885616307405306, w0=-8.22497803369991e-08, w1=-0.001431897818366024\n",
      "Regularized Logistic Regression(4/99): loss=0.6871259530052786, w0=-1.0228593901084923e-07, w1=-0.0017769544244776343\n",
      "Regularized Logistic Regression(5/99): loss=0.6857343056230828, w0=-1.2211621801676715e-07, w1=-0.0021170051406546424\n",
      "Regularized Logistic Regression(6/99): loss=0.6843853221576848, w0=-1.4174288106443043e-07, w1=-0.002452128249584036\n",
      "Regularized Logistic Regression(7/99): loss=0.6830776789108608, w0=-1.6116816203707062e-07, w1=-0.002782400728980684\n",
      "Regularized Logistic Regression(8/99): loss=0.6818100935929228, w0=-1.803942655235933e-07, w1=-0.0031078982722825468\n",
      "Regularized Logistic Regression(9/99): loss=0.6805813240327646, w0=-1.9942336733863663e-07, w1=-0.0034286953092240373\n",
      "Regularized Logistic Regression(10/99): loss=0.6793901669264669, w0=-2.1825761503082792e-07, w1=-0.003744865026270114\n",
      "Regularized Logistic Regression(11/99): loss=0.6782354566234853, w0=-2.368991283794964e-07, w1=-0.004056479386895273\n",
      "Regularized Logistic Regression(12/99): loss=0.677116063949456, w0=-2.553499998800989e-07, w1=-0.004363609151693511\n",
      "Regularized Logistic Regression(13/99): loss=0.6760308950646551, w0=-2.736122952186129e-07, w1=-0.004666323898305762\n",
      "Regularized Logistic Regression(14/99): loss=0.6749788903571698, w0=-2.9168805373514934e-07, w1=-0.004964692041153607\n",
      "Regularized Logistic Regression(15/99): loss=0.6739590233698397, w0=-3.0957928887703486e-07, w1=-0.005258780850968626\n",
      "Regularized Logistic Regression(16/99): loss=0.6729702997600534, w0=-3.272879886416101e-07, w1=-0.005548656474108082\n",
      "Regularized Logistic Regression(17/99): loss=0.6720117562914919, w0=-3.4481611600898726e-07, w1=-0.0058343839516489995\n",
      "Regularized Logistic Regression(18/99): loss=0.6710824598569229, w0=-3.6216560936500765e-07, w1=-0.006116027238252714\n",
      "Regularized Logistic Regression(19/99): loss=0.6701815065311797, w0=-3.7933838291463546e-07, w1=-0.006393649220794519\n",
      "Regularized Logistic Regression(20/99): loss=0.6693080206534551, w0=-3.9633632708602046e-07, w1=-0.006667311736751995\n",
      "Regularized Logistic Regression(21/99): loss=0.6684611539380813, w0=-4.13161308925459e-07, w1=-0.006937075592347969\n",
      "Regularized Logistic Regression(22/99): loss=0.6676400846129609, w0=-4.29815172483478e-07, w1=-0.007203000580443894\n",
      "Regularized Logistic Regression(23/99): loss=0.6668440165848487, w0=-4.46299739192263e-07, w1=-0.007465145498180636\n",
      "Regularized Logistic Regression(24/99): loss=0.6660721786306957, w0=-4.6261680823464667e-07, w1=-0.007723568164363712\n",
      "Regularized Logistic Regression(25/99): loss=0.6653238236142842, w0=-4.78768156904871e-07, w1=-0.007978325436591154\n",
      "Regularized Logistic Regression(26/99): loss=0.6645982277274026, w0=-4.947555409613305e-07, w1=-0.008229473228122767\n",
      "Regularized Logistic Regression(27/99): loss=0.6638946897548245, w0=-5.105806949715015e-07, w1=-0.008477066524489195\n",
      "Regularized Logistic Regression(28/99): loss=0.6632125303623808, w0=-5.262453326492555e-07, w1=-0.008721159399840934\n",
      "Regularized Logistic Regression(29/99): loss=0.6625510914074222, w0=-5.417511471847536e-07, w1=-0.008961805033036696\n",
      "Regularized Logistic Regression(30/99): loss=0.6619097352709974, w0=-5.57099811567112e-07, w1=-0.009199055723471811\n",
      "Regularized Logistic Regression(31/99): loss=0.661287844211081, w0=-5.72292978900025e-07, w1=-0.009432962906646974\n",
      "Regularized Logistic Regression(32/99): loss=0.6606848197362093, w0=-5.873322827105278e-07, w1=-0.009663577169478594\n",
      "Regularized Logistic Regression(33/99): loss=0.6601000819988982, w0=-6.022193372510782e-07, w1=-0.009890948265351895\n",
      "Regularized Logistic Regression(34/99): loss=0.6595330692082289, w0=-6.169557377951289e-07, w1=-0.010115125128918285\n",
      "Regularized Logistic Regression(35/99): loss=0.658983237061017, w0=-6.315430609263621e-07, w1=-0.010336155890638752\n",
      "Regularized Logistic Regression(36/99): loss=0.6584500581909778, w0=-6.459828648217496e-07, w1=-0.010554087891075628\n",
      "Regularized Logistic Regression(37/99): loss=0.6579330216353392, w0=-6.602766895286017e-07, w1=-0.010768967694934368\n",
      "Regularized Logistic Regression(38/99): loss=0.6574316323183473, w0=-6.744260572357601e-07, w1=-0.01098084110485829\n",
      "Regularized Logistic Regression(39/99): loss=0.6569454105511489, w0=-6.884324725390895e-07, w1=-0.011189753174978414\n",
      "Regularized Logistic Regression(40/99): loss=0.6564738915475244, w0=-7.02297422701416e-07, w1=-0.011395748224221295\n",
      "Regularized Logistic Regression(41/99): loss=0.656016624954985, w0=-7.160223779070584e-07, w1=-0.011598869849377644\n",
      "Regularized Logistic Regression(42/99): loss=0.6555731744007448, w0=-7.296087915110933e-07, w1=-0.011799160937934606\n",
      "Regularized Logistic Regression(43/99): loss=0.6551431170520983, w0=-7.430581002834917e-07, w1=-0.01199666368067483\n",
      "Regularized Logistic Regression(44/99): loss=0.6547260431907549, w0=-7.563717246482623e-07, w1=-0.01219141958404522\n",
      "Regularized Logistic Regression(45/99): loss=0.6543215558006814, w0=-7.695510689177299e-07, w1=-0.012383469482298841\n",
      "Regularized Logistic Regression(46/99): loss=0.6539292701690314, w0=-7.825975215220783e-07, w1=-0.012572853549412937\n",
      "Regularized Logistic Regression(47/99): loss=0.6535488134997415, w0=-7.955124552342791e-07, w1=-0.012759611310786578\n",
      "Regularized Logistic Regression(48/99): loss=0.6531798245393972, w0=-8.082972273905268e-07, w1=-0.01294378165472111\n",
      "Regularized Logistic Regression(49/99): loss=0.6528219532149743, w0=-8.209531801062992e-07, w1=-0.013125402843686766\n",
      "Regularized Logistic Regression(50/99): loss=0.6524748602830823, w0=-8.33481640488153e-07, w1=-0.013304512525379047\n",
      "Regularized Logistic Regression(51/99): loss=0.6521382169903406, w0=-8.458839208413679e-07, w1=-0.013481147743567881\n",
      "Regularized Logistic Regression(52/99): loss=0.6518117047445345, w0=-8.581613188735458e-07, w1=-0.013655344948743296\n",
      "Regularized Logistic Regression(53/99): loss=0.6514950147962064, w0=-8.703151178942679e-07, w1=-0.013827140008561\n",
      "Regularized Logistic Regression(54/99): loss=0.6511878479303497, w0=-8.823465870109136e-07, w1=-0.01399656821809099\n",
      "Regularized Logistic Regression(55/99): loss=0.6508899141678853, w0=-8.942569813207374e-07, w1=-0.014163664309872922\n",
      "Regularized Logistic Regression(56/99): loss=0.6506009324766032, w0=-9.060475420993006e-07, w1=-0.014328462463781447\n",
      "Regularized Logistic Regression(57/99): loss=0.6503206304912726, w0=-9.177194969853516e-07, w1=-0.014490996316704956\n",
      "Regularized Logistic Regression(58/99): loss=0.6500487442426232, w0=-9.292740601622426e-07, w1=-0.014651298972041047\n",
      "Regularized Logistic Regression(59/99): loss=0.649785017894916, w0=-9.40712432535973e-07, w1=-0.014809403009012007\n",
      "Regularized Logistic Regression(60/99): loss=0.6495292034918293, w0=-9.520358019099438e-07, w1=-0.014965340491803846\n",
      "Regularized Logistic Regression(61/99): loss=0.6492810607103923, w0=-9.632453431565056e-07, w1=-0.015119142978531612\n",
      "Regularized Logistic Regression(62/99): loss=0.6490403566227122, w0=-9.743422183853803e-07, w1=-0.015270841530034968\n",
      "Regularized Logistic Regression(63/99): loss=0.6488068654652402, w0=-9.853275771090361e-07, w1=-0.015420466718506543\n",
      "Regularized Logistic Regression(64/99): loss=0.6485803684153401, w0=-9.9620255640509e-07, w1=-0.01556804863595664\n",
      "Regularized Logistic Regression(65/99): loss=0.6483606533749229, w0=-1.006968281075812e-06, w1=-0.015713616902517328\n",
      "Regularized Logistic Regression(66/99): loss=0.6481475147609255, w0=-1.0176258638048036e-06, w1=-0.015857200674588873\n",
      "Regularized Logistic Regression(67/99): loss=0.6479407533024055, w0=-1.0281764053109179e-06, w1=-0.015998828652831743\n",
      "Regularized Logistic Regression(68/99): loss=0.6477401758440525, w0=-1.0386209944994915e-06, w1=-0.016138529090007073\n",
      "Regularized Logistic Regression(69/99): loss=0.6475455951559005, w0=-1.0489607086109518e-06, w1=-0.01627632979866859\n",
      "Regularized Logistic Regression(70/99): loss=0.6473568297490486, w0=-1.0591966133668623e-06, w1=-0.016412258158708826\n",
      "Regularized Logistic Regression(71/99): loss=0.6471737036971972, w0=-1.0693297631134727e-06, w1=-0.016546341124762713\n",
      "Regularized Logistic Regression(72/99): loss=0.646996046463809, w0=-1.0793612009628273e-06, w1=-0.016678605233470974\n",
      "Regularized Logistic Regression(73/99): loss=0.6468236927347252, w0=-1.0892919589314941e-06, w1=-0.016809076610606603\n",
      "Regularized Logistic Regression(74/99): loss=0.6466564822560492, w0=-1.0991230580769714e-06, w1=-0.016937780978066717\n",
      "Regularized Logistic Regression(75/99): loss=0.6464942596771427, w0=-1.1088555086318233e-06, w1=-0.017064743660732712\n",
      "Regularized Logistic Regression(76/99): loss=0.646336874398559, w0=-1.118490310135603e-06, w1=-0.017189989593201244\n",
      "Regularized Logistic Regression(77/99): loss=0.6461841804247682, w0=-1.1280284515646093e-06, w1=-0.01731354332638873\n",
      "Regularized Logistic Regression(78/99): loss=0.6460360362215062, w0=-1.1374709114595333e-06, w1=-0.017435429034011834\n",
      "Regularized Logistic Regression(79/99): loss=0.645892304577617, w0=-1.146818658051038e-06, w1=-0.017555670518946347\n",
      "Regularized Logistic Regression(80/99): loss=0.6457528524712305, w0=-1.1560726493833242e-06, w1=-0.017674291219467123\n",
      "Regularized Logistic Regression(81/99): loss=0.645617550940145, w0=-1.165233833435723e-06, w1=-0.017791314215371386\n",
      "Regularized Logistic Regression(82/99): loss=0.64548627495628, w0=-1.1743031482423659e-06, w1=-0.017906762233987558\n",
      "Regularized Logistic Regression(83/99): loss=0.6453589033040671, w0=-1.1832815220099703e-06, w1=-0.01802065765607239\n",
      "Regularized Logistic Regression(84/99): loss=0.645235318462654, w0=-1.1921698732337867e-06, w1=-0.0181330225215981\n",
      "Regularized Logistic Regression(85/99): loss=0.6451154064918042, w0=-1.2009691108117468e-06, w1=-0.018243878535432155\n",
      "Regularized Logistic Regression(86/99): loss=0.6449990569213667, w0=-1.2096801341568537e-06, w1=-0.018353247072911753\n",
      "Regularized Logistic Regression(87/99): loss=0.6448861626442124, w0=-1.2183038333078519e-06, w1=-0.018461149185315075\n",
      "Regularized Logistic Regression(88/99): loss=0.6447766198125171, w0=-1.2268410890382149e-06, w1=-0.018567605605231442\n",
      "Regularized Logistic Regression(89/99): loss=0.6446703277372916, w0=-1.2352927729634886e-06, w1=-0.018672636751832533\n",
      "Regularized Logistic Regression(90/99): loss=0.6445671887910529, w0=-1.2436597476470248e-06, w1=-0.018776262736046465\n",
      "Regularized Logistic Regression(91/99): loss=0.6444671083135372, w0=-1.251942866704138e-06, w1=-0.018878503365636828\n",
      "Regularized Logistic Regression(92/99): loss=0.644369994520358, w0=-1.2601429749047234e-06, w1=-0.01897937815018859\n",
      "Regularized Logistic Regression(93/99): loss=0.644275758414516, w0=-1.268260908274365e-06, w1=-0.019078906306002934\n",
      "Regularized Logistic Regression(94/99): loss=0.6441843137006713, w0=-1.2762974941939673e-06, w1=-0.01917710676090226\n",
      "Regularized Logistic Regression(95/99): loss=0.6440955767020871, w0=-1.2842535514979403e-06, w1=-0.019273998158948185\n",
      "Regularized Logistic Regression(96/99): loss=0.6440094662801665, w0=-1.2921298905709704e-06, w1=-0.019369598865073126\n",
      "Regularized Logistic Regression(97/99): loss=0.6439259037564935, w0=-1.2999273134434039e-06, w1=-0.019463926969628238\n",
      "Regularized Logistic Regression(98/99): loss=0.6438448128373018, w0=-1.3076466138852724e-06, w1=-0.019557000292848813\n",
      "Regularized Logistic Regression(99/99): loss=0.6437661195402974, w0=-1.3152885774989891e-06, w1=-0.01964883638923885\n",
      "Regularized Logistic Regression(0/199): loss=0.693147180559945, w0=-2.0882930292778682e-08, w1=-0.0003658898216597753\n",
      "Regularized Logistic Regression(1/199): loss=0.691570764967098, w0=-4.155061673532941e-08, w1=-0.000726447155859764\n",
      "Regularized Logistic Regression(2/199): loss=0.6900427493731593, w0=-6.200544791465143e-08, w1=-0.0010817557138626016\n",
      "Regularized Logistic Regression(3/199): loss=0.6885616307405306, w0=-8.22497803369991e-08, w1=-0.001431897818366024\n",
      "Regularized Logistic Regression(4/199): loss=0.6871259530052786, w0=-1.0228593901084923e-07, w1=-0.0017769544244776343\n",
      "Regularized Logistic Regression(5/199): loss=0.6857343056230828, w0=-1.2211621801676715e-07, w1=-0.0021170051406546424\n",
      "Regularized Logistic Regression(6/199): loss=0.6843853221576848, w0=-1.4174288106443043e-07, w1=-0.002452128249584036\n",
      "Regularized Logistic Regression(7/199): loss=0.6830776789108608, w0=-1.6116816203707062e-07, w1=-0.002782400728980684\n",
      "Regularized Logistic Regression(8/199): loss=0.6818100935929228, w0=-1.803942655235933e-07, w1=-0.0031078982722825468\n",
      "Regularized Logistic Regression(9/199): loss=0.6805813240327646, w0=-1.9942336733863663e-07, w1=-0.0034286953092240373\n",
      "Regularized Logistic Regression(10/199): loss=0.6793901669264669, w0=-2.1825761503082792e-07, w1=-0.003744865026270114\n",
      "Regularized Logistic Regression(11/199): loss=0.6782354566234853, w0=-2.368991283794964e-07, w1=-0.004056479386895273\n",
      "Regularized Logistic Regression(12/199): loss=0.677116063949456, w0=-2.553499998800989e-07, w1=-0.004363609151693511\n",
      "Regularized Logistic Regression(13/199): loss=0.6760308950646551, w0=-2.736122952186129e-07, w1=-0.004666323898305762\n",
      "Regularized Logistic Regression(14/199): loss=0.6749788903571698, w0=-2.9168805373514934e-07, w1=-0.004964692041153607\n",
      "Regularized Logistic Regression(15/199): loss=0.6739590233698397, w0=-3.0957928887703486e-07, w1=-0.005258780850968626\n",
      "Regularized Logistic Regression(16/199): loss=0.6729702997600534, w0=-3.272879886416101e-07, w1=-0.005548656474108082\n",
      "Regularized Logistic Regression(17/199): loss=0.6720117562914919, w0=-3.4481611600898726e-07, w1=-0.0058343839516489995\n",
      "Regularized Logistic Regression(18/199): loss=0.6710824598569229, w0=-3.6216560936500765e-07, w1=-0.006116027238252714\n",
      "Regularized Logistic Regression(19/199): loss=0.6701815065311797, w0=-3.7933838291463546e-07, w1=-0.006393649220794519\n",
      "Regularized Logistic Regression(20/199): loss=0.6693080206534551, w0=-3.9633632708602046e-07, w1=-0.006667311736751995\n",
      "Regularized Logistic Regression(21/199): loss=0.6684611539380813, w0=-4.13161308925459e-07, w1=-0.006937075592347969\n",
      "Regularized Logistic Regression(22/199): loss=0.6676400846129609, w0=-4.29815172483478e-07, w1=-0.007203000580443894\n",
      "Regularized Logistic Regression(23/199): loss=0.6668440165848487, w0=-4.46299739192263e-07, w1=-0.007465145498180636\n",
      "Regularized Logistic Regression(24/199): loss=0.6660721786306957, w0=-4.6261680823464667e-07, w1=-0.007723568164363712\n",
      "Regularized Logistic Regression(25/199): loss=0.6653238236142842, w0=-4.78768156904871e-07, w1=-0.007978325436591154\n",
      "Regularized Logistic Regression(26/199): loss=0.6645982277274026, w0=-4.947555409613305e-07, w1=-0.008229473228122767\n",
      "Regularized Logistic Regression(27/199): loss=0.6638946897548245, w0=-5.105806949715015e-07, w1=-0.008477066524489195\n",
      "Regularized Logistic Regression(28/199): loss=0.6632125303623808, w0=-5.262453326492555e-07, w1=-0.008721159399840934\n",
      "Regularized Logistic Regression(29/199): loss=0.6625510914074222, w0=-5.417511471847536e-07, w1=-0.008961805033036696\n",
      "Regularized Logistic Regression(30/199): loss=0.6619097352709974, w0=-5.57099811567112e-07, w1=-0.009199055723471811\n",
      "Regularized Logistic Regression(31/199): loss=0.661287844211081, w0=-5.72292978900025e-07, w1=-0.009432962906646974\n",
      "Regularized Logistic Regression(32/199): loss=0.6606848197362093, w0=-5.873322827105278e-07, w1=-0.009663577169478594\n",
      "Regularized Logistic Regression(33/199): loss=0.6601000819988982, w0=-6.022193372510782e-07, w1=-0.009890948265351895\n",
      "Regularized Logistic Regression(34/199): loss=0.6595330692082289, w0=-6.169557377951289e-07, w1=-0.010115125128918285\n",
      "Regularized Logistic Regression(35/199): loss=0.658983237061017, w0=-6.315430609263621e-07, w1=-0.010336155890638752\n",
      "Regularized Logistic Regression(36/199): loss=0.6584500581909778, w0=-6.459828648217496e-07, w1=-0.010554087891075628\n",
      "Regularized Logistic Regression(37/199): loss=0.6579330216353392, w0=-6.602766895286017e-07, w1=-0.010768967694934368\n",
      "Regularized Logistic Regression(38/199): loss=0.6574316323183473, w0=-6.744260572357601e-07, w1=-0.01098084110485829\n",
      "Regularized Logistic Regression(39/199): loss=0.6569454105511489, w0=-6.884324725390895e-07, w1=-0.011189753174978414\n",
      "Regularized Logistic Regression(40/199): loss=0.6564738915475244, w0=-7.02297422701416e-07, w1=-0.011395748224221295\n",
      "Regularized Logistic Regression(41/199): loss=0.656016624954985, w0=-7.160223779070584e-07, w1=-0.011598869849377644\n",
      "Regularized Logistic Regression(42/199): loss=0.6555731744007448, w0=-7.296087915110933e-07, w1=-0.011799160937934606\n",
      "Regularized Logistic Regression(43/199): loss=0.6551431170520983, w0=-7.430581002834917e-07, w1=-0.01199666368067483\n",
      "Regularized Logistic Regression(44/199): loss=0.6547260431907549, w0=-7.563717246482623e-07, w1=-0.01219141958404522\n",
      "Regularized Logistic Regression(45/199): loss=0.6543215558006814, w0=-7.695510689177299e-07, w1=-0.012383469482298841\n",
      "Regularized Logistic Regression(46/199): loss=0.6539292701690314, w0=-7.825975215220783e-07, w1=-0.012572853549412937\n",
      "Regularized Logistic Regression(47/199): loss=0.6535488134997415, w0=-7.955124552342791e-07, w1=-0.012759611310786578\n",
      "Regularized Logistic Regression(48/199): loss=0.6531798245393972, w0=-8.082972273905268e-07, w1=-0.01294378165472111\n",
      "Regularized Logistic Regression(49/199): loss=0.6528219532149743, w0=-8.209531801062992e-07, w1=-0.013125402843686766\n",
      "Regularized Logistic Regression(50/199): loss=0.6524748602830823, w0=-8.33481640488153e-07, w1=-0.013304512525379047\n",
      "Regularized Logistic Regression(51/199): loss=0.6521382169903406, w0=-8.458839208413679e-07, w1=-0.013481147743567881\n",
      "Regularized Logistic Regression(52/199): loss=0.6518117047445345, w0=-8.581613188735458e-07, w1=-0.013655344948743296\n",
      "Regularized Logistic Regression(53/199): loss=0.6514950147962064, w0=-8.703151178942679e-07, w1=-0.013827140008561\n",
      "Regularized Logistic Regression(54/199): loss=0.6511878479303497, w0=-8.823465870109136e-07, w1=-0.01399656821809099\n",
      "Regularized Logistic Regression(55/199): loss=0.6508899141678853, w0=-8.942569813207374e-07, w1=-0.014163664309872922\n",
      "Regularized Logistic Regression(56/199): loss=0.6506009324766032, w0=-9.060475420993006e-07, w1=-0.014328462463781447\n",
      "Regularized Logistic Regression(57/199): loss=0.6503206304912726, w0=-9.177194969853516e-07, w1=-0.014490996316704956\n",
      "Regularized Logistic Regression(58/199): loss=0.6500487442426232, w0=-9.292740601622426e-07, w1=-0.014651298972041047\n",
      "Regularized Logistic Regression(59/199): loss=0.649785017894916, w0=-9.40712432535973e-07, w1=-0.014809403009012007\n",
      "Regularized Logistic Regression(60/199): loss=0.6495292034918293, w0=-9.520358019099438e-07, w1=-0.014965340491803846\n",
      "Regularized Logistic Regression(61/199): loss=0.6492810607103923, w0=-9.632453431565056e-07, w1=-0.015119142978531612\n",
      "Regularized Logistic Regression(62/199): loss=0.6490403566227122, w0=-9.743422183853803e-07, w1=-0.015270841530034968\n",
      "Regularized Logistic Regression(63/199): loss=0.6488068654652402, w0=-9.853275771090361e-07, w1=-0.015420466718506543\n",
      "Regularized Logistic Regression(64/199): loss=0.6485803684153401, w0=-9.9620255640509e-07, w1=-0.01556804863595664\n",
      "Regularized Logistic Regression(65/199): loss=0.6483606533749229, w0=-1.006968281075812e-06, w1=-0.015713616902517328\n",
      "Regularized Logistic Regression(66/199): loss=0.6481475147609255, w0=-1.0176258638048036e-06, w1=-0.015857200674588873\n",
      "Regularized Logistic Regression(67/199): loss=0.6479407533024055, w0=-1.0281764053109179e-06, w1=-0.015998828652831743\n",
      "Regularized Logistic Regression(68/199): loss=0.6477401758440525, w0=-1.0386209944994915e-06, w1=-0.016138529090007073\n",
      "Regularized Logistic Regression(69/199): loss=0.6475455951559005, w0=-1.0489607086109518e-06, w1=-0.01627632979866859\n",
      "Regularized Logistic Regression(70/199): loss=0.6473568297490486, w0=-1.0591966133668623e-06, w1=-0.016412258158708826\n",
      "Regularized Logistic Regression(71/199): loss=0.6471737036971972, w0=-1.0693297631134727e-06, w1=-0.016546341124762713\n",
      "Regularized Logistic Regression(72/199): loss=0.646996046463809, w0=-1.0793612009628273e-06, w1=-0.016678605233470974\n",
      "Regularized Logistic Regression(73/199): loss=0.6468236927347252, w0=-1.0892919589314941e-06, w1=-0.016809076610606603\n",
      "Regularized Logistic Regression(74/199): loss=0.6466564822560492, w0=-1.0991230580769714e-06, w1=-0.016937780978066717\n",
      "Regularized Logistic Regression(75/199): loss=0.6464942596771427, w0=-1.1088555086318233e-06, w1=-0.017064743660732712\n",
      "Regularized Logistic Regression(76/199): loss=0.646336874398559, w0=-1.118490310135603e-06, w1=-0.017189989593201244\n",
      "Regularized Logistic Regression(77/199): loss=0.6461841804247682, w0=-1.1280284515646093e-06, w1=-0.01731354332638873\n",
      "Regularized Logistic Regression(78/199): loss=0.6460360362215062, w0=-1.1374709114595333e-06, w1=-0.017435429034011834\n",
      "Regularized Logistic Regression(79/199): loss=0.645892304577617, w0=-1.146818658051038e-06, w1=-0.017555670518946347\n",
      "Regularized Logistic Regression(80/199): loss=0.6457528524712305, w0=-1.1560726493833242e-06, w1=-0.017674291219467123\n",
      "Regularized Logistic Regression(81/199): loss=0.645617550940145, w0=-1.165233833435723e-06, w1=-0.017791314215371386\n",
      "Regularized Logistic Regression(82/199): loss=0.64548627495628, w0=-1.1743031482423659e-06, w1=-0.017906762233987558\n",
      "Regularized Logistic Regression(83/199): loss=0.6453589033040671, w0=-1.1832815220099703e-06, w1=-0.01802065765607239\n",
      "Regularized Logistic Regression(84/199): loss=0.645235318462654, w0=-1.1921698732337867e-06, w1=-0.0181330225215981\n",
      "Regularized Logistic Regression(85/199): loss=0.6451154064918042, w0=-1.2009691108117468e-06, w1=-0.018243878535432155\n",
      "Regularized Logistic Regression(86/199): loss=0.6449990569213667, w0=-1.2096801341568537e-06, w1=-0.018353247072911753\n",
      "Regularized Logistic Regression(87/199): loss=0.6448861626442124, w0=-1.2183038333078519e-06, w1=-0.018461149185315075\n",
      "Regularized Logistic Regression(88/199): loss=0.6447766198125171, w0=-1.2268410890382149e-06, w1=-0.018567605605231442\n",
      "Regularized Logistic Regression(89/199): loss=0.6446703277372916, w0=-1.2352927729634886e-06, w1=-0.018672636751832533\n",
      "Regularized Logistic Regression(90/199): loss=0.6445671887910529, w0=-1.2436597476470248e-06, w1=-0.018776262736046465\n",
      "Regularized Logistic Regression(91/199): loss=0.6444671083135372, w0=-1.251942866704138e-06, w1=-0.018878503365636828\n",
      "Regularized Logistic Regression(92/199): loss=0.644369994520358, w0=-1.2601429749047234e-06, w1=-0.01897937815018859\n",
      "Regularized Logistic Regression(93/199): loss=0.644275758414516, w0=-1.268260908274365e-06, w1=-0.019078906306002934\n",
      "Regularized Logistic Regression(94/199): loss=0.6441843137006713, w0=-1.2762974941939673e-06, w1=-0.01917710676090226\n",
      "Regularized Logistic Regression(95/199): loss=0.6440955767020871, w0=-1.2842535514979403e-06, w1=-0.019273998158948185\n",
      "Regularized Logistic Regression(96/199): loss=0.6440094662801665, w0=-1.2921298905709704e-06, w1=-0.019369598865073126\n",
      "Regularized Logistic Regression(97/199): loss=0.6439259037564935, w0=-1.2999273134434039e-06, w1=-0.019463926969628238\n",
      "Regularized Logistic Regression(98/199): loss=0.6438448128373018, w0=-1.3076466138852724e-06, w1=-0.019557000292848813\n",
      "Regularized Logistic Regression(99/199): loss=0.6437661195402974, w0=-1.3152885774989891e-06, w1=-0.01964883638923885\n",
      "Regularized Logistic Regression(100/199): loss=0.6436897521237556, w0=-1.3228539818107404e-06, w1=-0.01973945255187667\n",
      "Regularized Logistic Regression(101/199): loss=0.6436156410178232, w0=-1.3303435963606014e-06, w1=-0.019828865816642942\n",
      "Regularized Logistic Regression(102/199): loss=0.6435437187579572, w0=-1.3377581827913992e-06, w1=-0.019917092966372747\n",
      "Regularized Logistic Regression(103/199): loss=0.64347391992043, w0=-1.3450984949363503e-06, w1=-0.020004150534933273\n",
      "Regularized Logistic Regression(104/199): loss=0.643406181059838, w0=-1.352365278905494e-06, w1=-0.020090054811228466\n",
      "Regularized Logistic Regression(105/199): loss=0.6433404406485498, w0=-1.3595592731709487e-06, w1=-0.020174821843132327\n",
      "Regularized Logistic Regression(106/199): loss=0.6432766390180316, w0=-1.3666812086510092e-06, w1=-0.020258467441352\n",
      "Regularized Logistic Regression(107/199): loss=0.6432147183019936, w0=-1.3737318087931132e-06, w1=-0.020341007183222386\n",
      "Regularized Logistic Regression(108/199): loss=0.6431546223812978, w0=-1.3807117896556934e-06, w1=-0.020422456416433254\n",
      "Regularized Logistic Regression(109/199): loss=0.6430962968305707, w0=-1.3876218599889383e-06, w1=-0.02050283026269061\n",
      "Regularized Logistic Regression(110/199): loss=0.6430396888664712, w0=-1.3944627213144844e-06, w1=-0.020582143621313165\n",
      "Regularized Logistic Regression(111/199): loss=0.6429847472975585, w0=-1.4012350680040558e-06, w1=-0.02066041117276577\n",
      "Regularized Logistic Regression(112/199): loss=0.6429314224757101, w0=-1.407939587357074e-06, w1=-0.02073764738213043\n",
      "Regularized Logistic Regression(113/199): loss=0.6428796662490434, w0=-1.414576959677255e-06, w1=-0.020813866502516543\n",
      "Regularized Logistic Regression(114/199): loss=0.6428294319162897, w0=-1.4211478583482136e-06, w1=-0.020889082578411523\n",
      "Regularized Logistic Regression(115/199): loss=0.6427806741825797, w0=-1.4276529499080907e-06, w1=-0.020963309448972733\n",
      "Regularized Logistic Regression(116/199): loss=0.642733349116593, w0=-1.4340928941232255e-06, w1=-0.02103656075126223\n",
      "Regularized Logistic Regression(117/199): loss=0.6426874141090299, w0=-1.4404683440608843e-06, w1=-0.021108849923425148\n",
      "Regularized Logistic Regression(118/199): loss=0.6426428278323646, w0=-1.4467799461610664e-06, w1=-0.021180190207812973\n",
      "Regularized Logistic Regression(119/199): loss=0.6425995502018383, w0=-1.4530283403074027e-06, w1=-0.021250594654052653\n",
      "Regularized Logistic Regression(120/199): loss=0.642557542337657, w0=-1.4592141598971616e-06, w1=-0.021320076122062877\n",
      "Regularized Logistic Regression(121/199): loss=0.6425167665283483, w0=-1.4653380319103791e-06, w1=-0.021388647285018138\n",
      "Regularized Logistic Regression(122/199): loss=0.6424771861952525, w0=-1.4714005769781282e-06, w1=-0.021456320632261962\n",
      "Regularized Logistic Regression(123/199): loss=0.6424387658581022, w0=-1.4774024094499407e-06, w1=-0.02152310847217009\n",
      "Regularized Logistic Regression(124/199): loss=0.6424014711016631, w0=-1.4833441374603975e-06, w1=-0.021589022934964498\n",
      "Regularized Logistic Regression(125/199): loss=0.6423652685433984, w0=-1.4892263629949014e-06, w1=-0.021654075975479598\n",
      "Regularized Logistic Regression(126/199): loss=0.6423301258021301, w0=-1.4950496819546449e-06, w1=-0.021718279375880944\n",
      "Regularized Logistic Regression(127/199): loss=0.6422960114676588, w0=-1.500814684220787e-06, w1=-0.021781644748337946\n",
      "Regularized Logistic Regression(128/199): loss=0.642262895071319, w0=-1.506521953717853e-06, w1=-0.021844183537650903\n",
      "Regularized Logistic Regression(129/199): loss=0.6422307470574389, w0=-1.512172068476368e-06, w1=-0.021905907023833868\n",
      "Regularized Logistic Regression(130/199): loss=0.6421995387556739, w0=-1.5177656006947384e-06, w1=-0.02196682632465349\n",
      "Regularized Logistic Regression(131/199): loss=0.6421692423541888, w0=-1.5233031168003927e-06, w1=-0.02202695239812521\n",
      "Regularized Logistic Regression(132/199): loss=0.6421398308736629, w0=-1.528785177510194e-06, w1=-0.022086296044967386\n",
      "Regularized Logistic Regression(133/199): loss=0.6421112781420909, w0=-1.5342123378901347e-06, w1=-0.022144867911014166\n",
      "Regularized Logistic Regression(134/199): loss=0.6420835587703558, w0=-1.5395851474143258e-06, w1=-0.022202678489587804\n",
      "Regularized Logistic Regression(135/199): loss=0.6420566481285497, w0=-1.5449041500232922e-06, w1=-0.022259738123831443\n",
      "Regularized Logistic Regression(136/199): loss=0.642030522323019, w0=-1.550169884181583e-06, w1=-0.022316057009002757\n",
      "Regularized Logistic Regression(137/199): loss=0.6420051581741119, w0=-1.555382882934709e-06, w1=-0.022371645194729485\n",
      "Regularized Logistic Regression(138/199): loss=0.6419805331946058, w0=-1.5605436739654186e-06, w1=-0.02242651258722737\n",
      "Regularized Logistic Regression(139/199): loss=0.6419566255687954, w0=-1.5656527796493183e-06, w1=-0.022480668951481333\n",
      "Regularized Logistic Regression(140/199): loss=0.6419334141322182, w0=-1.570710717109853e-06, w1=-0.022534123913390394\n",
      "Regularized Logistic Regression(141/199): loss=0.641910878351998, w0=-1.5757179982726509e-06, w1=-0.022586886961877266\n",
      "Regularized Logistic Regression(142/199): loss=0.6418889983077939, w0=-1.5806751299192462e-06, w1=-0.02263896745096302\n",
      "Regularized Logistic Regression(143/199): loss=0.641867754673323, w0=-1.5855826137401866e-06, w1=-0.022690374601807672\n",
      "Regularized Logistic Regression(144/199): loss=0.6418471286984522, w0=-1.5904409463875355e-06, w1=-0.022741117504717072\n",
      "Regularized Logistic Regression(145/199): loss=0.6418271021918349, w0=-1.5952506195267775e-06, w1=-0.022791205121117042\n",
      "Regularized Logistic Regression(146/199): loss=0.6418076575040746, w0=-1.6000121198881364e-06, w1=-0.02284064628549506\n",
      "Regularized Logistic Regression(147/199): loss=0.6417887775114044, w0=-1.6047259293173134e-06, w1=-0.022889449707310258\n",
      "Regularized Logistic Regression(148/199): loss=0.6417704455998614, w0=-1.6093925248256542e-06, w1=-0.022937623972872186\n",
      "Regularized Logistic Regression(149/199): loss=0.6417526456499447, w0=-1.6140123786397542e-06, w1=-0.0229851775471891\n",
      "Regularized Logistic Regression(150/199): loss=0.6417353620217376, w0=-1.6185859582505068e-06, w1=-0.023032118775786082\n",
      "Regularized Logistic Regression(151/199): loss=0.6417185795404863, w0=-1.6231137264616065e-06, w1=-0.023078455886493803\n",
      "Regularized Logistic Regression(152/199): loss=0.6417022834826114, w0=-1.6275961414375122e-06, w1=-0.023124196991208143\n",
      "Regularized Logistic Regression(153/199): loss=0.641686459562152, w0=-1.6320336567508772e-06, w1=-0.02316935008762149\n",
      "Regularized Logistic Regression(154/199): loss=0.6416710939176145, w0=-1.636426721429456e-06, w1=-0.02321392306092611\n",
      "Regularized Logistic Regression(155/199): loss=0.6416561730992255, w0=-1.6407757800024937e-06, w1=-0.023257923685489984\n",
      "Regularized Logistic Regression(156/199): loss=0.6416416840565694, w0=-1.6450812725466037e-06, w1=-0.02330135962650577\n",
      "Regularized Logistic Regression(157/199): loss=0.6416276141266026, w0=-1.6493436347311437e-06, w1=-0.02334423844161329\n",
      "Regularized Logistic Regression(158/199): loss=0.6416139510220306, w0=-1.6535632978630945e-06, w1=-0.023386567582496003\n",
      "Regularized Logistic Regression(159/199): loss=0.6416006828200375, w0=-1.6577406889314479e-06, w1=-0.023428354396451977\n",
      "Regularized Logistic Regression(160/199): loss=0.6415877979513569, w0=-1.6618762306511128e-06, w1=-0.023469606127939725\n",
      "Regularized Logistic Regression(161/199): loss=0.6415752851896767, w0=-1.6659703415063433e-06, w1=-0.02351032992009943\n",
      "Regularized Logistic Regression(162/199): loss=0.641563133641359, w0=-1.670023435793696e-06, w1=-0.023550532816249983\n",
      "Regularized Logistic Regression(163/199): loss=0.6415513327354785, w0=-1.674035923664524e-06, w1=-0.023590221761362207\n",
      "Regularized Logistic Regression(164/199): loss=0.6415398722141571, w0=-1.6780082111670104e-06, w1=-0.02362940360350873\n",
      "Regularized Logistic Regression(165/199): loss=0.6415287421231933, w0=-1.6819407002877515e-06, w1=-0.0236680850952909\n",
      "Regularized Logistic Regression(166/199): loss=0.6415179328029751, w0=-1.6858337889928914e-06, w1=-0.023706272895243147\n",
      "Regularized Logistic Regression(167/199): loss=0.6415074348796663, w0=-1.689687871268817e-06, w1=-0.023743973569215156\n",
      "Regularized Logistic Regression(168/199): loss=0.6414972392566611, w0=-1.693503337162417e-06, w1=-0.023781193591732305\n",
      "Regularized Logistic Regression(169/199): loss=0.6414873371062956, w0=-1.6972805728209114e-06, w1=-0.023817939347334713\n",
      "Regularized Logistic Regression(170/199): loss=0.64147771986181, w0=-1.7010199605312564e-06, w1=-0.023854217131895134\n",
      "Regularized Logistic Regression(171/199): loss=0.6414683792095541, w0=-1.7047218787591301e-06, w1=-0.023890033153916346\n",
      "Regularized Logistic Regression(172/199): loss=0.6414593070814274, w0=-1.7083867021875052e-06, w1=-0.023925393535808154\n",
      "Regularized Logistic Regression(173/199): loss=0.6414504956475479, w0=-1.7120148017548123e-06, w1=-0.023960304315144402\n",
      "Regularized Logistic Regression(174/199): loss=0.641441937309141, w0=-1.7156065446926997e-06, w1=-0.02399477144590036\n",
      "Regularized Logistic Regression(175/199): loss=0.6414336246916446, w0=-1.719162294563395e-06, w1=-0.024028800799670954\n",
      "Regularized Logistic Regression(176/199): loss=0.6414255506380212, w0=-1.722682411296673e-06, w1=-0.024062398166869853\n",
      "Regularized Logistic Regression(177/199): loss=0.6414177082022704, w0=-1.7261672512264333e-06, w1=-0.02409556925791003\n",
      "Regularized Logistic Regression(178/199): loss=0.6414100906431388, w0=-1.7296171671268962e-06, w1=-0.024128319704366054\n",
      "Regularized Logistic Regression(179/199): loss=0.6414026914180184, w0=-1.733032508248417e-06, w1=-0.024160655060118167\n",
      "Regularized Logistic Regression(180/199): loss=0.641395504177028, w0=-1.7364136203529259e-06, w1=-0.024192580802478946\n",
      "Regularized Logistic Regression(181/199): loss=0.6413885227572753, w0=-1.7397608457489982e-06, w1=-0.02422410233330231\n",
      "Regularized Logistic Regression(182/199): loss=0.641381741177288, w0=-1.7430745233265574e-06, w1=-0.024255224980075558\n",
      "Regularized Logistic Regression(183/199): loss=0.6413751536316155, w0=-1.7463549885912166e-06, w1=-0.024285953996994562\n",
      "Regularized Logistic Regression(184/199): loss=0.6413687544855908, w0=-1.7496025736982642e-06, w1=-0.02431629456602238\n",
      "Regularized Logistic Regression(185/199): loss=0.6413625382702516, w0=-1.7528176074862939e-06, w1=-0.024346251797931626\n",
      "Regularized Logistic Regression(186/199): loss=0.6413564996774126, w0=-1.7560004155104878e-06, w1=-0.024375830733330862\n",
      "Regularized Logistic Regression(187/199): loss=0.6413506335548868, w0=-1.759151320075553e-06, w1=-0.024405036343675274\n",
      "Regularized Logistic Regression(188/199): loss=0.6413449349018501, w0=-1.7622706402683185e-06, w1=-0.024433873532261837\n",
      "Regularized Logistic Regression(189/199): loss=0.641339398864344, w0=-1.765358691989994e-06, w1=-0.024462347135209273\n",
      "Regularized Logistic Regression(190/199): loss=0.6413340207309142, w0=-1.7684157879880964e-06, w1=-0.024490461922423164\n",
      "Regularized Logistic Regression(191/199): loss=0.6413287959283818, w0=-1.7714422378880474e-06, w1=-0.024518222598546226\n",
      "Regularized Logistic Regression(192/199): loss=0.6413237200177369, w0=-1.7744383482244441e-06, w1=-0.02454563380389416\n",
      "Regularized Logistic Regression(193/199): loss=0.64131878869016, w0=-1.7774044224720096e-06, w1=-0.02457270011537732\n",
      "Regularized Logistic Regression(194/199): loss=0.6413139977631597, w0=-1.780340761076224e-06, w1=-0.02459942604740837\n",
      "Regularized Logistic Regression(195/199): loss=0.6413093431768269, w0=-1.7832476614836424e-06, w1=-0.02462581605279607\n",
      "Regularized Logistic Regression(196/199): loss=0.6413048209902015, w0=-1.7861254181719013e-06, w1=-0.02465187452362573\n",
      "Regularized Logistic Regression(197/199): loss=0.6413004273777484, w0=-1.7889743226794174e-06, w1=-0.02467760579212625\n",
      "Regularized Logistic Regression(198/199): loss=0.6412961586259374, w0=-1.7917946636347821e-06, w1=-0.02470301413152396\n",
      "Regularized Logistic Regression(199/199): loss=0.6412920111299265, w0=-1.7945867267858575e-06, w1=-0.024728103756883713\n",
      "Regularized Logistic Regression(0/299): loss=0.693147180559945, w0=-2.0882930292778682e-08, w1=-0.0003658898216597753\n",
      "Regularized Logistic Regression(1/299): loss=0.691570764967098, w0=-4.155061673532941e-08, w1=-0.000726447155859764\n",
      "Regularized Logistic Regression(2/299): loss=0.6900427493731593, w0=-6.200544791465143e-08, w1=-0.0010817557138626016\n",
      "Regularized Logistic Regression(3/299): loss=0.6885616307405306, w0=-8.22497803369991e-08, w1=-0.001431897818366024\n",
      "Regularized Logistic Regression(4/299): loss=0.6871259530052786, w0=-1.0228593901084923e-07, w1=-0.0017769544244776343\n",
      "Regularized Logistic Regression(5/299): loss=0.6857343056230828, w0=-1.2211621801676715e-07, w1=-0.0021170051406546424\n",
      "Regularized Logistic Regression(6/299): loss=0.6843853221576848, w0=-1.4174288106443043e-07, w1=-0.002452128249584036\n",
      "Regularized Logistic Regression(7/299): loss=0.6830776789108608, w0=-1.6116816203707062e-07, w1=-0.002782400728980684\n",
      "Regularized Logistic Regression(8/299): loss=0.6818100935929228, w0=-1.803942655235933e-07, w1=-0.0031078982722825468\n",
      "Regularized Logistic Regression(9/299): loss=0.6805813240327646, w0=-1.9942336733863663e-07, w1=-0.0034286953092240373\n",
      "Regularized Logistic Regression(10/299): loss=0.6793901669264669, w0=-2.1825761503082792e-07, w1=-0.003744865026270114\n",
      "Regularized Logistic Regression(11/299): loss=0.6782354566234853, w0=-2.368991283794964e-07, w1=-0.004056479386895273\n",
      "Regularized Logistic Regression(12/299): loss=0.677116063949456, w0=-2.553499998800989e-07, w1=-0.004363609151693511\n",
      "Regularized Logistic Regression(13/299): loss=0.6760308950646551, w0=-2.736122952186129e-07, w1=-0.004666323898305762\n",
      "Regularized Logistic Regression(14/299): loss=0.6749788903571698, w0=-2.9168805373514934e-07, w1=-0.004964692041153607\n",
      "Regularized Logistic Regression(15/299): loss=0.6739590233698397, w0=-3.0957928887703486e-07, w1=-0.005258780850968626\n",
      "Regularized Logistic Regression(16/299): loss=0.6729702997600534, w0=-3.272879886416101e-07, w1=-0.005548656474108082\n",
      "Regularized Logistic Regression(17/299): loss=0.6720117562914919, w0=-3.4481611600898726e-07, w1=-0.0058343839516489995\n",
      "Regularized Logistic Regression(18/299): loss=0.6710824598569229, w0=-3.6216560936500765e-07, w1=-0.006116027238252714\n",
      "Regularized Logistic Regression(19/299): loss=0.6701815065311797, w0=-3.7933838291463546e-07, w1=-0.006393649220794519\n",
      "Regularized Logistic Regression(20/299): loss=0.6693080206534551, w0=-3.9633632708602046e-07, w1=-0.006667311736751995\n",
      "Regularized Logistic Regression(21/299): loss=0.6684611539380813, w0=-4.13161308925459e-07, w1=-0.006937075592347969\n",
      "Regularized Logistic Regression(22/299): loss=0.6676400846129609, w0=-4.29815172483478e-07, w1=-0.007203000580443894\n",
      "Regularized Logistic Regression(23/299): loss=0.6668440165848487, w0=-4.46299739192263e-07, w1=-0.007465145498180636\n",
      "Regularized Logistic Regression(24/299): loss=0.6660721786306957, w0=-4.6261680823464667e-07, w1=-0.007723568164363712\n",
      "Regularized Logistic Regression(25/299): loss=0.6653238236142842, w0=-4.78768156904871e-07, w1=-0.007978325436591154\n",
      "Regularized Logistic Regression(26/299): loss=0.6645982277274026, w0=-4.947555409613305e-07, w1=-0.008229473228122767\n",
      "Regularized Logistic Regression(27/299): loss=0.6638946897548245, w0=-5.105806949715015e-07, w1=-0.008477066524489195\n",
      "Regularized Logistic Regression(28/299): loss=0.6632125303623808, w0=-5.262453326492555e-07, w1=-0.008721159399840934\n",
      "Regularized Logistic Regression(29/299): loss=0.6625510914074222, w0=-5.417511471847536e-07, w1=-0.008961805033036696\n",
      "Regularized Logistic Regression(30/299): loss=0.6619097352709974, w0=-5.57099811567112e-07, w1=-0.009199055723471811\n",
      "Regularized Logistic Regression(31/299): loss=0.661287844211081, w0=-5.72292978900025e-07, w1=-0.009432962906646974\n",
      "Regularized Logistic Regression(32/299): loss=0.6606848197362093, w0=-5.873322827105278e-07, w1=-0.009663577169478594\n",
      "Regularized Logistic Regression(33/299): loss=0.6601000819988982, w0=-6.022193372510782e-07, w1=-0.009890948265351895\n",
      "Regularized Logistic Regression(34/299): loss=0.6595330692082289, w0=-6.169557377951289e-07, w1=-0.010115125128918285\n",
      "Regularized Logistic Regression(35/299): loss=0.658983237061017, w0=-6.315430609263621e-07, w1=-0.010336155890638752\n",
      "Regularized Logistic Regression(36/299): loss=0.6584500581909778, w0=-6.459828648217496e-07, w1=-0.010554087891075628\n",
      "Regularized Logistic Regression(37/299): loss=0.6579330216353392, w0=-6.602766895286017e-07, w1=-0.010768967694934368\n",
      "Regularized Logistic Regression(38/299): loss=0.6574316323183473, w0=-6.744260572357601e-07, w1=-0.01098084110485829\n",
      "Regularized Logistic Regression(39/299): loss=0.6569454105511489, w0=-6.884324725390895e-07, w1=-0.011189753174978414\n",
      "Regularized Logistic Regression(40/299): loss=0.6564738915475244, w0=-7.02297422701416e-07, w1=-0.011395748224221295\n",
      "Regularized Logistic Regression(41/299): loss=0.656016624954985, w0=-7.160223779070584e-07, w1=-0.011598869849377644\n",
      "Regularized Logistic Regression(42/299): loss=0.6555731744007448, w0=-7.296087915110933e-07, w1=-0.011799160937934606\n",
      "Regularized Logistic Regression(43/299): loss=0.6551431170520983, w0=-7.430581002834917e-07, w1=-0.01199666368067483\n",
      "Regularized Logistic Regression(44/299): loss=0.6547260431907549, w0=-7.563717246482623e-07, w1=-0.01219141958404522\n",
      "Regularized Logistic Regression(45/299): loss=0.6543215558006814, w0=-7.695510689177299e-07, w1=-0.012383469482298841\n",
      "Regularized Logistic Regression(46/299): loss=0.6539292701690314, w0=-7.825975215220783e-07, w1=-0.012572853549412937\n",
      "Regularized Logistic Regression(47/299): loss=0.6535488134997415, w0=-7.955124552342791e-07, w1=-0.012759611310786578\n",
      "Regularized Logistic Regression(48/299): loss=0.6531798245393972, w0=-8.082972273905268e-07, w1=-0.01294378165472111\n",
      "Regularized Logistic Regression(49/299): loss=0.6528219532149743, w0=-8.209531801062992e-07, w1=-0.013125402843686766\n",
      "Regularized Logistic Regression(50/299): loss=0.6524748602830823, w0=-8.33481640488153e-07, w1=-0.013304512525379047\n",
      "Regularized Logistic Regression(51/299): loss=0.6521382169903406, w0=-8.458839208413679e-07, w1=-0.013481147743567881\n",
      "Regularized Logistic Regression(52/299): loss=0.6518117047445345, w0=-8.581613188735458e-07, w1=-0.013655344948743296\n",
      "Regularized Logistic Regression(53/299): loss=0.6514950147962064, w0=-8.703151178942679e-07, w1=-0.013827140008561\n",
      "Regularized Logistic Regression(54/299): loss=0.6511878479303497, w0=-8.823465870109136e-07, w1=-0.01399656821809099\n",
      "Regularized Logistic Regression(55/299): loss=0.6508899141678853, w0=-8.942569813207374e-07, w1=-0.014163664309872922\n",
      "Regularized Logistic Regression(56/299): loss=0.6506009324766032, w0=-9.060475420993006e-07, w1=-0.014328462463781447\n",
      "Regularized Logistic Regression(57/299): loss=0.6503206304912726, w0=-9.177194969853516e-07, w1=-0.014490996316704956\n",
      "Regularized Logistic Regression(58/299): loss=0.6500487442426232, w0=-9.292740601622426e-07, w1=-0.014651298972041047\n",
      "Regularized Logistic Regression(59/299): loss=0.649785017894916, w0=-9.40712432535973e-07, w1=-0.014809403009012007\n",
      "Regularized Logistic Regression(60/299): loss=0.6495292034918293, w0=-9.520358019099438e-07, w1=-0.014965340491803846\n",
      "Regularized Logistic Regression(61/299): loss=0.6492810607103923, w0=-9.632453431565056e-07, w1=-0.015119142978531612\n",
      "Regularized Logistic Regression(62/299): loss=0.6490403566227122, w0=-9.743422183853803e-07, w1=-0.015270841530034968\n",
      "Regularized Logistic Regression(63/299): loss=0.6488068654652402, w0=-9.853275771090361e-07, w1=-0.015420466718506543\n",
      "Regularized Logistic Regression(64/299): loss=0.6485803684153401, w0=-9.9620255640509e-07, w1=-0.01556804863595664\n",
      "Regularized Logistic Regression(65/299): loss=0.6483606533749229, w0=-1.006968281075812e-06, w1=-0.015713616902517328\n",
      "Regularized Logistic Regression(66/299): loss=0.6481475147609255, w0=-1.0176258638048036e-06, w1=-0.015857200674588873\n",
      "Regularized Logistic Regression(67/299): loss=0.6479407533024055, w0=-1.0281764053109179e-06, w1=-0.015998828652831743\n",
      "Regularized Logistic Regression(68/299): loss=0.6477401758440525, w0=-1.0386209944994915e-06, w1=-0.016138529090007073\n",
      "Regularized Logistic Regression(69/299): loss=0.6475455951559005, w0=-1.0489607086109518e-06, w1=-0.01627632979866859\n",
      "Regularized Logistic Regression(70/299): loss=0.6473568297490486, w0=-1.0591966133668623e-06, w1=-0.016412258158708826\n",
      "Regularized Logistic Regression(71/299): loss=0.6471737036971972, w0=-1.0693297631134727e-06, w1=-0.016546341124762713\n",
      "Regularized Logistic Regression(72/299): loss=0.646996046463809, w0=-1.0793612009628273e-06, w1=-0.016678605233470974\n",
      "Regularized Logistic Regression(73/299): loss=0.6468236927347252, w0=-1.0892919589314941e-06, w1=-0.016809076610606603\n",
      "Regularized Logistic Regression(74/299): loss=0.6466564822560492, w0=-1.0991230580769714e-06, w1=-0.016937780978066717\n",
      "Regularized Logistic Regression(75/299): loss=0.6464942596771427, w0=-1.1088555086318233e-06, w1=-0.017064743660732712\n",
      "Regularized Logistic Regression(76/299): loss=0.646336874398559, w0=-1.118490310135603e-06, w1=-0.017189989593201244\n",
      "Regularized Logistic Regression(77/299): loss=0.6461841804247682, w0=-1.1280284515646093e-06, w1=-0.01731354332638873\n",
      "Regularized Logistic Regression(78/299): loss=0.6460360362215062, w0=-1.1374709114595333e-06, w1=-0.017435429034011834\n",
      "Regularized Logistic Regression(79/299): loss=0.645892304577617, w0=-1.146818658051038e-06, w1=-0.017555670518946347\n",
      "Regularized Logistic Regression(80/299): loss=0.6457528524712305, w0=-1.1560726493833242e-06, w1=-0.017674291219467123\n",
      "Regularized Logistic Regression(81/299): loss=0.645617550940145, w0=-1.165233833435723e-06, w1=-0.017791314215371386\n",
      "Regularized Logistic Regression(82/299): loss=0.64548627495628, w0=-1.1743031482423659e-06, w1=-0.017906762233987558\n",
      "Regularized Logistic Regression(83/299): loss=0.6453589033040671, w0=-1.1832815220099703e-06, w1=-0.01802065765607239\n",
      "Regularized Logistic Regression(84/299): loss=0.645235318462654, w0=-1.1921698732337867e-06, w1=-0.0181330225215981\n",
      "Regularized Logistic Regression(85/299): loss=0.6451154064918042, w0=-1.2009691108117468e-06, w1=-0.018243878535432155\n",
      "Regularized Logistic Regression(86/299): loss=0.6449990569213667, w0=-1.2096801341568537e-06, w1=-0.018353247072911753\n",
      "Regularized Logistic Regression(87/299): loss=0.6448861626442124, w0=-1.2183038333078519e-06, w1=-0.018461149185315075\n",
      "Regularized Logistic Regression(88/299): loss=0.6447766198125171, w0=-1.2268410890382149e-06, w1=-0.018567605605231442\n",
      "Regularized Logistic Regression(89/299): loss=0.6446703277372916, w0=-1.2352927729634886e-06, w1=-0.018672636751832533\n",
      "Regularized Logistic Regression(90/299): loss=0.6445671887910529, w0=-1.2436597476470248e-06, w1=-0.018776262736046465\n",
      "Regularized Logistic Regression(91/299): loss=0.6444671083135372, w0=-1.251942866704138e-06, w1=-0.018878503365636828\n",
      "Regularized Logistic Regression(92/299): loss=0.644369994520358, w0=-1.2601429749047234e-06, w1=-0.01897937815018859\n",
      "Regularized Logistic Regression(93/299): loss=0.644275758414516, w0=-1.268260908274365e-06, w1=-0.019078906306002934\n",
      "Regularized Logistic Regression(94/299): loss=0.6441843137006713, w0=-1.2762974941939673e-06, w1=-0.01917710676090226\n",
      "Regularized Logistic Regression(95/299): loss=0.6440955767020871, w0=-1.2842535514979403e-06, w1=-0.019273998158948185\n",
      "Regularized Logistic Regression(96/299): loss=0.6440094662801665, w0=-1.2921298905709704e-06, w1=-0.019369598865073126\n",
      "Regularized Logistic Regression(97/299): loss=0.6439259037564935, w0=-1.2999273134434039e-06, w1=-0.019463926969628238\n",
      "Regularized Logistic Regression(98/299): loss=0.6438448128373018, w0=-1.3076466138852724e-06, w1=-0.019557000292848813\n",
      "Regularized Logistic Regression(99/299): loss=0.6437661195402974, w0=-1.3152885774989891e-06, w1=-0.01964883638923885\n",
      "Regularized Logistic Regression(100/299): loss=0.6436897521237556, w0=-1.3228539818107404e-06, w1=-0.01973945255187667\n",
      "Regularized Logistic Regression(101/299): loss=0.6436156410178232, w0=-1.3303435963606014e-06, w1=-0.019828865816642942\n",
      "Regularized Logistic Regression(102/299): loss=0.6435437187579572, w0=-1.3377581827913992e-06, w1=-0.019917092966372747\n",
      "Regularized Logistic Regression(103/299): loss=0.64347391992043, w0=-1.3450984949363503e-06, w1=-0.020004150534933273\n",
      "Regularized Logistic Regression(104/299): loss=0.643406181059838, w0=-1.352365278905494e-06, w1=-0.020090054811228466\n",
      "Regularized Logistic Regression(105/299): loss=0.6433404406485498, w0=-1.3595592731709487e-06, w1=-0.020174821843132327\n",
      "Regularized Logistic Regression(106/299): loss=0.6432766390180316, w0=-1.3666812086510092e-06, w1=-0.020258467441352\n",
      "Regularized Logistic Regression(107/299): loss=0.6432147183019936, w0=-1.3737318087931132e-06, w1=-0.020341007183222386\n",
      "Regularized Logistic Regression(108/299): loss=0.6431546223812978, w0=-1.3807117896556934e-06, w1=-0.020422456416433254\n",
      "Regularized Logistic Regression(109/299): loss=0.6430962968305707, w0=-1.3876218599889383e-06, w1=-0.02050283026269061\n",
      "Regularized Logistic Regression(110/299): loss=0.6430396888664712, w0=-1.3944627213144844e-06, w1=-0.020582143621313165\n",
      "Regularized Logistic Regression(111/299): loss=0.6429847472975585, w0=-1.4012350680040558e-06, w1=-0.02066041117276577\n",
      "Regularized Logistic Regression(112/299): loss=0.6429314224757101, w0=-1.407939587357074e-06, w1=-0.02073764738213043\n",
      "Regularized Logistic Regression(113/299): loss=0.6428796662490434, w0=-1.414576959677255e-06, w1=-0.020813866502516543\n",
      "Regularized Logistic Regression(114/299): loss=0.6428294319162897, w0=-1.4211478583482136e-06, w1=-0.020889082578411523\n",
      "Regularized Logistic Regression(115/299): loss=0.6427806741825797, w0=-1.4276529499080907e-06, w1=-0.020963309448972733\n",
      "Regularized Logistic Regression(116/299): loss=0.642733349116593, w0=-1.4340928941232255e-06, w1=-0.02103656075126223\n",
      "Regularized Logistic Regression(117/299): loss=0.6426874141090299, w0=-1.4404683440608843e-06, w1=-0.021108849923425148\n",
      "Regularized Logistic Regression(118/299): loss=0.6426428278323646, w0=-1.4467799461610664e-06, w1=-0.021180190207812973\n",
      "Regularized Logistic Regression(119/299): loss=0.6425995502018383, w0=-1.4530283403074027e-06, w1=-0.021250594654052653\n",
      "Regularized Logistic Regression(120/299): loss=0.642557542337657, w0=-1.4592141598971616e-06, w1=-0.021320076122062877\n",
      "Regularized Logistic Regression(121/299): loss=0.6425167665283483, w0=-1.4653380319103791e-06, w1=-0.021388647285018138\n",
      "Regularized Logistic Regression(122/299): loss=0.6424771861952525, w0=-1.4714005769781282e-06, w1=-0.021456320632261962\n",
      "Regularized Logistic Regression(123/299): loss=0.6424387658581022, w0=-1.4774024094499407e-06, w1=-0.02152310847217009\n",
      "Regularized Logistic Regression(124/299): loss=0.6424014711016631, w0=-1.4833441374603975e-06, w1=-0.021589022934964498\n",
      "Regularized Logistic Regression(125/299): loss=0.6423652685433984, w0=-1.4892263629949014e-06, w1=-0.021654075975479598\n",
      "Regularized Logistic Regression(126/299): loss=0.6423301258021301, w0=-1.4950496819546449e-06, w1=-0.021718279375880944\n",
      "Regularized Logistic Regression(127/299): loss=0.6422960114676588, w0=-1.500814684220787e-06, w1=-0.021781644748337946\n",
      "Regularized Logistic Regression(128/299): loss=0.642262895071319, w0=-1.506521953717853e-06, w1=-0.021844183537650903\n",
      "Regularized Logistic Regression(129/299): loss=0.6422307470574389, w0=-1.512172068476368e-06, w1=-0.021905907023833868\n",
      "Regularized Logistic Regression(130/299): loss=0.6421995387556739, w0=-1.5177656006947384e-06, w1=-0.02196682632465349\n",
      "Regularized Logistic Regression(131/299): loss=0.6421692423541888, w0=-1.5233031168003927e-06, w1=-0.02202695239812521\n",
      "Regularized Logistic Regression(132/299): loss=0.6421398308736629, w0=-1.528785177510194e-06, w1=-0.022086296044967386\n",
      "Regularized Logistic Regression(133/299): loss=0.6421112781420909, w0=-1.5342123378901347e-06, w1=-0.022144867911014166\n",
      "Regularized Logistic Regression(134/299): loss=0.6420835587703558, w0=-1.5395851474143258e-06, w1=-0.022202678489587804\n",
      "Regularized Logistic Regression(135/299): loss=0.6420566481285497, w0=-1.5449041500232922e-06, w1=-0.022259738123831443\n",
      "Regularized Logistic Regression(136/299): loss=0.642030522323019, w0=-1.550169884181583e-06, w1=-0.022316057009002757\n",
      "Regularized Logistic Regression(137/299): loss=0.6420051581741119, w0=-1.555382882934709e-06, w1=-0.022371645194729485\n",
      "Regularized Logistic Regression(138/299): loss=0.6419805331946058, w0=-1.5605436739654186e-06, w1=-0.02242651258722737\n",
      "Regularized Logistic Regression(139/299): loss=0.6419566255687954, w0=-1.5656527796493183e-06, w1=-0.022480668951481333\n",
      "Regularized Logistic Regression(140/299): loss=0.6419334141322182, w0=-1.570710717109853e-06, w1=-0.022534123913390394\n",
      "Regularized Logistic Regression(141/299): loss=0.641910878351998, w0=-1.5757179982726509e-06, w1=-0.022586886961877266\n",
      "Regularized Logistic Regression(142/299): loss=0.6418889983077939, w0=-1.5806751299192462e-06, w1=-0.02263896745096302\n",
      "Regularized Logistic Regression(143/299): loss=0.641867754673323, w0=-1.5855826137401866e-06, w1=-0.022690374601807672\n",
      "Regularized Logistic Regression(144/299): loss=0.6418471286984522, w0=-1.5904409463875355e-06, w1=-0.022741117504717072\n",
      "Regularized Logistic Regression(145/299): loss=0.6418271021918349, w0=-1.5952506195267775e-06, w1=-0.022791205121117042\n",
      "Regularized Logistic Regression(146/299): loss=0.6418076575040746, w0=-1.6000121198881364e-06, w1=-0.02284064628549506\n",
      "Regularized Logistic Regression(147/299): loss=0.6417887775114044, w0=-1.6047259293173134e-06, w1=-0.022889449707310258\n",
      "Regularized Logistic Regression(148/299): loss=0.6417704455998614, w0=-1.6093925248256542e-06, w1=-0.022937623972872186\n",
      "Regularized Logistic Regression(149/299): loss=0.6417526456499447, w0=-1.6140123786397542e-06, w1=-0.0229851775471891\n",
      "Regularized Logistic Regression(150/299): loss=0.6417353620217376, w0=-1.6185859582505068e-06, w1=-0.023032118775786082\n",
      "Regularized Logistic Regression(151/299): loss=0.6417185795404863, w0=-1.6231137264616065e-06, w1=-0.023078455886493803\n",
      "Regularized Logistic Regression(152/299): loss=0.6417022834826114, w0=-1.6275961414375122e-06, w1=-0.023124196991208143\n",
      "Regularized Logistic Regression(153/299): loss=0.641686459562152, w0=-1.6320336567508772e-06, w1=-0.02316935008762149\n",
      "Regularized Logistic Regression(154/299): loss=0.6416710939176145, w0=-1.636426721429456e-06, w1=-0.02321392306092611\n",
      "Regularized Logistic Regression(155/299): loss=0.6416561730992255, w0=-1.6407757800024937e-06, w1=-0.023257923685489984\n",
      "Regularized Logistic Regression(156/299): loss=0.6416416840565694, w0=-1.6450812725466037e-06, w1=-0.02330135962650577\n",
      "Regularized Logistic Regression(157/299): loss=0.6416276141266026, w0=-1.6493436347311437e-06, w1=-0.02334423844161329\n",
      "Regularized Logistic Regression(158/299): loss=0.6416139510220306, w0=-1.6535632978630945e-06, w1=-0.023386567582496003\n",
      "Regularized Logistic Regression(159/299): loss=0.6416006828200375, w0=-1.6577406889314479e-06, w1=-0.023428354396451977\n",
      "Regularized Logistic Regression(160/299): loss=0.6415877979513569, w0=-1.6618762306511128e-06, w1=-0.023469606127939725\n",
      "Regularized Logistic Regression(161/299): loss=0.6415752851896767, w0=-1.6659703415063433e-06, w1=-0.02351032992009943\n",
      "Regularized Logistic Regression(162/299): loss=0.641563133641359, w0=-1.670023435793696e-06, w1=-0.023550532816249983\n",
      "Regularized Logistic Regression(163/299): loss=0.6415513327354785, w0=-1.674035923664524e-06, w1=-0.023590221761362207\n",
      "Regularized Logistic Regression(164/299): loss=0.6415398722141571, w0=-1.6780082111670104e-06, w1=-0.02362940360350873\n",
      "Regularized Logistic Regression(165/299): loss=0.6415287421231933, w0=-1.6819407002877515e-06, w1=-0.0236680850952909\n",
      "Regularized Logistic Regression(166/299): loss=0.6415179328029751, w0=-1.6858337889928914e-06, w1=-0.023706272895243147\n",
      "Regularized Logistic Regression(167/299): loss=0.6415074348796663, w0=-1.689687871268817e-06, w1=-0.023743973569215156\n",
      "Regularized Logistic Regression(168/299): loss=0.6414972392566611, w0=-1.693503337162417e-06, w1=-0.023781193591732305\n",
      "Regularized Logistic Regression(169/299): loss=0.6414873371062956, w0=-1.6972805728209114e-06, w1=-0.023817939347334713\n",
      "Regularized Logistic Regression(170/299): loss=0.64147771986181, w0=-1.7010199605312564e-06, w1=-0.023854217131895134\n",
      "Regularized Logistic Regression(171/299): loss=0.6414683792095541, w0=-1.7047218787591301e-06, w1=-0.023890033153916346\n",
      "Regularized Logistic Regression(172/299): loss=0.6414593070814274, w0=-1.7083867021875052e-06, w1=-0.023925393535808154\n",
      "Regularized Logistic Regression(173/299): loss=0.6414504956475479, w0=-1.7120148017548123e-06, w1=-0.023960304315144402\n",
      "Regularized Logistic Regression(174/299): loss=0.641441937309141, w0=-1.7156065446926997e-06, w1=-0.02399477144590036\n",
      "Regularized Logistic Regression(175/299): loss=0.6414336246916446, w0=-1.719162294563395e-06, w1=-0.024028800799670954\n",
      "Regularized Logistic Regression(176/299): loss=0.6414255506380212, w0=-1.722682411296673e-06, w1=-0.024062398166869853\n",
      "Regularized Logistic Regression(177/299): loss=0.6414177082022704, w0=-1.7261672512264333e-06, w1=-0.02409556925791003\n",
      "Regularized Logistic Regression(178/299): loss=0.6414100906431388, w0=-1.7296171671268962e-06, w1=-0.024128319704366054\n",
      "Regularized Logistic Regression(179/299): loss=0.6414026914180184, w0=-1.733032508248417e-06, w1=-0.024160655060118167\n",
      "Regularized Logistic Regression(180/299): loss=0.641395504177028, w0=-1.7364136203529259e-06, w1=-0.024192580802478946\n",
      "Regularized Logistic Regression(181/299): loss=0.6413885227572753, w0=-1.7397608457489982e-06, w1=-0.02422410233330231\n",
      "Regularized Logistic Regression(182/299): loss=0.641381741177288, w0=-1.7430745233265574e-06, w1=-0.024255224980075558\n",
      "Regularized Logistic Regression(183/299): loss=0.6413751536316155, w0=-1.7463549885912166e-06, w1=-0.024285953996994562\n",
      "Regularized Logistic Regression(184/299): loss=0.6413687544855908, w0=-1.7496025736982642e-06, w1=-0.02431629456602238\n",
      "Regularized Logistic Regression(185/299): loss=0.6413625382702516, w0=-1.7528176074862939e-06, w1=-0.024346251797931626\n",
      "Regularized Logistic Regression(186/299): loss=0.6413564996774126, w0=-1.7560004155104878e-06, w1=-0.024375830733330862\n",
      "Regularized Logistic Regression(187/299): loss=0.6413506335548868, w0=-1.759151320075553e-06, w1=-0.024405036343675274\n",
      "Regularized Logistic Regression(188/299): loss=0.6413449349018501, w0=-1.7622706402683185e-06, w1=-0.024433873532261837\n",
      "Regularized Logistic Regression(189/299): loss=0.641339398864344, w0=-1.765358691989994e-06, w1=-0.024462347135209273\n",
      "Regularized Logistic Regression(190/299): loss=0.6413340207309142, w0=-1.7684157879880964e-06, w1=-0.024490461922423164\n",
      "Regularized Logistic Regression(191/299): loss=0.6413287959283818, w0=-1.7714422378880474e-06, w1=-0.024518222598546226\n",
      "Regularized Logistic Regression(192/299): loss=0.6413237200177369, w0=-1.7744383482244441e-06, w1=-0.02454563380389416\n",
      "Regularized Logistic Regression(193/299): loss=0.64131878869016, w0=-1.7774044224720096e-06, w1=-0.02457270011537732\n",
      "Regularized Logistic Regression(194/299): loss=0.6413139977631597, w0=-1.780340761076224e-06, w1=-0.02459942604740837\n",
      "Regularized Logistic Regression(195/299): loss=0.6413093431768269, w0=-1.7832476614836424e-06, w1=-0.02462581605279607\n",
      "Regularized Logistic Regression(196/299): loss=0.6413048209902015, w0=-1.7861254181719013e-06, w1=-0.02465187452362573\n",
      "Regularized Logistic Regression(197/299): loss=0.6413004273777484, w0=-1.7889743226794174e-06, w1=-0.02467760579212625\n",
      "Regularized Logistic Regression(198/299): loss=0.6412961586259374, w0=-1.7917946636347821e-06, w1=-0.02470301413152396\n",
      "Regularized Logistic Regression(199/299): loss=0.6412920111299265, w0=-1.7945867267858575e-06, w1=-0.024728103756883713\n",
      "Regularized Logistic Regression(200/299): loss=0.6412879813903455, w0=-1.7973507950285728e-06, w1=-0.024752878825937318\n",
      "Regularized Logistic Regression(201/299): loss=0.641284066010172, w0=-1.800087148435428e-06, w1=-0.024777343439899367\n",
      "Regularized Logistic Regression(202/299): loss=0.6412802616917067, w0=-1.8027960642837072e-06, w1=-0.02480150164427091\n",
      "Regularized Logistic Regression(203/299): loss=0.6412765652336339, w0=-1.8054778170834042e-06, w1=-0.024825357429631056\n",
      "Regularized Logistic Regression(204/299): loss=0.6412729735281711, w0=-1.8081326786048635e-06, w1=-0.024848914732416606\n",
      "Regularized Logistic Regression(205/299): loss=0.6412694835583069, w0=-1.8107609179061413e-06, w1=-0.024872177435690163\n",
      "Regularized Logistic Regression(206/299): loss=0.6412660923951161, w0=-1.8133628013600878e-06, w1=-0.024895149369896574\n",
      "Regularized Logistic Regression(207/299): loss=0.64126279719516, w0=-1.8159385926811543e-06, w1=-0.024917834313608198\n",
      "Regularized Logistic Regression(208/299): loss=0.6412595951979598, w0=-1.818488552951929e-06, w1=-0.024940235994259024\n",
      "Regularized Logistic Regression(209/299): loss=0.6412564837235507, w0=-1.8210129406494044e-06, w1=-0.0249623580888677\n",
      "Regularized Logistic Regression(210/299): loss=0.6412534601701028, w0=-1.8235120116709761e-06, w1=-0.024984204224750048\n",
      "Regularized Logistic Regression(211/299): loss=0.6412505220116175, w0=-1.8259860193601823e-06, w1=-0.025005777980220748\n",
      "Regularized Logistic Regression(212/299): loss=0.6412476667956909, w0=-1.8284352145321793e-06, w1=-0.02502708288528466\n",
      "Regularized Logistic Regression(213/299): loss=0.6412448921413447, w0=-1.8308598454989624e-06, w1=-0.02504812242231795\n",
      "Regularized Logistic Regression(214/299): loss=0.6412421957369197, w0=-1.8332601580943294e-06, w1=-0.02506890002673905\n",
      "Regularized Logistic Regression(215/299): loss=0.641239575338034, w0=-1.835636395698595e-06, w1=-0.025089419087669595\n",
      "Regularized Logistic Regression(216/299): loss=0.6412370287656016, w0=-1.8379887992630532e-06, w1=-0.025109682948585815\n",
      "Regularized Logistic Regression(217/299): loss=0.64123455390391, w0=-1.8403176073341954e-06, w1=-0.025129694907959986\n",
      "Regularized Logistic Regression(218/299): loss=0.6412321486987538, w0=-1.8426230560776824e-06, w1=-0.025149458219892624\n",
      "Regularized Logistic Regression(219/299): loss=0.6412298111556259, w0=-1.8449053793020766e-06, w1=-0.02516897609473532\n",
      "Regularized Logistic Regression(220/299): loss=0.6412275393379602, w0=-1.8471648084823348e-06, w1=-0.02518825169970427\n",
      "Regularized Logistic Regression(221/299): loss=0.641225331365429, w0=-1.8494015727830644e-06, w1=-0.025207288159484895\n",
      "Regularized Logistic Regression(222/299): loss=0.6412231854122883, w0=-1.8516158990815464e-06, w1=-0.02522608855682759\n",
      "Regularized Logistic Regression(223/299): loss=0.6412210997057755, w0=-1.8538080119905261e-06, w1=-0.025244655933134543\n",
      "Regularized Logistic Regression(224/299): loss=0.64121907252455, w0=-1.8559781338807765e-06, w1=-0.025262993289038076\n",
      "Regularized Logistic Regression(225/299): loss=0.6412171021971868, w0=-1.8581264849034335e-06, w1=-0.025281103584970453\n",
      "Regularized Logistic Regression(226/299): loss=0.6412151871007076, w0=-1.8602532830121082e-06, w1=-0.025298989741725397\n",
      "Regularized Logistic Regression(227/299): loss=0.6412133256591609, w0=-1.8623587439847767e-06, w1=-0.025316654641011294\n",
      "Regularized Logistic Regression(228/299): loss=0.6412115163422409, w0=-1.8644430814454512e-06, w1=-0.025334101125996322\n",
      "Regularized Logistic Regression(229/299): loss=0.64120975766395, w0=-1.866506506885633e-06, w1=-0.02535133200184572\n",
      "Regularized Logistic Regression(230/299): loss=0.6412080481812981, w0=-1.8685492296855516e-06, w1=-0.025368350036250986\n",
      "Regularized Logistic Regression(231/299): loss=0.6412063864930437, w0=-1.8705714571351899e-06, w1=-0.02538515795995164\n",
      "Regularized Logistic Regression(232/299): loss=0.6412047712384683, w0=-1.8725733944551012e-06, w1=-0.02540175846724911\n",
      "Regularized Logistic Regression(233/299): loss=0.6412032010961927, w0=-1.8745552448170152e-06, w1=-0.025418154216513316\n",
      "Regularized Logistic Regression(234/299): loss=0.6412016747830213, w0=-1.8765172093642398e-06, w1=-0.02543434783068184\n",
      "Regularized Logistic Regression(235/299): loss=0.6412001910528269, w0=-1.8784594872318583e-06, w1=-0.025450341897751763\n",
      "Regularized Logistic Regression(236/299): loss=0.6411987486954653, w0=-1.8803822755667248e-06, w1=-0.025466138971264433\n",
      "Regularized Logistic Regression(237/299): loss=0.641197346535722, w0=-1.8822857695472596e-06, w1=-0.02548174157078318\n",
      "Regularized Logistic Regression(238/299): loss=0.6411959834322917, w0=-1.8841701624030482e-06, w1=-0.025497152182364154\n",
      "Regularized Logistic Regression(239/299): loss=0.6411946582767862, w0=-1.8860356454342423e-06, w1=-0.025512373259020216\n",
      "Regularized Logistic Regression(240/299): loss=0.6411933699927737, w0=-1.8878824080307692e-06, w1=-0.02552740722117821\n",
      "Regularized Logistic Regression(241/299): loss=0.6411921175348425, w0=-1.8897106376913485e-06, w1=-0.025542256457129587\n",
      "Regularized Logistic Regression(242/299): loss=0.6411908998876992, w0=-1.8915205200423194e-06, w1=-0.02555692332347455\n",
      "Regularized Logistic Regression(243/299): loss=0.6411897160652852, w0=-1.89331223885628e-06, w1=-0.025571410145559758\n",
      "Regularized Logistic Regression(244/299): loss=0.6411885651099268, w0=-1.8950859760705418e-06, w1=-0.025585719217909697\n",
      "Regularized Logistic Regression(245/299): loss=0.6411874460915045, w0=-1.896841911805398e-06, w1=-0.02559985280465188\n",
      "Regularized Logistic Regression(246/299): loss=0.6411863581066511, w0=-1.8985802243822124e-06, w1=-0.0256138131399358\n",
      "Regularized Logistic Regression(247/299): loss=0.6411853002779717, w0=-1.9003010903413263e-06, w1=-0.025627602428346132\n",
      "Regularized Logistic Regression(248/299): loss=0.6411842717532855, w0=-1.9020046844597877e-06, w1=-0.025641222845309602\n",
      "Regularized Logistic Regression(249/299): loss=0.6411832717048923, w0=-1.9036911797689034e-06, w1=-0.025654676537496337\n",
      "Regularized Logistic Regression(250/299): loss=0.6411822993288596, w0=-1.9053607475716177e-06, w1=-0.0256679656232153\n",
      "Regularized Logistic Regression(251/299): loss=0.6411813538443294, w0=-1.9070135574597162e-06, w1=-0.02568109219280405\n",
      "Regularized Logistic Regression(252/299): loss=0.641180434492849, w0=-1.90864977733086e-06, w1=-0.025694058309012925\n",
      "Regularized Logistic Regression(253/299): loss=0.6411795405377154, w0=-1.9102695734054486e-06, w1=-0.025706866007383825\n",
      "Regularized Logistic Regression(254/299): loss=0.6411786712633473, w0=-1.9118731102433178e-06, w1=-0.025719517296623397\n",
      "Regularized Logistic Regression(255/299): loss=0.641177825974667, w0=-1.913460550760269e-06, w1=-0.025732014158970952\n",
      "Regularized Logistic Regression(256/299): loss=0.6411770039965071, w0=-1.9150320562444348e-06, w1=-0.025744358550561258\n",
      "Regularized Logistic Regression(257/299): loss=0.641176204673031, w0=-1.916587786372482e-06, w1=-0.02575655240178186\n",
      "Regularized Logistic Regression(258/299): loss=0.6411754273671716, w0=-1.9181278992256542e-06, w1=-0.02576859761762561\n",
      "Regularized Logistic Regression(259/299): loss=0.6411746714600864, w0=-1.9196525513056535e-06, w1=-0.025780496078037965\n",
      "Regularized Logistic Regression(260/299): loss=0.6411739363506291, w0=-1.9211618975503652e-06, w1=-0.025792249638259362\n",
      "Regularized Logistic Regression(261/299): loss=0.6411732214548354, w0=-1.922656091349427e-06, w1=-0.025803860129162817\n",
      "Regularized Logistic Regression(262/299): loss=0.6411725262054246, w0=-1.9241352845596406e-06, w1=-0.025815329357586663\n",
      "Regularized Logistic Regression(263/299): loss=0.6411718500513178, w0=-1.9255996275202344e-06, w1=-0.025826659106662427\n",
      "Regularized Logistic Regression(264/299): loss=0.6411711924571657, w0=-1.927049269067971e-06, w1=-0.02583785113613827\n",
      "Regularized Logistic Regression(265/299): loss=0.6411705529028954, w0=-1.928484356552106e-06, w1=-0.025848907182697715\n",
      "Regularized Logistic Regression(266/299): loss=0.6411699308832658, w0=-1.9299050358491985e-06, w1=-0.025859828960273827\n",
      "Regularized Logistic Regression(267/299): loss=0.6411693259074404, w0=-1.931311451377774e-06, w1=-0.02587061816035904\n",
      "Regularized Logistic Regression(268/299): loss=0.641168737498569, w0=-1.9327037461128417e-06, w1=-0.025881276452310385\n",
      "Regularized Logistic Regression(269/299): loss=0.6411681651933834, w0=-1.9340820616002684e-06, w1=-0.02589180548365071\n",
      "Regularized Logistic Regression(270/299): loss=0.6411676085418053, w0=-1.935446537971008e-06, w1=-0.025902206880365312\n",
      "Regularized Logistic Regression(271/299): loss=0.6411670671065648, w0=-1.9367973139551916e-06, w1=-0.025912482247194637\n",
      "Regularized Logistic Regression(272/299): loss=0.6411665404628295, w0=-1.938134526896077e-06, w1=-0.02592263316792263\n",
      "Regularized Logistic Regression(273/299): loss=0.6411660281978465, w0=-1.9394583127638583e-06, w1=-0.02593266120566119\n",
      "Regularized Logistic Regression(274/299): loss=0.641165529910593, w0=-1.9407688061693416e-06, w1=-0.02594256790313045\n",
      "Regularized Logistic Regression(275/299): loss=0.6411650452114374, w0=-1.942066140377482e-06, w1=-0.02595235478293525\n",
      "Regularized Logistic Regression(276/299): loss=0.6411645737218123, w0=-1.943350447320787e-06, w1=-0.025962023347837614\n",
      "Regularized Logistic Regression(277/299): loss=0.6411641150738933, w0=-1.944621857612588e-06, w1=-0.02597157508102537\n",
      "Regularized Logistic Regression(278/299): loss=0.6411636689102904, w0=-1.9458805005601796e-06, w1=-0.02598101144637714\n",
      "Regularized Logistic Regression(279/299): loss=0.6411632348837477, w0=-1.9471265041778265e-06, w1=-0.025990333888723342\n",
      "Regularized Logistic Regression(280/299): loss=0.641162812656851, w0=-1.948359995199645e-06, w1=-0.025999543834103855\n",
      "Regularized Logistic Regression(281/299): loss=0.6411624019017439, w0=-1.9495810990923534e-06, w1=-0.026008642690021733\n",
      "Regularized Logistic Regression(282/299): loss=0.6411620022998538, w0=-1.9507899400678977e-06, w1=-0.02601763184569359\n",
      "Regularized Logistic Regression(283/299): loss=0.6411616135416239, w0=-1.9519866410959513e-06, w1=-0.026026512672296317\n",
      "Regularized Logistic Regression(284/299): loss=0.6411612353262549, w0=-1.953171323916291e-06, w1=-0.026035286523210462\n",
      "Regularized Logistic Regression(285/299): loss=0.6411608673614517, w0=-1.9543441090510506e-06, w1=-0.026043954734260123\n",
      "Regularized Logistic Regression(286/299): loss=0.6411605093631816, w0=-1.9555051158168512e-06, w1=-0.026052518623949496\n",
      "Regularized Logistic Regression(287/299): loss=0.6411601610554327, w0=-1.9566544623368134e-06, w1=-0.026060979493696096\n",
      "Regularized Logistic Regression(288/299): loss=0.6411598221699892, w0=-1.9577922655524484e-06, w1=-0.026069338628060795\n",
      "Regularized Logistic Regression(289/299): loss=0.6411594924462022, w0=-1.958918641235433e-06, w1=-0.026077597294974473\n",
      "Regularized Logistic Regression(290/299): loss=0.6411591716307765, w0=-1.960033703999266e-06, w1=-0.026085756745961742\n",
      "Regularized Logistic Regression(291/299): loss=0.6411588594775582, w0=-1.961137567310811e-06, w1=-0.02609381821636126\n",
      "Regularized Logistic Regression(292/299): loss=0.6411585557473294, w0=-1.9622303435017206e-06, w1=-0.026101782925543267\n",
      "Regularized Logistic Regression(293/299): loss=0.6411582602076108, w0=-1.963312143779753e-06, w1=-0.026109652077123836\n",
      "Regularized Logistic Regression(294/299): loss=0.6411579726324675, w0=-1.9643830782399725e-06, w1=-0.026117426859176306\n",
      "Regularized Logistic Regression(295/299): loss=0.6411576928023225, w0=-1.965443255875839e-06, w1=-0.02612510844443963\n",
      "Regularized Logistic Regression(296/299): loss=0.6411574205037742, w0=-1.966492784590189e-06, w1=-0.02613269799052394\n",
      "Regularized Logistic Regression(297/299): loss=0.6411571555294201, w0=-1.967531771206107e-06, w1=-0.026140196640113234\n",
      "Regularized Logistic Regression(298/299): loss=0.6411568976776852, w0=-1.9685603214776877e-06, w1=-0.026147605521165188\n",
      "Regularized Logistic Regression(299/299): loss=0.641156646752655, w0=-1.969578540100695e-06, w1=-0.026154925747108138\n",
      "Regularized Logistic Regression(0/99): loss=0.693147180559945, w0=-1.0441465146389343e-07, w1=-0.0018294491082988765\n",
      "Regularized Logistic Regression(1/99): loss=0.6853125380829753, w0=-2.0866371863375381e-07, w1=-0.003616973688170504\n",
      "Regularized Logistic Regression(2/99): loss=0.6779004681202152, w0=-3.127687135927536e-07, w1=-0.00536431456060506\n",
      "Regularized Logistic Regression(3/99): loss=0.6708859087536877, w0=-4.167485525992631e-07, w1=-0.007073135275884425\n",
      "Regularized Logistic Regression(4/99): loss=0.6642452751233161, w0=-5.206198194040118e-07, w1=-0.008745022998325505\n",
      "Regularized Logistic Regression(5/99): loss=0.6579563821726515, w0=-6.243970082611126e-07, w1=-0.010381490106797113\n",
      "Regularized Logistic Regression(6/99): loss=0.6519983700669729, w0=-7.280927469355394e-07, w1=-0.011983976326401672\n",
      "Regularized Logistic Regression(7/99): loss=0.646351632440934, w0=-8.317180003325502e-07, w1=-0.013553851238812404\n",
      "Regularized Logistic Regression(8/99): loss=0.6409977475306177, w0=-9.352822556078162e-07, w1=-0.01509241704718383\n",
      "Regularized Logistic Regression(9/99): loss=0.635919412185358, w0=-1.0387936897760044e-06, w1=-0.016600911496195227\n",
      "Regularized Logistic Regression(10/99): loss=0.6311003787228301, w0=-1.1422593209341476e-06, w1=-0.018080510868782797\n",
      "Regularized Logistic Regression(11/99): loss=0.6265253945757006, w0=-1.2456851442666442e-06, w1=-0.019532332998719835\n",
      "Regularized Logistic Regression(12/99): loss=0.622180144672111, w0=-1.3490762540117748e-06, w1=-0.020957440252772682\n",
      "Regularized Logistic Regression(13/99): loss=0.6180511964906327, w0=-1.4524369525542484e-06, w1=-0.022356842448059085\n",
      "Regularized Logistic Regression(14/99): loss=0.614125947730143, w0=-1.555770847771997e-06, w1=-0.023731499679840815\n",
      "Regularized Logistic Regression(15/99): loss=0.6103925765348261, w0=-1.6590809397144002e-06, w1=-0.025082325042642\n",
      "Regularized Logistic Regression(16/99): loss=0.606839994213414, w0=-1.762369697628296e-06, w1=-0.02641018723362656\n",
      "Regularized Logistic Regression(17/99): loss=0.6034578003897876, w0=-1.8656391282814278e-06, w1=-0.027715913031862514\n",
      "Regularized Logistic Regression(18/99): loss=0.6002362405192376, w0=-1.9688908364634805e-06, w1=-0.029000289650705804\n",
      "Regularized Logistic Regression(19/99): loss=0.5971661657013418, w0=-2.0721260784748306e-06, w1=-0.03026406696325413\n",
      "Regularized Logistic Regression(20/99): loss=0.5942389947168618, w0=-2.1753458093443424e-06, w1=-0.03150795960283059\n",
      "Regularized Logistic Regression(21/99): loss=0.5914466782126053, w0=-2.278550724451173e-06, w1=-0.03273264894190425\n",
      "Regularized Logistic Regression(22/99): loss=0.5887816649550504, w0=-2.3817412961624583e-06, w1=-0.03393878495385966\n",
      "Regularized Logistic Regression(23/99): loss=0.5862368700709052, w0=-2.484917806039481e-06, w1=-0.03512698796268799\n",
      "Regularized Logistic Regression(24/99): loss=0.5838056451907699, w0=-2.5880803731097447e-06, w1=-0.03629785028607079\n",
      "Regularized Logistic Regression(25/99): loss=0.5814817504107485, w0=-2.6912289786514294e-06, w1=-0.03745193777752216\n",
      "Regularized Logistic Regression(26/99): loss=0.579259327986234, w0=-2.7943634878899814e-06, w1=-0.03858979127330232\n",
      "Regularized Logistic Regression(27/99): loss=0.5771328776721557, w0=-2.897483668963947e-06, w1=-0.03971192794974981\n",
      "Regularized Logistic Regression(28/99): loss=0.5750972336246686, w0=-3.000589209478464e-06, w1=-0.04081884259653704\n",
      "Regularized Logistic Regression(29/99): loss=0.5731475427805313, w0=-3.1036797309298325e-06, w1=-0.04191100881115546\n",
      "Regularized Logistic Regression(30/99): loss=0.5712792446321786, w0=-3.2067548012530513e-06, w1=-0.04298888011970368\n",
      "Regularized Logistic Regression(31/99): loss=0.5694880523186792, w0=-3.309813945715898e-06, w1=-0.04405289102879344\n",
      "Regularized Logistic Regression(32/99): loss=0.567769934955296, w0=-3.412856656357748e-06, w1=-0.04510345801312465\n",
      "Regularized Logistic Regression(33/99): loss=0.5661211011271634, w0=-3.5158824001486604e-06, w1=-0.046140980443008475\n",
      "Regularized Logistic Regression(34/99): loss=0.5645379834755953, w0=-3.6188906260240285e-06, w1=-0.04716584145585192\n",
      "Regularized Logistic Regression(35/99): loss=0.5630172243086801, w0=-3.7218807709320795e-06, w1=-0.04817840877535493\n",
      "Regularized Logistic Regression(36/99): loss=0.561555662171042, w0=-3.824852265015497e-06, w1=-0.04917903548192365\n",
      "Regularized Logistic Regression(37/99): loss=0.5601503193109175, w0=-3.927804536034214e-06, w1=-0.05016806073756128\n",
      "Regularized Logistic Regression(38/99): loss=0.5587983899859603, w0=-4.030737013123819e-06, w1=-0.05114581046827397\n",
      "Regularized Logistic Regression(39/99): loss=0.557497229552415, w0=-4.13364912997285e-06, w1=-0.05211259800681282\n",
      "Regularized Logistic Regression(40/99): loss=0.5562443442854731, w0=-4.236540327492327e-06, w1=-0.05306872469837783\n",
      "Regularized Logistic Regression(41/99): loss=0.5550373818817061, w0=-4.339410056042183e-06, w1=-0.05401448047171797\n",
      "Regularized Logistic Regression(42/99): loss=0.5538741225974572, w0=-4.442257777271452e-06, w1=-0.054950144377889795\n",
      "Regularized Logistic Regression(43/99): loss=0.5527524709799448, w0=-4.545082965622303e-06, w1=-0.05587598509877583\n",
      "Regularized Logistic Regression(44/99): loss=0.5516704481505792, w0=-4.6478851095419165e-06, w1=-0.056792261427311176\n",
      "Regularized Logistic Regression(45/99): loss=0.5506261846026171, w0=-4.750663712440934e-06, w1=-0.05769922272123167\n",
      "Regularized Logistic Regression(46/99): loss=0.5496179134777713, w0=-4.853418293432462e-06, w1=-0.05859710933202238\n",
      "Regularized Logistic Regression(47/99): loss=0.5486439642887512, w0=-4.956148387881512e-06, w1=-0.059486153010632724\n",
      "Regularized Logistic Regression(48/99): loss=0.5477027570569372, w0=-5.0588535477910594e-06, w1=-0.06036657729140773\n",
      "Regularized Logistic Regression(49/99): loss=0.5467927968364894, w0=-5.161533342047747e-06, w1=-0.061238597855587724\n",
      "Regularized Logistic Regression(50/99): loss=0.545912668598172, w0=-5.2641873565473466e-06, w1=-0.06210242287563279\n",
      "Regularized Logistic Regression(51/99): loss=0.5450610324480191, w0=-5.366815194217664e-06, w1=-0.062958253341541\n",
      "Regularized Logistic Regression(52/99): loss=0.5442366191576963, w0=-5.469416474954324e-06, w1=-0.063806283370249\n",
      "Regularized Logistic Regression(53/99): loss=0.5434382259850498, w0=-5.571990835482955e-06, w1=-0.06464670049913018\n",
      "Regularized Logistic Regression(54/99): loss=0.5426647127648341, w0=-5.674537929159581e-06, w1=-0.06547968596453599\n",
      "Regularized Logistic Regression(55/99): loss=0.5419149982510238, w0=-5.7770574257195276e-06, w1=-0.06630541496626184\n",
      "Regularized Logistic Regression(56/99): loss=0.5411880566934413, w0=-5.8795490109838295e-06, w1=-0.06712405691876108\n",
      "Regularized Logistic Regression(57/99): loss=0.5404829146326429, w0=-5.982012386530962e-06, w1=-0.0679357756898757\n",
      "Regularized Logistic Regression(58/99): loss=0.539798647898158, w0=-6.084447269340697e-06, w1=-0.06874072982780205\n",
      "Regularized Logistic Regression(59/99): loss=0.5391343787962299, w0=-6.1868533914159855e-06, w1=-0.06953907277696197\n",
      "Regularized Logistic Regression(60/99): loss=0.5384892734741897, w0=-6.2892304993879805e-06, w1=-0.0703309530834081\n",
      "Regularized Logistic Regression(61/99): loss=0.5378625394495172, w0=-6.39157835410862e-06, w1=-0.07111651459034986\n",
      "Regularized Logistic Regression(62/99): loss=0.5372534232924818, w0=-6.4938967302345786e-06, w1=-0.07189589662435118\n",
      "Regularized Logistic Regression(63/99): loss=0.5366612084520516, w0=-6.59618541580586e-06, w1=-0.07266923417271455\n",
      "Regularized Logistic Regression(64/99): loss=0.5360852132154863, w0=-6.6984442118218435e-06, w1=-0.0734366580525351\n",
      "Regularized Logistic Regression(65/99): loss=0.5355247887927103, w0=-6.80067293181717e-06, w1=-0.07419829507187722\n",
      "Regularized Logistic Regression(66/99): loss=0.5349793175171851, w0=-6.902871401439511e-06, w1=-0.07495426818349867\n",
      "Regularized Logistic Regression(67/99): loss=0.5344482111555908, w0=-7.005039458030935e-06, w1=-0.07570469663152243\n",
      "Regularized Logistic Regression(68/99): loss=0.5339309093191639, w0=-7.107176950214311e-06, w1=-0.07644969609142944\n",
      "Regularized Logistic Regression(69/99): loss=0.5334268779700317, w0=-7.209283737485966e-06, w1=-0.07718937880372591\n",
      "Regularized Logistic Regression(70/99): loss=0.5329356080163683, w0=-7.311359689815571e-06, w1=-0.07792385370161559\n",
      "Regularized Logistic Regression(71/99): loss=0.5324566139906017, w0=-7.4134046872540826e-06, w1=-0.07865322653298883\n",
      "Regularized Logistic Regression(72/99): loss=0.5319894328053274, w0=-7.5154186195503715e-06, w1=-0.07937759997702165\n",
      "Regularized Logistic Regression(73/99): loss=0.5315336225819283, w0=-7.61740138577707e-06, w1=-0.08009707375566076\n",
      "Regularized Logistic Regression(74/99): loss=0.5310887615472719, w0=-7.719352893966014e-06, w1=-0.08081174474025467\n",
      "Regularized Logistic Regression(75/99): loss=0.5306544469941455, w0=-7.821273060753579e-06, w1=-0.08152170705357567\n",
      "Regularized Logistic Regression(76/99): loss=0.5302302943014116, w0=-7.923161811036097e-06, w1=-0.08222705216746443\n",
      "Regularized Logistic Regression(77/99): loss=0.5298159360101212, w0=-8.025019077635488e-06, w1=-0.08292786899631385\n",
      "Regularized Logistic Regression(78/99): loss=0.5294110209520888, w0=-8.12684480097515e-06, w1=-0.08362424398659915\n",
      "Regularized Logistic Regression(79/99): loss=0.5290152134276629, w0=-8.228638928766121e-06, w1=-0.08431626120264761\n",
      "Regularized Logistic Regression(80/99): loss=0.5286281924296461, w0=-8.330401415703444e-06, w1=-0.08500400240883116\n",
      "Regularized Logistic Regression(81/99): loss=0.528249650910525, w0=-8.432132223172662e-06, w1=-0.08568754714835512\n",
      "Regularized Logistic Regression(82/99): loss=0.5278792950903601, w0=-8.533831318966315e-06, w1=-0.08636697281880718\n",
      "Regularized Logistic Regression(83/99): loss=0.5275168438028527, w0=-8.63549867701028e-06, w1=-0.08704235474462096\n",
      "Regularized Logistic Regression(84/99): loss=0.527162027877283, w0=-8.737134277099786e-06, w1=-0.08771376624660028\n",
      "Regularized Logistic Regression(85/99): loss=0.5268145895541553, w0=-8.838738104644908e-06, w1=-0.08838127870864369\n",
      "Regularized Logistic Regression(86/99): loss=0.5264742819325287, w0=-8.94031015042533e-06, w1=-0.08904496164179876\n",
      "Regularized Logistic Regression(87/99): loss=0.5261408684471476, w0=-9.04185041035415e-06, w1=-0.08970488274577232\n",
      "Regularized Logistic Regression(88/99): loss=0.5258141223736061, w0=-9.143358885250496e-06, w1=-0.09036110796801218\n",
      "Regularized Logistic Regression(89/99): loss=0.5254938263598883, w0=-9.244835580620728e-06, w1=-0.0910137015604731\n",
      "Regularized Logistic Regression(90/99): loss=0.5251797719827468, w0=-9.346280506447966e-06, w1=-0.09166272613417262\n",
      "Regularized Logistic Regression(91/99): loss=0.5248717593274621, w0=-9.447693676989703e-06, w1=-0.09230824271163621\n",
      "Regularized Logistic Regression(92/99): loss=0.5245695965896325, w0=-9.549075110583261e-06, w1=-0.0929503107773275\n",
      "Regularized Logistic Regression(93/99): loss=0.524273099697723, w0=-9.650424829458846e-06, w1=-0.09358898832615312\n",
      "Regularized Logistic Regression(94/99): loss=0.5239820919551803, w0=-9.751742859559934e-06, w1=-0.09422433191012826\n",
      "Regularized Logistic Regression(95/99): loss=0.5236964037009979, w0=-9.853029230370784e-06, w1=-0.09485639668328433\n",
      "Regularized Logistic Regression(96/99): loss=0.5234158719876871, w0=-9.954283974750792e-06, w1=-0.09548523644489505\n",
      "Regularized Logistic Regression(97/99): loss=0.523140340275668, w0=-1.005550712877548e-05, w1=-0.09611090368109539\n",
      "Regularized Logistic Regression(98/99): loss=0.5228696581431604, w0=-1.015669873158389e-05, w1=-0.0967334496049626\n",
      "Regularized Logistic Regression(99/99): loss=0.5226036810107086, w0=-1.025785882523212e-05, w1=-0.09735292419512515\n",
      "Regularized Logistic Regression(0/199): loss=0.693147180559945, w0=-1.0441465146389343e-07, w1=-0.0018294491082988765\n",
      "Regularized Logistic Regression(1/199): loss=0.6853125380829753, w0=-2.0866371863375381e-07, w1=-0.003616973688170504\n",
      "Regularized Logistic Regression(2/199): loss=0.6779004681202152, w0=-3.127687135927536e-07, w1=-0.00536431456060506\n",
      "Regularized Logistic Regression(3/199): loss=0.6708859087536877, w0=-4.167485525992631e-07, w1=-0.007073135275884425\n",
      "Regularized Logistic Regression(4/199): loss=0.6642452751233161, w0=-5.206198194040118e-07, w1=-0.008745022998325505\n",
      "Regularized Logistic Regression(5/199): loss=0.6579563821726515, w0=-6.243970082611126e-07, w1=-0.010381490106797113\n",
      "Regularized Logistic Regression(6/199): loss=0.6519983700669729, w0=-7.280927469355394e-07, w1=-0.011983976326401672\n",
      "Regularized Logistic Regression(7/199): loss=0.646351632440934, w0=-8.317180003325502e-07, w1=-0.013553851238812404\n",
      "Regularized Logistic Regression(8/199): loss=0.6409977475306177, w0=-9.352822556078162e-07, w1=-0.01509241704718383\n",
      "Regularized Logistic Regression(9/199): loss=0.635919412185358, w0=-1.0387936897760044e-06, w1=-0.016600911496195227\n",
      "Regularized Logistic Regression(10/199): loss=0.6311003787228301, w0=-1.1422593209341476e-06, w1=-0.018080510868782797\n",
      "Regularized Logistic Regression(11/199): loss=0.6265253945757006, w0=-1.2456851442666442e-06, w1=-0.019532332998719835\n",
      "Regularized Logistic Regression(12/199): loss=0.622180144672111, w0=-1.3490762540117748e-06, w1=-0.020957440252772682\n",
      "Regularized Logistic Regression(13/199): loss=0.6180511964906327, w0=-1.4524369525542484e-06, w1=-0.022356842448059085\n",
      "Regularized Logistic Regression(14/199): loss=0.614125947730143, w0=-1.555770847771997e-06, w1=-0.023731499679840815\n",
      "Regularized Logistic Regression(15/199): loss=0.6103925765348261, w0=-1.6590809397144002e-06, w1=-0.025082325042642\n",
      "Regularized Logistic Regression(16/199): loss=0.606839994213414, w0=-1.762369697628296e-06, w1=-0.02641018723362656\n",
      "Regularized Logistic Regression(17/199): loss=0.6034578003897876, w0=-1.8656391282814278e-06, w1=-0.027715913031862514\n",
      "Regularized Logistic Regression(18/199): loss=0.6002362405192376, w0=-1.9688908364634805e-06, w1=-0.029000289650705804\n",
      "Regularized Logistic Regression(19/199): loss=0.5971661657013418, w0=-2.0721260784748306e-06, w1=-0.03026406696325413\n",
      "Regularized Logistic Regression(20/199): loss=0.5942389947168618, w0=-2.1753458093443424e-06, w1=-0.03150795960283059\n",
      "Regularized Logistic Regression(21/199): loss=0.5914466782126053, w0=-2.278550724451173e-06, w1=-0.03273264894190425\n",
      "Regularized Logistic Regression(22/199): loss=0.5887816649550504, w0=-2.3817412961624583e-06, w1=-0.03393878495385966\n",
      "Regularized Logistic Regression(23/199): loss=0.5862368700709052, w0=-2.484917806039481e-06, w1=-0.03512698796268799\n",
      "Regularized Logistic Regression(24/199): loss=0.5838056451907699, w0=-2.5880803731097447e-06, w1=-0.03629785028607079\n",
      "Regularized Logistic Regression(25/199): loss=0.5814817504107485, w0=-2.6912289786514294e-06, w1=-0.03745193777752216\n",
      "Regularized Logistic Regression(26/199): loss=0.579259327986234, w0=-2.7943634878899814e-06, w1=-0.03858979127330232\n",
      "Regularized Logistic Regression(27/199): loss=0.5771328776721557, w0=-2.897483668963947e-06, w1=-0.03971192794974981\n",
      "Regularized Logistic Regression(28/199): loss=0.5750972336246686, w0=-3.000589209478464e-06, w1=-0.04081884259653704\n",
      "Regularized Logistic Regression(29/199): loss=0.5731475427805313, w0=-3.1036797309298325e-06, w1=-0.04191100881115546\n",
      "Regularized Logistic Regression(30/199): loss=0.5712792446321786, w0=-3.2067548012530513e-06, w1=-0.04298888011970368\n",
      "Regularized Logistic Regression(31/199): loss=0.5694880523186792, w0=-3.309813945715898e-06, w1=-0.04405289102879344\n",
      "Regularized Logistic Regression(32/199): loss=0.567769934955296, w0=-3.412856656357748e-06, w1=-0.04510345801312465\n",
      "Regularized Logistic Regression(33/199): loss=0.5661211011271634, w0=-3.5158824001486604e-06, w1=-0.046140980443008475\n",
      "Regularized Logistic Regression(34/199): loss=0.5645379834755953, w0=-3.6188906260240285e-06, w1=-0.04716584145585192\n",
      "Regularized Logistic Regression(35/199): loss=0.5630172243086801, w0=-3.7218807709320795e-06, w1=-0.04817840877535493\n",
      "Regularized Logistic Regression(36/199): loss=0.561555662171042, w0=-3.824852265015497e-06, w1=-0.04917903548192365\n",
      "Regularized Logistic Regression(37/199): loss=0.5601503193109175, w0=-3.927804536034214e-06, w1=-0.05016806073756128\n",
      "Regularized Logistic Regression(38/199): loss=0.5587983899859603, w0=-4.030737013123819e-06, w1=-0.05114581046827397\n",
      "Regularized Logistic Regression(39/199): loss=0.557497229552415, w0=-4.13364912997285e-06, w1=-0.05211259800681282\n",
      "Regularized Logistic Regression(40/199): loss=0.5562443442854731, w0=-4.236540327492327e-06, w1=-0.05306872469837783\n",
      "Regularized Logistic Regression(41/199): loss=0.5550373818817061, w0=-4.339410056042183e-06, w1=-0.05401448047171797\n",
      "Regularized Logistic Regression(42/199): loss=0.5538741225974572, w0=-4.442257777271452e-06, w1=-0.054950144377889795\n",
      "Regularized Logistic Regression(43/199): loss=0.5527524709799448, w0=-4.545082965622303e-06, w1=-0.05587598509877583\n",
      "Regularized Logistic Regression(44/199): loss=0.5516704481505792, w0=-4.6478851095419165e-06, w1=-0.056792261427311176\n",
      "Regularized Logistic Regression(45/199): loss=0.5506261846026171, w0=-4.750663712440934e-06, w1=-0.05769922272123167\n",
      "Regularized Logistic Regression(46/199): loss=0.5496179134777713, w0=-4.853418293432462e-06, w1=-0.05859710933202238\n",
      "Regularized Logistic Regression(47/199): loss=0.5486439642887512, w0=-4.956148387881512e-06, w1=-0.059486153010632724\n",
      "Regularized Logistic Regression(48/199): loss=0.5477027570569372, w0=-5.0588535477910594e-06, w1=-0.06036657729140773\n",
      "Regularized Logistic Regression(49/199): loss=0.5467927968364894, w0=-5.161533342047747e-06, w1=-0.061238597855587724\n",
      "Regularized Logistic Regression(50/199): loss=0.545912668598172, w0=-5.2641873565473466e-06, w1=-0.06210242287563279\n",
      "Regularized Logistic Regression(51/199): loss=0.5450610324480191, w0=-5.366815194217664e-06, w1=-0.062958253341541\n",
      "Regularized Logistic Regression(52/199): loss=0.5442366191576963, w0=-5.469416474954324e-06, w1=-0.063806283370249\n",
      "Regularized Logistic Regression(53/199): loss=0.5434382259850498, w0=-5.571990835482955e-06, w1=-0.06464670049913018\n",
      "Regularized Logistic Regression(54/199): loss=0.5426647127648341, w0=-5.674537929159581e-06, w1=-0.06547968596453599\n",
      "Regularized Logistic Regression(55/199): loss=0.5419149982510238, w0=-5.7770574257195276e-06, w1=-0.06630541496626184\n",
      "Regularized Logistic Regression(56/199): loss=0.5411880566934413, w0=-5.8795490109838295e-06, w1=-0.06712405691876108\n",
      "Regularized Logistic Regression(57/199): loss=0.5404829146326429, w0=-5.982012386530962e-06, w1=-0.0679357756898757\n",
      "Regularized Logistic Regression(58/199): loss=0.539798647898158, w0=-6.084447269340697e-06, w1=-0.06874072982780205\n",
      "Regularized Logistic Regression(59/199): loss=0.5391343787962299, w0=-6.1868533914159855e-06, w1=-0.06953907277696197\n",
      "Regularized Logistic Regression(60/199): loss=0.5384892734741897, w0=-6.2892304993879805e-06, w1=-0.0703309530834081\n",
      "Regularized Logistic Regression(61/199): loss=0.5378625394495172, w0=-6.39157835410862e-06, w1=-0.07111651459034986\n",
      "Regularized Logistic Regression(62/199): loss=0.5372534232924818, w0=-6.4938967302345786e-06, w1=-0.07189589662435118\n",
      "Regularized Logistic Regression(63/199): loss=0.5366612084520516, w0=-6.59618541580586e-06, w1=-0.07266923417271455\n",
      "Regularized Logistic Regression(64/199): loss=0.5360852132154863, w0=-6.6984442118218435e-06, w1=-0.0734366580525351\n",
      "Regularized Logistic Regression(65/199): loss=0.5355247887927103, w0=-6.80067293181717e-06, w1=-0.07419829507187722\n",
      "Regularized Logistic Regression(66/199): loss=0.5349793175171851, w0=-6.902871401439511e-06, w1=-0.07495426818349867\n",
      "Regularized Logistic Regression(67/199): loss=0.5344482111555908, w0=-7.005039458030935e-06, w1=-0.07570469663152243\n",
      "Regularized Logistic Regression(68/199): loss=0.5339309093191639, w0=-7.107176950214311e-06, w1=-0.07644969609142944\n",
      "Regularized Logistic Regression(69/199): loss=0.5334268779700317, w0=-7.209283737485966e-06, w1=-0.07718937880372591\n",
      "Regularized Logistic Regression(70/199): loss=0.5329356080163683, w0=-7.311359689815571e-06, w1=-0.07792385370161559\n",
      "Regularized Logistic Regression(71/199): loss=0.5324566139906017, w0=-7.4134046872540826e-06, w1=-0.07865322653298883\n",
      "Regularized Logistic Regression(72/199): loss=0.5319894328053274, w0=-7.5154186195503715e-06, w1=-0.07937759997702165\n",
      "Regularized Logistic Regression(73/199): loss=0.5315336225819283, w0=-7.61740138577707e-06, w1=-0.08009707375566076\n",
      "Regularized Logistic Regression(74/199): loss=0.5310887615472719, w0=-7.719352893966014e-06, w1=-0.08081174474025467\n",
      "Regularized Logistic Regression(75/199): loss=0.5306544469941455, w0=-7.821273060753579e-06, w1=-0.08152170705357567\n",
      "Regularized Logistic Regression(76/199): loss=0.5302302943014116, w0=-7.923161811036097e-06, w1=-0.08222705216746443\n",
      "Regularized Logistic Regression(77/199): loss=0.5298159360101212, w0=-8.025019077635488e-06, w1=-0.08292786899631385\n",
      "Regularized Logistic Regression(78/199): loss=0.5294110209520888, w0=-8.12684480097515e-06, w1=-0.08362424398659915\n",
      "Regularized Logistic Regression(79/199): loss=0.5290152134276629, w0=-8.228638928766121e-06, w1=-0.08431626120264761\n",
      "Regularized Logistic Regression(80/199): loss=0.5286281924296461, w0=-8.330401415703444e-06, w1=-0.08500400240883116\n",
      "Regularized Logistic Regression(81/199): loss=0.528249650910525, w0=-8.432132223172662e-06, w1=-0.08568754714835512\n",
      "Regularized Logistic Regression(82/199): loss=0.5278792950903601, w0=-8.533831318966315e-06, w1=-0.08636697281880718\n",
      "Regularized Logistic Regression(83/199): loss=0.5275168438028527, w0=-8.63549867701028e-06, w1=-0.08704235474462096\n",
      "Regularized Logistic Regression(84/199): loss=0.527162027877283, w0=-8.737134277099786e-06, w1=-0.08771376624660028\n",
      "Regularized Logistic Regression(85/199): loss=0.5268145895541553, w0=-8.838738104644908e-06, w1=-0.08838127870864369\n",
      "Regularized Logistic Regression(86/199): loss=0.5264742819325287, w0=-8.94031015042533e-06, w1=-0.08904496164179876\n",
      "Regularized Logistic Regression(87/199): loss=0.5261408684471476, w0=-9.04185041035415e-06, w1=-0.08970488274577232\n",
      "Regularized Logistic Regression(88/199): loss=0.5258141223736061, w0=-9.143358885250496e-06, w1=-0.09036110796801218\n",
      "Regularized Logistic Regression(89/199): loss=0.5254938263598883, w0=-9.244835580620728e-06, w1=-0.0910137015604731\n",
      "Regularized Logistic Regression(90/199): loss=0.5251797719827468, w0=-9.346280506447966e-06, w1=-0.09166272613417262\n",
      "Regularized Logistic Regression(91/199): loss=0.5248717593274621, w0=-9.447693676989703e-06, w1=-0.09230824271163621\n",
      "Regularized Logistic Regression(92/199): loss=0.5245695965896325, w0=-9.549075110583261e-06, w1=-0.0929503107773275\n",
      "Regularized Logistic Regression(93/199): loss=0.524273099697723, w0=-9.650424829458846e-06, w1=-0.09358898832615312\n",
      "Regularized Logistic Regression(94/199): loss=0.5239820919551803, w0=-9.751742859559934e-06, w1=-0.09422433191012826\n",
      "Regularized Logistic Regression(95/199): loss=0.5236964037009979, w0=-9.853029230370784e-06, w1=-0.09485639668328433\n",
      "Regularized Logistic Regression(96/199): loss=0.5234158719876871, w0=-9.954283974750792e-06, w1=-0.09548523644489505\n",
      "Regularized Logistic Regression(97/199): loss=0.523140340275668, w0=-1.005550712877548e-05, w1=-0.09611090368109539\n",
      "Regularized Logistic Regression(98/199): loss=0.5228696581431604, w0=-1.015669873158389e-05, w1=-0.0967334496049626\n",
      "Regularized Logistic Regression(99/199): loss=0.5226036810107086, w0=-1.025785882523212e-05, w1=-0.09735292419512515\n",
      "Regularized Logistic Regression(100/199): loss=0.5223422698795286, w0=-1.0358987454552826e-05, w1=-0.09796937623296387\n",
      "Regularized Logistic Regression(101/199): loss=0.5220852910829112, w0=-1.0460084667020434e-05, w1=-0.09858285333846377\n",
      "Regularized Logistic Regression(102/199): loss=0.5218326160499677, w0=-1.0561150512621876e-05, w1=-0.09919340200477474\n",
      "Regularized Logistic Regression(103/199): loss=0.5215841210810396, w0=-1.0662185043732623e-05, w1=-0.099801067631535\n",
      "Regularized Logistic Regression(104/199): loss=0.5213396871341418, w0=-1.0763188314997836e-05, w1=-0.10040589455700898\n",
      "Regularized Logistic Regression(105/199): loss=0.5210991996218387, w0=-1.0864160383218418e-05, w1=-0.10100792608908901\n",
      "Regularized Logistic Regression(106/199): loss=0.5208625482179954, w0=-1.09651013072418e-05, w1=-0.10160720453520769\n",
      "Regularized Logistic Regression(107/199): loss=0.5206296266738746, w0=-1.1066011147857262e-05, w1=-0.10220377123120603\n",
      "Regularized Logistic Regression(108/199): loss=0.5204003326430795, w0=-1.1166889967695623e-05, w1=-0.1027976665691993\n",
      "Regularized Logistic Regression(109/199): loss=0.5201745675148775, w0=-1.1267737831133117e-05, w1=-0.10338893002448177\n",
      "Regularized Logistic Regression(110/199): loss=0.5199522362554612, w0=-1.1368554804199313e-05, w1=-0.10397760018150932\n",
      "Regularized Logistic Regression(111/199): loss=0.5197332472567303, w0=-1.146934095448889e-05, w1=-0.10456371475899609\n",
      "Regularized Logistic Regression(112/199): loss=0.5195175121922035, w0=-1.157009635107713e-05, w1=-0.10514731063416123\n",
      "Regularized Logistic Regression(113/199): loss=0.5193049458796902, w0=-1.1670821064438993e-05, w1=-0.10572842386615951\n",
      "Regularized Logistic Regression(114/199): loss=0.5190954661503717, w0=-1.1771515166371594e-05, w1=-0.1063070897187269\n",
      "Regularized Logistic Regression(115/199): loss=0.518888993723964, w0=-1.1872178729919993e-05, w1=-0.10688334268207356\n",
      "Regularized Logistic Regression(116/199): loss=0.5186854520896526, w0=-1.1972811829306115e-05, w1=-0.10745721649405209\n",
      "Regularized Logistic Regression(117/199): loss=0.5184847673925009, w0=-1.2073414539860715e-05, w1=-0.10802874416062977\n",
      "Regularized Logistic Regression(118/199): loss=0.518286868325064, w0=-1.2173986937958242e-05, w1=-0.10859795797569165\n",
      "Regularized Logistic Regression(119/199): loss=0.5180916860239329, w0=-1.2274529100954488e-05, w1=-0.10916488954019993\n",
      "Regularized Logistic Regression(120/199): loss=0.5178991539709774, w0=-1.2375041107126911e-05, w1=-0.10972956978073409\n",
      "Regularized Logistic Regression(121/199): loss=0.5177092078990408, w0=-1.2475523035617524e-05, w1=-0.11029202896743551\n",
      "Regularized Logistic Regression(122/199): loss=0.5175217857018714, w0=-1.2575974966378243e-05, w1=-0.11085229673137878\n",
      "Regularized Logistic Regression(123/199): loss=0.5173368273480817, w0=-1.2676396980118588e-05, w1=-0.11141040208139145\n",
      "Regularized Logistic Regression(124/199): loss=0.5171542747989343, w0=-1.2776789158255652e-05, w1=-0.11196637342034216\n",
      "Regularized Logistic Regression(125/199): loss=0.516974071929772, w0=-1.2877151582866236e-05, w1=-0.11252023856091728\n",
      "Regularized Logistic Regression(126/199): loss=0.5167961644549086, w0=-1.2977484336641063e-05, w1=-0.11307202474090552\n",
      "Regularized Logistic Regression(127/199): loss=0.5166204998558194, w0=-1.3077787502840989e-05, w1=-0.11362175863800682\n",
      "Regularized Logistic Regression(128/199): loss=0.5164470273124668, w0=-1.3178061165255115e-05, w1=-0.11416946638418483\n",
      "Regularized Logistic Regression(129/199): loss=0.5162756976376139, w0=-1.3278305408160749e-05, w1=-0.11471517357957803\n",
      "Regularized Logistic Regression(130/199): loss=0.5161064632139813, w0=-1.3378520316285105e-05, w1=-0.11525890530598606\n",
      "Regularized Logistic Regression(131/199): loss=0.5159392779341146, w0=-1.34787059747687e-05, w1=-0.11580068613994686\n",
      "Regularized Logistic Regression(132/199): loss=0.5157740971428302, w0=-1.357886246913036e-05, w1=-0.11634054016541769\n",
      "Regularized Logistic Regression(133/199): loss=0.5156108775821215, w0=-1.3678989885233776e-05, w1=-0.11687849098607632\n",
      "Regularized Logistic Regression(134/199): loss=0.5154495773384092, w0=-1.3779088309255533e-05, w1=-0.11741456173725383\n",
      "Regularized Logistic Regression(135/199): loss=0.5152901557920242, w0=-1.3879157827654573e-05, w1=-0.11794877509751281\n",
      "Regularized Logistic Regression(136/199): loss=0.5151325735688221, w0=-1.3979198527143014e-05, w1=-0.11848115329988336\n",
      "Regularized Logistic Regression(137/199): loss=0.5149767924938307, w0=-1.4079210494658274e-05, w1=-0.11901171814276856\n",
      "Regularized Logistic Regression(138/199): loss=0.5148227755468322, w0=-1.4179193817336449e-05, w1=-0.1195404910005309\n",
      "Regularized Logistic Regression(139/199): loss=0.5146704868197964, w0=-1.4279148582486883e-05, w1=-0.1200674928337706\n",
      "Regularized Logistic Regression(140/199): loss=0.5145198914760812, w0=-1.4379074877567905e-05, w1=-0.12059274419930632\n",
      "Regularized Logistic Regression(141/199): loss=0.5143709557113126, w0=-1.447897279016364e-05, w1=-0.12111626525986847\n",
      "Regularized Logistic Regression(142/199): loss=0.5142236467158761, w0=-1.4578842407961896e-05, w1=-0.12163807579351493\n",
      "Regularized Logistic Regression(143/199): loss=0.5140779326389429, w0=-1.4678683818733057e-05, w1=-0.1221581952027779\n",
      "Regularized Logistic Regression(144/199): loss=0.5139337825539637, w0=-1.4778497110309944e-05, w1=-0.12267664252355194\n",
      "Regularized Logistic Regression(145/199): loss=0.5137911664255616, w0=-1.48782823705686e-05, w1=-0.12319343643373076\n",
      "Regularized Logistic Regression(146/199): loss=0.5136500550777644, w0=-1.497803968740997e-05, w1=-0.12370859526160187\n",
      "Regularized Logistic Regression(147/199): loss=0.5135104201635177, w0=-1.5077769148742433e-05, w1=-0.1242221369940064\n",
      "Regularized Logistic Regression(148/199): loss=0.5133722341354187, w0=-1.5177470842465147e-05, w1=-0.12473407928427255\n",
      "Regularized Logistic Regression(149/199): loss=0.5132354702176203, w0=-1.527714485645218e-05, w1=-0.12524443945992908\n",
      "Regularized Logistic Regression(150/199): loss=0.5131001023788523, w0=-1.5376791278537386e-05, w1=-0.1257532345302073\n",
      "Regularized Logistic Regression(151/199): loss=0.5129661053065123, w0=-1.5476410196500006e-05, w1=-0.12626048119333658\n",
      "Regularized Logistic Regression(152/199): loss=0.5128334543817779, w0=-1.557600169805095e-05, w1=-0.12676619584364215\n",
      "Regularized Logistic Regression(153/199): loss=0.5127021256556993, w0=-1.5675565870819733e-05, w1=-0.12727039457844969\n",
      "Regularized Logistic Regression(154/199): loss=0.5125720958262246, w0=-1.5775102802342072e-05, w1=-0.12777309320480365\n",
      "Regularized Logistic Regression(155/199): loss=0.5124433422161263, w0=-1.587461258004804e-05, w1=-0.12827430724600547\n",
      "Regularized Logistic Regression(156/199): loss=0.5123158427517784, w0=-1.5974095291250835e-05, w1=-0.12877405194797667\n",
      "Regularized Logistic Regression(157/199): loss=0.5121895759427618, w0=-1.6073551023136103e-05, w1=-0.12927234228545273\n",
      "Regularized Logistic Regression(158/199): loss=0.5120645208622499, w0=-1.617297986275176e-05, w1=-0.12976919296801326\n",
      "Regularized Logistic Regression(159/199): loss=0.5119406571281506, w0=-1.6272381896998365e-05, w1=-0.1302646184459525\n",
      "Regularized Logistic Regression(160/199): loss=0.5118179648849694, w0=-1.6371757212619958e-05, w1=-0.13075863291599688\n",
      "Regularized Logistic Regression(161/199): loss=0.5116964247863609, w0=-1.6471105896195352e-05, w1=-0.13125125032687235\n",
      "Regularized Logistic Regression(162/199): loss=0.5115760179783453, w0=-1.6570428034129905e-05, w1=-0.13174248438472735\n",
      "Regularized Logistic Regression(163/199): loss=0.5114567260831574, w0=-1.6669723712647694e-05, w1=-0.13223234855841554\n",
      "Regularized Logistic Regression(164/199): loss=0.5113385311837056, w0=-1.6768993017784113e-05, w1=-0.13272085608464249\n",
      "Regularized Logistic Regression(165/199): loss=0.511221415808611, w0=-1.6868236035378846e-05, w1=-0.13320801997297999\n",
      "Regularized Logistic Regression(166/199): loss=0.5111053629178097, w0=-1.6967452851069234e-05, w1=-0.133693853010753\n",
      "Regularized Logistic Regression(167/199): loss=0.5109903558886866, w0=-1.7066643550283998e-05, w1=-0.134178367767802\n",
      "Regularized Logistic Regression(168/199): loss=0.5108763785027262, w0=-1.71658082182373e-05, w1=-0.13466157660112493\n",
      "Regularized Logistic Regression(169/199): loss=0.5107634149326564, w0=-1.7264946939923128e-05, w1=-0.13514349165940207\n",
      "Regularized Logistic Regression(170/199): loss=0.5106514497300644, w0=-1.736405980011002e-05, w1=-0.13562412488740783\n",
      "Regularized Logistic Regression(171/199): loss=0.5105404678134672, w0=-1.746314688333606e-05, w1=-0.13610348803031205\n",
      "Regularized Logistic Regression(172/199): loss=0.5104304544568172, w0=-1.7562208273904185e-05, w1=-0.1365815926378743\n",
      "Regularized Logistic Regression(173/199): loss=0.510321395278427, w0=-1.766124405587776e-05, w1=-0.13705845006853484\n",
      "Regularized Logistic Regression(174/199): loss=0.510213276230296, w0=-1.7760254313076426e-05, w1=-0.13753407149340396\n",
      "Regularized Logistic Regression(175/199): loss=0.5101060835878194, w0=-1.785923912907217e-05, w1=-0.1380084679001541\n",
      "Regularized Logistic Regression(176/199): loss=0.5099998039398713, w0=-1.7958198587185694e-05, w1=-0.13848165009681657\n",
      "Regularized Logistic Regression(177/199): loss=0.5098944241792409, w0=-1.805713277048295e-05, w1=-0.1389536287154856\n",
      "Regularized Logistic Regression(178/199): loss=0.5097899314934106, w0=-1.815604176177195e-05, w1=-0.13942441421593316\n",
      "Regularized Logistic Regression(179/199): loss=0.5096863133556617, w0=-1.8254925643599766e-05, w1=-0.13989401688913594\n",
      "Regularized Logistic Regression(180/199): loss=0.5095835575164969, w0=-1.8353784498249713e-05, w1=-0.14036244686071792\n",
      "Regularized Logistic Regression(181/199): loss=0.5094816519953627, w0=-1.8452618407738777e-05, w1=-0.14082971409431033\n",
      "Regularized Logistic Regression(182/199): loss=0.5093805850726651, w0=-1.8551427453815168e-05, w1=-0.14129582839483146\n",
      "Regularized Logistic Regression(183/199): loss=0.5092803452820649, w0=-1.86502117179561e-05, w1=-0.1417607994116888\n",
      "Regularized Logistic Regression(184/199): loss=0.5091809214030407, w0=-1.8748971281365714e-05, w1=-0.14222463664190504\n",
      "Regularized Logistic Regression(185/199): loss=0.5090823024537107, w0=-1.884770622497316e-05, w1=-0.1426873494331709\n",
      "Regularized Logistic Regression(186/199): loss=0.5089844776839036, w0=-1.8946416629430853e-05, w1=-0.14314894698682573\n",
      "Regularized Logistic Regression(187/199): loss=0.5088874365684695, w0=-1.904510257511286e-05, w1=-0.1436094383607689\n",
      "Regularized Logistic Regression(188/199): loss=0.5087911688008168, w0=-1.914376414211344e-05, w1=-0.14406883247230323\n",
      "Regularized Logistic Regression(189/199): loss=0.5086956642866758, w0=-1.9242401410245697e-05, w1=-0.14452713810091208\n",
      "Regularized Logistic Regression(190/199): loss=0.5086009131380708, w0=-1.93410144590404e-05, w1=-0.14498436389097313\n",
      "Regularized Logistic Regression(191/199): loss=0.5085069056674979, w0=-1.9439603367744884e-05, w1=-0.14544051835440883\n",
      "Regularized Logistic Regression(192/199): loss=0.5084136323823018, w0=-1.95381682153221e-05, w1=-0.14589560987327632\n",
      "Regularized Logistic Regression(193/199): loss=0.508321083979239, w0=-1.9636709080449745e-05, w1=-0.1463496467022983\n",
      "Regularized Logistic Regression(194/199): loss=0.5082292513392263, w0=-1.973522604151955e-05, w1=-0.14680263697133622\n",
      "Regularized Logistic Regression(195/199): loss=0.5081381255222633, w0=-1.9833719176636607e-05, w1=-0.1472545886878072\n",
      "Regularized Logistic Regression(196/199): loss=0.508047697762525, w0=-1.993218856361884e-05, w1=-0.1477055097390466\n",
      "Regularized Logistic Regression(197/199): loss=0.5079579594636154, w0=-2.0030634279996547e-05, w1=-0.14815540789461729\n",
      "Regularized Logistic Regression(198/199): loss=0.5078689021939818, w0=-2.0129056403012042e-05, w1=-0.1486042908085668\n",
      "Regularized Logistic Regression(199/199): loss=0.5077805176824751, w0=-2.0227455009619366e-05, w1=-0.14905216602163465\n",
      "Regularized Logistic Regression(0/299): loss=0.693147180559945, w0=-1.0441465146389343e-07, w1=-0.0018294491082988765\n",
      "Regularized Logistic Regression(1/299): loss=0.6853125380829753, w0=-2.0866371863375381e-07, w1=-0.003616973688170504\n",
      "Regularized Logistic Regression(2/299): loss=0.6779004681202152, w0=-3.127687135927536e-07, w1=-0.00536431456060506\n",
      "Regularized Logistic Regression(3/299): loss=0.6708859087536877, w0=-4.167485525992631e-07, w1=-0.007073135275884425\n",
      "Regularized Logistic Regression(4/299): loss=0.6642452751233161, w0=-5.206198194040118e-07, w1=-0.008745022998325505\n",
      "Regularized Logistic Regression(5/299): loss=0.6579563821726515, w0=-6.243970082611126e-07, w1=-0.010381490106797113\n",
      "Regularized Logistic Regression(6/299): loss=0.6519983700669729, w0=-7.280927469355394e-07, w1=-0.011983976326401672\n",
      "Regularized Logistic Regression(7/299): loss=0.646351632440934, w0=-8.317180003325502e-07, w1=-0.013553851238812404\n",
      "Regularized Logistic Regression(8/299): loss=0.6409977475306177, w0=-9.352822556078162e-07, w1=-0.01509241704718383\n",
      "Regularized Logistic Regression(9/299): loss=0.635919412185358, w0=-1.0387936897760044e-06, w1=-0.016600911496195227\n",
      "Regularized Logistic Regression(10/299): loss=0.6311003787228301, w0=-1.1422593209341476e-06, w1=-0.018080510868782797\n",
      "Regularized Logistic Regression(11/299): loss=0.6265253945757006, w0=-1.2456851442666442e-06, w1=-0.019532332998719835\n",
      "Regularized Logistic Regression(12/299): loss=0.622180144672111, w0=-1.3490762540117748e-06, w1=-0.020957440252772682\n",
      "Regularized Logistic Regression(13/299): loss=0.6180511964906327, w0=-1.4524369525542484e-06, w1=-0.022356842448059085\n",
      "Regularized Logistic Regression(14/299): loss=0.614125947730143, w0=-1.555770847771997e-06, w1=-0.023731499679840815\n",
      "Regularized Logistic Regression(15/299): loss=0.6103925765348261, w0=-1.6590809397144002e-06, w1=-0.025082325042642\n",
      "Regularized Logistic Regression(16/299): loss=0.606839994213414, w0=-1.762369697628296e-06, w1=-0.02641018723362656\n",
      "Regularized Logistic Regression(17/299): loss=0.6034578003897876, w0=-1.8656391282814278e-06, w1=-0.027715913031862514\n",
      "Regularized Logistic Regression(18/299): loss=0.6002362405192376, w0=-1.9688908364634805e-06, w1=-0.029000289650705804\n",
      "Regularized Logistic Regression(19/299): loss=0.5971661657013418, w0=-2.0721260784748306e-06, w1=-0.03026406696325413\n",
      "Regularized Logistic Regression(20/299): loss=0.5942389947168618, w0=-2.1753458093443424e-06, w1=-0.03150795960283059\n",
      "Regularized Logistic Regression(21/299): loss=0.5914466782126053, w0=-2.278550724451173e-06, w1=-0.03273264894190425\n",
      "Regularized Logistic Regression(22/299): loss=0.5887816649550504, w0=-2.3817412961624583e-06, w1=-0.03393878495385966\n",
      "Regularized Logistic Regression(23/299): loss=0.5862368700709052, w0=-2.484917806039481e-06, w1=-0.03512698796268799\n",
      "Regularized Logistic Regression(24/299): loss=0.5838056451907699, w0=-2.5880803731097447e-06, w1=-0.03629785028607079\n",
      "Regularized Logistic Regression(25/299): loss=0.5814817504107485, w0=-2.6912289786514294e-06, w1=-0.03745193777752216\n",
      "Regularized Logistic Regression(26/299): loss=0.579259327986234, w0=-2.7943634878899814e-06, w1=-0.03858979127330232\n",
      "Regularized Logistic Regression(27/299): loss=0.5771328776721557, w0=-2.897483668963947e-06, w1=-0.03971192794974981\n",
      "Regularized Logistic Regression(28/299): loss=0.5750972336246686, w0=-3.000589209478464e-06, w1=-0.04081884259653704\n",
      "Regularized Logistic Regression(29/299): loss=0.5731475427805313, w0=-3.1036797309298325e-06, w1=-0.04191100881115546\n",
      "Regularized Logistic Regression(30/299): loss=0.5712792446321786, w0=-3.2067548012530513e-06, w1=-0.04298888011970368\n",
      "Regularized Logistic Regression(31/299): loss=0.5694880523186792, w0=-3.309813945715898e-06, w1=-0.04405289102879344\n",
      "Regularized Logistic Regression(32/299): loss=0.567769934955296, w0=-3.412856656357748e-06, w1=-0.04510345801312465\n",
      "Regularized Logistic Regression(33/299): loss=0.5661211011271634, w0=-3.5158824001486604e-06, w1=-0.046140980443008475\n",
      "Regularized Logistic Regression(34/299): loss=0.5645379834755953, w0=-3.6188906260240285e-06, w1=-0.04716584145585192\n",
      "Regularized Logistic Regression(35/299): loss=0.5630172243086801, w0=-3.7218807709320795e-06, w1=-0.04817840877535493\n",
      "Regularized Logistic Regression(36/299): loss=0.561555662171042, w0=-3.824852265015497e-06, w1=-0.04917903548192365\n",
      "Regularized Logistic Regression(37/299): loss=0.5601503193109175, w0=-3.927804536034214e-06, w1=-0.05016806073756128\n",
      "Regularized Logistic Regression(38/299): loss=0.5587983899859603, w0=-4.030737013123819e-06, w1=-0.05114581046827397\n",
      "Regularized Logistic Regression(39/299): loss=0.557497229552415, w0=-4.13364912997285e-06, w1=-0.05211259800681282\n",
      "Regularized Logistic Regression(40/299): loss=0.5562443442854731, w0=-4.236540327492327e-06, w1=-0.05306872469837783\n",
      "Regularized Logistic Regression(41/299): loss=0.5550373818817061, w0=-4.339410056042183e-06, w1=-0.05401448047171797\n",
      "Regularized Logistic Regression(42/299): loss=0.5538741225974572, w0=-4.442257777271452e-06, w1=-0.054950144377889795\n",
      "Regularized Logistic Regression(43/299): loss=0.5527524709799448, w0=-4.545082965622303e-06, w1=-0.05587598509877583\n",
      "Regularized Logistic Regression(44/299): loss=0.5516704481505792, w0=-4.6478851095419165e-06, w1=-0.056792261427311176\n",
      "Regularized Logistic Regression(45/299): loss=0.5506261846026171, w0=-4.750663712440934e-06, w1=-0.05769922272123167\n",
      "Regularized Logistic Regression(46/299): loss=0.5496179134777713, w0=-4.853418293432462e-06, w1=-0.05859710933202238\n",
      "Regularized Logistic Regression(47/299): loss=0.5486439642887512, w0=-4.956148387881512e-06, w1=-0.059486153010632724\n",
      "Regularized Logistic Regression(48/299): loss=0.5477027570569372, w0=-5.0588535477910594e-06, w1=-0.06036657729140773\n",
      "Regularized Logistic Regression(49/299): loss=0.5467927968364894, w0=-5.161533342047747e-06, w1=-0.061238597855587724\n",
      "Regularized Logistic Regression(50/299): loss=0.545912668598172, w0=-5.2641873565473466e-06, w1=-0.06210242287563279\n",
      "Regularized Logistic Regression(51/299): loss=0.5450610324480191, w0=-5.366815194217664e-06, w1=-0.062958253341541\n",
      "Regularized Logistic Regression(52/299): loss=0.5442366191576963, w0=-5.469416474954324e-06, w1=-0.063806283370249\n",
      "Regularized Logistic Regression(53/299): loss=0.5434382259850498, w0=-5.571990835482955e-06, w1=-0.06464670049913018\n",
      "Regularized Logistic Regression(54/299): loss=0.5426647127648341, w0=-5.674537929159581e-06, w1=-0.06547968596453599\n",
      "Regularized Logistic Regression(55/299): loss=0.5419149982510238, w0=-5.7770574257195276e-06, w1=-0.06630541496626184\n",
      "Regularized Logistic Regression(56/299): loss=0.5411880566934413, w0=-5.8795490109838295e-06, w1=-0.06712405691876108\n",
      "Regularized Logistic Regression(57/299): loss=0.5404829146326429, w0=-5.982012386530962e-06, w1=-0.0679357756898757\n",
      "Regularized Logistic Regression(58/299): loss=0.539798647898158, w0=-6.084447269340697e-06, w1=-0.06874072982780205\n",
      "Regularized Logistic Regression(59/299): loss=0.5391343787962299, w0=-6.1868533914159855e-06, w1=-0.06953907277696197\n",
      "Regularized Logistic Regression(60/299): loss=0.5384892734741897, w0=-6.2892304993879805e-06, w1=-0.0703309530834081\n",
      "Regularized Logistic Regression(61/299): loss=0.5378625394495172, w0=-6.39157835410862e-06, w1=-0.07111651459034986\n",
      "Regularized Logistic Regression(62/299): loss=0.5372534232924818, w0=-6.4938967302345786e-06, w1=-0.07189589662435118\n",
      "Regularized Logistic Regression(63/299): loss=0.5366612084520516, w0=-6.59618541580586e-06, w1=-0.07266923417271455\n",
      "Regularized Logistic Regression(64/299): loss=0.5360852132154863, w0=-6.6984442118218435e-06, w1=-0.0734366580525351\n",
      "Regularized Logistic Regression(65/299): loss=0.5355247887927103, w0=-6.80067293181717e-06, w1=-0.07419829507187722\n",
      "Regularized Logistic Regression(66/299): loss=0.5349793175171851, w0=-6.902871401439511e-06, w1=-0.07495426818349867\n",
      "Regularized Logistic Regression(67/299): loss=0.5344482111555908, w0=-7.005039458030935e-06, w1=-0.07570469663152243\n",
      "Regularized Logistic Regression(68/299): loss=0.5339309093191639, w0=-7.107176950214311e-06, w1=-0.07644969609142944\n",
      "Regularized Logistic Regression(69/299): loss=0.5334268779700317, w0=-7.209283737485966e-06, w1=-0.07718937880372591\n",
      "Regularized Logistic Regression(70/299): loss=0.5329356080163683, w0=-7.311359689815571e-06, w1=-0.07792385370161559\n",
      "Regularized Logistic Regression(71/299): loss=0.5324566139906017, w0=-7.4134046872540826e-06, w1=-0.07865322653298883\n",
      "Regularized Logistic Regression(72/299): loss=0.5319894328053274, w0=-7.5154186195503715e-06, w1=-0.07937759997702165\n",
      "Regularized Logistic Regression(73/299): loss=0.5315336225819283, w0=-7.61740138577707e-06, w1=-0.08009707375566076\n",
      "Regularized Logistic Regression(74/299): loss=0.5310887615472719, w0=-7.719352893966014e-06, w1=-0.08081174474025467\n",
      "Regularized Logistic Regression(75/299): loss=0.5306544469941455, w0=-7.821273060753579e-06, w1=-0.08152170705357567\n",
      "Regularized Logistic Regression(76/299): loss=0.5302302943014116, w0=-7.923161811036097e-06, w1=-0.08222705216746443\n",
      "Regularized Logistic Regression(77/299): loss=0.5298159360101212, w0=-8.025019077635488e-06, w1=-0.08292786899631385\n",
      "Regularized Logistic Regression(78/299): loss=0.5294110209520888, w0=-8.12684480097515e-06, w1=-0.08362424398659915\n",
      "Regularized Logistic Regression(79/299): loss=0.5290152134276629, w0=-8.228638928766121e-06, w1=-0.08431626120264761\n",
      "Regularized Logistic Regression(80/299): loss=0.5286281924296461, w0=-8.330401415703444e-06, w1=-0.08500400240883116\n",
      "Regularized Logistic Regression(81/299): loss=0.528249650910525, w0=-8.432132223172662e-06, w1=-0.08568754714835512\n",
      "Regularized Logistic Regression(82/299): loss=0.5278792950903601, w0=-8.533831318966315e-06, w1=-0.08636697281880718\n",
      "Regularized Logistic Regression(83/299): loss=0.5275168438028527, w0=-8.63549867701028e-06, w1=-0.08704235474462096\n",
      "Regularized Logistic Regression(84/299): loss=0.527162027877283, w0=-8.737134277099786e-06, w1=-0.08771376624660028\n",
      "Regularized Logistic Regression(85/299): loss=0.5268145895541553, w0=-8.838738104644908e-06, w1=-0.08838127870864369\n",
      "Regularized Logistic Regression(86/299): loss=0.5264742819325287, w0=-8.94031015042533e-06, w1=-0.08904496164179876\n",
      "Regularized Logistic Regression(87/299): loss=0.5261408684471476, w0=-9.04185041035415e-06, w1=-0.08970488274577232\n",
      "Regularized Logistic Regression(88/299): loss=0.5258141223736061, w0=-9.143358885250496e-06, w1=-0.09036110796801218\n",
      "Regularized Logistic Regression(89/299): loss=0.5254938263598883, w0=-9.244835580620728e-06, w1=-0.0910137015604731\n",
      "Regularized Logistic Regression(90/299): loss=0.5251797719827468, w0=-9.346280506447966e-06, w1=-0.09166272613417262\n",
      "Regularized Logistic Regression(91/299): loss=0.5248717593274621, w0=-9.447693676989703e-06, w1=-0.09230824271163621\n",
      "Regularized Logistic Regression(92/299): loss=0.5245695965896325, w0=-9.549075110583261e-06, w1=-0.0929503107773275\n",
      "Regularized Logistic Regression(93/299): loss=0.524273099697723, w0=-9.650424829458846e-06, w1=-0.09358898832615312\n",
      "Regularized Logistic Regression(94/299): loss=0.5239820919551803, w0=-9.751742859559934e-06, w1=-0.09422433191012826\n",
      "Regularized Logistic Regression(95/299): loss=0.5236964037009979, w0=-9.853029230370784e-06, w1=-0.09485639668328433\n",
      "Regularized Logistic Regression(96/299): loss=0.5234158719876871, w0=-9.954283974750792e-06, w1=-0.09548523644489505\n",
      "Regularized Logistic Regression(97/299): loss=0.523140340275668, w0=-1.005550712877548e-05, w1=-0.09611090368109539\n",
      "Regularized Logistic Regression(98/299): loss=0.5228696581431604, w0=-1.015669873158389e-05, w1=-0.0967334496049626\n",
      "Regularized Logistic Regression(99/299): loss=0.5226036810107086, w0=-1.025785882523212e-05, w1=-0.09735292419512515\n",
      "Regularized Logistic Regression(100/299): loss=0.5223422698795286, w0=-1.0358987454552826e-05, w1=-0.09796937623296387\n",
      "Regularized Logistic Regression(101/299): loss=0.5220852910829112, w0=-1.0460084667020434e-05, w1=-0.09858285333846377\n",
      "Regularized Logistic Regression(102/299): loss=0.5218326160499677, w0=-1.0561150512621876e-05, w1=-0.09919340200477474\n",
      "Regularized Logistic Regression(103/299): loss=0.5215841210810396, w0=-1.0662185043732623e-05, w1=-0.099801067631535\n",
      "Regularized Logistic Regression(104/299): loss=0.5213396871341418, w0=-1.0763188314997836e-05, w1=-0.10040589455700898\n",
      "Regularized Logistic Regression(105/299): loss=0.5210991996218387, w0=-1.0864160383218418e-05, w1=-0.10100792608908901\n",
      "Regularized Logistic Regression(106/299): loss=0.5208625482179954, w0=-1.09651013072418e-05, w1=-0.10160720453520769\n",
      "Regularized Logistic Regression(107/299): loss=0.5206296266738746, w0=-1.1066011147857262e-05, w1=-0.10220377123120603\n",
      "Regularized Logistic Regression(108/299): loss=0.5204003326430795, w0=-1.1166889967695623e-05, w1=-0.1027976665691993\n",
      "Regularized Logistic Regression(109/299): loss=0.5201745675148775, w0=-1.1267737831133117e-05, w1=-0.10338893002448177\n",
      "Regularized Logistic Regression(110/299): loss=0.5199522362554612, w0=-1.1368554804199313e-05, w1=-0.10397760018150932\n",
      "Regularized Logistic Regression(111/299): loss=0.5197332472567303, w0=-1.146934095448889e-05, w1=-0.10456371475899609\n",
      "Regularized Logistic Regression(112/299): loss=0.5195175121922035, w0=-1.157009635107713e-05, w1=-0.10514731063416123\n",
      "Regularized Logistic Regression(113/299): loss=0.5193049458796902, w0=-1.1670821064438993e-05, w1=-0.10572842386615951\n",
      "Regularized Logistic Regression(114/299): loss=0.5190954661503717, w0=-1.1771515166371594e-05, w1=-0.1063070897187269\n",
      "Regularized Logistic Regression(115/299): loss=0.518888993723964, w0=-1.1872178729919993e-05, w1=-0.10688334268207356\n",
      "Regularized Logistic Regression(116/299): loss=0.5186854520896526, w0=-1.1972811829306115e-05, w1=-0.10745721649405209\n",
      "Regularized Logistic Regression(117/299): loss=0.5184847673925009, w0=-1.2073414539860715e-05, w1=-0.10802874416062977\n",
      "Regularized Logistic Regression(118/299): loss=0.518286868325064, w0=-1.2173986937958242e-05, w1=-0.10859795797569165\n",
      "Regularized Logistic Regression(119/299): loss=0.5180916860239329, w0=-1.2274529100954488e-05, w1=-0.10916488954019993\n",
      "Regularized Logistic Regression(120/299): loss=0.5178991539709774, w0=-1.2375041107126911e-05, w1=-0.10972956978073409\n",
      "Regularized Logistic Regression(121/299): loss=0.5177092078990408, w0=-1.2475523035617524e-05, w1=-0.11029202896743551\n",
      "Regularized Logistic Regression(122/299): loss=0.5175217857018714, w0=-1.2575974966378243e-05, w1=-0.11085229673137878\n",
      "Regularized Logistic Regression(123/299): loss=0.5173368273480817, w0=-1.2676396980118588e-05, w1=-0.11141040208139145\n",
      "Regularized Logistic Regression(124/299): loss=0.5171542747989343, w0=-1.2776789158255652e-05, w1=-0.11196637342034216\n",
      "Regularized Logistic Regression(125/299): loss=0.516974071929772, w0=-1.2877151582866236e-05, w1=-0.11252023856091728\n",
      "Regularized Logistic Regression(126/299): loss=0.5167961644549086, w0=-1.2977484336641063e-05, w1=-0.11307202474090552\n",
      "Regularized Logistic Regression(127/299): loss=0.5166204998558194, w0=-1.3077787502840989e-05, w1=-0.11362175863800682\n",
      "Regularized Logistic Regression(128/299): loss=0.5164470273124668, w0=-1.3178061165255115e-05, w1=-0.11416946638418483\n",
      "Regularized Logistic Regression(129/299): loss=0.5162756976376139, w0=-1.3278305408160749e-05, w1=-0.11471517357957803\n",
      "Regularized Logistic Regression(130/299): loss=0.5161064632139813, w0=-1.3378520316285105e-05, w1=-0.11525890530598606\n",
      "Regularized Logistic Regression(131/299): loss=0.5159392779341146, w0=-1.34787059747687e-05, w1=-0.11580068613994686\n",
      "Regularized Logistic Regression(132/299): loss=0.5157740971428302, w0=-1.357886246913036e-05, w1=-0.11634054016541769\n",
      "Regularized Logistic Regression(133/299): loss=0.5156108775821215, w0=-1.3678989885233776e-05, w1=-0.11687849098607632\n",
      "Regularized Logistic Regression(134/299): loss=0.5154495773384092, w0=-1.3779088309255533e-05, w1=-0.11741456173725383\n",
      "Regularized Logistic Regression(135/299): loss=0.5152901557920242, w0=-1.3879157827654573e-05, w1=-0.11794877509751281\n",
      "Regularized Logistic Regression(136/299): loss=0.5151325735688221, w0=-1.3979198527143014e-05, w1=-0.11848115329988336\n",
      "Regularized Logistic Regression(137/299): loss=0.5149767924938307, w0=-1.4079210494658274e-05, w1=-0.11901171814276856\n",
      "Regularized Logistic Regression(138/299): loss=0.5148227755468322, w0=-1.4179193817336449e-05, w1=-0.1195404910005309\n",
      "Regularized Logistic Regression(139/299): loss=0.5146704868197964, w0=-1.4279148582486883e-05, w1=-0.1200674928337706\n",
      "Regularized Logistic Regression(140/299): loss=0.5145198914760812, w0=-1.4379074877567905e-05, w1=-0.12059274419930632\n",
      "Regularized Logistic Regression(141/299): loss=0.5143709557113126, w0=-1.447897279016364e-05, w1=-0.12111626525986847\n",
      "Regularized Logistic Regression(142/299): loss=0.5142236467158761, w0=-1.4578842407961896e-05, w1=-0.12163807579351493\n",
      "Regularized Logistic Regression(143/299): loss=0.5140779326389429, w0=-1.4678683818733057e-05, w1=-0.1221581952027779\n",
      "Regularized Logistic Regression(144/299): loss=0.5139337825539637, w0=-1.4778497110309944e-05, w1=-0.12267664252355194\n",
      "Regularized Logistic Regression(145/299): loss=0.5137911664255616, w0=-1.48782823705686e-05, w1=-0.12319343643373076\n",
      "Regularized Logistic Regression(146/299): loss=0.5136500550777644, w0=-1.497803968740997e-05, w1=-0.12370859526160187\n",
      "Regularized Logistic Regression(147/299): loss=0.5135104201635177, w0=-1.5077769148742433e-05, w1=-0.1242221369940064\n",
      "Regularized Logistic Regression(148/299): loss=0.5133722341354187, w0=-1.5177470842465147e-05, w1=-0.12473407928427255\n",
      "Regularized Logistic Regression(149/299): loss=0.5132354702176203, w0=-1.527714485645218e-05, w1=-0.12524443945992908\n",
      "Regularized Logistic Regression(150/299): loss=0.5131001023788523, w0=-1.5376791278537386e-05, w1=-0.1257532345302073\n",
      "Regularized Logistic Regression(151/299): loss=0.5129661053065123, w0=-1.5476410196500006e-05, w1=-0.12626048119333658\n",
      "Regularized Logistic Regression(152/299): loss=0.5128334543817779, w0=-1.557600169805095e-05, w1=-0.12676619584364215\n",
      "Regularized Logistic Regression(153/299): loss=0.5127021256556993, w0=-1.5675565870819733e-05, w1=-0.12727039457844969\n",
      "Regularized Logistic Regression(154/299): loss=0.5125720958262246, w0=-1.5775102802342072e-05, w1=-0.12777309320480365\n",
      "Regularized Logistic Regression(155/299): loss=0.5124433422161263, w0=-1.587461258004804e-05, w1=-0.12827430724600547\n",
      "Regularized Logistic Regression(156/299): loss=0.5123158427517784, w0=-1.5974095291250835e-05, w1=-0.12877405194797667\n",
      "Regularized Logistic Regression(157/299): loss=0.5121895759427618, w0=-1.6073551023136103e-05, w1=-0.12927234228545273\n",
      "Regularized Logistic Regression(158/299): loss=0.5120645208622499, w0=-1.617297986275176e-05, w1=-0.12976919296801326\n",
      "Regularized Logistic Regression(159/299): loss=0.5119406571281506, w0=-1.6272381896998365e-05, w1=-0.1302646184459525\n",
      "Regularized Logistic Regression(160/299): loss=0.5118179648849694, w0=-1.6371757212619958e-05, w1=-0.13075863291599688\n",
      "Regularized Logistic Regression(161/299): loss=0.5116964247863609, w0=-1.6471105896195352e-05, w1=-0.13125125032687235\n",
      "Regularized Logistic Regression(162/299): loss=0.5115760179783453, w0=-1.6570428034129905e-05, w1=-0.13174248438472735\n",
      "Regularized Logistic Regression(163/299): loss=0.5114567260831574, w0=-1.6669723712647694e-05, w1=-0.13223234855841554\n",
      "Regularized Logistic Regression(164/299): loss=0.5113385311837056, w0=-1.6768993017784113e-05, w1=-0.13272085608464249\n",
      "Regularized Logistic Regression(165/299): loss=0.511221415808611, w0=-1.6868236035378846e-05, w1=-0.13320801997297999\n",
      "Regularized Logistic Regression(166/299): loss=0.5111053629178097, w0=-1.6967452851069234e-05, w1=-0.133693853010753\n",
      "Regularized Logistic Regression(167/299): loss=0.5109903558886866, w0=-1.7066643550283998e-05, w1=-0.134178367767802\n",
      "Regularized Logistic Regression(168/299): loss=0.5108763785027262, w0=-1.71658082182373e-05, w1=-0.13466157660112493\n",
      "Regularized Logistic Regression(169/299): loss=0.5107634149326564, w0=-1.7264946939923128e-05, w1=-0.13514349165940207\n",
      "Regularized Logistic Regression(170/299): loss=0.5106514497300644, w0=-1.736405980011002e-05, w1=-0.13562412488740783\n",
      "Regularized Logistic Regression(171/299): loss=0.5105404678134672, w0=-1.746314688333606e-05, w1=-0.13610348803031205\n",
      "Regularized Logistic Regression(172/299): loss=0.5104304544568172, w0=-1.7562208273904185e-05, w1=-0.1365815926378743\n",
      "Regularized Logistic Regression(173/299): loss=0.510321395278427, w0=-1.766124405587776e-05, w1=-0.13705845006853484\n",
      "Regularized Logistic Regression(174/299): loss=0.510213276230296, w0=-1.7760254313076426e-05, w1=-0.13753407149340396\n",
      "Regularized Logistic Regression(175/299): loss=0.5101060835878194, w0=-1.785923912907217e-05, w1=-0.1380084679001541\n",
      "Regularized Logistic Regression(176/299): loss=0.5099998039398713, w0=-1.7958198587185694e-05, w1=-0.13848165009681657\n",
      "Regularized Logistic Regression(177/299): loss=0.5098944241792409, w0=-1.805713277048295e-05, w1=-0.1389536287154856\n",
      "Regularized Logistic Regression(178/299): loss=0.5097899314934106, w0=-1.815604176177195e-05, w1=-0.13942441421593316\n",
      "Regularized Logistic Regression(179/299): loss=0.5096863133556617, w0=-1.8254925643599766e-05, w1=-0.13989401688913594\n",
      "Regularized Logistic Regression(180/299): loss=0.5095835575164969, w0=-1.8353784498249713e-05, w1=-0.14036244686071792\n",
      "Regularized Logistic Regression(181/299): loss=0.5094816519953627, w0=-1.8452618407738777e-05, w1=-0.14082971409431033\n",
      "Regularized Logistic Regression(182/299): loss=0.5093805850726651, w0=-1.8551427453815168e-05, w1=-0.14129582839483146\n",
      "Regularized Logistic Regression(183/299): loss=0.5092803452820649, w0=-1.86502117179561e-05, w1=-0.1417607994116888\n",
      "Regularized Logistic Regression(184/299): loss=0.5091809214030407, w0=-1.8748971281365714e-05, w1=-0.14222463664190504\n",
      "Regularized Logistic Regression(185/299): loss=0.5090823024537107, w0=-1.884770622497316e-05, w1=-0.1426873494331709\n",
      "Regularized Logistic Regression(186/299): loss=0.5089844776839036, w0=-1.8946416629430853e-05, w1=-0.14314894698682573\n",
      "Regularized Logistic Regression(187/299): loss=0.5088874365684695, w0=-1.904510257511286e-05, w1=-0.1436094383607689\n",
      "Regularized Logistic Regression(188/299): loss=0.5087911688008168, w0=-1.914376414211344e-05, w1=-0.14406883247230323\n",
      "Regularized Logistic Regression(189/299): loss=0.5086956642866758, w0=-1.9242401410245697e-05, w1=-0.14452713810091208\n",
      "Regularized Logistic Regression(190/299): loss=0.5086009131380708, w0=-1.93410144590404e-05, w1=-0.14498436389097313\n",
      "Regularized Logistic Regression(191/299): loss=0.5085069056674979, w0=-1.9439603367744884e-05, w1=-0.14544051835440883\n",
      "Regularized Logistic Regression(192/299): loss=0.5084136323823018, w0=-1.95381682153221e-05, w1=-0.14589560987327632\n",
      "Regularized Logistic Regression(193/299): loss=0.508321083979239, w0=-1.9636709080449745e-05, w1=-0.1463496467022983\n",
      "Regularized Logistic Regression(194/299): loss=0.5082292513392263, w0=-1.973522604151955e-05, w1=-0.14680263697133622\n",
      "Regularized Logistic Regression(195/299): loss=0.5081381255222633, w0=-1.9833719176636607e-05, w1=-0.1472545886878072\n",
      "Regularized Logistic Regression(196/299): loss=0.508047697762525, w0=-1.993218856361884e-05, w1=-0.1477055097390466\n",
      "Regularized Logistic Regression(197/299): loss=0.5079579594636154, w0=-2.0030634279996547e-05, w1=-0.14815540789461729\n",
      "Regularized Logistic Regression(198/299): loss=0.5078689021939818, w0=-2.0129056403012042e-05, w1=-0.1486042908085668\n",
      "Regularized Logistic Regression(199/299): loss=0.5077805176824751, w0=-2.0227455009619366e-05, w1=-0.14905216602163465\n",
      "Regularized Logistic Regression(200/299): loss=0.50769279781406, w0=-2.03258301764841e-05, w1=-0.1494990409634098\n",
      "Regularized Logistic Regression(201/299): loss=0.5076057346256625, w0=-2.0424181979983223e-05, w1=-0.1499449229544406\n",
      "Regularized Logistic Regression(202/299): loss=0.5075193203021546, w0=-2.0522510496205083e-05, w1=-0.15038981920829825\n",
      "Regularized Logistic Regression(203/299): loss=0.5074335471724672, w0=-2.06208158009494e-05, w1=-0.1508337368335941\n",
      "Regularized Logistic Regression(204/299): loss=0.5073484077058287, w0=-2.071909796972736e-05, w1=-0.15127668283595358\n",
      "Regularized Logistic Regression(205/299): loss=0.5072638945081268, w0=-2.0817357077761752e-05, w1=-0.15171866411994578\n",
      "Regularized Logistic Regression(206/299): loss=0.5071800003183831, w0=-2.0915593199987194e-05, w1=-0.15215968749097208\n",
      "Regularized Logistic Regression(207/299): loss=0.5070967180053433, w0=-2.1013806411050373e-05, w1=-0.15259975965711292\n",
      "Regularized Logistic Regression(208/299): loss=0.5070140405641735, w0=-2.1111996785310378e-05, w1=-0.15303888723093478\n",
      "Regularized Logistic Regression(209/299): loss=0.5069319611132623, w0=-2.1210164396839066e-05, w1=-0.15347707673125824\n",
      "Regularized Logistic Regression(210/299): loss=0.5068504728911223, w0=-2.130830931942148e-05, w1=-0.15391433458488785\n",
      "Regularized Logistic Regression(211/299): loss=0.5067695692533903, w0=-2.1406431626556306e-05, w1=-0.15435066712830495\n",
      "Regularized Logistic Regression(212/299): loss=0.5066892436699197, w0=-2.1504531391456384e-05, w1=-0.15478608060932417\n",
      "Regularized Logistic Regression(213/299): loss=0.506609489721964, w0=-2.160260868704926e-05, w1=-0.15522058118871498\n",
      "Regularized Logistic Regression(214/299): loss=0.506530301099447, w0=-2.1700663585977775e-05, w1=-0.15565417494178801\n",
      "Regularized Logistic Regression(215/299): loss=0.5064516715983185, w0=-2.1798696160600677e-05, w1=-0.15608686785994874\n",
      "Regularized Logistic Regression(216/299): loss=0.5063735951179896, w0=-2.1896706482993297e-05, w1=-0.1565186658522177\n",
      "Regularized Logistic Regression(217/299): loss=0.5062960656588468, w0=-2.199469462494823e-05, w1=-0.15694957474671875\n",
      "Regularized Logistic Regression(218/299): loss=0.5062190773198415, w0=-2.2092660657976064e-05, w1=-0.15737960029213657\n",
      "Regularized Logistic Regression(219/299): loss=0.5061426242961538, w0=-2.2190604653306146e-05, w1=-0.15780874815914325\n",
      "Regularized Logistic Regression(220/299): loss=0.5060667008769252, w0=-2.2288526681887343e-05, w1=-0.15823702394179498\n",
      "Regularized Logistic Regression(221/299): loss=0.5059913014430603, w0=-2.2386426814388867e-05, w1=-0.1586644331589003\n",
      "Regularized Logistic Regression(222/299): loss=0.5059164204650933, w0=-2.2484305121201102e-05, w1=-0.15909098125535942\n",
      "Regularized Logistic Regression(223/299): loss=0.5058420525011202, w0=-2.2582161672436464e-05, w1=-0.15951667360347632\n",
      "Regularized Logistic Regression(224/299): loss=0.5057681921947883, w0=-2.2679996537930274e-05, w1=-0.1599415155042434\n",
      "Regularized Logistic Regression(225/299): loss=0.5056948342733488, w0=-2.2777809787241673e-05, w1=-0.16036551218859996\n",
      "Regularized Logistic Regression(226/299): loss=0.5056219735457658, w0=-2.287560148965452e-05, w1=-0.16078866881866485\n",
      "Regularized Logistic Regression(227/299): loss=0.5055496049008774, w0=-2.297337171417835e-05, w1=-0.16121099048894416\n",
      "Regularized Logistic Regression(228/299): loss=0.5054777233056159, w0=-2.307112052954931e-05, w1=-0.16163248222751386\n",
      "Regularized Logistic Regression(229/299): loss=0.5054063238032731, w0=-2.3168848004231142e-05, w1=-0.16205314899717896\n",
      "Regularized Logistic Regression(230/299): loss=0.5053354015118217, w0=-2.3266554206416162e-05, w1=-0.16247299569660878\n",
      "Regularized Logistic Regression(231/299): loss=0.5052649516222799, w0=-2.336423920402626e-05, w1=-0.16289202716144952\n",
      "Regularized Logistic Regression(232/299): loss=0.5051949693971262, w0=-2.346190306471391e-05, w1=-0.16331024816541434\n",
      "Regularized Logistic Regression(233/299): loss=0.5051254501687563, w0=-2.3559545855863198e-05, w1=-0.16372766342135167\n",
      "Regularized Logistic Regression(234/299): loss=0.5050563893379865, w0=-2.3657167644590847e-05, w1=-0.16414427758229205\n",
      "Regularized Logistic Regression(235/299): loss=0.5049877823725971, w0=-2.375476849774727e-05, w1=-0.16456009524247397\n",
      "Regularized Logistic Regression(236/299): loss=0.5049196248059172, w0=-2.385234848191762e-05, w1=-0.1649751209383496\n",
      "Regularized Logistic Regression(237/299): loss=0.5048519122354491, w0=-2.3949907663422847e-05, w1=-0.16538935914957006\n",
      "Regularized Logistic Regression(238/299): loss=0.5047846403215307, w0=-2.4047446108320778e-05, w1=-0.1658028142999518\n",
      "Regularized Logistic Regression(239/299): loss=0.5047178047860351, w0=-2.414496388240718e-05, w1=-0.16621549075842335\n",
      "Regularized Logistic Regression(240/299): loss=0.5046514014111051, w0=-2.424246105121686e-05, w1=-0.16662739283995381\n",
      "Regularized Logistic Regression(241/299): loss=0.5045854260379228, w0=-2.4339937680024737e-05, w1=-0.1670385248064626\n",
      "Regularized Logistic Regression(242/299): loss=0.5045198745655131, w0=-2.4437393833846943e-05, w1=-0.1674488908677123\n",
      "Regularized Logistic Regression(243/299): loss=0.5044547429495784, w0=-2.4534829577441924e-05, w1=-0.1678584951821828\n",
      "Regularized Logistic Regression(244/299): loss=0.5043900272013654, w0=-2.4632244975311535e-05, w1=-0.16826734185792955\n",
      "Regularized Logistic Regression(245/299): loss=0.5043257233865625, w0=-2.4729640091702152e-05, w1=-0.16867543495342452\n",
      "Regularized Logistic Regression(246/299): loss=0.504261827624225, w0=-2.4827014990605777e-05, w1=-0.16908277847838096\n",
      "Regularized Logistic Regression(247/299): loss=0.5041983360857314, w0=-2.492436973576115e-05, w1=-0.16948937639456227\n",
      "Regularized Logistic Regression(248/299): loss=0.5041352449937643, w0=-2.5021704390654857e-05, w1=-0.16989523261657524\n",
      "Regularized Logistic Regression(249/299): loss=0.5040725506213197, w0=-2.511901901852245e-05, w1=-0.17030035101264812\n",
      "Regularized Logistic Regression(250/299): loss=0.5040102492907423, w0=-2.521631368234956e-05, w1=-0.1707047354053933\n",
      "Regularized Logistic Regression(251/299): loss=0.5039483373727844, w0=-2.531358844487301e-05, w1=-0.17110838957255622\n",
      "Regularized Logistic Regression(252/299): loss=0.5038868112856908, w0=-2.5410843368581947e-05, w1=-0.1715113172477492\n",
      "Regularized Logistic Regression(253/299): loss=0.5038256674943049, w0=-2.5508078515718928e-05, w1=-0.17191352212117172\n",
      "Regularized Logistic Regression(254/299): loss=0.5037649025092006, w0=-2.5605293948281063e-05, w1=-0.17231500784031684\n",
      "Regularized Logistic Regression(255/299): loss=0.5037045128858333, w0=-2.5702489728021118e-05, w1=-0.1727157780106642\n",
      "Regularized Logistic Regression(256/299): loss=0.5036444952237144, w0=-2.579966591644863e-05, w1=-0.17311583619636015\n",
      "Regularized Logistic Regression(257/299): loss=0.5035848461656048, w0=-2.589682257483102e-05, w1=-0.17351518592088472\n",
      "Regularized Logistic Regression(258/299): loss=0.5035255623967302, w0=-2.5993959764194714e-05, w1=-0.17391383066770622\n",
      "Regularized Logistic Regression(259/299): loss=0.5034666406440148, w0=-2.6091077545326227e-05, w1=-0.17431177388092356\n",
      "Regularized Logistic Regression(260/299): loss=0.5034080776753341, w0=-2.6188175978773302e-05, w1=-0.17470901896589652\n",
      "Regularized Logistic Regression(261/299): loss=0.5033498702987856, w0=-2.628525512484599e-05, w1=-0.17510556928986407\n",
      "Regularized Logistic Regression(262/299): loss=0.5032920153619789, w0=-2.6382315043617763e-05, w1=-0.17550142818255157\n",
      "Regularized Logistic Regression(263/299): loss=0.5032345097513407, w0=-2.647935579492661e-05, w1=-0.1758965989367659\n",
      "Regularized Logistic Regression(264/299): loss=0.5031773503914378, w0=-2.6576377438376144e-05, w1=-0.17629108480898065\n",
      "Regularized Logistic Regression(265/299): loss=0.5031205342443175, w0=-2.667338003333668e-05, w1=-0.17668488901990945\n",
      "Regularized Logistic Regression(266/299): loss=0.5030640583088609, w0=-2.677036363894633e-05, w1=-0.1770780147550692\n",
      "Regularized Logistic Regression(267/299): loss=0.503007919620154, w0=-2.6867328314112098e-05, w1=-0.177470465165333\n",
      "Regularized Logistic Regression(268/299): loss=0.5029521152488724, w0=-2.6964274117510952e-05, w1=-0.17786224336747244\n",
      "Regularized Logistic Regression(269/299): loss=0.5028966423006803, w0=-2.7061201107590908e-05, w1=-0.17825335244469068\n",
      "Regularized Logistic Regression(270/299): loss=0.5028414979156448, w0=-2.715810934257211e-05, w1=-0.1786437954471448\n",
      "Regularized Logistic Regression(271/299): loss=0.5027866792676622, w0=-2.7254998880447886e-05, w1=-0.17903357539245943\n",
      "Regularized Logistic Regression(272/299): loss=0.5027321835638978, w0=-2.7351869778985834e-05, w1=-0.1794226952662308\n",
      "Regularized Logistic Regression(273/299): loss=0.5026780080442399, w0=-2.7448722095728872e-05, w1=-0.17981115802252123\n",
      "Regularized Logistic Regression(274/299): loss=0.5026241499807635, w0=-2.7545555887996304e-05, w1=-0.18019896658434517\n",
      "Regularized Logistic Regression(275/299): loss=0.5025706066772103, w0=-2.764237121288486e-05, w1=-0.1805861238441461\n",
      "Regularized Logistic Regression(276/299): loss=0.5025173754684752, w0=-2.773916812726977e-05, w1=-0.18097263266426505\n",
      "Regularized Logistic Regression(277/299): loss=0.5024644537201075, w0=-2.7835946687805778e-05, w1=-0.18135849587740038\n",
      "Regularized Logistic Regression(278/299): loss=0.5024118388278247, w0=-2.7932706950928207e-05, w1=-0.18174371628705946\n",
      "Regularized Logistic Regression(279/299): loss=0.5023595282170314, w0=-2.8029448972853984e-05, w1=-0.1821282966680022\n",
      "Regularized Logistic Regression(280/299): loss=0.5023075193423562, w0=-2.8126172809582668e-05, w1=-0.1825122397666769\n",
      "Regularized Logistic Regression(281/299): loss=0.5022558096871901, w0=-2.822287851689748e-05, w1=-0.18289554830164764\n",
      "Regularized Logistic Regression(282/299): loss=0.502204396763244, w0=-2.831956615036631e-05, w1=-0.18327822496401466\n",
      "Regularized Logistic Regression(283/299): loss=0.5021532781101088, w0=-2.8416235765342762e-05, w1=-0.18366027241782715\n",
      "Regularized Logistic Regression(284/299): loss=0.5021024512948268, w0=-2.851288741696713e-05, w1=-0.18404169330048847\n",
      "Regularized Logistic Regression(285/299): loss=0.5020519139114753, w0=-2.8609521160167424e-05, w1=-0.1844224902231542\n",
      "Regularized Logistic Regression(286/299): loss=0.5020016635807543, w0=-2.870613704966036e-05, w1=-0.18480266577112336\n",
      "Regularized Logistic Regression(287/299): loss=0.5019516979495848, w0=-2.880273513995236e-05, w1=-0.18518222250422273\n",
      "Regularized Logistic Regression(288/299): loss=0.5019020146907177, w0=-2.889931548534054e-05, w1=-0.18556116295718403\n",
      "Regularized Logistic Regression(289/299): loss=0.5018526115023473, w0=-2.8995878139913683e-05, w1=-0.1859394896400149\n",
      "Regularized Logistic Regression(290/299): loss=0.5018034861077344, w0=-2.9092423157553237e-05, w1=-0.18631720503836302\n",
      "Regularized Logistic Regression(291/299): loss=0.5017546362548375, w0=-2.9188950591934275e-05, w1=-0.18669431161387415\n",
      "Regularized Logistic Regression(292/299): loss=0.5017060597159506, w0=-2.928546049652647e-05, w1=-0.18707081180454366\n",
      "Regularized Logistic Regression(293/299): loss=0.5016577542873492, w0=-2.9381952924595042e-05, w1=-0.18744670802506203\n",
      "Regularized Logistic Regression(294/299): loss=0.5016097177889418, w0=-2.9478427929201743e-05, w1=-0.18782200266715438\n",
      "Regularized Logistic Regression(295/299): loss=0.501561948063931, w0=-2.9574885563205782e-05, w1=-0.18819669809991402\n",
      "Regularized Logistic Regression(296/299): loss=0.5015144429784794, w0=-2.9671325879264784e-05, w1=-0.18857079667013021\n",
      "Regularized Logistic Regression(297/299): loss=0.5014672004213816, w0=-2.9767748929835728e-05, w1=-0.18894430070261028\n",
      "Regularized Logistic Regression(298/299): loss=0.5014202183037447, w0=-2.9864154767175886e-05, w1=-0.18931721250049627\n",
      "Regularized Logistic Regression(299/299): loss=0.5013734945586734, w0=-2.996054344334375e-05, w1=-0.1896895343455758\n",
      "Regularized Logistic Regression(0/99): loss=0.693147180559945, w0=-1.0441465146389343e-07, w1=-0.0018294491082988765\n",
      "Regularized Logistic Regression(1/99): loss=0.6853143253785982, w0=-2.086167320405951e-07, w1=-0.0036161504360717692\n",
      "Regularized Logistic Regression(2/99): loss=0.6779108012236981, w0=-3.1262792398257315e-07, w1=-0.005361882873841788\n",
      "Regularized Logistic Regression(3/99): loss=0.6709108090029711, w0=-4.164672845894852e-07, w1=-0.007068345581339656\n",
      "Regularized Logistic Regression(4/99): loss=0.6642900936075742, w0=-5.201515142799473e-07, w1=-0.00873715899314691\n",
      "Regularized Logistic Regression(5/99): loss=0.658025861475162, w0=-6.236952037609146e-07, w1=-0.010369866549107891\n",
      "Regularized Logistic Regression(6/99): loss=0.6520967010861011, w0=-7.271110606435485e-07, w1=-0.011967936959915368\n",
      "Regularized Logistic Regression(7/99): loss=0.646482506565076, w0=-8.30410116180797e-07, w1=-0.01353276685160415\n",
      "Regularized Logistic Regression(8/99): loss=0.6411644044496249, w0=-9.336019130360815e-07, w1=-0.015065683662118214\n",
      "Regularized Logistic Regression(9/99): loss=0.6361246836216448, w0=-1.0366946751504977e-06, w1=-0.016567948688575418\n",
      "Regularized Logistic Regression(10/99): loss=0.6313467283615873, w0=-1.1396954608722516e-06, w1=-0.018040760205484452\n",
      "Regularized Logistic Regression(11/99): loss=0.6268149544666675, w0=-1.2426103005594397e-06, w1=-0.019485256592273653\n",
      "Regularized Logistic Regression(12/99): loss=0.6225147483660237, w0=-1.345444319876716e-06, w1=-0.02090251942342655\n",
      "Regularized Logistic Regression(13/99): loss=0.6184324091624464, w0=-1.4482018499871769e-06, w1=-0.022293576486703715\n",
      "Regularized Logistic Regression(14/99): loss=0.614555093528953, w0=-1.550886525800655e-06, w1=-0.02365940470472913\n",
      "Regularized Logistic Regression(15/99): loss=0.6108707633874094, w0=-1.6535013733848189e-06, w1=-0.025000932943025914\n",
      "Regularized Logistic Regression(16/99): loss=0.6073681362949297, w0=-1.756048887581114e-06, w1=-0.026319044693717538\n",
      "Regularized Logistic Regression(17/99): loss=0.6040366384615804, w0=-1.858531100797604e-06, w1=-0.02761458062886722\n",
      "Regularized Logistic Regression(18/99): loss=0.6008663603201432, w0=-1.960949643878272e-06, w1=-0.02888834102106483\n",
      "Regularized Logistic Regression(19/99): loss=0.5978480145655964, w0=-2.063305799875636e-06, w1=-0.03014108803160403\n",
      "Regularized Logistic Regression(20/99): loss=0.5949728965787674, w0=-2.165600551482343e-06, w1=-0.03137354786860153\n",
      "Regularized Logistic Regression(21/99): loss=0.5922328471456526, w0=-2.2678346228089217e-06, w1=-0.0325864128188494\n",
      "Regularized Logistic Regression(22/99): loss=0.5896202173812948, w0=-2.3700085161299575e-06, w1=-0.033780343158179266\n",
      "Regularized Logistic Regression(23/99): loss=0.5871278357651369, w0=-2.472122544160045e-06, w1=-0.034955968945764256\n",
      "Regularized Logistic Regression(24/99): loss=0.5847489771933901, w0=-2.5741768583643415e-06, w1=-0.036113891708153094\n",
      "Regularized Logistic Regression(25/99): loss=0.5824773339533362, w0=-2.676171473756393e-06, w1=-0.0372546860190145\n",
      "Regularized Logistic Regression(26/99): loss=0.5803069885245352, w0=-2.7781062905881478e-06, w1=-0.03837890098058182\n",
      "Regularized Logistic Regression(27/99): loss=0.5782323881126451, w0=-2.879981113293594e-06, w1=-0.03948706161271239\n",
      "Regularized Logistic Regression(28/99): loss=0.5762483208229164, w0=-2.9817956670079893e-06, w1=-0.040579670155304635\n",
      "Regularized Logistic Regression(29/99): loss=0.574349893382313, w0=-3.0835496119490626e-06, w1=-0.04165720728960063\n",
      "Regularized Logistic Regression(30/99): loss=0.5725325103215859, w0=-3.1852425559144884e-06, w1=-0.042720133283649486\n",
      "Regularized Logistic Regression(31/99): loss=0.5707918545313705, w0=-3.286874065121194e-06, w1=-0.04376888906693131\n",
      "Regularized Logistic Regression(32/99): loss=0.5691238691094499, w0=-3.3884436735863026e-06, w1=-0.04480389723886087\n",
      "Regularized Logistic Regression(33/99): loss=0.5675247404196169, w0=-3.489950891226544e-06, w1=-0.04582556301560417\n",
      "Regularized Logistic Regression(34/99): loss=0.5659908822860324, w0=-3.591395210832465e-06, w1=-0.046834275119361286\n",
      "Regularized Logistic Regression(35/99): loss=0.5645189212505511, w0=-3.6927761140555603e-06, w1=-0.04783040661399466\n",
      "Regularized Logistic Regression(36/99): loss=0.5631056828241002, w0=-3.7940930765302364e-06, w1=-0.048814315690621196\n",
      "Regularized Logistic Regression(37/99): loss=0.5617481786668302, w0=-3.895345572238173e-06, w1=-0.049786346406536834\n",
      "Regularized Logistic Regression(38/99): loss=0.560443594635346, w0=-3.996533077209897e-06, w1=-0.05074682938060561\n",
      "Regularized Logistic Regression(39/99): loss=0.5591892796388643, w0=-4.097655072647122e-06, w1=-0.051696082448025\n",
      "Regularized Logistic Regression(40/99): loss=0.5579827352495806, w0=-4.198711047539439e-06, w1=-0.05263441127716985\n",
      "Regularized Logistic Regression(41/99): loss=0.5568216060158652, w0=-4.299700500840103e-06, w1=-0.053562109951024214\n",
      "Regularized Logistic Regression(42/99): loss=0.5557036704301143, w0=-4.400622943257912e-06, w1=-0.05447946151552926\n",
      "Regularized Logistic Regression(43/99): loss=0.5546268325061664, w0=-4.501477898715273e-06, w1=-0.055386738497008224\n",
      "Regularized Logistic Regression(44/99): loss=0.5535891139241153, w0=-4.602264905516478e-06, w1=-0.056284203390674276\n",
      "Regularized Logistic Regression(45/99): loss=0.5525886467031461, w0=-4.702983517264898e-06, w1=-0.05717210912208156\n",
      "Regularized Logistic Regression(46/99): loss=0.5516236663656673, w0=-4.803633303563029e-06, w1=-0.05805069948324789\n",
      "Regularized Logistic Regression(47/99): loss=0.5506925055584929, w0=-4.904213850525232e-06, w1=-0.05892020954505438\n",
      "Regularized Logistic Regression(48/99): loss=0.5497935880991937, w0=-5.004724761129275e-06, w1=-0.059780866047411965\n",
      "Regularized Logistic Regression(49/99): loss=0.5489254234179252, w0=-5.105165655429618e-06, w1=-0.060632887768581605\n",
      "Regularized Logistic Regression(50/99): loss=0.548086601367139, w0=-5.205536170652496e-06, w1=-0.0614764858749361\n",
      "Regularized Logistic Regression(51/99): loss=0.5472757873734934, w0=-5.305835961190376e-06, w1=-0.062311864252361814\n",
      "Regularized Logistic Regression(52/99): loss=0.546491717908114, w0=-5.406064698511154e-06, w1=-0.06313921982041791\n",
      "Regularized Logistic Regression(53/99): loss=0.5457331962530396, w0=-5.5062220709955124e-06, w1=-0.06395874283028984\n",
      "Regularized Logistic Regression(54/99): loss=0.5449990885432655, w0=-5.606307783714162e-06, w1=-0.06477061714750726\n",
      "Regularized Logistic Regression(55/99): loss=0.5442883200652691, w0=-5.7063215581551955e-06, w1=-0.0655750205203284\n",
      "Regularized Logistic Regression(56/99): loss=0.5435998717942704, w0=-5.806263131910441e-06, w1=-0.06637212483463323\n",
      "Regularized Logistic Regression(57/99): loss=0.54293277715375, w0=-5.9061322583285615e-06, w1=-0.06716209635611105\n",
      "Regularized Logistic Regression(58/99): loss=0.5422861189819307, w0=-6.0059287061416266e-06, w1=-0.06794509596047717\n",
      "Regularized Logistic Regression(59/99): loss=0.5416590266910268, w0=-6.10565225907097e-06, w1=-0.06872127935240428\n",
      "Regularized Logistic Regression(60/99): loss=0.5410506736060811, w0=-6.205302715417385e-06, w1=-0.06949079727381059\n",
      "Regularized Logistic Regression(61/99): loss=0.5404602744711562, w0=-6.304879887639994e-06, w1=-0.07025379570210324\n",
      "Regularized Logistic Regression(62/99): loss=0.5398870831115257, w0=-6.404383601927539e-06, w1=-0.0710104160389403\n",
      "Regularized Logistic Regression(63/99): loss=0.539330390241316, w0=-6.503813697765307e-06, w1=-0.0717607952900362\n",
      "Regularized Logistic Regression(64/99): loss=0.5387895214068121, w0=-6.603170027500427e-06, w1=-0.07250506623650378\n",
      "Regularized Logistic Regression(65/99): loss=0.5382638350563296, w0=-6.70245245590788e-06, w1=-0.07324335759819516\n",
      "Regularized Logistic Regression(66/99): loss=0.5377527207282089, w0=-6.801660859759208e-06, w1=-0.0739757941894744\n",
      "Regularized Logistic Regression(67/99): loss=0.5372555973490811, w0=-6.90079512739557e-06, w1=-0.07470249706782943\n",
      "Regularized Logistic Regression(68/99): loss=0.5367719116351166, w0=-6.99985515830656e-06, w1=-0.07542358367570448\n",
      "Regularized Logistic Regression(69/99): loss=0.5363011365894736, w0=-7.098840862715932e-06, w1=-0.07613916797591214\n",
      "Regularized Logistic Regression(70/99): loss=0.5358427700896484, w0=-7.197752161175188e-06, w1=-0.0768493605809625\n",
      "Regularized Logistic Regression(71/99): loss=0.5353963335588653, w0=-7.296588984165787e-06, w1=-0.07755426887662656\n",
      "Regularized Logistic Regression(72/99): loss=0.5349613707160544, w0=-7.395351271710596e-06, w1=-0.07825399714003188\n",
      "Regularized Logistic Regression(73/99): loss=0.5345374463993482, w0=-7.4940389729950546e-06, w1=-0.0789486466525714\n",
      "Regularized Logistic Regression(74/99): loss=0.5341241454583718, w0=-7.59265204599841e-06, w1=-0.07963831580789098\n",
      "Regularized Logistic Regression(75/99): loss=0.5337210717109381, w0=-7.691190457135282e-06, w1=-0.08032310021520328\n",
      "Regularized Logistic Regression(76/99): loss=0.5333278469600472, w0=-7.789654180907718e-06, w1=-0.0810030927981638\n",
      "Regularized Logistic Regression(77/99): loss=0.5329441100673864, w0=-7.888043199567843e-06, w1=-0.08167838388953122\n",
      "Regularized Logistic Regression(78/99): loss=0.5325695160797702, w0=-7.986357502791117e-06, w1=-0.08234906132181889\n",
      "Regularized Logistic Regression(79/99): loss=0.5322037354052143, w0=-8.084597087360194e-06, w1=-0.08301521051413731\n",
      "Regularized Logistic Regression(80/99): loss=0.5318464530355504, w0=-8.182761956859296e-06, w1=-0.08367691455541171\n",
      "Regularized Logistic Regression(81/99): loss=0.5314973678127076, w0=-8.280852121378984e-06, w1=-0.0843342542841517\n",
      "Regularized Logistic Regression(82/99): loss=0.5311561917359652, w0=-8.378867597231208e-06, w1=-0.08498730836493887\n",
      "Regularized Logistic Regression(83/99): loss=0.5308226493076759, w0=-8.476808406674435e-06, w1=-0.08563615336178913\n",
      "Regularized Logistic Regression(84/99): loss=0.5304964769151104, w0=-8.574674577648677e-06, w1=-0.08628086380853947\n",
      "Regularized Logistic Regression(85/99): loss=0.5301774222462425, w0=-8.672466143520211e-06, w1=-0.08692151227639809\n",
      "Regularized Logistic Regression(86/99): loss=0.5298652437374215, w0=-8.77018314283575e-06, w1=-0.0875581694387922\n",
      "Regularized Logistic Regression(87/99): loss=0.5295597100510311, w0=-8.867825619085842e-06, w1=-0.08819090413363902\n",
      "Regularized Logistic Regression(88/99): loss=0.5292605995813422, w0=-8.965393620477246e-06, w1=-0.08881978342315888\n",
      "Regularized Logistic Regression(89/99): loss=0.5289676999868885, w0=-9.062887199714033e-06, w1=-0.08944487265134421\n",
      "Regularized Logistic Regression(90/99): loss=0.5286808077478075, w0=-9.160306413787164e-06, w1=-0.09006623549919082\n",
      "Regularized Logistic Regression(91/99): loss=0.5283997277466751, w0=-9.257651323772268e-06, w1=-0.09068393403779311\n",
      "Regularized Logistic Regression(92/99): loss=0.5281242728714729, w0=-9.354921994635397e-06, w1=-0.09129802877940035\n",
      "Regularized Logistic Regression(93/99): loss=0.527854263639397, w0=-9.452118495046466e-06, w1=-0.09190857872652418\n",
      "Regularized Logistic Regression(94/99): loss=0.527589527840312, w0=-9.549240897200145e-06, w1=-0.09251564141918474\n",
      "Regularized Logistic Regression(95/99): loss=0.5273299001987205, w0=-9.64628927664394e-06, w1=-0.09311927298037818\n",
      "Regularized Logistic Regression(96/99): loss=0.5270752220531921, w0=-9.743263712113225e-06, w1=-0.09371952815984251\n",
      "Regularized Logistic Regression(97/99): loss=0.5268253410522633, w0=-9.84016428537297e-06, w1=-0.09431646037619741\n",
      "Regularized Logistic Regression(98/99): loss=0.5265801108658736, w0=-9.936991081065929e-06, w1=-0.09491012175752779\n",
      "Regularized Logistic Regression(99/99): loss=0.5263393909114729, w0=-1.0033744186567057e-05, w1=-0.09550056318047855\n",
      "Regularized Logistic Regression(0/199): loss=0.693147180559945, w0=-1.0441465146389343e-07, w1=-0.0018294491082988765\n",
      "Regularized Logistic Regression(1/199): loss=0.6853143253785982, w0=-2.086167320405951e-07, w1=-0.0036161504360717692\n",
      "Regularized Logistic Regression(2/199): loss=0.6779108012236981, w0=-3.1262792398257315e-07, w1=-0.005361882873841788\n",
      "Regularized Logistic Regression(3/199): loss=0.6709108090029711, w0=-4.164672845894852e-07, w1=-0.007068345581339656\n",
      "Regularized Logistic Regression(4/199): loss=0.6642900936075742, w0=-5.201515142799473e-07, w1=-0.00873715899314691\n",
      "Regularized Logistic Regression(5/199): loss=0.658025861475162, w0=-6.236952037609146e-07, w1=-0.010369866549107891\n",
      "Regularized Logistic Regression(6/199): loss=0.6520967010861011, w0=-7.271110606435485e-07, w1=-0.011967936959915368\n",
      "Regularized Logistic Regression(7/199): loss=0.646482506565076, w0=-8.30410116180797e-07, w1=-0.01353276685160415\n",
      "Regularized Logistic Regression(8/199): loss=0.6411644044496249, w0=-9.336019130360815e-07, w1=-0.015065683662118214\n",
      "Regularized Logistic Regression(9/199): loss=0.6361246836216448, w0=-1.0366946751504977e-06, w1=-0.016567948688575418\n",
      "Regularized Logistic Regression(10/199): loss=0.6313467283615873, w0=-1.1396954608722516e-06, w1=-0.018040760205484452\n",
      "Regularized Logistic Regression(11/199): loss=0.6268149544666675, w0=-1.2426103005594397e-06, w1=-0.019485256592273653\n",
      "Regularized Logistic Regression(12/199): loss=0.6225147483660237, w0=-1.345444319876716e-06, w1=-0.02090251942342655\n",
      "Regularized Logistic Regression(13/199): loss=0.6184324091624464, w0=-1.4482018499871769e-06, w1=-0.022293576486703715\n",
      "Regularized Logistic Regression(14/199): loss=0.614555093528953, w0=-1.550886525800655e-06, w1=-0.02365940470472913\n",
      "Regularized Logistic Regression(15/199): loss=0.6108707633874094, w0=-1.6535013733848189e-06, w1=-0.025000932943025914\n",
      "Regularized Logistic Regression(16/199): loss=0.6073681362949297, w0=-1.756048887581114e-06, w1=-0.026319044693717538\n",
      "Regularized Logistic Regression(17/199): loss=0.6040366384615804, w0=-1.858531100797604e-06, w1=-0.02761458062886722\n",
      "Regularized Logistic Regression(18/199): loss=0.6008663603201432, w0=-1.960949643878272e-06, w1=-0.02888834102106483\n",
      "Regularized Logistic Regression(19/199): loss=0.5978480145655964, w0=-2.063305799875636e-06, w1=-0.03014108803160403\n",
      "Regularized Logistic Regression(20/199): loss=0.5949728965787674, w0=-2.165600551482343e-06, w1=-0.03137354786860153\n",
      "Regularized Logistic Regression(21/199): loss=0.5922328471456526, w0=-2.2678346228089217e-06, w1=-0.0325864128188494\n",
      "Regularized Logistic Regression(22/199): loss=0.5896202173812948, w0=-2.3700085161299575e-06, w1=-0.033780343158179266\n",
      "Regularized Logistic Regression(23/199): loss=0.5871278357651369, w0=-2.472122544160045e-06, w1=-0.034955968945764256\n",
      "Regularized Logistic Regression(24/199): loss=0.5847489771933901, w0=-2.5741768583643415e-06, w1=-0.036113891708153094\n",
      "Regularized Logistic Regression(25/199): loss=0.5824773339533362, w0=-2.676171473756393e-06, w1=-0.0372546860190145\n",
      "Regularized Logistic Regression(26/199): loss=0.5803069885245352, w0=-2.7781062905881478e-06, w1=-0.03837890098058182\n",
      "Regularized Logistic Regression(27/199): loss=0.5782323881126451, w0=-2.879981113293594e-06, w1=-0.03948706161271239\n",
      "Regularized Logistic Regression(28/199): loss=0.5762483208229164, w0=-2.9817956670079893e-06, w1=-0.040579670155304635\n",
      "Regularized Logistic Regression(29/199): loss=0.574349893382313, w0=-3.0835496119490626e-06, w1=-0.04165720728960063\n",
      "Regularized Logistic Regression(30/199): loss=0.5725325103215859, w0=-3.1852425559144884e-06, w1=-0.042720133283649486\n",
      "Regularized Logistic Regression(31/199): loss=0.5707918545313705, w0=-3.286874065121194e-06, w1=-0.04376888906693131\n",
      "Regularized Logistic Regression(32/199): loss=0.5691238691094499, w0=-3.3884436735863026e-06, w1=-0.04480389723886087\n",
      "Regularized Logistic Regression(33/199): loss=0.5675247404196169, w0=-3.489950891226544e-06, w1=-0.04582556301560417\n",
      "Regularized Logistic Regression(34/199): loss=0.5659908822860324, w0=-3.591395210832465e-06, w1=-0.046834275119361286\n",
      "Regularized Logistic Regression(35/199): loss=0.5645189212505511, w0=-3.6927761140555603e-06, w1=-0.04783040661399466\n",
      "Regularized Logistic Regression(36/199): loss=0.5631056828241002, w0=-3.7940930765302364e-06, w1=-0.048814315690621196\n",
      "Regularized Logistic Regression(37/199): loss=0.5617481786668302, w0=-3.895345572238173e-06, w1=-0.049786346406536834\n",
      "Regularized Logistic Regression(38/199): loss=0.560443594635346, w0=-3.996533077209897e-06, w1=-0.05074682938060561\n",
      "Regularized Logistic Regression(39/199): loss=0.5591892796388643, w0=-4.097655072647122e-06, w1=-0.051696082448025\n",
      "Regularized Logistic Regression(40/199): loss=0.5579827352495806, w0=-4.198711047539439e-06, w1=-0.05263441127716985\n",
      "Regularized Logistic Regression(41/199): loss=0.5568216060158652, w0=-4.299700500840103e-06, w1=-0.053562109951024214\n",
      "Regularized Logistic Regression(42/199): loss=0.5557036704301143, w0=-4.400622943257912e-06, w1=-0.05447946151552926\n",
      "Regularized Logistic Regression(43/199): loss=0.5546268325061664, w0=-4.501477898715273e-06, w1=-0.055386738497008224\n",
      "Regularized Logistic Regression(44/199): loss=0.5535891139241153, w0=-4.602264905516478e-06, w1=-0.056284203390674276\n",
      "Regularized Logistic Regression(45/199): loss=0.5525886467031461, w0=-4.702983517264898e-06, w1=-0.05717210912208156\n",
      "Regularized Logistic Regression(46/199): loss=0.5516236663656673, w0=-4.803633303563029e-06, w1=-0.05805069948324789\n",
      "Regularized Logistic Regression(47/199): loss=0.5506925055584929, w0=-4.904213850525232e-06, w1=-0.05892020954505438\n",
      "Regularized Logistic Regression(48/199): loss=0.5497935880991937, w0=-5.004724761129275e-06, w1=-0.059780866047411965\n",
      "Regularized Logistic Regression(49/199): loss=0.5489254234179252, w0=-5.105165655429618e-06, w1=-0.060632887768581605\n",
      "Regularized Logistic Regression(50/199): loss=0.548086601367139, w0=-5.205536170652496e-06, w1=-0.0614764858749361\n",
      "Regularized Logistic Regression(51/199): loss=0.5472757873734934, w0=-5.305835961190376e-06, w1=-0.062311864252361814\n",
      "Regularized Logistic Regression(52/199): loss=0.546491717908114, w0=-5.406064698511154e-06, w1=-0.06313921982041791\n",
      "Regularized Logistic Regression(53/199): loss=0.5457331962530396, w0=-5.5062220709955124e-06, w1=-0.06395874283028984\n",
      "Regularized Logistic Regression(54/199): loss=0.5449990885432655, w0=-5.606307783714162e-06, w1=-0.06477061714750726\n",
      "Regularized Logistic Regression(55/199): loss=0.5442883200652691, w0=-5.7063215581551955e-06, w1=-0.0655750205203284\n",
      "Regularized Logistic Regression(56/199): loss=0.5435998717942704, w0=-5.806263131910441e-06, w1=-0.06637212483463323\n",
      "Regularized Logistic Regression(57/199): loss=0.54293277715375, w0=-5.9061322583285615e-06, w1=-0.06716209635611105\n",
      "Regularized Logistic Regression(58/199): loss=0.5422861189819307, w0=-6.0059287061416266e-06, w1=-0.06794509596047717\n",
      "Regularized Logistic Regression(59/199): loss=0.5416590266910268, w0=-6.10565225907097e-06, w1=-0.06872127935240428\n",
      "Regularized Logistic Regression(60/199): loss=0.5410506736060811, w0=-6.205302715417385e-06, w1=-0.06949079727381059\n",
      "Regularized Logistic Regression(61/199): loss=0.5404602744711562, w0=-6.304879887639994e-06, w1=-0.07025379570210324\n",
      "Regularized Logistic Regression(62/199): loss=0.5398870831115257, w0=-6.404383601927539e-06, w1=-0.0710104160389403\n",
      "Regularized Logistic Regression(63/199): loss=0.539330390241316, w0=-6.503813697765307e-06, w1=-0.0717607952900362\n",
      "Regularized Logistic Regression(64/199): loss=0.5387895214068121, w0=-6.603170027500427e-06, w1=-0.07250506623650378\n",
      "Regularized Logistic Regression(65/199): loss=0.5382638350563296, w0=-6.70245245590788e-06, w1=-0.07324335759819516\n",
      "Regularized Logistic Regression(66/199): loss=0.5377527207282089, w0=-6.801660859759208e-06, w1=-0.0739757941894744\n",
      "Regularized Logistic Regression(67/199): loss=0.5372555973490811, w0=-6.90079512739557e-06, w1=-0.07470249706782943\n",
      "Regularized Logistic Regression(68/199): loss=0.5367719116351166, w0=-6.99985515830656e-06, w1=-0.07542358367570448\n",
      "Regularized Logistic Regression(69/199): loss=0.5363011365894736, w0=-7.098840862715932e-06, w1=-0.07613916797591214\n",
      "Regularized Logistic Regression(70/199): loss=0.5358427700896484, w0=-7.197752161175188e-06, w1=-0.0768493605809625\n",
      "Regularized Logistic Regression(71/199): loss=0.5353963335588653, w0=-7.296588984165787e-06, w1=-0.07755426887662656\n",
      "Regularized Logistic Regression(72/199): loss=0.5349613707160544, w0=-7.395351271710596e-06, w1=-0.07825399714003188\n",
      "Regularized Logistic Regression(73/199): loss=0.5345374463993482, w0=-7.4940389729950546e-06, w1=-0.0789486466525714\n",
      "Regularized Logistic Regression(74/199): loss=0.5341241454583718, w0=-7.59265204599841e-06, w1=-0.07963831580789098\n",
      "Regularized Logistic Regression(75/199): loss=0.5337210717109381, w0=-7.691190457135282e-06, w1=-0.08032310021520328\n",
      "Regularized Logistic Regression(76/199): loss=0.5333278469600472, w0=-7.789654180907718e-06, w1=-0.0810030927981638\n",
      "Regularized Logistic Regression(77/199): loss=0.5329441100673864, w0=-7.888043199567843e-06, w1=-0.08167838388953122\n",
      "Regularized Logistic Regression(78/199): loss=0.5325695160797702, w0=-7.986357502791117e-06, w1=-0.08234906132181889\n",
      "Regularized Logistic Regression(79/199): loss=0.5322037354052143, w0=-8.084597087360194e-06, w1=-0.08301521051413731\n",
      "Regularized Logistic Regression(80/199): loss=0.5318464530355504, w0=-8.182761956859296e-06, w1=-0.08367691455541171\n",
      "Regularized Logistic Regression(81/199): loss=0.5314973678127076, w0=-8.280852121378984e-06, w1=-0.0843342542841517\n",
      "Regularized Logistic Regression(82/199): loss=0.5311561917359652, w0=-8.378867597231208e-06, w1=-0.08498730836493887\n",
      "Regularized Logistic Regression(83/199): loss=0.5308226493076759, w0=-8.476808406674435e-06, w1=-0.08563615336178913\n",
      "Regularized Logistic Regression(84/199): loss=0.5304964769151104, w0=-8.574674577648677e-06, w1=-0.08628086380853947\n",
      "Regularized Logistic Regression(85/199): loss=0.5301774222462425, w0=-8.672466143520211e-06, w1=-0.08692151227639809\n",
      "Regularized Logistic Regression(86/199): loss=0.5298652437374215, w0=-8.77018314283575e-06, w1=-0.0875581694387922\n",
      "Regularized Logistic Regression(87/199): loss=0.5295597100510311, w0=-8.867825619085842e-06, w1=-0.08819090413363902\n",
      "Regularized Logistic Regression(88/199): loss=0.5292605995813422, w0=-8.965393620477246e-06, w1=-0.08881978342315888\n",
      "Regularized Logistic Regression(89/199): loss=0.5289676999868885, w0=-9.062887199714033e-06, w1=-0.08944487265134421\n",
      "Regularized Logistic Regression(90/199): loss=0.5286808077478075, w0=-9.160306413787164e-06, w1=-0.09006623549919082\n",
      "Regularized Logistic Regression(91/199): loss=0.5283997277466751, w0=-9.257651323772268e-06, w1=-0.09068393403779311\n",
      "Regularized Logistic Regression(92/199): loss=0.5281242728714729, w0=-9.354921994635397e-06, w1=-0.09129802877940035\n",
      "Regularized Logistic Regression(93/199): loss=0.527854263639397, w0=-9.452118495046466e-06, w1=-0.09190857872652418\n",
      "Regularized Logistic Regression(94/199): loss=0.527589527840312, w0=-9.549240897200145e-06, w1=-0.09251564141918474\n",
      "Regularized Logistic Regression(95/199): loss=0.5273299001987205, w0=-9.64628927664394e-06, w1=-0.09311927298037818\n",
      "Regularized Logistic Regression(96/199): loss=0.5270752220531921, w0=-9.743263712113225e-06, w1=-0.09371952815984251\n",
      "Regularized Logistic Regression(97/199): loss=0.5268253410522633, w0=-9.84016428537297e-06, w1=-0.09431646037619741\n",
      "Regularized Logistic Regression(98/199): loss=0.5265801108658736, w0=-9.936991081065929e-06, w1=-0.09491012175752779\n",
      "Regularized Logistic Regression(99/199): loss=0.5263393909114729, w0=-1.0033744186567057e-05, w1=-0.09550056318047855\n",
      "Regularized Logistic Regression(100/199): loss=0.526103046093973, w0=-1.0130423691843926e-05, w1=-0.09608783430792418\n",
      "Regularized Logistic Regression(101/199): loss=0.5258709465587813, w0=-1.0227029689322914e-05, w1=-0.0966719836252743\n",
      "Regularized Logistic Regression(102/199): loss=0.5256429674571877, w0=-1.0323562273760947e-05, w1=-0.09725305847547198\n",
      "Regularized Logistic Regression(103/199): loss=0.5254189887234323, w0=-1.0420021542122598e-05, w1=-0.09783110509274127\n",
      "Regularized Logistic Regression(104/199): loss=0.5251988948628071, w0=-1.051640759346232e-05, w1=-0.09840616863513496\n",
      "Regularized Logistic Regression(105/199): loss=0.5249825747501997, w0=-1.0612720528811621e-05, w1=-0.09897829321593293\n",
      "Regularized Logistic Regression(106/199): loss=0.5247699214385083, w0=-1.0708960451070995e-05, w1=-0.099547521933939\n",
      "Regularized Logistic Regression(107/199): loss=0.5245608319764015, w0=-1.0805127464906415e-05, w1=-0.10011389690272073\n",
      "Regularized Logistic Regression(108/199): loss=0.5243552072349142, w0=-1.0901221676650212e-05, w1=-0.10067745927883612\n",
      "Regularized Logistic Regression(109/199): loss=0.524152951742421, w0=-1.0997243194206162e-05, w1=-0.10123824928908734\n",
      "Regularized Logistic Regression(110/199): loss=0.5239539735275311, w0=-1.109319212695862e-05, w1=-0.10179630625684205\n",
      "Regularized Logistic Regression(111/199): loss=0.5237581839694957, w0=-1.1189068585685533e-05, w1=-0.10235166862745843\n",
      "Regularized Logistic Regression(112/199): loss=0.5235654976557271, w0=-1.1284872682475175e-05, w1=-0.10290437399285067\n",
      "Regularized Logistic Regression(113/199): loss=0.523375832246065, w0=-1.1380604530646467e-05, w1=-0.10345445911522799\n",
      "Regularized Logistic Regression(114/199): loss=0.5231891083434318, w0=-1.1476264244672711e-05, w1=-0.10400195995004098\n",
      "Regularized Logistic Regression(115/199): loss=0.5230052493705541, w0=-1.1571851940108634e-05, w1=-0.10454691166816513\n",
      "Regularized Logistic Regression(116/199): loss=0.5228241814524333, w0=-1.1667367733520565e-05, w1=-0.10508934867735173\n",
      "Regularized Logistic Regression(117/199): loss=0.5226458333042722, w0=-1.1762811742419666e-05, w1=-0.105629304642975\n",
      "Regularized Logistic Regression(118/199): loss=0.5224701361245818, w0=-1.1858184085198034e-05, w1=-0.1061668125081017\n",
      "Regularized Logistic Regression(119/199): loss=0.5222970234932032, w0=-1.1953484881067618e-05, w1=-0.10670190451290953\n",
      "Regularized Logistic Regression(120/199): loss=0.5221264312739977, w0=-1.2048714250001783e-05, w1=-0.10723461221347945\n",
      "Regularized Logistic Regression(121/199): loss=0.5219582975219703, w0=-1.214387231267944e-05, w1=-0.107764966499985\n",
      "Regularized Logistic Regression(122/199): loss=0.5217925623946067, w0=-1.2238959190431633e-05, w1=-0.10829299761430158\n",
      "Regularized Logistic Regression(123/199): loss=0.5216291680672122, w0=-1.2333975005190468e-05, w1=-0.10881873516705756\n",
      "Regularized Logistic Regression(124/199): loss=0.5214680586520537, w0=-1.24289198794403e-05, w1=-0.10934220815414787\n",
      "Regularized Logistic Regression(125/199): loss=0.5213091801211246, w0=-1.252379393617108e-05, w1=-0.10986344497272972\n",
      "Regularized Logistic Regression(126/199): loss=0.5211524802323407, w0=-1.2618597298833776e-05, w1=-0.11038247343671984\n",
      "Regularized Logistic Regression(127/199): loss=0.5209979084590177, w0=-1.271333009129776e-05, w1=-0.11089932079181128\n",
      "Regularized Logistic Regression(128/199): loss=0.5208454159224539, w0=-1.2807992437810126e-05, w1=-0.11141401373002761\n",
      "Regularized Logistic Regression(129/199): loss=0.5206949553274809, w0=-1.2902584462956791e-05, w1=-0.11192657840383037\n",
      "Regularized Logistic Regression(130/199): loss=0.5205464809008308, w0=-1.2997106291625381e-05, w1=-0.11243704043979713\n",
      "Regularized Logistic Regression(131/199): loss=0.5203999483321899, w0=-1.3091558048969753e-05, w1=-0.11294542495188428\n",
      "Regularized Logistic Regression(132/199): loss=0.5202553147178088, w0=-1.3185939860376146e-05, w1=-0.11345175655429021\n",
      "Regularized Logistic Regression(133/199): loss=0.5201125385065472, w0=-1.3280251851430849e-05, w1=-0.1139560593739328\n",
      "Regularized Logistic Regression(134/199): loss=0.519971579448239, w0=-1.337449414788935e-05, w1=-0.11445835706255433\n",
      "Regularized Logistic Regression(135/199): loss=0.5198323985442679, w0=-1.3468666875646883e-05, w1=-0.1149586728084676\n",
      "Regularized Logistic Regression(136/199): loss=0.5196949580002521, w0=-1.356277016071034e-05, w1=-0.11545702934795514\n",
      "Regularized Logistic Regression(137/199): loss=0.5195592211807343, w0=-1.3656804129171458e-05, w1=-0.11595344897633408\n",
      "Regularized Logistic Regression(138/199): loss=0.5194251525657906, w0=-1.375076890718125e-05, w1=-0.11644795355869758\n",
      "Regularized Logistic Regression(139/199): loss=0.5192927177094641, w0=-1.384466462092563e-05, w1=-0.1169405645403445\n",
      "Regularized Logistic Regression(140/199): loss=0.5191618831999415, w0=-1.3938491396602162e-05, w1=-0.11743130295690768\n",
      "Regularized Logistic Regression(141/199): loss=0.519032616621394, w0=-1.4032249360397903e-05, w1=-0.11792018944419057\n",
      "Regularized Logistic Regression(142/199): loss=0.5189048865174017, w0=-1.4125938638468284e-05, w1=-0.11840724424772328\n",
      "Regularized Logistic Regression(143/199): loss=0.5187786623558962, w0=-1.4219559356916998e-05, w1=-0.11889248723204626\n",
      "Regularized Logistic Regression(144/199): loss=0.5186539144955449, w0=-1.431311164177683e-05, w1=-0.1193759378897311\n",
      "Regularized Logistic Regression(145/199): loss=0.51853061415352, w0=-1.4406595618991413e-05, w1=-0.11985761535014741\n",
      "Regularized Logistic Regression(146/199): loss=0.5184087333745836, w0=-1.4500011414397852e-05, w1=-0.1203375383879841\n",
      "Regularized Logistic Regression(147/199): loss=0.5182882450014323, w0=-1.4593359153710198e-05, w1=-0.12081572543153284\n",
      "Regularized Logistic Regression(148/199): loss=0.5181691226462461, w0=-1.468663896250371e-05, w1=-0.12129219457074156\n",
      "Regularized Logistic Regression(149/199): loss=0.5180513406633862, w0=-1.4779850966199912e-05, w1=-0.12176696356504607\n",
      "Regularized Logistic Regression(150/199): loss=0.5179348741231927, w0=-1.4872995290052353e-05, w1=-0.12224004985098597\n",
      "Regularized Logistic Regression(151/199): loss=0.517819698786832, w0=-1.49660720591331e-05, w1=-0.1227114705496126\n",
      "Regularized Logistic Regression(152/199): loss=0.5177057910821499, w0=-1.5059081398319899e-05, w1=-0.12318124247369525\n",
      "Regularized Logistic Regression(153/199): loss=0.5175931280804831, w0=-1.5152023432283974e-05, w1=-0.12364938213473263\n",
      "Regularized Logistic Regression(154/199): loss=0.5174816874743919, w0=-1.5244898285478455e-05, w1=-0.12411590574977471\n",
      "Regularized Logistic Regression(155/199): loss=0.5173714475562696, w0=-1.5337706082127397e-05, w1=-0.12458082924806232\n",
      "Regularized Logistic Regression(156/199): loss=0.5172623871977945, w0=-1.5430446946215365e-05, w1=-0.1250441682774888\n",
      "Regularized Logistic Regression(157/199): loss=0.5171544858301851, w0=-1.5523121001477562e-05, w1=-0.12550593821089046\n",
      "Regularized Logistic Regression(158/199): loss=0.5170477234252264, w0=-1.561572837139048e-05, w1=-0.12596615415217063\n",
      "Regularized Logistic Regression(159/199): loss=0.5169420804770327, w0=-1.5708269179163042e-05, w1=-0.12642483094226212\n",
      "Regularized Logistic Regression(160/199): loss=0.5168375379845164, w0=-1.580074354772823e-05, w1=-0.12688198316493376\n",
      "Regularized Logistic Regression(161/199): loss=0.5167340774345334, w0=-1.589315159973516e-05, w1=-0.1273376251524456\n",
      "Regularized Logistic Regression(162/199): loss=0.5166316807856745, w0=-1.5985493457541603e-05, w1=-0.1277917709910568\n",
      "Regularized Logistic Regression(163/199): loss=0.5165303304526779, w0=-1.607776924320691e-05, w1=-0.12824443452639153\n",
      "Regularized Logistic Regression(164/199): loss=0.5164300092914359, w0=-1.6169979078485353e-05, w1=-0.12869562936866671\n",
      "Regularized Logistic Regression(165/199): loss=0.5163307005845679, w0=-1.6262123084819837e-05, w1=-0.12914536889778566\n",
      "Regularized Logistic Regression(166/199): loss=0.516232388027544, w0=-1.635420138333598e-05, w1=-0.1295936662683019\n",
      "Regularized Logistic Regression(167/199): loss=0.5161350557153268, w0=-1.6446214094836547e-05, w1=-0.13004053441425717\n",
      "Regularized Logistic Regression(168/199): loss=0.5160386881295159, w0=-1.653816133979621e-05, w1=-0.1304859860538966\n",
      "Regularized Logistic Regression(169/199): loss=0.5159432701259746, w0=-1.663004323835664e-05, w1=-0.1309300336942656\n",
      "Regularized Logistic Regression(170/199): loss=0.5158487869229146, w0=-1.6721859910321905e-05, w1=-0.13137268963569126\n",
      "Regularized Logistic Regression(171/199): loss=0.5157552240894238, w0=-1.6813611475154148e-05, w1=-0.1318139659761521\n",
      "Regularized Logistic Regression(172/199): loss=0.515662567534419, w0=-1.6905298051969554e-05, w1=-0.13225387461553906\n",
      "Regularized Logistic Regression(173/199): loss=0.5155708034960037, w0=-1.6996919759534588e-05, w1=-0.132692427259811\n",
      "Regularized Logistic Regression(174/199): loss=0.5154799185312169, w0=-1.708847671626248e-05, w1=-0.133129635425048\n",
      "Regularized Logistic Regression(175/199): loss=0.515389899506155, w0=-1.717996904020997e-05, w1=-0.13356551044140488\n",
      "Regularized Logistic Regression(176/199): loss=0.5153007335864567, w0=-1.7271396849074267e-05, w1=-0.1340000634569682\n",
      "Regularized Logistic Regression(177/199): loss=0.5152124082281274, w0=-1.736276026019026e-05, w1=-0.13443330544151946\n",
      "Regularized Logistic Regression(178/199): loss=0.5151249111686987, w0=-1.7454059390527905e-05, w1=-0.13486524719020695\n",
      "Regularized Logistic Regression(179/199): loss=0.5150382304187053, w0=-1.7545294356689853e-05, w1=-0.13529589932712896\n",
      "Regularized Logistic Regression(180/199): loss=0.5149523542534657, w0=-1.7636465274909253e-05, w1=-0.13572527230883064\n",
      "Regularized Logistic Regression(181/199): loss=0.5148672712051572, w0=-1.7727572261047744e-05, w1=-0.13615337642771755\n",
      "Regularized Logistic Regression(182/199): loss=0.5147829700551733, w0=-1.7818615430593634e-05, w1=-0.13658022181538712\n",
      "Regularized Logistic Regression(183/199): loss=0.5146994398267495, w0=-1.790959489866024e-05, w1=-0.13700581844588144\n",
      "Regularized Logistic Regression(184/199): loss=0.5146166697778508, w0=-1.8000510779984396e-05, w1=-0.13743017613886288\n",
      "Regularized Logistic Regression(185/199): loss=0.5145346493943096, w0=-1.8091363188925108e-05, w1=-0.1378533045627145\n",
      "Regularized Logistic Regression(186/199): loss=0.5144533683832012, w0=-1.8182152239462376e-05, w1=-0.13827521323756825\n",
      "Regularized Logistic Regression(187/199): loss=0.5143728166664525, w0=-1.8272878045196132e-05, w1=-0.1386959115382617\n",
      "Regularized Logistic Regression(188/199): loss=0.5142929843746711, w0=-1.836354071934534e-05, w1=-0.139115408697226\n",
      "Regularized Logistic Regression(189/199): loss=0.514213861841189, w0=-1.8454140374747203e-05, w1=-0.13953371380730703\n",
      "Regularized Logistic Regression(190/199): loss=0.5141354395963096, w0=-1.8544677123856497e-05, w1=-0.13995083582452095\n",
      "Regularized Logistic Regression(191/199): loss=0.5140577083617537, w0=-1.8635151078745045e-05, w1=-0.1403667835707461\n",
      "Regularized Logistic Regression(192/199): loss=0.5139806590452938, w0=-1.872556235110126e-05, w1=-0.14078156573635345\n",
      "Regularized Logistic Regression(193/199): loss=0.5139042827355725, w0=-1.881591105222984e-05, w1=-0.14119519088277643\n",
      "Regularized Logistic Regression(194/199): loss=0.5138285706970941, w0=-1.8906197293051533e-05, w1=-0.14160766744502262\n",
      "Regularized Logistic Regression(195/199): loss=0.5137535143653879, w0=-1.899642118410301e-05, w1=-0.1420190037341277\n",
      "Regularized Logistic Regression(196/199): loss=0.5136791053423316, w0=-1.9086582835536835e-05, w1=-0.14242920793955463\n",
      "Regularized Logistic Regression(197/199): loss=0.5136053353916329, w0=-1.917668235712151e-05, w1=-0.1428382881315377\n",
      "Regularized Logistic Regression(198/199): loss=0.5135321964344607, w0=-1.9266719858241617e-05, w1=-0.14324625226337492\n",
      "Regularized Logistic Regression(199/199): loss=0.5134596805452201, w0=-1.9356695447898036e-05, w1=-0.14365310817366828\n",
      "Regularized Logistic Regression(0/299): loss=0.693147180559945, w0=-1.0441465146389343e-07, w1=-0.0018294491082988765\n",
      "Regularized Logistic Regression(1/299): loss=0.6853143253785982, w0=-2.086167320405951e-07, w1=-0.0036161504360717692\n",
      "Regularized Logistic Regression(2/299): loss=0.6779108012236981, w0=-3.1262792398257315e-07, w1=-0.005361882873841788\n",
      "Regularized Logistic Regression(3/299): loss=0.6709108090029711, w0=-4.164672845894852e-07, w1=-0.007068345581339656\n",
      "Regularized Logistic Regression(4/299): loss=0.6642900936075742, w0=-5.201515142799473e-07, w1=-0.00873715899314691\n",
      "Regularized Logistic Regression(5/299): loss=0.658025861475162, w0=-6.236952037609146e-07, w1=-0.010369866549107891\n",
      "Regularized Logistic Regression(6/299): loss=0.6520967010861011, w0=-7.271110606435485e-07, w1=-0.011967936959915368\n",
      "Regularized Logistic Regression(7/299): loss=0.646482506565076, w0=-8.30410116180797e-07, w1=-0.01353276685160415\n",
      "Regularized Logistic Regression(8/299): loss=0.6411644044496249, w0=-9.336019130360815e-07, w1=-0.015065683662118214\n",
      "Regularized Logistic Regression(9/299): loss=0.6361246836216448, w0=-1.0366946751504977e-06, w1=-0.016567948688575418\n",
      "Regularized Logistic Regression(10/299): loss=0.6313467283615873, w0=-1.1396954608722516e-06, w1=-0.018040760205484452\n",
      "Regularized Logistic Regression(11/299): loss=0.6268149544666675, w0=-1.2426103005594397e-06, w1=-0.019485256592273653\n",
      "Regularized Logistic Regression(12/299): loss=0.6225147483660237, w0=-1.345444319876716e-06, w1=-0.02090251942342655\n",
      "Regularized Logistic Regression(13/299): loss=0.6184324091624464, w0=-1.4482018499871769e-06, w1=-0.022293576486703715\n",
      "Regularized Logistic Regression(14/299): loss=0.614555093528953, w0=-1.550886525800655e-06, w1=-0.02365940470472913\n",
      "Regularized Logistic Regression(15/299): loss=0.6108707633874094, w0=-1.6535013733848189e-06, w1=-0.025000932943025914\n",
      "Regularized Logistic Regression(16/299): loss=0.6073681362949297, w0=-1.756048887581114e-06, w1=-0.026319044693717538\n",
      "Regularized Logistic Regression(17/299): loss=0.6040366384615804, w0=-1.858531100797604e-06, w1=-0.02761458062886722\n",
      "Regularized Logistic Regression(18/299): loss=0.6008663603201432, w0=-1.960949643878272e-06, w1=-0.02888834102106483\n",
      "Regularized Logistic Regression(19/299): loss=0.5978480145655964, w0=-2.063305799875636e-06, w1=-0.03014108803160403\n",
      "Regularized Logistic Regression(20/299): loss=0.5949728965787674, w0=-2.165600551482343e-06, w1=-0.03137354786860153\n",
      "Regularized Logistic Regression(21/299): loss=0.5922328471456526, w0=-2.2678346228089217e-06, w1=-0.0325864128188494\n",
      "Regularized Logistic Regression(22/299): loss=0.5896202173812948, w0=-2.3700085161299575e-06, w1=-0.033780343158179266\n",
      "Regularized Logistic Regression(23/299): loss=0.5871278357651369, w0=-2.472122544160045e-06, w1=-0.034955968945764256\n",
      "Regularized Logistic Regression(24/299): loss=0.5847489771933901, w0=-2.5741768583643415e-06, w1=-0.036113891708153094\n",
      "Regularized Logistic Regression(25/299): loss=0.5824773339533362, w0=-2.676171473756393e-06, w1=-0.0372546860190145\n",
      "Regularized Logistic Regression(26/299): loss=0.5803069885245352, w0=-2.7781062905881478e-06, w1=-0.03837890098058182\n",
      "Regularized Logistic Regression(27/299): loss=0.5782323881126451, w0=-2.879981113293594e-06, w1=-0.03948706161271239\n",
      "Regularized Logistic Regression(28/299): loss=0.5762483208229164, w0=-2.9817956670079893e-06, w1=-0.040579670155304635\n",
      "Regularized Logistic Regression(29/299): loss=0.574349893382313, w0=-3.0835496119490626e-06, w1=-0.04165720728960063\n",
      "Regularized Logistic Regression(30/299): loss=0.5725325103215859, w0=-3.1852425559144884e-06, w1=-0.042720133283649486\n",
      "Regularized Logistic Regression(31/299): loss=0.5707918545313705, w0=-3.286874065121194e-06, w1=-0.04376888906693131\n",
      "Regularized Logistic Regression(32/299): loss=0.5691238691094499, w0=-3.3884436735863026e-06, w1=-0.04480389723886087\n",
      "Regularized Logistic Regression(33/299): loss=0.5675247404196169, w0=-3.489950891226544e-06, w1=-0.04582556301560417\n",
      "Regularized Logistic Regression(34/299): loss=0.5659908822860324, w0=-3.591395210832465e-06, w1=-0.046834275119361286\n",
      "Regularized Logistic Regression(35/299): loss=0.5645189212505511, w0=-3.6927761140555603e-06, w1=-0.04783040661399466\n",
      "Regularized Logistic Regression(36/299): loss=0.5631056828241002, w0=-3.7940930765302364e-06, w1=-0.048814315690621196\n",
      "Regularized Logistic Regression(37/299): loss=0.5617481786668302, w0=-3.895345572238173e-06, w1=-0.049786346406536834\n",
      "Regularized Logistic Regression(38/299): loss=0.560443594635346, w0=-3.996533077209897e-06, w1=-0.05074682938060561\n",
      "Regularized Logistic Regression(39/299): loss=0.5591892796388643, w0=-4.097655072647122e-06, w1=-0.051696082448025\n",
      "Regularized Logistic Regression(40/299): loss=0.5579827352495806, w0=-4.198711047539439e-06, w1=-0.05263441127716985\n",
      "Regularized Logistic Regression(41/299): loss=0.5568216060158652, w0=-4.299700500840103e-06, w1=-0.053562109951024214\n",
      "Regularized Logistic Regression(42/299): loss=0.5557036704301143, w0=-4.400622943257912e-06, w1=-0.05447946151552926\n",
      "Regularized Logistic Regression(43/299): loss=0.5546268325061664, w0=-4.501477898715273e-06, w1=-0.055386738497008224\n",
      "Regularized Logistic Regression(44/299): loss=0.5535891139241153, w0=-4.602264905516478e-06, w1=-0.056284203390674276\n",
      "Regularized Logistic Regression(45/299): loss=0.5525886467031461, w0=-4.702983517264898e-06, w1=-0.05717210912208156\n",
      "Regularized Logistic Regression(46/299): loss=0.5516236663656673, w0=-4.803633303563029e-06, w1=-0.05805069948324789\n",
      "Regularized Logistic Regression(47/299): loss=0.5506925055584929, w0=-4.904213850525232e-06, w1=-0.05892020954505438\n",
      "Regularized Logistic Regression(48/299): loss=0.5497935880991937, w0=-5.004724761129275e-06, w1=-0.059780866047411965\n",
      "Regularized Logistic Regression(49/299): loss=0.5489254234179252, w0=-5.105165655429618e-06, w1=-0.060632887768581605\n",
      "Regularized Logistic Regression(50/299): loss=0.548086601367139, w0=-5.205536170652496e-06, w1=-0.0614764858749361\n",
      "Regularized Logistic Regression(51/299): loss=0.5472757873734934, w0=-5.305835961190376e-06, w1=-0.062311864252361814\n",
      "Regularized Logistic Regression(52/299): loss=0.546491717908114, w0=-5.406064698511154e-06, w1=-0.06313921982041791\n",
      "Regularized Logistic Regression(53/299): loss=0.5457331962530396, w0=-5.5062220709955124e-06, w1=-0.06395874283028984\n",
      "Regularized Logistic Regression(54/299): loss=0.5449990885432655, w0=-5.606307783714162e-06, w1=-0.06477061714750726\n",
      "Regularized Logistic Regression(55/299): loss=0.5442883200652691, w0=-5.7063215581551955e-06, w1=-0.0655750205203284\n",
      "Regularized Logistic Regression(56/299): loss=0.5435998717942704, w0=-5.806263131910441e-06, w1=-0.06637212483463323\n",
      "Regularized Logistic Regression(57/299): loss=0.54293277715375, w0=-5.9061322583285615e-06, w1=-0.06716209635611105\n",
      "Regularized Logistic Regression(58/299): loss=0.5422861189819307, w0=-6.0059287061416266e-06, w1=-0.06794509596047717\n",
      "Regularized Logistic Regression(59/299): loss=0.5416590266910268, w0=-6.10565225907097e-06, w1=-0.06872127935240428\n",
      "Regularized Logistic Regression(60/299): loss=0.5410506736060811, w0=-6.205302715417385e-06, w1=-0.06949079727381059\n",
      "Regularized Logistic Regression(61/299): loss=0.5404602744711562, w0=-6.304879887639994e-06, w1=-0.07025379570210324\n",
      "Regularized Logistic Regression(62/299): loss=0.5398870831115257, w0=-6.404383601927539e-06, w1=-0.0710104160389403\n",
      "Regularized Logistic Regression(63/299): loss=0.539330390241316, w0=-6.503813697765307e-06, w1=-0.0717607952900362\n",
      "Regularized Logistic Regression(64/299): loss=0.5387895214068121, w0=-6.603170027500427e-06, w1=-0.07250506623650378\n",
      "Regularized Logistic Regression(65/299): loss=0.5382638350563296, w0=-6.70245245590788e-06, w1=-0.07324335759819516\n",
      "Regularized Logistic Regression(66/299): loss=0.5377527207282089, w0=-6.801660859759208e-06, w1=-0.0739757941894744\n",
      "Regularized Logistic Regression(67/299): loss=0.5372555973490811, w0=-6.90079512739557e-06, w1=-0.07470249706782943\n",
      "Regularized Logistic Regression(68/299): loss=0.5367719116351166, w0=-6.99985515830656e-06, w1=-0.07542358367570448\n",
      "Regularized Logistic Regression(69/299): loss=0.5363011365894736, w0=-7.098840862715932e-06, w1=-0.07613916797591214\n",
      "Regularized Logistic Regression(70/299): loss=0.5358427700896484, w0=-7.197752161175188e-06, w1=-0.0768493605809625\n",
      "Regularized Logistic Regression(71/299): loss=0.5353963335588653, w0=-7.296588984165787e-06, w1=-0.07755426887662656\n",
      "Regularized Logistic Regression(72/299): loss=0.5349613707160544, w0=-7.395351271710596e-06, w1=-0.07825399714003188\n",
      "Regularized Logistic Regression(73/299): loss=0.5345374463993482, w0=-7.4940389729950546e-06, w1=-0.0789486466525714\n",
      "Regularized Logistic Regression(74/299): loss=0.5341241454583718, w0=-7.59265204599841e-06, w1=-0.07963831580789098\n",
      "Regularized Logistic Regression(75/299): loss=0.5337210717109381, w0=-7.691190457135282e-06, w1=-0.08032310021520328\n",
      "Regularized Logistic Regression(76/299): loss=0.5333278469600472, w0=-7.789654180907718e-06, w1=-0.0810030927981638\n",
      "Regularized Logistic Regression(77/299): loss=0.5329441100673864, w0=-7.888043199567843e-06, w1=-0.08167838388953122\n",
      "Regularized Logistic Regression(78/299): loss=0.5325695160797702, w0=-7.986357502791117e-06, w1=-0.08234906132181889\n",
      "Regularized Logistic Regression(79/299): loss=0.5322037354052143, w0=-8.084597087360194e-06, w1=-0.08301521051413731\n",
      "Regularized Logistic Regression(80/299): loss=0.5318464530355504, w0=-8.182761956859296e-06, w1=-0.08367691455541171\n",
      "Regularized Logistic Regression(81/299): loss=0.5314973678127076, w0=-8.280852121378984e-06, w1=-0.0843342542841517\n",
      "Regularized Logistic Regression(82/299): loss=0.5311561917359652, w0=-8.378867597231208e-06, w1=-0.08498730836493887\n",
      "Regularized Logistic Regression(83/299): loss=0.5308226493076759, w0=-8.476808406674435e-06, w1=-0.08563615336178913\n",
      "Regularized Logistic Regression(84/299): loss=0.5304964769151104, w0=-8.574674577648677e-06, w1=-0.08628086380853947\n",
      "Regularized Logistic Regression(85/299): loss=0.5301774222462425, w0=-8.672466143520211e-06, w1=-0.08692151227639809\n",
      "Regularized Logistic Regression(86/299): loss=0.5298652437374215, w0=-8.77018314283575e-06, w1=-0.0875581694387922\n",
      "Regularized Logistic Regression(87/299): loss=0.5295597100510311, w0=-8.867825619085842e-06, w1=-0.08819090413363902\n",
      "Regularized Logistic Regression(88/299): loss=0.5292605995813422, w0=-8.965393620477246e-06, w1=-0.08881978342315888\n",
      "Regularized Logistic Regression(89/299): loss=0.5289676999868885, w0=-9.062887199714033e-06, w1=-0.08944487265134421\n",
      "Regularized Logistic Regression(90/299): loss=0.5286808077478075, w0=-9.160306413787164e-06, w1=-0.09006623549919082\n",
      "Regularized Logistic Regression(91/299): loss=0.5283997277466751, w0=-9.257651323772268e-06, w1=-0.09068393403779311\n",
      "Regularized Logistic Regression(92/299): loss=0.5281242728714729, w0=-9.354921994635397e-06, w1=-0.09129802877940035\n",
      "Regularized Logistic Regression(93/299): loss=0.527854263639397, w0=-9.452118495046466e-06, w1=-0.09190857872652418\n",
      "Regularized Logistic Regression(94/299): loss=0.527589527840312, w0=-9.549240897200145e-06, w1=-0.09251564141918474\n",
      "Regularized Logistic Regression(95/299): loss=0.5273299001987205, w0=-9.64628927664394e-06, w1=-0.09311927298037818\n",
      "Regularized Logistic Regression(96/299): loss=0.5270752220531921, w0=-9.743263712113225e-06, w1=-0.09371952815984251\n",
      "Regularized Logistic Regression(97/299): loss=0.5268253410522633, w0=-9.84016428537297e-06, w1=-0.09431646037619741\n",
      "Regularized Logistic Regression(98/299): loss=0.5265801108658736, w0=-9.936991081065929e-06, w1=-0.09491012175752779\n",
      "Regularized Logistic Regression(99/299): loss=0.5263393909114729, w0=-1.0033744186567057e-05, w1=-0.09550056318047855\n",
      "Regularized Logistic Regression(100/299): loss=0.526103046093973, w0=-1.0130423691843926e-05, w1=-0.09608783430792418\n",
      "Regularized Logistic Regression(101/299): loss=0.5258709465587813, w0=-1.0227029689322914e-05, w1=-0.0966719836252743\n",
      "Regularized Logistic Regression(102/299): loss=0.5256429674571877, w0=-1.0323562273760947e-05, w1=-0.09725305847547198\n",
      "Regularized Logistic Regression(103/299): loss=0.5254189887234323, w0=-1.0420021542122598e-05, w1=-0.09783110509274127\n",
      "Regularized Logistic Regression(104/299): loss=0.5251988948628071, w0=-1.051640759346232e-05, w1=-0.09840616863513496\n",
      "Regularized Logistic Regression(105/299): loss=0.5249825747501997, w0=-1.0612720528811621e-05, w1=-0.09897829321593293\n",
      "Regularized Logistic Regression(106/299): loss=0.5247699214385083, w0=-1.0708960451070995e-05, w1=-0.099547521933939\n",
      "Regularized Logistic Regression(107/299): loss=0.5245608319764015, w0=-1.0805127464906415e-05, w1=-0.10011389690272073\n",
      "Regularized Logistic Regression(108/299): loss=0.5243552072349142, w0=-1.0901221676650212e-05, w1=-0.10067745927883612\n",
      "Regularized Logistic Regression(109/299): loss=0.524152951742421, w0=-1.0997243194206162e-05, w1=-0.10123824928908734\n",
      "Regularized Logistic Regression(110/299): loss=0.5239539735275311, w0=-1.109319212695862e-05, w1=-0.10179630625684205\n",
      "Regularized Logistic Regression(111/299): loss=0.5237581839694957, w0=-1.1189068585685533e-05, w1=-0.10235166862745843\n",
      "Regularized Logistic Regression(112/299): loss=0.5235654976557271, w0=-1.1284872682475175e-05, w1=-0.10290437399285067\n",
      "Regularized Logistic Regression(113/299): loss=0.523375832246065, w0=-1.1380604530646467e-05, w1=-0.10345445911522799\n",
      "Regularized Logistic Regression(114/299): loss=0.5231891083434318, w0=-1.1476264244672711e-05, w1=-0.10400195995004098\n",
      "Regularized Logistic Regression(115/299): loss=0.5230052493705541, w0=-1.1571851940108634e-05, w1=-0.10454691166816513\n",
      "Regularized Logistic Regression(116/299): loss=0.5228241814524333, w0=-1.1667367733520565e-05, w1=-0.10508934867735173\n",
      "Regularized Logistic Regression(117/299): loss=0.5226458333042722, w0=-1.1762811742419666e-05, w1=-0.105629304642975\n",
      "Regularized Logistic Regression(118/299): loss=0.5224701361245818, w0=-1.1858184085198034e-05, w1=-0.1061668125081017\n",
      "Regularized Logistic Regression(119/299): loss=0.5222970234932032, w0=-1.1953484881067618e-05, w1=-0.10670190451290953\n",
      "Regularized Logistic Regression(120/299): loss=0.5221264312739977, w0=-1.2048714250001783e-05, w1=-0.10723461221347945\n",
      "Regularized Logistic Regression(121/299): loss=0.5219582975219703, w0=-1.214387231267944e-05, w1=-0.107764966499985\n",
      "Regularized Logistic Regression(122/299): loss=0.5217925623946067, w0=-1.2238959190431633e-05, w1=-0.10829299761430158\n",
      "Regularized Logistic Regression(123/299): loss=0.5216291680672122, w0=-1.2333975005190468e-05, w1=-0.10881873516705756\n",
      "Regularized Logistic Regression(124/299): loss=0.5214680586520537, w0=-1.24289198794403e-05, w1=-0.10934220815414787\n",
      "Regularized Logistic Regression(125/299): loss=0.5213091801211246, w0=-1.252379393617108e-05, w1=-0.10986344497272972\n",
      "Regularized Logistic Regression(126/299): loss=0.5211524802323407, w0=-1.2618597298833776e-05, w1=-0.11038247343671984\n",
      "Regularized Logistic Regression(127/299): loss=0.5209979084590177, w0=-1.271333009129776e-05, w1=-0.11089932079181128\n",
      "Regularized Logistic Regression(128/299): loss=0.5208454159224539, w0=-1.2807992437810126e-05, w1=-0.11141401373002761\n",
      "Regularized Logistic Regression(129/299): loss=0.5206949553274809, w0=-1.2902584462956791e-05, w1=-0.11192657840383037\n",
      "Regularized Logistic Regression(130/299): loss=0.5205464809008308, w0=-1.2997106291625381e-05, w1=-0.11243704043979713\n",
      "Regularized Logistic Regression(131/299): loss=0.5203999483321899, w0=-1.3091558048969753e-05, w1=-0.11294542495188428\n",
      "Regularized Logistic Regression(132/299): loss=0.5202553147178088, w0=-1.3185939860376146e-05, w1=-0.11345175655429021\n",
      "Regularized Logistic Regression(133/299): loss=0.5201125385065472, w0=-1.3280251851430849e-05, w1=-0.1139560593739328\n",
      "Regularized Logistic Regression(134/299): loss=0.519971579448239, w0=-1.337449414788935e-05, w1=-0.11445835706255433\n",
      "Regularized Logistic Regression(135/299): loss=0.5198323985442679, w0=-1.3468666875646883e-05, w1=-0.1149586728084676\n",
      "Regularized Logistic Regression(136/299): loss=0.5196949580002521, w0=-1.356277016071034e-05, w1=-0.11545702934795514\n",
      "Regularized Logistic Regression(137/299): loss=0.5195592211807343, w0=-1.3656804129171458e-05, w1=-0.11595344897633408\n",
      "Regularized Logistic Regression(138/299): loss=0.5194251525657906, w0=-1.375076890718125e-05, w1=-0.11644795355869758\n",
      "Regularized Logistic Regression(139/299): loss=0.5192927177094641, w0=-1.384466462092563e-05, w1=-0.1169405645403445\n",
      "Regularized Logistic Regression(140/299): loss=0.5191618831999415, w0=-1.3938491396602162e-05, w1=-0.11743130295690768\n",
      "Regularized Logistic Regression(141/299): loss=0.519032616621394, w0=-1.4032249360397903e-05, w1=-0.11792018944419057\n",
      "Regularized Logistic Regression(142/299): loss=0.5189048865174017, w0=-1.4125938638468284e-05, w1=-0.11840724424772328\n",
      "Regularized Logistic Regression(143/299): loss=0.5187786623558962, w0=-1.4219559356916998e-05, w1=-0.11889248723204626\n",
      "Regularized Logistic Regression(144/299): loss=0.5186539144955449, w0=-1.431311164177683e-05, w1=-0.1193759378897311\n",
      "Regularized Logistic Regression(145/299): loss=0.51853061415352, w0=-1.4406595618991413e-05, w1=-0.11985761535014741\n",
      "Regularized Logistic Regression(146/299): loss=0.5184087333745836, w0=-1.4500011414397852e-05, w1=-0.1203375383879841\n",
      "Regularized Logistic Regression(147/299): loss=0.5182882450014323, w0=-1.4593359153710198e-05, w1=-0.12081572543153284\n",
      "Regularized Logistic Regression(148/299): loss=0.5181691226462461, w0=-1.468663896250371e-05, w1=-0.12129219457074156\n",
      "Regularized Logistic Regression(149/299): loss=0.5180513406633862, w0=-1.4779850966199912e-05, w1=-0.12176696356504607\n",
      "Regularized Logistic Regression(150/299): loss=0.5179348741231927, w0=-1.4872995290052353e-05, w1=-0.12224004985098597\n",
      "Regularized Logistic Regression(151/299): loss=0.517819698786832, w0=-1.49660720591331e-05, w1=-0.1227114705496126\n",
      "Regularized Logistic Regression(152/299): loss=0.5177057910821499, w0=-1.5059081398319899e-05, w1=-0.12318124247369525\n",
      "Regularized Logistic Regression(153/299): loss=0.5175931280804831, w0=-1.5152023432283974e-05, w1=-0.12364938213473263\n",
      "Regularized Logistic Regression(154/299): loss=0.5174816874743919, w0=-1.5244898285478455e-05, w1=-0.12411590574977471\n",
      "Regularized Logistic Regression(155/299): loss=0.5173714475562696, w0=-1.5337706082127397e-05, w1=-0.12458082924806232\n",
      "Regularized Logistic Regression(156/299): loss=0.5172623871977945, w0=-1.5430446946215365e-05, w1=-0.1250441682774888\n",
      "Regularized Logistic Regression(157/299): loss=0.5171544858301851, w0=-1.5523121001477562e-05, w1=-0.12550593821089046\n",
      "Regularized Logistic Regression(158/299): loss=0.5170477234252264, w0=-1.561572837139048e-05, w1=-0.12596615415217063\n",
      "Regularized Logistic Regression(159/299): loss=0.5169420804770327, w0=-1.5708269179163042e-05, w1=-0.12642483094226212\n",
      "Regularized Logistic Regression(160/299): loss=0.5168375379845164, w0=-1.580074354772823e-05, w1=-0.12688198316493376\n",
      "Regularized Logistic Regression(161/299): loss=0.5167340774345334, w0=-1.589315159973516e-05, w1=-0.1273376251524456\n",
      "Regularized Logistic Regression(162/299): loss=0.5166316807856745, w0=-1.5985493457541603e-05, w1=-0.1277917709910568\n",
      "Regularized Logistic Regression(163/299): loss=0.5165303304526779, w0=-1.607776924320691e-05, w1=-0.12824443452639153\n",
      "Regularized Logistic Regression(164/299): loss=0.5164300092914359, w0=-1.6169979078485353e-05, w1=-0.12869562936866671\n",
      "Regularized Logistic Regression(165/299): loss=0.5163307005845679, w0=-1.6262123084819837e-05, w1=-0.12914536889778566\n",
      "Regularized Logistic Regression(166/299): loss=0.516232388027544, w0=-1.635420138333598e-05, w1=-0.1295936662683019\n",
      "Regularized Logistic Regression(167/299): loss=0.5161350557153268, w0=-1.6446214094836547e-05, w1=-0.13004053441425717\n",
      "Regularized Logistic Regression(168/299): loss=0.5160386881295159, w0=-1.653816133979621e-05, w1=-0.1304859860538966\n",
      "Regularized Logistic Regression(169/299): loss=0.5159432701259746, w0=-1.663004323835664e-05, w1=-0.1309300336942656\n",
      "Regularized Logistic Regression(170/299): loss=0.5158487869229146, w0=-1.6721859910321905e-05, w1=-0.13137268963569126\n",
      "Regularized Logistic Regression(171/299): loss=0.5157552240894238, w0=-1.6813611475154148e-05, w1=-0.1318139659761521\n",
      "Regularized Logistic Regression(172/299): loss=0.515662567534419, w0=-1.6905298051969554e-05, w1=-0.13225387461553906\n",
      "Regularized Logistic Regression(173/299): loss=0.5155708034960037, w0=-1.6996919759534588e-05, w1=-0.132692427259811\n",
      "Regularized Logistic Regression(174/299): loss=0.5154799185312169, w0=-1.708847671626248e-05, w1=-0.133129635425048\n",
      "Regularized Logistic Regression(175/299): loss=0.515389899506155, w0=-1.717996904020997e-05, w1=-0.13356551044140488\n",
      "Regularized Logistic Regression(176/299): loss=0.5153007335864567, w0=-1.7271396849074267e-05, w1=-0.1340000634569682\n",
      "Regularized Logistic Regression(177/299): loss=0.5152124082281274, w0=-1.736276026019026e-05, w1=-0.13443330544151946\n",
      "Regularized Logistic Regression(178/299): loss=0.5151249111686987, w0=-1.7454059390527905e-05, w1=-0.13486524719020695\n",
      "Regularized Logistic Regression(179/299): loss=0.5150382304187053, w0=-1.7545294356689853e-05, w1=-0.13529589932712896\n",
      "Regularized Logistic Regression(180/299): loss=0.5149523542534657, w0=-1.7636465274909253e-05, w1=-0.13572527230883064\n",
      "Regularized Logistic Regression(181/299): loss=0.5148672712051572, w0=-1.7727572261047744e-05, w1=-0.13615337642771755\n",
      "Regularized Logistic Regression(182/299): loss=0.5147829700551733, w0=-1.7818615430593634e-05, w1=-0.13658022181538712\n",
      "Regularized Logistic Regression(183/299): loss=0.5146994398267495, w0=-1.790959489866024e-05, w1=-0.13700581844588144\n",
      "Regularized Logistic Regression(184/299): loss=0.5146166697778508, w0=-1.8000510779984396e-05, w1=-0.13743017613886288\n",
      "Regularized Logistic Regression(185/299): loss=0.5145346493943096, w0=-1.8091363188925108e-05, w1=-0.1378533045627145\n",
      "Regularized Logistic Regression(186/299): loss=0.5144533683832012, w0=-1.8182152239462376e-05, w1=-0.13827521323756825\n",
      "Regularized Logistic Regression(187/299): loss=0.5143728166664525, w0=-1.8272878045196132e-05, w1=-0.1386959115382617\n",
      "Regularized Logistic Regression(188/299): loss=0.5142929843746711, w0=-1.836354071934534e-05, w1=-0.139115408697226\n",
      "Regularized Logistic Regression(189/299): loss=0.514213861841189, w0=-1.8454140374747203e-05, w1=-0.13953371380730703\n",
      "Regularized Logistic Regression(190/299): loss=0.5141354395963096, w0=-1.8544677123856497e-05, w1=-0.13995083582452095\n",
      "Regularized Logistic Regression(191/299): loss=0.5140577083617537, w0=-1.8635151078745045e-05, w1=-0.1403667835707461\n",
      "Regularized Logistic Regression(192/299): loss=0.5139806590452938, w0=-1.872556235110126e-05, w1=-0.14078156573635345\n",
      "Regularized Logistic Regression(193/299): loss=0.5139042827355725, w0=-1.881591105222984e-05, w1=-0.14119519088277643\n",
      "Regularized Logistic Regression(194/299): loss=0.5138285706970941, w0=-1.8906197293051533e-05, w1=-0.14160766744502262\n",
      "Regularized Logistic Regression(195/299): loss=0.5137535143653879, w0=-1.899642118410301e-05, w1=-0.1420190037341277\n",
      "Regularized Logistic Regression(196/299): loss=0.5136791053423316, w0=-1.9086582835536835e-05, w1=-0.14242920793955463\n",
      "Regularized Logistic Regression(197/299): loss=0.5136053353916329, w0=-1.917668235712151e-05, w1=-0.1428382881315377\n",
      "Regularized Logistic Regression(198/299): loss=0.5135321964344607, w0=-1.9266719858241617e-05, w1=-0.14324625226337492\n",
      "Regularized Logistic Regression(199/299): loss=0.5134596805452201, w0=-1.9356695447898036e-05, w1=-0.14365310817366828\n",
      "Regularized Logistic Regression(200/299): loss=0.5133877799474706, w0=-1.9446609234708224e-05, w1=-0.14405886358851422\n",
      "Regularized Logistic Regression(201/299): loss=0.5133164870099753, w0=-1.9536461326906598e-05, w1=-0.1444635261236455\n",
      "Regularized Logistic Regression(202/299): loss=0.5132457942428804, w0=-1.9626251832344947e-05, w1=-0.14486710328652516\n",
      "Regularized Logistic Regression(203/299): loss=0.5131756942940192, w0=-1.9715980858492945e-05, w1=-0.14526960247839468\n",
      "Regularized Logistic Regression(204/299): loss=0.5131061799453375, w0=-1.9805648512438697e-05, w1=-0.14567103099627665\n",
      "Regularized Logistic Regression(205/299): loss=0.5130372441094324, w0=-1.9895254900889365e-05, w1=-0.1460713960349336\n",
      "Regularized Logistic Regression(206/299): loss=0.5129688798262049, w0=-1.998480013017184e-05, w1=-0.1464707046887839\n",
      "Regularized Logistic Regression(207/299): loss=0.5129010802596167, w0=-2.0074284306233464e-05, w1=-0.14686896395377602\n",
      "Regularized Logistic Regression(208/299): loss=0.5128338386945542, w0=-2.016370753464281e-05, w1=-0.14726618072922174\n",
      "Regularized Logistic Regression(209/299): loss=0.5127671485337899, w0=-2.0253069920590504e-05, w1=-0.14766236181958986\n",
      "Regularized Logistic Regression(210/299): loss=0.5127010032950395, w0=-2.0342371568890093e-05, w1=-0.14805751393626115\n",
      "Regularized Logistic Regression(211/299): loss=0.5126353966081136, w0=-2.043161258397896e-05, w1=-0.1484516436992453\n",
      "Regularized Logistic Regression(212/299): loss=0.5125703222121569, w0=-2.0520793069919272e-05, w1=-0.1488447576388612\n",
      "Regularized Logistic Regression(213/299): loss=0.5125057739529745, w0=-2.0609913130398967e-05, w1=-0.1492368621973813\n",
      "Regularized Logistic Regression(214/299): loss=0.5124417457804414, w0=-2.0698972868732786e-05, w1=-0.14962796373064063\n",
      "Regularized Logistic Regression(215/299): loss=0.512378231745991, w0=-2.078797238786333e-05, w1=-0.1500180685096116\n",
      "Regularized Logistic Regression(216/299): loss=0.5123152260001823, w0=-2.087691179036215e-05, w1=-0.15040718272194595\n",
      "Regularized Logistic Regression(217/299): loss=0.5122527227903404, w0=-2.096579117843087e-05, w1=-0.15079531247348332\n",
      "Regularized Logistic Regression(218/299): loss=0.51219071645827, w0=-2.1054610653902328e-05, w1=-0.1511824637897286\n",
      "Regularized Logistic Regression(219/299): loss=0.5121292014380375, w0=-2.1143370318241777e-05, w1=-0.1515686426172978\n",
      "Regularized Logistic Regression(220/299): loss=0.5120681722538201, w0=-2.1232070272548064e-05, w1=-0.15195385482533416\n",
      "Regularized Logistic Regression(221/299): loss=0.5120076235178208, w0=-2.132071061755486e-05, w1=-0.1523381062068939\n",
      "Regularized Logistic Regression(222/299): loss=0.5119475499282434, w0=-2.1409291453631917e-05, w1=-0.15272140248030408\n",
      "Regularized Logistic Regression(223/299): loss=0.5118879462673305, w0=-2.1497812880786328e-05, w1=-0.15310374929049125\n",
      "Regularized Logistic Regression(224/299): loss=0.5118288073994567, w0=-2.1586274998663814e-05, w1=-0.1534851522102834\n",
      "Regularized Logistic Regression(225/299): loss=0.5117701282692811, w0=-2.1674677906550034e-05, w1=-0.15386561674168442\n",
      "Regularized Logistic Regression(226/299): loss=0.5117119038999524, w0=-2.176302170337191e-05, w1=-0.15424514831712277\n",
      "Regularized Logistic Regression(227/299): loss=0.5116541293913645, w0=-2.1851306487698954e-05, w1=-0.15462375230067418\n",
      "Regularized Logistic Regression(228/299): loss=0.511596799918467, w0=-2.1939532357744633e-05, w1=-0.15500143398925945\n",
      "Regularized Logistic Regression(229/299): loss=0.5115399107296211, w0=-2.202769941136773e-05, w1=-0.15537819861381788\n",
      "Regularized Logistic Regression(230/299): loss=0.5114834571450039, w0=-2.211580774607372e-05, w1=-0.15575405134045647\n",
      "Regularized Logistic Regression(231/299): loss=0.5114274345550578, w0=-2.2203857459016158e-05, w1=-0.15612899727157628\n",
      "Regularized Logistic Regression(232/299): loss=0.5113718384189861, w0=-2.229184864699809e-05, w1=-0.15650304144697533\n",
      "Regularized Logistic Regression(233/299): loss=0.5113166642632878, w0=-2.2379781406473453e-05, w1=-0.15687618884493004\n",
      "Regularized Logistic Regression(234/299): loss=0.5112619076803356, w0=-2.2467655833548493e-05, w1=-0.15724844438325405\n",
      "Regularized Logistic Regression(235/299): loss=0.511207564326995, w0=-2.2555472023983197e-05, w1=-0.15761981292033628\n",
      "Regularized Logistic Regression(236/299): loss=0.5111536299232784, w0=-2.2643230073192726e-05, w1=-0.15799029925615798\n",
      "Regularized Logistic Regression(237/299): loss=0.5111001002510398, w0=-2.2730930076248847e-05, w1=-0.15835990813328976\n",
      "Regularized Logistic Regression(238/299): loss=0.5110469711527038, w0=-2.2818572127881387e-05, w1=-0.15872864423786803\n",
      "Regularized Logistic Regression(239/299): loss=0.510994238530029, w0=-2.290615632247968e-05, w1=-0.15909651220055301\n",
      "Regularized Logistic Regression(240/299): loss=0.5109418983429074, w0=-2.2993682754094026e-05, w1=-0.15946351659746683\n",
      "Regularized Logistic Regression(241/299): loss=0.5108899466081935, w0=-2.3081151516437138e-05, w1=-0.1598296619511136\n",
      "Regularized Logistic Regression(242/299): loss=0.5108383793985665, w0=-2.3168562702885615e-05, w1=-0.1601949527312813\n",
      "Regularized Logistic Regression(243/299): loss=0.5107871928414232, w0=-2.3255916406481404e-05, w1=-0.16055939335592553\n",
      "Regularized Logistic Regression(244/299): loss=0.5107363831177996, w0=-2.3343212719933255e-05, w1=-0.16092298819203657\n",
      "Regularized Logistic Regression(245/299): loss=0.5106859464613226, w0=-2.343045173561821e-05, w1=-0.16128574155648856\n",
      "Regularized Logistic Regression(246/299): loss=0.510635879157186, w0=-2.3517633545583046e-05, w1=-0.16164765771687298\n",
      "Regularized Logistic Regression(247/299): loss=0.5105861775411574, w0=-2.3604758241545766e-05, w1=-0.16200874089231546\n",
      "Regularized Logistic Regression(248/299): loss=0.5105368379986087, w0=-2.369182591489706e-05, w1=-0.1623689952542766\n",
      "Regularized Logistic Regression(249/299): loss=0.5104878569635721, w0=-2.3778836656701766e-05, w1=-0.16272842492733747\n",
      "Regularized Logistic Regression(250/299): loss=0.5104392309178205, w0=-2.3865790557700354e-05, w1=-0.16308703398996996\n",
      "Regularized Logistic Regression(251/299): loss=0.5103909563899712, w0=-2.3952687708310377e-05, w1=-0.16344482647529204\n",
      "Regularized Logistic Regression(252/299): loss=0.5103430299546131, w0=-2.4039528198627948e-05, w1=-0.16380180637180858\n",
      "Regularized Logistic Regression(253/299): loss=0.5102954482314552, w0=-2.4126312118429205e-05, w1=-0.16415797762413803\n",
      "Regularized Logistic Regression(254/299): loss=0.5102482078844974, w0=-2.4213039557171764e-05, w1=-0.16451334413372498\n",
      "Regularized Logistic Regression(255/299): loss=0.510201305621221, w0=-2.4299710603996184e-05, w1=-0.1648679097595393\n",
      "Regularized Logistic Regression(256/299): loss=0.5101547381918003, w0=-2.4386325347727424e-05, w1=-0.16522167831876178\n",
      "Regularized Logistic Regression(257/299): loss=0.5101085023883328, w0=-2.4472883876876297e-05, w1=-0.16557465358745688\n",
      "Regularized Logistic Regression(258/299): loss=0.5100625950440896, w0=-2.455938627964092e-05, w1=-0.1659268393012325\n",
      "Regularized Logistic Regression(259/299): loss=0.5100170130327822, w0=-2.464583264390816e-05, w1=-0.16627823915588755\n",
      "Regularized Logistic Regression(260/299): loss=0.5099717532678494, w0=-2.473222305725508e-05, w1=-0.16662885680804687\n",
      "Regularized Logistic Regression(261/299): loss=0.5099268127017593, w0=-2.481855760695038e-05, w1=-0.16697869587578454\n",
      "Regularized Logistic Regression(262/299): loss=0.5098821883253299, w0=-2.4904836379955833e-05, w1=-0.1673277599392353\n",
      "Regularized Logistic Regression(263/299): loss=0.5098378771670633, w0=-2.4991059462927704e-05, w1=-0.16767605254119453\n",
      "Regularized Logistic Regression(264/299): loss=0.5097938762924997, w0=-2.5077226942218196e-05, w1=-0.16802357718770683\n",
      "Regularized Logistic Regression(265/299): loss=0.509750182803582, w0=-2.5163338903876858e-05, w1=-0.1683703373486441\n",
      "Regularized Logistic Regression(266/299): loss=0.5097067938380391, w0=-2.524939543365201e-05, w1=-0.16871633645827203\n",
      "Regularized Logistic Regression(267/299): loss=0.5096637065687811, w0=-2.5335396616992155e-05, w1=-0.1690615779158069\n",
      "Regularized Logistic Regression(268/299): loss=0.50962091820331, w0=-2.5421342539047378e-05, w1=-0.16940606508596137\n",
      "Regularized Logistic Regression(269/299): loss=0.5095784259831427, w0=-2.5507233284670757e-05, w1=-0.16974980129948064\n",
      "Regularized Logistic Regression(270/299): loss=0.509536227183248, w0=-2.5593068938419762e-05, w1=-0.17009278985366835\n",
      "Regularized Logistic Regression(271/299): loss=0.5094943191114962, w0=-2.5678849584557633e-05, w1=-0.17043503401290286\n",
      "Regularized Logistic Regression(272/299): loss=0.5094526991081211, w0=-2.576457530705478e-05, w1=-0.17077653700914436\n",
      "Regularized Logistic Regression(273/299): loss=0.5094113645451939, w0=-2.585024618959015e-05, w1=-0.1711173020424321\n",
      "Regularized Logistic Regression(274/299): loss=0.5093703128261088, w0=-2.593586231555261e-05, w1=-0.17145733228137303\n",
      "Regularized Logistic Regression(275/299): loss=0.5093295413850804, w0=-2.6021423768042315e-05, w1=-0.17179663086362099\n",
      "Regularized Logistic Regression(276/299): loss=0.5092890476866514, w0=-2.6106930629872064e-05, w1=-0.17213520089634776\n",
      "Regularized Logistic Regression(277/299): loss=0.5092488292252124, w0=-2.6192382983568662e-05, w1=-0.17247304545670497\n",
      "Regularized Logistic Regression(278/299): loss=0.5092088835245301, w0=-2.6277780911374264e-05, w1=-0.1728101675922779\n",
      "Regularized Logistic Regression(279/299): loss=0.5091692081372875, w0=-2.6363124495247724e-05, w1=-0.17314657032153122\n",
      "Regularized Logistic Regression(280/299): loss=0.5091298006446329, w0=-2.644841381686593e-05, w1=-0.17348225663424607\n",
      "Regularized Logistic Regression(281/299): loss=0.5090906586557395, w0=-2.653364895762514e-05, w1=-0.1738172294919498\n",
      "Regularized Logistic Regression(282/299): loss=0.5090517798073728, w0=-2.6618829998642296e-05, w1=-0.17415149182833783\n",
      "Regularized Logistic Regression(283/299): loss=0.5090131617634687, w0=-2.6703957020756352e-05, w1=-0.17448504654968788\n",
      "Regularized Logistic Regression(284/299): loss=0.508974802214719, w0=-2.678903010452958e-05, w1=-0.17481789653526666\n",
      "Regularized Logistic Regression(285/299): loss=0.5089366988781677, w0=-2.687404933024888e-05, w1=-0.17515004463772937\n",
      "Regularized Logistic Regression(286/299): loss=0.5088988494968121, w0=-2.695901477792708e-05, w1=-0.1754814936835123\n",
      "Regularized Logistic Regression(287/299): loss=0.5088612518392155, w0=-2.7043926527304215e-05, w1=-0.17581224647321783\n",
      "Regularized Logistic Regression(288/299): loss=0.508823903699126, w0=-2.7128784657848836e-05, w1=-0.17614230578199316\n",
      "Regularized Logistic Regression(289/299): loss=0.5087868028951037, w0=-2.7213589248759268e-05, w1=-0.17647167435990213\n",
      "Regularized Logistic Regression(290/299): loss=0.5087499472701545, w0=-2.729834037896489e-05, w1=-0.17680035493229018\n",
      "Regularized Logistic Regression(291/299): loss=0.5087133346913728, w0=-2.738303812712741e-05, w1=-0.17712835020014345\n",
      "Regularized Logistic Regression(292/299): loss=0.5086769630495893, w0=-2.7467682571642108e-05, w1=-0.17745566284044068\n",
      "Regularized Logistic Regression(293/299): loss=0.5086408302590276, w0=-2.75522737906391e-05, w1=-0.17778229550649968\n",
      "Regularized Logistic Regression(294/299): loss=0.5086049342569664, w0=-2.763681186198458e-05, w1=-0.17810825082831755\n",
      "Regularized Logistic Regression(295/299): loss=0.5085692730034084, w0=-2.772129686328206e-05, w1=-0.17843353141290452\n",
      "Regularized Logistic Regression(296/299): loss=0.5085338444807571, w0=-2.78057288718736e-05, w1=-0.17875813984461228\n",
      "Regularized Logistic Regression(297/299): loss=0.508498646693496, w0=-2.7890107964841046e-05, w1=-0.17908207868545664\n",
      "Regularized Logistic Regression(298/299): loss=0.5084636776678796, w0=-2.797443421900723e-05, w1=-0.17940535047543435\n",
      "Regularized Logistic Regression(299/299): loss=0.5084289354516248, w0=-2.80587077109372e-05, w1=-0.17972795773283443\n",
      "Regularized Logistic Regression(0/99): loss=0.693147180559945, w0=-1.0441465146389343e-07, w1=-0.0018294491082988765\n",
      "Regularized Logistic Regression(1/99): loss=0.6853321983348266, w0=-2.0814686610900756e-07, w1=-0.0036079179150844244\n",
      "Regularized Logistic Regression(2/99): loss=0.678013786124486, w0=-3.1122235371714565e-07, w1=-0.005337606758097223\n",
      "Regularized Logistic Regression(3/99): loss=0.6711581553565382, w0=-4.136638842120526e-07, w1=-0.007020608635746807\n",
      "Regularized Logistic Regression(4/99): loss=0.6647338392638477, w0=-5.154916062837092e-07, w1=-0.008658911703110983\n",
      "Regularized Logistic Regression(5/99): loss=0.6587115444874669, w0=-6.167233385231585e-07, w1=-0.01025440256754603\n",
      "Regularized Logistic Regression(6/99): loss=0.6530640096784435, w0=-7.173748339759431e-07, w1=-0.011808870140670301\n",
      "Regularized Logistic Regression(7/99): loss=0.6477658713425308, w0=-8.174600193758582e-07, w1=-0.013324009850961572\n",
      "Regularized Logistic Regression(8/99): loss=0.6427935369625349, w0=-9.169912105546656e-07, w1=-0.014801428062096153\n",
      "Regularized Logistic Regression(9/99): loss=0.6381250652992081, w0=-1.0159793056616813e-06, w1=-0.01624264657666697\n",
      "Regularized Logistic Regression(10/99): loss=0.6337400536886384, w0=-1.114433957890718e-06, w1=-0.017649107133562214\n",
      "Regularized Logistic Regression(11/99): loss=0.629619532104521, w0=-1.2123637294185508e-06, w1=-0.01902217583069931\n",
      "Regularized Logistic Regression(12/99): loss=0.6257458637253942, w0=-1.3097762282237568e-06, w1=-0.020363147423687056\n",
      "Regularized Logistic Regression(13/99): loss=0.6221026517318929, w0=-1.4066782293895718e-06, w1=-0.021673249466006853\n",
      "Regularized Logistic Regression(14/99): loss=0.618674652052166, w0=-1.5030757824090514e-06, w1=-0.022953646268099356\n",
      "Regularized Logistic Regression(15/99): loss=0.6154476917716364, w0=-1.5989743059129627e-06, w1=-0.024205442661879187\n",
      "Regularized Logistic Regression(16/99): loss=0.6124085929243788, w0=-1.6943786711363607e-06, w1=-0.025429687564168628\n",
      "Regularized Logistic Regression(17/99): loss=0.6095451013865079, w0=-1.78929327533317e-06, w1=-0.026627377337774667\n",
      "Regularized Logistic Regression(18/99): loss=0.6068458205965005, w0=-1.8837221062425678e-06, w1=-0.027799458952776197\n",
      "Regularized Logistic Regression(19/99): loss=0.6043001498330294, w0=-1.977668798608837e-06, w1=-0.02894683295333671\n",
      "Regularized Logistic Regression(20/99): loss=0.6018982267875101, w0=-2.0711366836592425e-06, w1=-0.03007035623726003\n",
      "Regularized Logistic Regression(21/99): loss=0.5996308741760193, w0=-2.1641288323533366e-06, w1=-0.031170844656742823\n",
      "Regularized Logistic Regression(22/99): loss=0.5974895501434535, w0=-2.2566480931324747e-06, w1=-0.0322490754495186\n",
      "Regularized Logistic Regression(23/99): loss=0.5954663022216752, w0=-2.3486971248204647e-06, w1=-0.033305789509949286\n",
      "Regularized Logistic Regression(24/99): loss=0.593553724612823, w0=-2.440278425255143e-06, w1=-0.03434169350970087\n",
      "Regularized Logistic Regression(25/99): loss=0.5917449185788206, w0=-2.5313943561660865e-06, w1=-0.03535746187752005\n",
      "Regularized Logistic Regression(26/99): loss=0.5900334557283097, w0=-2.622047164755338e-06, w1=-0.0363537386473657\n",
      "Regularized Logistic Regression(27/99): loss=0.5884133440026073, w0=-2.7122390023855498e-06, w1=-0.0373311391837849\n",
      "Regularized Logistic Regression(28/99): loss=0.5868789961727745, w0=-2.801971940732958e-06, w1=-0.0382902517930005\n",
      "Regularized Logistic Regression(29/99): loss=0.5854252006703118, w0=-2.8912479857205903e-06, w1=-0.039231639227718905\n",
      "Regularized Logistic Regression(30/99): loss=0.5840470945843409, w0=-2.9800690895097458e-06, w1=-0.04015584009318504\n",
      "Regularized Logistic Regression(31/99): loss=0.5827401386682722, w0=-3.0684371607945244e-06, w1=-0.04106337016153506\n",
      "Regularized Logistic Regression(32/99): loss=0.5815000942088068, w0=-3.1563540736147423e-06, w1=-0.04195472360102344\n",
      "Regularized Logistic Regression(33/99): loss=0.5803230016196828, w0=-3.2438216748764705e-06, w1=-0.04283037412624477\n",
      "Regularized Logistic Regression(34/99): loss=0.5792051606317383, w0=-3.330841790746385e-06, w1=-0.04369077607503049\n",
      "Regularized Logistic Regression(35/99): loss=0.5781431119596381, w0=-3.417416232065759e-06, w1=-0.044536365417287824\n",
      "Regularized Logistic Regression(36/99): loss=0.5771336203339692, w0=-3.5035467989120014e-06, w1=-0.04536756070065502\n",
      "Regularized Logistic Regression(37/99): loss=0.5761736587953171, w0=-3.5892352844198174e-06, w1=-0.04618476393748005\n",
      "Regularized Logistic Regression(38/99): loss=0.5752603941544134, w0=-3.674483477960184e-06, w1=-0.04698836143729021\n",
      "Regularized Logistic Regression(39/99): loss=0.5743911735294772, w0=-3.7592931677630823e-06, w1=-0.047778724588599435\n",
      "Regularized Logistic Regression(40/99): loss=0.5735635118784687, w0=-3.843666143059191e-06, w1=-0.04855621059360696\n",
      "Regularized Logistic Regression(41/99): loss=0.5727750804501537, w0=-3.927604195806302e-06, w1=-0.0493211631590704\n",
      "Regularized Logistic Regression(42/99): loss=0.5720236960836367, w0=-4.011109122057924e-06, w1=-0.050073913146381245\n",
      "Regularized Logistic Regression(43/99): loss=0.5713073112913937, w0=-4.094182723024281e-06, w1=-0.050814779183642786\n",
      "Regularized Logistic Regression(44/99): loss=0.5706240050658221, w0=-4.17682680586952e-06, w1=-0.05154406824233479\n",
      "Regularized Logistic Regression(45/99): loss=0.5699719743539675, w0=-4.259043184283355e-06, w1=-0.05226207618095408\n",
      "Regularized Logistic Regression(46/99): loss=0.569349526149377, w0=-4.340833678860464e-06, w1=-0.05296908825783958\n",
      "Regularized Logistic Regression(47/99): loss=0.5687550701540078, w0=-4.422200117316677e-06, w1=-0.053665379615223704\n",
      "Regularized Logistic Regression(48/99): loss=0.568187111966795, w0=-4.5031443345672014e-06, w1=-0.05435121573640064\n",
      "Regularized Logistic Regression(49/99): loss=0.5676442467588828, w0=-4.583668172688855e-06, w1=-0.055026852877760025\n",
      "Regularized Logistic Regression(50/99): loss=0.5671251533986523, w0=-4.663773480785386e-06, w1=-0.055692538477308436\n",
      "Regularized Logistic Regression(51/99): loss=0.5666285889925807, w0=-4.7434621147724305e-06, w1=-0.056348511541178384\n",
      "Regularized Logistic Regression(52/99): loss=0.5661533838106185, w0=-4.822735937096437e-06, w1=-0.056995003009519755\n",
      "Regularized Logistic Regression(53/99): loss=0.5656984365672415, w0=-4.9015968163999835e-06, w1=-0.05763223610306638\n",
      "Regularized Logistic Regression(54/99): loss=0.5652627100315837, w0=-4.980046627144183e-06, w1=-0.05826042665157634\n",
      "Regularized Logistic Regression(55/99): loss=0.5648452269421471, w0=-5.058087249197425e-06, w1=-0.05887978340526317\n",
      "Regularized Logistic Regression(56/99): loss=0.5644450662034921, w0=-5.135720567398393e-06, w1=-0.05949050833025401\n",
      "Regularized Logistic Regression(57/99): loss=0.5640613593440864, w0=-5.2129484711001784e-06, w1=-0.06009279688903919\n",
      "Regularized Logistic Regression(58/99): loss=0.5636932872160929, w0=-5.2897728537013215e-06, w1=-0.060686838306811985\n",
      "Regularized Logistic Regression(59/99): loss=0.5633400769193947, w0=-5.3661956121687545e-06, w1=-0.06127281582453406\n",
      "Regularized Logistic Regression(60/99): loss=0.5630009989334952, w0=-5.442218646556869e-06, w1=-0.06185090693950737\n",
      "Regularized Logistic Regression(61/99): loss=0.5626753644422269, w0=-5.517843859526287e-06, w1=-0.06242128363417903\n",
      "Regularized Logistic Regression(62/99): loss=0.5623625228373325, w0=-5.593073155865332e-06, w1=-0.06298411259385796\n",
      "Regularized Logistic Regression(63/99): loss=0.5620618593880716, w0=-5.667908442016728e-06, w1=-0.06353955541397716\n",
      "Regularized Logistic Regression(64/99): loss=0.5617727930649736, w0=-5.7423516256115904e-06, w1=-0.06408776879749409\n",
      "Regularized Logistic Regression(65/99): loss=0.561494774506774, w0=-5.816404615012436e-06, w1=-0.06462890474298255\n",
      "Regularized Logistic Regression(66/99): loss=0.5612272841203892, w0=-5.890069318866606e-06, w1=-0.065163110723934\n",
      "Regularized Logistic Regression(67/99): loss=0.5609698303045612, w0=-5.963347645671194e-06, w1=-0.06569052985975349\n",
      "Regularized Logistic Regression(68/99): loss=0.5607219477885043, w0=-6.036241503350381e-06, w1=-0.06621130107890426\n",
      "Regularized Logistic Regression(69/99): loss=0.5604831960775296, w0=-6.108752798845816e-06, w1=-0.06672555927462659\n",
      "Regularized Logistic Regression(70/99): loss=0.5602531579982334, w0=-6.180883437720544e-06, w1=-0.06723343545363039\n",
      "Regularized Logistic Regression(71/99): loss=0.5600314383363724, w0=-6.252635323776823e-06, w1=-0.06773505687813494\n",
      "Regularized Logistic Regression(72/99): loss=0.5598176625610617, w0=-6.324010358688019e-06, w1=-0.06823054720160869\n",
      "Regularized Logistic Regression(73/99): loss=0.5596114756294046, w0=-6.395010441644694e-06, w1=-0.06872002659853826\n",
      "Regularized Logistic Regression(74/99): loss=0.5594125408660835, w0=-6.465637469014874e-06, w1=-0.06920361188853691\n",
      "Regularized Logistic Regression(75/99): loss=0.5592205389128491, w0=-6.535893334018425e-06, w1=-0.06968141665508498\n",
      "Regularized Logistic Regression(76/99): loss=0.5590351667432055, w0=-6.605779926415406e-06, w1=-0.07015355135917542\n",
      "Regularized Logistic Regression(77/99): loss=0.5588561367379328, w0=-6.675299132208181e-06, w1=-0.07062012344812373\n",
      "Regularized Logistic Regression(78/99): loss=0.5586831758173929, w0=-6.744452833357067e-06, w1=-0.07108123745978534\n",
      "Regularized Logistic Regression(79/99): loss=0.5585160246268688, w0=-6.813242907509222e-06, w1=-0.07153699512240845\n",
      "Regularized Logistic Regression(80/99): loss=0.5583544367714353, w0=-6.881671227740484e-06, w1=-0.0719874954503397\n",
      "Regularized Logistic Regression(81/99): loss=0.5581981780971214, w0=-6.949739662309812e-06, w1=-0.07243283483578516\n",
      "Regularized Logistic Regression(82/99): loss=0.5580470260153458, w0=-7.0174500744259865e-06, w1=-0.0728731071368192\n",
      "Regularized Logistic Regression(83/99): loss=0.5579007688678189, w0=-7.08480432202621e-06, w1=-0.07330840376182343\n",
      "Regularized Logistic Regression(84/99): loss=0.5577592053292988, w0=-7.1518042575662385e-06, w1=-0.07373881375052428\n",
      "Regularized Logistic Regression(85/99): loss=0.5576221438457746, w0=-7.218451727821652e-06, w1=-0.07416442385179421\n",
      "Regularized Logistic Regression(86/99): loss=0.5574894021058121, w0=-7.284748573699906e-06, w1=-0.07458531859836669\n",
      "Regularized Logistic Regression(87/99): loss=0.5573608065429542, w0=-7.350696630062767e-06, w1=-0.07500158037861179\n",
      "Regularized Logistic Regression(88/99): loss=0.5572361918672115, w0=-7.416297725558769e-06, w1=-0.07541328950550674\n",
      "Regularized Logistic Regression(89/99): loss=0.5571154006238166, w0=-7.4815536824652955e-06, w1=-0.07582052428293314\n",
      "Regularized Logistic Regression(90/99): loss=0.5569982827775293, w0=-7.546466316539949e-06, w1=-0.07622336106942146\n",
      "Regularized Logistic Regression(91/99): loss=0.5568846953209058, w0=-7.611037436880811e-06, w1=-0.07662187433945986\n",
      "Regularized Logistic Regression(92/99): loss=0.5567745019050413, w0=-7.67526884579527e-06, w1=-0.07701613674247704\n",
      "Regularized Logistic Regression(93/99): loss=0.5566675724914036, w0=-7.739162338677035e-06, w1=-0.07740621915960297\n",
      "Regularized Logistic Regression(94/99): loss=0.556563783023459, w0=-7.802719703891038e-06, w1=-0.0777921907583071\n",
      "Regularized Logistic Regression(95/99): loss=0.5564630151168826, w0=-7.86594272266585e-06, w1=-0.07817411904500673\n",
      "Regularized Logistic Regression(96/99): loss=0.5563651557672197, w0=-7.928833168993327e-06, w1=-0.07855206991573478\n",
      "Regularized Logistic Regression(97/99): loss=0.5562700970739484, w0=-7.99139280953516e-06, w1=-0.07892610770495247\n",
      "Regularized Logistic Regression(98/99): loss=0.5561777359799421, w0=-8.053623403536023e-06, w1=-0.07929629523258554\n",
      "Regularized Logistic Regression(99/99): loss=0.5560879740254255, w0=-8.115526702743043e-06, w1=-0.07966269384936024\n",
      "Regularized Logistic Regression(0/199): loss=0.693147180559945, w0=-1.0441465146389343e-07, w1=-0.0018294491082988765\n",
      "Regularized Logistic Regression(1/199): loss=0.6853321983348266, w0=-2.0814686610900756e-07, w1=-0.0036079179150844244\n",
      "Regularized Logistic Regression(2/199): loss=0.678013786124486, w0=-3.1122235371714565e-07, w1=-0.005337606758097223\n",
      "Regularized Logistic Regression(3/199): loss=0.6711581553565382, w0=-4.136638842120526e-07, w1=-0.007020608635746807\n",
      "Regularized Logistic Regression(4/199): loss=0.6647338392638477, w0=-5.154916062837092e-07, w1=-0.008658911703110983\n",
      "Regularized Logistic Regression(5/199): loss=0.6587115444874669, w0=-6.167233385231585e-07, w1=-0.01025440256754603\n",
      "Regularized Logistic Regression(6/199): loss=0.6530640096784435, w0=-7.173748339759431e-07, w1=-0.011808870140670301\n",
      "Regularized Logistic Regression(7/199): loss=0.6477658713425308, w0=-8.174600193758582e-07, w1=-0.013324009850961572\n",
      "Regularized Logistic Regression(8/199): loss=0.6427935369625349, w0=-9.169912105546656e-07, w1=-0.014801428062096153\n",
      "Regularized Logistic Regression(9/199): loss=0.6381250652992081, w0=-1.0159793056616813e-06, w1=-0.01624264657666697\n",
      "Regularized Logistic Regression(10/199): loss=0.6337400536886384, w0=-1.114433957890718e-06, w1=-0.017649107133562214\n",
      "Regularized Logistic Regression(11/199): loss=0.629619532104521, w0=-1.2123637294185508e-06, w1=-0.01902217583069931\n",
      "Regularized Logistic Regression(12/199): loss=0.6257458637253942, w0=-1.3097762282237568e-06, w1=-0.020363147423687056\n",
      "Regularized Logistic Regression(13/199): loss=0.6221026517318929, w0=-1.4066782293895718e-06, w1=-0.021673249466006853\n",
      "Regularized Logistic Regression(14/199): loss=0.618674652052166, w0=-1.5030757824090514e-06, w1=-0.022953646268099356\n",
      "Regularized Logistic Regression(15/199): loss=0.6154476917716364, w0=-1.5989743059129627e-06, w1=-0.024205442661879187\n",
      "Regularized Logistic Regression(16/199): loss=0.6124085929243788, w0=-1.6943786711363607e-06, w1=-0.025429687564168628\n",
      "Regularized Logistic Regression(17/199): loss=0.6095451013865079, w0=-1.78929327533317e-06, w1=-0.026627377337774667\n",
      "Regularized Logistic Regression(18/199): loss=0.6068458205965005, w0=-1.8837221062425678e-06, w1=-0.027799458952776197\n",
      "Regularized Logistic Regression(19/199): loss=0.6043001498330294, w0=-1.977668798608837e-06, w1=-0.02894683295333671\n",
      "Regularized Logistic Regression(20/199): loss=0.6018982267875101, w0=-2.0711366836592425e-06, w1=-0.03007035623726003\n",
      "Regularized Logistic Regression(21/199): loss=0.5996308741760193, w0=-2.1641288323533366e-06, w1=-0.031170844656742823\n",
      "Regularized Logistic Regression(22/199): loss=0.5974895501434535, w0=-2.2566480931324747e-06, w1=-0.0322490754495186\n",
      "Regularized Logistic Regression(23/199): loss=0.5954663022216752, w0=-2.3486971248204647e-06, w1=-0.033305789509949286\n",
      "Regularized Logistic Regression(24/199): loss=0.593553724612823, w0=-2.440278425255143e-06, w1=-0.03434169350970087\n",
      "Regularized Logistic Regression(25/199): loss=0.5917449185788206, w0=-2.5313943561660865e-06, w1=-0.03535746187752005\n",
      "Regularized Logistic Regression(26/199): loss=0.5900334557283097, w0=-2.622047164755338e-06, w1=-0.0363537386473657\n",
      "Regularized Logistic Regression(27/199): loss=0.5884133440026073, w0=-2.7122390023855498e-06, w1=-0.0373311391837849\n",
      "Regularized Logistic Regression(28/199): loss=0.5868789961727745, w0=-2.801971940732958e-06, w1=-0.0382902517930005\n",
      "Regularized Logistic Regression(29/199): loss=0.5854252006703118, w0=-2.8912479857205903e-06, w1=-0.039231639227718905\n",
      "Regularized Logistic Regression(30/199): loss=0.5840470945843409, w0=-2.9800690895097458e-06, w1=-0.04015584009318504\n",
      "Regularized Logistic Regression(31/199): loss=0.5827401386682722, w0=-3.0684371607945244e-06, w1=-0.04106337016153506\n",
      "Regularized Logistic Regression(32/199): loss=0.5815000942088068, w0=-3.1563540736147423e-06, w1=-0.04195472360102344\n",
      "Regularized Logistic Regression(33/199): loss=0.5803230016196828, w0=-3.2438216748764705e-06, w1=-0.04283037412624477\n",
      "Regularized Logistic Regression(34/199): loss=0.5792051606317383, w0=-3.330841790746385e-06, w1=-0.04369077607503049\n",
      "Regularized Logistic Regression(35/199): loss=0.5781431119596381, w0=-3.417416232065759e-06, w1=-0.044536365417287824\n",
      "Regularized Logistic Regression(36/199): loss=0.5771336203339692, w0=-3.5035467989120014e-06, w1=-0.04536756070065502\n",
      "Regularized Logistic Regression(37/199): loss=0.5761736587953171, w0=-3.5892352844198174e-06, w1=-0.04618476393748005\n",
      "Regularized Logistic Regression(38/199): loss=0.5752603941544134, w0=-3.674483477960184e-06, w1=-0.04698836143729021\n",
      "Regularized Logistic Regression(39/199): loss=0.5743911735294772, w0=-3.7592931677630823e-06, w1=-0.047778724588599435\n",
      "Regularized Logistic Regression(40/199): loss=0.5735635118784687, w0=-3.843666143059191e-06, w1=-0.04855621059360696\n",
      "Regularized Logistic Regression(41/199): loss=0.5727750804501537, w0=-3.927604195806302e-06, w1=-0.0493211631590704\n",
      "Regularized Logistic Regression(42/199): loss=0.5720236960836367, w0=-4.011109122057924e-06, w1=-0.050073913146381245\n",
      "Regularized Logistic Regression(43/199): loss=0.5713073112913937, w0=-4.094182723024281e-06, w1=-0.050814779183642786\n",
      "Regularized Logistic Regression(44/199): loss=0.5706240050658221, w0=-4.17682680586952e-06, w1=-0.05154406824233479\n",
      "Regularized Logistic Regression(45/199): loss=0.5699719743539675, w0=-4.259043184283355e-06, w1=-0.05226207618095408\n",
      "Regularized Logistic Regression(46/199): loss=0.569349526149377, w0=-4.340833678860464e-06, w1=-0.05296908825783958\n",
      "Regularized Logistic Regression(47/199): loss=0.5687550701540078, w0=-4.422200117316677e-06, w1=-0.053665379615223704\n",
      "Regularized Logistic Regression(48/199): loss=0.568187111966795, w0=-4.5031443345672014e-06, w1=-0.05435121573640064\n",
      "Regularized Logistic Regression(49/199): loss=0.5676442467588828, w0=-4.583668172688855e-06, w1=-0.055026852877760025\n",
      "Regularized Logistic Regression(50/199): loss=0.5671251533986523, w0=-4.663773480785386e-06, w1=-0.055692538477308436\n",
      "Regularized Logistic Regression(51/199): loss=0.5666285889925807, w0=-4.7434621147724305e-06, w1=-0.056348511541178384\n",
      "Regularized Logistic Regression(52/199): loss=0.5661533838106185, w0=-4.822735937096437e-06, w1=-0.056995003009519755\n",
      "Regularized Logistic Regression(53/199): loss=0.5656984365672415, w0=-4.9015968163999835e-06, w1=-0.05763223610306638\n",
      "Regularized Logistic Regression(54/199): loss=0.5652627100315837, w0=-4.980046627144183e-06, w1=-0.05826042665157634\n",
      "Regularized Logistic Regression(55/199): loss=0.5648452269421471, w0=-5.058087249197425e-06, w1=-0.05887978340526317\n",
      "Regularized Logistic Regression(56/199): loss=0.5644450662034921, w0=-5.135720567398393e-06, w1=-0.05949050833025401\n",
      "Regularized Logistic Regression(57/199): loss=0.5640613593440864, w0=-5.2129484711001784e-06, w1=-0.06009279688903919\n",
      "Regularized Logistic Regression(58/199): loss=0.5636932872160929, w0=-5.2897728537013215e-06, w1=-0.060686838306811985\n",
      "Regularized Logistic Regression(59/199): loss=0.5633400769193947, w0=-5.3661956121687545e-06, w1=-0.06127281582453406\n",
      "Regularized Logistic Regression(60/199): loss=0.5630009989334952, w0=-5.442218646556869e-06, w1=-0.06185090693950737\n",
      "Regularized Logistic Regression(61/199): loss=0.5626753644422269, w0=-5.517843859526287e-06, w1=-0.06242128363417903\n",
      "Regularized Logistic Regression(62/199): loss=0.5623625228373325, w0=-5.593073155865332e-06, w1=-0.06298411259385796\n",
      "Regularized Logistic Regression(63/199): loss=0.5620618593880716, w0=-5.667908442016728e-06, w1=-0.06353955541397716\n",
      "Regularized Logistic Regression(64/199): loss=0.5617727930649736, w0=-5.7423516256115904e-06, w1=-0.06408776879749409\n",
      "Regularized Logistic Regression(65/199): loss=0.561494774506774, w0=-5.816404615012436e-06, w1=-0.06462890474298255\n",
      "Regularized Logistic Regression(66/199): loss=0.5612272841203892, w0=-5.890069318866606e-06, w1=-0.065163110723934\n",
      "Regularized Logistic Regression(67/199): loss=0.5609698303045612, w0=-5.963347645671194e-06, w1=-0.06569052985975349\n",
      "Regularized Logistic Regression(68/199): loss=0.5607219477885043, w0=-6.036241503350381e-06, w1=-0.06621130107890426\n",
      "Regularized Logistic Regression(69/199): loss=0.5604831960775296, w0=-6.108752798845816e-06, w1=-0.06672555927462659\n",
      "Regularized Logistic Regression(70/199): loss=0.5602531579982334, w0=-6.180883437720544e-06, w1=-0.06723343545363039\n",
      "Regularized Logistic Regression(71/199): loss=0.5600314383363724, w0=-6.252635323776823e-06, w1=-0.06773505687813494\n",
      "Regularized Logistic Regression(72/199): loss=0.5598176625610617, w0=-6.324010358688019e-06, w1=-0.06823054720160869\n",
      "Regularized Logistic Regression(73/199): loss=0.5596114756294046, w0=-6.395010441644694e-06, w1=-0.06872002659853826\n",
      "Regularized Logistic Regression(74/199): loss=0.5594125408660835, w0=-6.465637469014874e-06, w1=-0.06920361188853691\n",
      "Regularized Logistic Regression(75/199): loss=0.5592205389128491, w0=-6.535893334018425e-06, w1=-0.06968141665508498\n",
      "Regularized Logistic Regression(76/199): loss=0.5590351667432055, w0=-6.605779926415406e-06, w1=-0.07015355135917542\n",
      "Regularized Logistic Regression(77/199): loss=0.5588561367379328, w0=-6.675299132208181e-06, w1=-0.07062012344812373\n",
      "Regularized Logistic Regression(78/199): loss=0.5586831758173929, w0=-6.744452833357067e-06, w1=-0.07108123745978534\n",
      "Regularized Logistic Regression(79/199): loss=0.5585160246268688, w0=-6.813242907509222e-06, w1=-0.07153699512240845\n",
      "Regularized Logistic Regression(80/199): loss=0.5583544367714353, w0=-6.881671227740484e-06, w1=-0.0719874954503397\n",
      "Regularized Logistic Regression(81/199): loss=0.5581981780971214, w0=-6.949739662309812e-06, w1=-0.07243283483578516\n",
      "Regularized Logistic Regression(82/199): loss=0.5580470260153458, w0=-7.0174500744259865e-06, w1=-0.0728731071368192\n",
      "Regularized Logistic Regression(83/199): loss=0.5579007688678189, w0=-7.08480432202621e-06, w1=-0.07330840376182343\n",
      "Regularized Logistic Regression(84/199): loss=0.5577592053292988, w0=-7.1518042575662385e-06, w1=-0.07373881375052428\n",
      "Regularized Logistic Regression(85/199): loss=0.5576221438457746, w0=-7.218451727821652e-06, w1=-0.07416442385179421\n",
      "Regularized Logistic Regression(86/199): loss=0.5574894021058121, w0=-7.284748573699906e-06, w1=-0.07458531859836669\n",
      "Regularized Logistic Regression(87/199): loss=0.5573608065429542, w0=-7.350696630062767e-06, w1=-0.07500158037861179\n",
      "Regularized Logistic Regression(88/199): loss=0.5572361918672115, w0=-7.416297725558769e-06, w1=-0.07541328950550674\n",
      "Regularized Logistic Regression(89/199): loss=0.5571154006238166, w0=-7.4815536824652955e-06, w1=-0.07582052428293314\n",
      "Regularized Logistic Regression(90/199): loss=0.5569982827775293, w0=-7.546466316539949e-06, w1=-0.07622336106942146\n",
      "Regularized Logistic Regression(91/199): loss=0.5568846953209058, w0=-7.611037436880811e-06, w1=-0.07662187433945986\n",
      "Regularized Logistic Regression(92/199): loss=0.5567745019050413, w0=-7.67526884579527e-06, w1=-0.07701613674247704\n",
      "Regularized Logistic Regression(93/199): loss=0.5566675724914036, w0=-7.739162338677035e-06, w1=-0.07740621915960297\n",
      "Regularized Logistic Regression(94/199): loss=0.556563783023459, w0=-7.802719703891038e-06, w1=-0.0777921907583071\n",
      "Regularized Logistic Regression(95/199): loss=0.5564630151168826, w0=-7.86594272266585e-06, w1=-0.07817411904500673\n",
      "Regularized Logistic Regression(96/199): loss=0.5563651557672197, w0=-7.928833168993327e-06, w1=-0.07855206991573478\n",
      "Regularized Logistic Regression(97/199): loss=0.5562700970739484, w0=-7.99139280953516e-06, w1=-0.07892610770495247\n",
      "Regularized Logistic Regression(98/199): loss=0.5561777359799421, w0=-8.053623403536023e-06, w1=-0.07929629523258554\n",
      "Regularized Logistic Regression(99/199): loss=0.5560879740254255, w0=-8.115526702743043e-06, w1=-0.07966269384936024\n",
      "Regularized Logistic Regression(100/199): loss=0.5560007171155391, w0=-8.1771044513313e-06, w1=-0.08002536348051364\n",
      "Regularized Logistic Regression(101/199): loss=0.55591587530072, w0=-8.238358385835096e-06, w1=-0.08038436266794427\n",
      "Regularized Logistic Regression(102/199): loss=0.555833362569129, w0=-8.299290235084716e-06, w1=-0.08073974861086994\n",
      "Regularized Logistic Regression(103/199): loss=0.5557530966504199, w0=-8.359901720148456e-06, w1=-0.0810915772050555\n",
      "Regularized Logistic Regression(104/199): loss=0.5556749988301833, w0=-8.420194554279646e-06, w1=-0.08143990308066815\n",
      "Regularized Logistic Regression(105/199): loss=0.5555989937744422, w0=-8.480170442868461e-06, w1=-0.08178477963881853\n",
      "Regularized Logistic Regression(106/199): loss=0.5555250093636145, w0=-8.539831083398287e-06, w1=-0.08212625908684058\n",
      "Regularized Logistic Regression(107/199): loss=0.5554529765353932, w0=-8.599178165406424e-06, w1=-0.08246439247236066\n",
      "Regularized Logistic Regression(108/199): loss=0.5553828291360318, w0=-8.658213370448926e-06, w1=-0.08279922971620535\n",
      "Regularized Logistic Regression(109/199): loss=0.5553145037795528, w0=-8.716938372069387e-06, w1=-0.0831308196441956\n",
      "Regularized Logistic Regression(110/199): loss=0.5552479397144208, w0=-8.775354835771468e-06, w1=-0.08345921001786907\n",
      "Regularized Logistic Regression(111/199): loss=0.5551830786972626, w0=-8.833464418995e-06, w1=-0.08378444756417461\n",
      "Regularized Logistic Regression(112/199): loss=0.5551198648732293, w0=-8.89126877109549e-06, w1=-0.08410657800417969\n",
      "Regularized Logistic Regression(113/199): loss=0.5550582446626291, w0=-8.94876953332684e-06, w1=-0.08442564608082755\n",
      "Regularized Logistic Regression(114/199): loss=0.5549981666534713, w0=-9.00596833882716e-06, w1=-0.08474169558578284\n",
      "Regularized Logistic Regression(115/199): loss=0.5549395814996012, w0=-9.062866812607497e-06, w1=-0.08505476938539885\n",
      "Regularized Logistic Regression(116/199): loss=0.5548824418241005, w0=-9.119466571543323e-06, w1=-0.08536490944584209\n",
      "Regularized Logistic Regression(117/199): loss=0.5548267021276744, w0=-9.175769224368695e-06, w1=-0.08567215685740413\n",
      "Regularized Logistic Regression(118/199): loss=0.5547723187017365, w0=-9.231776371672888e-06, w1=-0.08597655185803317\n",
      "Regularized Logistic Regression(119/199): loss=0.5547192495459421, w0=-9.287489605899429e-06, w1=-0.08627813385611355\n",
      "Regularized Logistic Regression(120/199): loss=0.5546674542899186, w0=-9.342910511347379e-06, w1=-0.0865769414525219\n",
      "Regularized Logistic Regression(121/199): loss=0.5546168941189688, w0=-9.398040664174765e-06, w1=-0.08687301246198587\n",
      "Regularized Logistic Regression(122/199): loss=0.554567531703525, w0=-9.452881632404038e-06, w1=-0.08716638393377239\n",
      "Regularized Logistic Regression(123/199): loss=0.5545193311321559, w0=-9.507434975929467e-06, w1=-0.08745709217172899\n",
      "Regularized Logistic Regression(124/199): loss=0.5544722578479276, w0=-9.561702246526355e-06, w1=-0.08774517275370233\n",
      "Regularized Logistic Regression(125/199): loss=0.5544262785879446, w0=-9.615684987861986e-06, w1=-0.08803066055035617\n",
      "Regularized Logistic Regression(126/199): loss=0.5543813613258934, w0=-9.669384735508221e-06, w1=-0.08831358974341025\n",
      "Regularized Logistic Regression(127/199): loss=0.5543374752174327, w0=-9.722803016955632e-06, w1=-0.08859399384332162\n",
      "Regularized Logistic Regression(128/199): loss=0.5542945905482769, w0=-9.775941351629122e-06, w1=-0.0888719057064271\n",
      "Regularized Logistic Regression(129/199): loss=0.5542526786848283, w0=-9.82880125090493e-06, w1=-0.08914735755156677\n",
      "Regularized Logistic Regression(130/199): loss=0.5542117120272254, w0=-9.881384218128937e-06, w1=-0.08942038097620636\n",
      "Regularized Logistic Regression(131/199): loss=0.5541716639646774, w0=-9.933691748636243e-06, w1=-0.08969100697207633\n",
      "Regularized Logistic Regression(132/199): loss=0.5541325088329675, w0=-9.98572532977189e-06, w1=-0.08995926594034392\n",
      "Regularized Logistic Regression(133/199): loss=0.5540942218740078, w0=-1.0037486440912708e-05, w1=-0.09022518770633504\n",
      "Regularized Logistic Regression(134/199): loss=0.5540567791973431, w0=-1.0088976553490202e-05, w1=-0.09048880153382063\n",
      "Regularized Logistic Regression(135/199): loss=0.554020157743497, w0=-1.014019713101443e-05, w1=-0.09075013613888354\n",
      "Regularized Logistic Regression(136/199): loss=0.5539843352490694, w0=-1.0191149629098797e-05, w1=-0.09100921970337846\n",
      "Regularized Logistic Regression(137/199): loss=0.5539492902134924, w0=-1.024183549548574e-05, w1=-0.09126607988800069\n",
      "Regularized Logistic Regression(138/199): loss=0.5539150018673592, w0=-1.0292256170073226e-05, w1=-0.09152074384497527\n",
      "Regularized Logistic Regression(139/199): loss=0.5538814501422467, w0=-1.0342413084942019e-05, w1=-0.09177323823038015\n",
      "Regularized Logistic Regression(140/199): loss=0.5538486156419541, w0=-1.0392307664383692e-05, w1=-0.09202358921611482\n",
      "Regularized Logistic Regression(141/199): loss=0.5538164796150855, w0=-1.04419413249293e-05, w1=-0.09227182250152693\n",
      "Regularized Logistic Regression(142/199): loss=0.5537850239289097, w0=-1.0491315475378707e-05, w1=-0.09251796332470762\n",
      "Regularized Logistic Regression(143/199): loss=0.5537542310444297, w0=-1.0540431516830498e-05, w1=-0.09276203647346637\n",
      "Regularized Logistic Regression(144/199): loss=0.5537240839926028, w0=-1.058929084271247e-05, w1=-0.09300406629599656\n",
      "Regularized Logistic Regression(145/199): loss=0.5536945663516543, w0=-1.0637894838812629e-05, w1=-0.09324407671124056\n",
      "Regularized Logistic Regression(146/199): loss=0.5536656622254288, w0=-1.0686244883310677e-05, w1=-0.09348209121896525\n",
      "Regularized Logistic Regression(147/199): loss=0.5536373562227249, w0=-1.0734342346809968e-05, w1=-0.09371813290955627\n",
      "Regularized Logistic Regression(148/199): loss=0.5536096334375702, w0=-1.0782188592369866e-05, w1=-0.09395222447354086\n",
      "Regularized Logistic Regression(149/199): loss=0.5535824794303839, w0=-1.0829784975538512e-05, w1=-0.09418438821084701\n",
      "Regularized Logistic Regression(150/199): loss=0.5535558802099865, w0=-1.0877132844385943e-05, w1=-0.09441464603980852\n",
      "Regularized Logistic Regression(151/199): loss=0.5535298222164161, w0=-1.0924233539537551e-05, w1=-0.0946430195059224\n",
      "Regularized Logistic Regression(152/199): loss=0.5535042923045058, w0=-1.0971088394207858e-05, w1=-0.09486952979036747\n",
      "Regularized Logistic Regression(153/199): loss=0.5534792777281926, w0=-1.1017698734234558e-05, w1=-0.09509419771829175\n",
      "Regularized Logistic Regression(154/199): loss=0.5534547661255165, w0=-1.1064065878112845e-05, w1=-0.09531704376687478\n",
      "Regularized Logistic Regression(155/199): loss=0.5534307455042781, w0=-1.1110191137029954e-05, w1=-0.09553808807317259\n",
      "Regularized Logistic Regression(156/199): loss=0.5534072042283242, w0=-1.1156075814899932e-05, w1=-0.09575735044175243\n",
      "Regularized Logistic Regression(157/199): loss=0.5533841310044285, w0=-1.1201721208398607e-05, w1=-0.09597485035212222\n",
      "Regularized Logistic Regression(158/199): loss=0.5533615148697405, w0=-1.1247128606998724e-05, w1=-0.09619060696596257\n",
      "Regularized Logistic Regression(159/199): loss=0.5533393451797747, w0=-1.1292299293005247e-05, w1=-0.09640463913416653\n",
      "Regularized Logistic Regression(160/199): loss=0.5533176115969152, w0=-1.1337234541590796e-05, w1=-0.09661696540369287\n",
      "Regularized Logistic Regression(161/199): loss=0.5532963040794101, w0=-1.1381935620831217e-05, w1=-0.09682760402423891\n",
      "Regularized Logistic Regression(162/199): loss=0.5532754128708319, w0=-1.1426403791741241e-05, w1=-0.09703657295473803\n",
      "Regularized Logistic Regression(163/199): loss=0.5532549284899845, w0=-1.1470640308310256e-05, w1=-0.09724388986968699\n",
      "Regularized Logistic Regression(164/199): loss=0.5532348417212335, w0=-1.1514646417538145e-05, w1=-0.09744957216530857\n",
      "Regularized Logistic Regression(165/199): loss=0.5532151436052389, w0=-1.1558423359471183e-05, w1=-0.09765363696555386\n",
      "Regularized Logistic Regression(166/199): loss=0.5531958254300767, w0=-1.1601972367238007e-05, w1=-0.09785610112794929\n",
      "Regularized Logistic Regression(167/199): loss=0.5531768787227233, w0=-1.1645294667085596e-05, w1=-0.09805698124929281\n",
      "Regularized Logistic Regression(168/199): loss=0.5531582952408931, w0=-1.1688391478415296e-05, w1=-0.09825629367120374\n",
      "Regularized Logistic Regression(169/199): loss=0.5531400669652099, w0=-1.1731264013818862e-05, w1=-0.09845405448553043\n",
      "Regularized Logistic Regression(170/199): loss=0.5531221860916948, w0=-1.1773913479114486e-05, w1=-0.09865027953962005\n",
      "Regularized Logistic Regression(171/199): loss=0.5531046450245607, w0=-1.1816341073382845e-05, w1=-0.09884498444145419\n",
      "Regularized Logistic Regression(172/199): loss=0.5530874363692945, w0=-1.1858547989003108e-05, w1=-0.09903818456465445\n",
      "Regularized Logistic Regression(173/199): loss=0.5530705529260183, w0=-1.1900535411688947e-05, w1=-0.09922989505336134\n",
      "Regularized Logistic Regression(174/199): loss=0.5530539876831129, w0=-1.1942304520524486e-05, w1=-0.09942013082699093\n",
      "Regularized Logistic Regression(175/199): loss=0.5530377338110962, w0=-1.198385648800023e-05, w1=-0.09960890658487156\n",
      "Regularized Logistic Regression(176/199): loss=0.5530217846567393, w0=-1.2025192480048938e-05, w1=-0.09979623681076524\n",
      "Regularized Logistic Regression(177/199): loss=0.5530061337374153, w0=-1.2066313656081437e-05, w1=-0.09998213577727581\n",
      "Regularized Logistic Regression(178/199): loss=0.5529907747356673, w0=-1.2107221169022376e-05, w1=-0.10016661755014818\n",
      "Regularized Logistic Regression(179/199): loss=0.5529757014939864, w0=-1.2147916165345912e-05, w1=-0.10034969599246067\n",
      "Regularized Logistic Regression(180/199): loss=0.5529609080097906, w0=-1.2188399785111314e-05, w1=-0.1005313847687146\n",
      "Regularized Logistic Regression(181/199): loss=0.5529463884305948, w0=-1.222867316199849e-05, w1=-0.10071169734882242\n",
      "Regularized Logistic Regression(182/199): loss=0.5529321370493665, w0=-1.226873742334342e-05, w1=-0.1008906470119992\n",
      "Regularized Logistic Regression(183/199): loss=0.5529181483000545, w0=-1.2308593690173501e-05, w1=-0.10106824685055804\n",
      "Regularized Logistic Regression(184/199): loss=0.5529044167532865, w0=-1.2348243077242797e-05, w1=-0.10124450977361395\n",
      "Regularized Logistic Regression(185/199): loss=0.552890937112228, w0=-1.2387686693067176e-05, w1=-0.10141944851069724\n",
      "Regularized Logistic Regression(186/199): loss=0.5528777042085938, w0=-1.2426925639959343e-05, w1=-0.10159307561527982\n",
      "Regularized Logistic Regression(187/199): loss=0.552864712998806, w0=-1.2465961014063766e-05, w1=-0.10176540346821647\n",
      "Regularized Logistic Regression(188/199): loss=0.552851958560295, w0=-1.2504793905391485e-05, w1=-0.10193644428110331\n",
      "Regularized Logistic Regression(189/199): loss=0.5528394360879314, w0=-1.254342539785479e-05, w1=-0.10210621009955592\n",
      "Regularized Logistic Regression(190/199): loss=0.5528271408905922, w0=-1.2581856569301795e-05, w1=-0.10227471280640917\n",
      "Regularized Logistic Regression(191/199): loss=0.5528150683878448, w0=-1.2620088491550865e-05, w1=-0.1024419641248409\n",
      "Regularized Logistic Regression(192/199): loss=0.5528032141067536, w0=-1.2658122230424932e-05, w1=-0.10260797562142143\n",
      "Regularized Logistic Regression(193/199): loss=0.5527915736787977, w0=-1.2695958845785666e-05, w1=-0.10277275870909114\n",
      "Regularized Logistic Regression(194/199): loss=0.552780142836899, w0=-1.2733599391567509e-05, w1=-0.10293632465006732\n",
      "Regularized Logistic Regression(195/199): loss=0.5527689174125516, w0=-1.2771044915811581e-05, w1=-0.10309868455868366\n",
      "Regularized Logistic Regression(196/199): loss=0.5527578933330553, w0=-1.280829646069944e-05, w1=-0.10325984940416225\n",
      "Regularized Logistic Regression(197/199): loss=0.5527470666188404, w0=-1.2845355062586693e-05, w1=-0.10341983001332146\n",
      "Regularized Logistic Regression(198/199): loss=0.5527364333808878, w0=-1.2882221752036479e-05, w1=-0.10357863707322061\n",
      "Regularized Logistic Regression(199/199): loss=0.5527259898182354, w0=-1.2918897553852782e-05, w1=-0.10373628113374338\n",
      "Regularized Logistic Regression(0/299): loss=0.693147180559945, w0=-1.0441465146389343e-07, w1=-0.0018294491082988765\n",
      "Regularized Logistic Regression(1/299): loss=0.6853321983348266, w0=-2.0814686610900756e-07, w1=-0.0036079179150844244\n",
      "Regularized Logistic Regression(2/299): loss=0.678013786124486, w0=-3.1122235371714565e-07, w1=-0.005337606758097223\n",
      "Regularized Logistic Regression(3/299): loss=0.6711581553565382, w0=-4.136638842120526e-07, w1=-0.007020608635746807\n",
      "Regularized Logistic Regression(4/299): loss=0.6647338392638477, w0=-5.154916062837092e-07, w1=-0.008658911703110983\n",
      "Regularized Logistic Regression(5/299): loss=0.6587115444874669, w0=-6.167233385231585e-07, w1=-0.01025440256754603\n",
      "Regularized Logistic Regression(6/299): loss=0.6530640096784435, w0=-7.173748339759431e-07, w1=-0.011808870140670301\n",
      "Regularized Logistic Regression(7/299): loss=0.6477658713425308, w0=-8.174600193758582e-07, w1=-0.013324009850961572\n",
      "Regularized Logistic Regression(8/299): loss=0.6427935369625349, w0=-9.169912105546656e-07, w1=-0.014801428062096153\n",
      "Regularized Logistic Regression(9/299): loss=0.6381250652992081, w0=-1.0159793056616813e-06, w1=-0.01624264657666697\n",
      "Regularized Logistic Regression(10/299): loss=0.6337400536886384, w0=-1.114433957890718e-06, w1=-0.017649107133562214\n",
      "Regularized Logistic Regression(11/299): loss=0.629619532104521, w0=-1.2123637294185508e-06, w1=-0.01902217583069931\n",
      "Regularized Logistic Regression(12/299): loss=0.6257458637253942, w0=-1.3097762282237568e-06, w1=-0.020363147423687056\n",
      "Regularized Logistic Regression(13/299): loss=0.6221026517318929, w0=-1.4066782293895718e-06, w1=-0.021673249466006853\n",
      "Regularized Logistic Regression(14/299): loss=0.618674652052166, w0=-1.5030757824090514e-06, w1=-0.022953646268099356\n",
      "Regularized Logistic Regression(15/299): loss=0.6154476917716364, w0=-1.5989743059129627e-06, w1=-0.024205442661879187\n",
      "Regularized Logistic Regression(16/299): loss=0.6124085929243788, w0=-1.6943786711363607e-06, w1=-0.025429687564168628\n",
      "Regularized Logistic Regression(17/299): loss=0.6095451013865079, w0=-1.78929327533317e-06, w1=-0.026627377337774667\n",
      "Regularized Logistic Regression(18/299): loss=0.6068458205965005, w0=-1.8837221062425678e-06, w1=-0.027799458952776197\n",
      "Regularized Logistic Regression(19/299): loss=0.6043001498330294, w0=-1.977668798608837e-06, w1=-0.02894683295333671\n",
      "Regularized Logistic Regression(20/299): loss=0.6018982267875101, w0=-2.0711366836592425e-06, w1=-0.03007035623726003\n",
      "Regularized Logistic Regression(21/299): loss=0.5996308741760193, w0=-2.1641288323533366e-06, w1=-0.031170844656742823\n",
      "Regularized Logistic Regression(22/299): loss=0.5974895501434535, w0=-2.2566480931324747e-06, w1=-0.0322490754495186\n",
      "Regularized Logistic Regression(23/299): loss=0.5954663022216752, w0=-2.3486971248204647e-06, w1=-0.033305789509949286\n",
      "Regularized Logistic Regression(24/299): loss=0.593553724612823, w0=-2.440278425255143e-06, w1=-0.03434169350970087\n",
      "Regularized Logistic Regression(25/299): loss=0.5917449185788206, w0=-2.5313943561660865e-06, w1=-0.03535746187752005\n",
      "Regularized Logistic Regression(26/299): loss=0.5900334557283097, w0=-2.622047164755338e-06, w1=-0.0363537386473657\n",
      "Regularized Logistic Regression(27/299): loss=0.5884133440026073, w0=-2.7122390023855498e-06, w1=-0.0373311391837849\n",
      "Regularized Logistic Regression(28/299): loss=0.5868789961727745, w0=-2.801971940732958e-06, w1=-0.0382902517930005\n",
      "Regularized Logistic Regression(29/299): loss=0.5854252006703118, w0=-2.8912479857205903e-06, w1=-0.039231639227718905\n",
      "Regularized Logistic Regression(30/299): loss=0.5840470945843409, w0=-2.9800690895097458e-06, w1=-0.04015584009318504\n",
      "Regularized Logistic Regression(31/299): loss=0.5827401386682722, w0=-3.0684371607945244e-06, w1=-0.04106337016153506\n",
      "Regularized Logistic Regression(32/299): loss=0.5815000942088068, w0=-3.1563540736147423e-06, w1=-0.04195472360102344\n",
      "Regularized Logistic Regression(33/299): loss=0.5803230016196828, w0=-3.2438216748764705e-06, w1=-0.04283037412624477\n",
      "Regularized Logistic Regression(34/299): loss=0.5792051606317383, w0=-3.330841790746385e-06, w1=-0.04369077607503049\n",
      "Regularized Logistic Regression(35/299): loss=0.5781431119596381, w0=-3.417416232065759e-06, w1=-0.044536365417287824\n",
      "Regularized Logistic Regression(36/299): loss=0.5771336203339692, w0=-3.5035467989120014e-06, w1=-0.04536756070065502\n",
      "Regularized Logistic Regression(37/299): loss=0.5761736587953171, w0=-3.5892352844198174e-06, w1=-0.04618476393748005\n",
      "Regularized Logistic Regression(38/299): loss=0.5752603941544134, w0=-3.674483477960184e-06, w1=-0.04698836143729021\n",
      "Regularized Logistic Regression(39/299): loss=0.5743911735294772, w0=-3.7592931677630823e-06, w1=-0.047778724588599435\n",
      "Regularized Logistic Regression(40/299): loss=0.5735635118784687, w0=-3.843666143059191e-06, w1=-0.04855621059360696\n",
      "Regularized Logistic Regression(41/299): loss=0.5727750804501537, w0=-3.927604195806302e-06, w1=-0.0493211631590704\n",
      "Regularized Logistic Regression(42/299): loss=0.5720236960836367, w0=-4.011109122057924e-06, w1=-0.050073913146381245\n",
      "Regularized Logistic Regression(43/299): loss=0.5713073112913937, w0=-4.094182723024281e-06, w1=-0.050814779183642786\n",
      "Regularized Logistic Regression(44/299): loss=0.5706240050658221, w0=-4.17682680586952e-06, w1=-0.05154406824233479\n",
      "Regularized Logistic Regression(45/299): loss=0.5699719743539675, w0=-4.259043184283355e-06, w1=-0.05226207618095408\n",
      "Regularized Logistic Regression(46/299): loss=0.569349526149377, w0=-4.340833678860464e-06, w1=-0.05296908825783958\n",
      "Regularized Logistic Regression(47/299): loss=0.5687550701540078, w0=-4.422200117316677e-06, w1=-0.053665379615223704\n",
      "Regularized Logistic Regression(48/299): loss=0.568187111966795, w0=-4.5031443345672014e-06, w1=-0.05435121573640064\n",
      "Regularized Logistic Regression(49/299): loss=0.5676442467588828, w0=-4.583668172688855e-06, w1=-0.055026852877760025\n",
      "Regularized Logistic Regression(50/299): loss=0.5671251533986523, w0=-4.663773480785386e-06, w1=-0.055692538477308436\n",
      "Regularized Logistic Regression(51/299): loss=0.5666285889925807, w0=-4.7434621147724305e-06, w1=-0.056348511541178384\n",
      "Regularized Logistic Regression(52/299): loss=0.5661533838106185, w0=-4.822735937096437e-06, w1=-0.056995003009519755\n",
      "Regularized Logistic Regression(53/299): loss=0.5656984365672415, w0=-4.9015968163999835e-06, w1=-0.05763223610306638\n",
      "Regularized Logistic Regression(54/299): loss=0.5652627100315837, w0=-4.980046627144183e-06, w1=-0.05826042665157634\n",
      "Regularized Logistic Regression(55/299): loss=0.5648452269421471, w0=-5.058087249197425e-06, w1=-0.05887978340526317\n",
      "Regularized Logistic Regression(56/299): loss=0.5644450662034921, w0=-5.135720567398393e-06, w1=-0.05949050833025401\n",
      "Regularized Logistic Regression(57/299): loss=0.5640613593440864, w0=-5.2129484711001784e-06, w1=-0.06009279688903919\n",
      "Regularized Logistic Regression(58/299): loss=0.5636932872160929, w0=-5.2897728537013215e-06, w1=-0.060686838306811985\n",
      "Regularized Logistic Regression(59/299): loss=0.5633400769193947, w0=-5.3661956121687545e-06, w1=-0.06127281582453406\n",
      "Regularized Logistic Regression(60/299): loss=0.5630009989334952, w0=-5.442218646556869e-06, w1=-0.06185090693950737\n",
      "Regularized Logistic Regression(61/299): loss=0.5626753644422269, w0=-5.517843859526287e-06, w1=-0.06242128363417903\n",
      "Regularized Logistic Regression(62/299): loss=0.5623625228373325, w0=-5.593073155865332e-06, w1=-0.06298411259385796\n",
      "Regularized Logistic Regression(63/299): loss=0.5620618593880716, w0=-5.667908442016728e-06, w1=-0.06353955541397716\n",
      "Regularized Logistic Regression(64/299): loss=0.5617727930649736, w0=-5.7423516256115904e-06, w1=-0.06408776879749409\n",
      "Regularized Logistic Regression(65/299): loss=0.561494774506774, w0=-5.816404615012436e-06, w1=-0.06462890474298255\n",
      "Regularized Logistic Regression(66/299): loss=0.5612272841203892, w0=-5.890069318866606e-06, w1=-0.065163110723934\n",
      "Regularized Logistic Regression(67/299): loss=0.5609698303045612, w0=-5.963347645671194e-06, w1=-0.06569052985975349\n",
      "Regularized Logistic Regression(68/299): loss=0.5607219477885043, w0=-6.036241503350381e-06, w1=-0.06621130107890426\n",
      "Regularized Logistic Regression(69/299): loss=0.5604831960775296, w0=-6.108752798845816e-06, w1=-0.06672555927462659\n",
      "Regularized Logistic Regression(70/299): loss=0.5602531579982334, w0=-6.180883437720544e-06, w1=-0.06723343545363039\n",
      "Regularized Logistic Regression(71/299): loss=0.5600314383363724, w0=-6.252635323776823e-06, w1=-0.06773505687813494\n",
      "Regularized Logistic Regression(72/299): loss=0.5598176625610617, w0=-6.324010358688019e-06, w1=-0.06823054720160869\n",
      "Regularized Logistic Regression(73/299): loss=0.5596114756294046, w0=-6.395010441644694e-06, w1=-0.06872002659853826\n",
      "Regularized Logistic Regression(74/299): loss=0.5594125408660835, w0=-6.465637469014874e-06, w1=-0.06920361188853691\n",
      "Regularized Logistic Regression(75/299): loss=0.5592205389128491, w0=-6.535893334018425e-06, w1=-0.06968141665508498\n",
      "Regularized Logistic Regression(76/299): loss=0.5590351667432055, w0=-6.605779926415406e-06, w1=-0.07015355135917542\n",
      "Regularized Logistic Regression(77/299): loss=0.5588561367379328, w0=-6.675299132208181e-06, w1=-0.07062012344812373\n",
      "Regularized Logistic Regression(78/299): loss=0.5586831758173929, w0=-6.744452833357067e-06, w1=-0.07108123745978534\n",
      "Regularized Logistic Regression(79/299): loss=0.5585160246268688, w0=-6.813242907509222e-06, w1=-0.07153699512240845\n",
      "Regularized Logistic Regression(80/299): loss=0.5583544367714353, w0=-6.881671227740484e-06, w1=-0.0719874954503397\n",
      "Regularized Logistic Regression(81/299): loss=0.5581981780971214, w0=-6.949739662309812e-06, w1=-0.07243283483578516\n",
      "Regularized Logistic Regression(82/299): loss=0.5580470260153458, w0=-7.0174500744259865e-06, w1=-0.0728731071368192\n",
      "Regularized Logistic Regression(83/299): loss=0.5579007688678189, w0=-7.08480432202621e-06, w1=-0.07330840376182343\n",
      "Regularized Logistic Regression(84/299): loss=0.5577592053292988, w0=-7.1518042575662385e-06, w1=-0.07373881375052428\n",
      "Regularized Logistic Regression(85/299): loss=0.5576221438457746, w0=-7.218451727821652e-06, w1=-0.07416442385179421\n",
      "Regularized Logistic Regression(86/299): loss=0.5574894021058121, w0=-7.284748573699906e-06, w1=-0.07458531859836669\n",
      "Regularized Logistic Regression(87/299): loss=0.5573608065429542, w0=-7.350696630062767e-06, w1=-0.07500158037861179\n",
      "Regularized Logistic Regression(88/299): loss=0.5572361918672115, w0=-7.416297725558769e-06, w1=-0.07541328950550674\n",
      "Regularized Logistic Regression(89/299): loss=0.5571154006238166, w0=-7.4815536824652955e-06, w1=-0.07582052428293314\n",
      "Regularized Logistic Regression(90/299): loss=0.5569982827775293, w0=-7.546466316539949e-06, w1=-0.07622336106942146\n",
      "Regularized Logistic Regression(91/299): loss=0.5568846953209058, w0=-7.611037436880811e-06, w1=-0.07662187433945986\n",
      "Regularized Logistic Regression(92/299): loss=0.5567745019050413, w0=-7.67526884579527e-06, w1=-0.07701613674247704\n",
      "Regularized Logistic Regression(93/299): loss=0.5566675724914036, w0=-7.739162338677035e-06, w1=-0.07740621915960297\n",
      "Regularized Logistic Regression(94/299): loss=0.556563783023459, w0=-7.802719703891038e-06, w1=-0.0777921907583071\n",
      "Regularized Logistic Regression(95/299): loss=0.5564630151168826, w0=-7.86594272266585e-06, w1=-0.07817411904500673\n",
      "Regularized Logistic Regression(96/299): loss=0.5563651557672197, w0=-7.928833168993327e-06, w1=-0.07855206991573478\n",
      "Regularized Logistic Regression(97/299): loss=0.5562700970739484, w0=-7.99139280953516e-06, w1=-0.07892610770495247\n",
      "Regularized Logistic Regression(98/299): loss=0.5561777359799421, w0=-8.053623403536023e-06, w1=-0.07929629523258554\n",
      "Regularized Logistic Regression(99/299): loss=0.5560879740254255, w0=-8.115526702743043e-06, w1=-0.07966269384936024\n",
      "Regularized Logistic Regression(100/299): loss=0.5560007171155391, w0=-8.1771044513313e-06, w1=-0.08002536348051364\n",
      "Regularized Logistic Regression(101/299): loss=0.55591587530072, w0=-8.238358385835096e-06, w1=-0.08038436266794427\n",
      "Regularized Logistic Regression(102/299): loss=0.555833362569129, w0=-8.299290235084716e-06, w1=-0.08073974861086994\n",
      "Regularized Logistic Regression(103/299): loss=0.5557530966504199, w0=-8.359901720148456e-06, w1=-0.0810915772050555\n",
      "Regularized Logistic Regression(104/299): loss=0.5556749988301833, w0=-8.420194554279646e-06, w1=-0.08143990308066815\n",
      "Regularized Logistic Regression(105/299): loss=0.5555989937744422, w0=-8.480170442868461e-06, w1=-0.08178477963881853\n",
      "Regularized Logistic Regression(106/299): loss=0.5555250093636145, w0=-8.539831083398287e-06, w1=-0.08212625908684058\n",
      "Regularized Logistic Regression(107/299): loss=0.5554529765353932, w0=-8.599178165406424e-06, w1=-0.08246439247236066\n",
      "Regularized Logistic Regression(108/299): loss=0.5553828291360318, w0=-8.658213370448926e-06, w1=-0.08279922971620535\n",
      "Regularized Logistic Regression(109/299): loss=0.5553145037795528, w0=-8.716938372069387e-06, w1=-0.0831308196441956\n",
      "Regularized Logistic Regression(110/299): loss=0.5552479397144208, w0=-8.775354835771468e-06, w1=-0.08345921001786907\n",
      "Regularized Logistic Regression(111/299): loss=0.5551830786972626, w0=-8.833464418995e-06, w1=-0.08378444756417461\n",
      "Regularized Logistic Regression(112/299): loss=0.5551198648732293, w0=-8.89126877109549e-06, w1=-0.08410657800417969\n",
      "Regularized Logistic Regression(113/299): loss=0.5550582446626291, w0=-8.94876953332684e-06, w1=-0.08442564608082755\n",
      "Regularized Logistic Regression(114/299): loss=0.5549981666534713, w0=-9.00596833882716e-06, w1=-0.08474169558578284\n",
      "Regularized Logistic Regression(115/299): loss=0.5549395814996012, w0=-9.062866812607497e-06, w1=-0.08505476938539885\n",
      "Regularized Logistic Regression(116/299): loss=0.5548824418241005, w0=-9.119466571543323e-06, w1=-0.08536490944584209\n",
      "Regularized Logistic Regression(117/299): loss=0.5548267021276744, w0=-9.175769224368695e-06, w1=-0.08567215685740413\n",
      "Regularized Logistic Regression(118/299): loss=0.5547723187017365, w0=-9.231776371672888e-06, w1=-0.08597655185803317\n",
      "Regularized Logistic Regression(119/299): loss=0.5547192495459421, w0=-9.287489605899429e-06, w1=-0.08627813385611355\n",
      "Regularized Logistic Regression(120/299): loss=0.5546674542899186, w0=-9.342910511347379e-06, w1=-0.0865769414525219\n",
      "Regularized Logistic Regression(121/299): loss=0.5546168941189688, w0=-9.398040664174765e-06, w1=-0.08687301246198587\n",
      "Regularized Logistic Regression(122/299): loss=0.554567531703525, w0=-9.452881632404038e-06, w1=-0.08716638393377239\n",
      "Regularized Logistic Regression(123/299): loss=0.5545193311321559, w0=-9.507434975929467e-06, w1=-0.08745709217172899\n",
      "Regularized Logistic Regression(124/299): loss=0.5544722578479276, w0=-9.561702246526355e-06, w1=-0.08774517275370233\n",
      "Regularized Logistic Regression(125/299): loss=0.5544262785879446, w0=-9.615684987861986e-06, w1=-0.08803066055035617\n",
      "Regularized Logistic Regression(126/299): loss=0.5543813613258934, w0=-9.669384735508221e-06, w1=-0.08831358974341025\n",
      "Regularized Logistic Regression(127/299): loss=0.5543374752174327, w0=-9.722803016955632e-06, w1=-0.08859399384332162\n",
      "Regularized Logistic Regression(128/299): loss=0.5542945905482769, w0=-9.775941351629122e-06, w1=-0.0888719057064271\n",
      "Regularized Logistic Regression(129/299): loss=0.5542526786848283, w0=-9.82880125090493e-06, w1=-0.08914735755156677\n",
      "Regularized Logistic Regression(130/299): loss=0.5542117120272254, w0=-9.881384218128937e-06, w1=-0.08942038097620636\n",
      "Regularized Logistic Regression(131/299): loss=0.5541716639646774, w0=-9.933691748636243e-06, w1=-0.08969100697207633\n",
      "Regularized Logistic Regression(132/299): loss=0.5541325088329675, w0=-9.98572532977189e-06, w1=-0.08995926594034392\n",
      "Regularized Logistic Regression(133/299): loss=0.5540942218740078, w0=-1.0037486440912708e-05, w1=-0.09022518770633504\n",
      "Regularized Logistic Regression(134/299): loss=0.5540567791973431, w0=-1.0088976553490202e-05, w1=-0.09048880153382063\n",
      "Regularized Logistic Regression(135/299): loss=0.554020157743497, w0=-1.014019713101443e-05, w1=-0.09075013613888354\n",
      "Regularized Logistic Regression(136/299): loss=0.5539843352490694, w0=-1.0191149629098797e-05, w1=-0.09100921970337846\n",
      "Regularized Logistic Regression(137/299): loss=0.5539492902134924, w0=-1.024183549548574e-05, w1=-0.09126607988800069\n",
      "Regularized Logistic Regression(138/299): loss=0.5539150018673592, w0=-1.0292256170073226e-05, w1=-0.09152074384497527\n",
      "Regularized Logistic Regression(139/299): loss=0.5538814501422467, w0=-1.0342413084942019e-05, w1=-0.09177323823038015\n",
      "Regularized Logistic Regression(140/299): loss=0.5538486156419541, w0=-1.0392307664383692e-05, w1=-0.09202358921611482\n",
      "Regularized Logistic Regression(141/299): loss=0.5538164796150855, w0=-1.04419413249293e-05, w1=-0.09227182250152693\n",
      "Regularized Logistic Regression(142/299): loss=0.5537850239289097, w0=-1.0491315475378707e-05, w1=-0.09251796332470762\n",
      "Regularized Logistic Regression(143/299): loss=0.5537542310444297, w0=-1.0540431516830498e-05, w1=-0.09276203647346637\n",
      "Regularized Logistic Regression(144/299): loss=0.5537240839926028, w0=-1.058929084271247e-05, w1=-0.09300406629599656\n",
      "Regularized Logistic Regression(145/299): loss=0.5536945663516543, w0=-1.0637894838812629e-05, w1=-0.09324407671124056\n",
      "Regularized Logistic Regression(146/299): loss=0.5536656622254288, w0=-1.0686244883310677e-05, w1=-0.09348209121896525\n",
      "Regularized Logistic Regression(147/299): loss=0.5536373562227249, w0=-1.0734342346809968e-05, w1=-0.09371813290955627\n",
      "Regularized Logistic Regression(148/299): loss=0.5536096334375702, w0=-1.0782188592369866e-05, w1=-0.09395222447354086\n",
      "Regularized Logistic Regression(149/299): loss=0.5535824794303839, w0=-1.0829784975538512e-05, w1=-0.09418438821084701\n",
      "Regularized Logistic Regression(150/299): loss=0.5535558802099865, w0=-1.0877132844385943e-05, w1=-0.09441464603980852\n",
      "Regularized Logistic Regression(151/299): loss=0.5535298222164161, w0=-1.0924233539537551e-05, w1=-0.0946430195059224\n",
      "Regularized Logistic Regression(152/299): loss=0.5535042923045058, w0=-1.0971088394207858e-05, w1=-0.09486952979036747\n",
      "Regularized Logistic Regression(153/299): loss=0.5534792777281926, w0=-1.1017698734234558e-05, w1=-0.09509419771829175\n",
      "Regularized Logistic Regression(154/299): loss=0.5534547661255165, w0=-1.1064065878112845e-05, w1=-0.09531704376687478\n",
      "Regularized Logistic Regression(155/299): loss=0.5534307455042781, w0=-1.1110191137029954e-05, w1=-0.09553808807317259\n",
      "Regularized Logistic Regression(156/299): loss=0.5534072042283242, w0=-1.1156075814899932e-05, w1=-0.09575735044175243\n",
      "Regularized Logistic Regression(157/299): loss=0.5533841310044285, w0=-1.1201721208398607e-05, w1=-0.09597485035212222\n",
      "Regularized Logistic Regression(158/299): loss=0.5533615148697405, w0=-1.1247128606998724e-05, w1=-0.09619060696596257\n",
      "Regularized Logistic Regression(159/299): loss=0.5533393451797747, w0=-1.1292299293005247e-05, w1=-0.09640463913416653\n",
      "Regularized Logistic Regression(160/299): loss=0.5533176115969152, w0=-1.1337234541590796e-05, w1=-0.09661696540369287\n",
      "Regularized Logistic Regression(161/299): loss=0.5532963040794101, w0=-1.1381935620831217e-05, w1=-0.09682760402423891\n",
      "Regularized Logistic Regression(162/299): loss=0.5532754128708319, w0=-1.1426403791741241e-05, w1=-0.09703657295473803\n",
      "Regularized Logistic Regression(163/299): loss=0.5532549284899845, w0=-1.1470640308310256e-05, w1=-0.09724388986968699\n",
      "Regularized Logistic Regression(164/299): loss=0.5532348417212335, w0=-1.1514646417538145e-05, w1=-0.09744957216530857\n",
      "Regularized Logistic Regression(165/299): loss=0.5532151436052389, w0=-1.1558423359471183e-05, w1=-0.09765363696555386\n",
      "Regularized Logistic Regression(166/299): loss=0.5531958254300767, w0=-1.1601972367238007e-05, w1=-0.09785610112794929\n",
      "Regularized Logistic Regression(167/299): loss=0.5531768787227233, w0=-1.1645294667085596e-05, w1=-0.09805698124929281\n",
      "Regularized Logistic Regression(168/299): loss=0.5531582952408931, w0=-1.1688391478415296e-05, w1=-0.09825629367120374\n",
      "Regularized Logistic Regression(169/299): loss=0.5531400669652099, w0=-1.1731264013818862e-05, w1=-0.09845405448553043\n",
      "Regularized Logistic Regression(170/299): loss=0.5531221860916948, w0=-1.1773913479114486e-05, w1=-0.09865027953962005\n",
      "Regularized Logistic Regression(171/299): loss=0.5531046450245607, w0=-1.1816341073382845e-05, w1=-0.09884498444145419\n",
      "Regularized Logistic Regression(172/299): loss=0.5530874363692945, w0=-1.1858547989003108e-05, w1=-0.09903818456465445\n",
      "Regularized Logistic Regression(173/299): loss=0.5530705529260183, w0=-1.1900535411688947e-05, w1=-0.09922989505336134\n",
      "Regularized Logistic Regression(174/299): loss=0.5530539876831129, w0=-1.1942304520524486e-05, w1=-0.09942013082699093\n",
      "Regularized Logistic Regression(175/299): loss=0.5530377338110962, w0=-1.198385648800023e-05, w1=-0.09960890658487156\n",
      "Regularized Logistic Regression(176/299): loss=0.5530217846567393, w0=-1.2025192480048938e-05, w1=-0.09979623681076524\n",
      "Regularized Logistic Regression(177/299): loss=0.5530061337374153, w0=-1.2066313656081437e-05, w1=-0.09998213577727581\n",
      "Regularized Logistic Regression(178/299): loss=0.5529907747356673, w0=-1.2107221169022376e-05, w1=-0.10016661755014818\n",
      "Regularized Logistic Regression(179/299): loss=0.5529757014939864, w0=-1.2147916165345912e-05, w1=-0.10034969599246067\n",
      "Regularized Logistic Regression(180/299): loss=0.5529609080097906, w0=-1.2188399785111314e-05, w1=-0.1005313847687146\n",
      "Regularized Logistic Regression(181/299): loss=0.5529463884305948, w0=-1.222867316199849e-05, w1=-0.10071169734882242\n",
      "Regularized Logistic Regression(182/299): loss=0.5529321370493665, w0=-1.226873742334342e-05, w1=-0.1008906470119992\n",
      "Regularized Logistic Regression(183/299): loss=0.5529181483000545, w0=-1.2308593690173501e-05, w1=-0.10106824685055804\n",
      "Regularized Logistic Regression(184/299): loss=0.5529044167532865, w0=-1.2348243077242797e-05, w1=-0.10124450977361395\n",
      "Regularized Logistic Regression(185/299): loss=0.552890937112228, w0=-1.2387686693067176e-05, w1=-0.10141944851069724\n",
      "Regularized Logistic Regression(186/299): loss=0.5528777042085938, w0=-1.2426925639959343e-05, w1=-0.10159307561527982\n",
      "Regularized Logistic Regression(187/299): loss=0.552864712998806, w0=-1.2465961014063766e-05, w1=-0.10176540346821647\n",
      "Regularized Logistic Regression(188/299): loss=0.552851958560295, w0=-1.2504793905391485e-05, w1=-0.10193644428110331\n",
      "Regularized Logistic Regression(189/299): loss=0.5528394360879314, w0=-1.254342539785479e-05, w1=-0.10210621009955592\n",
      "Regularized Logistic Regression(190/299): loss=0.5528271408905922, w0=-1.2581856569301795e-05, w1=-0.10227471280640917\n",
      "Regularized Logistic Regression(191/299): loss=0.5528150683878448, w0=-1.2620088491550865e-05, w1=-0.1024419641248409\n",
      "Regularized Logistic Regression(192/299): loss=0.5528032141067536, w0=-1.2658122230424932e-05, w1=-0.10260797562142143\n",
      "Regularized Logistic Regression(193/299): loss=0.5527915736787977, w0=-1.2695958845785666e-05, w1=-0.10277275870909114\n",
      "Regularized Logistic Regression(194/299): loss=0.552780142836899, w0=-1.2733599391567509e-05, w1=-0.10293632465006732\n",
      "Regularized Logistic Regression(195/299): loss=0.5527689174125516, w0=-1.2771044915811581e-05, w1=-0.10309868455868366\n",
      "Regularized Logistic Regression(196/299): loss=0.5527578933330553, w0=-1.280829646069944e-05, w1=-0.10325984940416225\n",
      "Regularized Logistic Regression(197/299): loss=0.5527470666188404, w0=-1.2845355062586693e-05, w1=-0.10341983001332146\n",
      "Regularized Logistic Regression(198/299): loss=0.5527364333808878, w0=-1.2882221752036479e-05, w1=-0.10357863707322061\n",
      "Regularized Logistic Regression(199/299): loss=0.5527259898182354, w0=-1.2918897553852782e-05, w1=-0.10373628113374338\n",
      "Regularized Logistic Regression(200/299): loss=0.5527157322155685, w0=-1.2955383487113611e-05, w1=-0.10389277261012092\n",
      "Regularized Logistic Regression(201/299): loss=0.5527056569408944, w0=-1.299168056520403e-05, w1=-0.10404812178539795\n",
      "Regularized Logistic Regression(202/299): loss=0.5526957604432899, w0=-1.3027789795849024e-05, w1=-0.10420233881284083\n",
      "Regularized Logistic Regression(203/299): loss=0.5526860392507298, w0=-1.306371218114622e-05, w1=-0.10435543371829122\n",
      "Regularized Logistic Regression(204/299): loss=0.5526764899679804, w0=-1.3099448717598454e-05, w1=-0.10450741640246615\n",
      "Regularized Logistic Regression(205/299): loss=0.5526671092745686, w0=-1.3135000396146175e-05, w1=-0.1046582966432051\n",
      "Regularized Logistic Regression(206/299): loss=0.5526578939228114, w0=-1.3170368202199695e-05, w1=-0.10480808409766645\n",
      "Regularized Logistic Regression(207/299): loss=0.5526488407359139, w0=-1.3205553115671284e-05, w1=-0.10495678830447429\n",
      "Regularized Logistic Regression(208/299): loss=0.5526399466061248, w0=-1.3240556111007098e-05, w1=-0.10510441868581674\n",
      "Regularized Logistic Regression(209/299): loss=0.5526312084929532, w0=-1.3275378157218952e-05, w1=-0.10525098454949688\n",
      "Regularized Logistic Regression(210/299): loss=0.5526226234214415, w0=-1.331002021791593e-05, w1=-0.10539649509093817\n",
      "Regularized Logistic Regression(211/299): loss=0.5526141884804906, w0=-1.3344483251335834e-05, w1=-0.10554095939514466\n",
      "Regularized Logistic Regression(212/299): loss=0.5526059008212415, w0=-1.3378768210376466e-05, w1=-0.10568438643861773\n",
      "Regularized Logistic Regression(213/299): loss=0.5525977576555032, w0=-1.3412876042626756e-05, w1=-0.10582678509123025\n",
      "Regularized Logistic Regression(214/299): loss=0.5525897562542319, w0=-1.3446807690397718e-05, w1=-0.10596816411805945\n",
      "Regularized Logistic Regression(215/299): loss=0.5525818939460555, w0=-1.3480564090753244e-05, w1=-0.10610853218117888\n",
      "Regularized Logistic Regression(216/299): loss=0.5525741681158434, w0=-1.3514146175540735e-05, w1=-0.10624789784141168\n",
      "Regularized Logistic Regression(217/299): loss=0.552566576203319, w0=-1.3547554871421568e-05, w1=-0.10638626956004493\n",
      "Regularized Logistic Regression(218/299): loss=0.5525591157017158, w0=-1.3580791099901398e-05, w1=-0.10652365570050644\n",
      "Regularized Logistic Regression(219/299): loss=0.5525517841564717, w0=-1.3613855777360288e-05, w1=-0.10666006453000576\n",
      "Regularized Logistic Regression(220/299): loss=0.5525445791639616, w0=-1.3646749815082688e-05, w1=-0.1067955042211387\n",
      "Regularized Logistic Regression(221/299): loss=0.5525374983702706, w0=-1.3679474119287236e-05, w1=-0.1069299828534577\n",
      "Regularized Logistic Regression(222/299): loss=0.5525305394699992, w0=-1.3712029591156396e-05, w1=-0.10706350841500822\n",
      "Regularized Logistic Regression(223/299): loss=0.5525237002051057, w0=-1.3744417126865937e-05, w1=-0.107196088803832\n",
      "Regularized Logistic Regression(224/299): loss=0.5525169783637809, w0=-1.3776637617614244e-05, w1=-0.10732773182943812\n",
      "Regularized Logistic Regression(225/299): loss=0.5525103717793561, w0=-1.3808691949651457e-05, w1=-0.10745844521424326\n",
      "Regularized Logistic Regression(226/299): loss=0.552503878329242, w0=-1.3840581004308456e-05, w1=-0.10758823659498025\n",
      "Regularized Logistic Regression(227/299): loss=0.5524974959338961, w0=-1.3872305658025673e-05, w1=-0.10771711352407748\n",
      "Regularized Logistic Regression(228/299): loss=0.5524912225558221, w0=-1.3903866782381748e-05, w1=-0.1078450834710092\n",
      "Regularized Logistic Regression(229/299): loss=0.552485056198594, w0=-1.3935265244122005e-05, w1=-0.10797215382361676\n",
      "Regularized Logistic Regression(230/299): loss=0.5524789949059106, w0=-1.3966501905186787e-05, w1=-0.10809833188940302\n",
      "Regularized Logistic Regression(231/299): loss=0.5524730367606738, w0=-1.3997577622739609e-05, w1=-0.10822362489679871\n",
      "Regularized Logistic Regression(232/299): loss=0.5524671798840922, w0=-1.4028493249195149e-05, w1=-0.10834803999640327\n",
      "Regularized Logistic Regression(233/299): loss=0.5524614224348119, w0=-1.4059249632247088e-05, w1=-0.1084715842621992\n",
      "Regularized Logistic Regression(234/299): loss=0.5524557626080664, w0=-1.408984761489578e-05, w1=-0.10859426469274187\n",
      "Regularized Logistic Regression(235/299): loss=0.5524501986348545, w0=-1.4120288035475756e-05, w1=-0.10871608821232426\n",
      "Regularized Logistic Regression(236/299): loss=0.5524447287811353, w0=-1.4150571727683075e-05, w1=-0.10883706167211787\n",
      "Regularized Logistic Regression(237/299): loss=0.5524393513470485, w0=-1.4180699520602504e-05, w1=-0.1089571918512906\n",
      "Regularized Logistic Regression(238/299): loss=0.5524340646661534, w0=-1.4210672238734553e-05, w1=-0.10907648545810135\n",
      "Regularized Logistic Regression(239/299): loss=0.5524288671046875, w0=-1.4240490702022328e-05, w1=-0.10919494913097279\n",
      "Regularized Logistic Regression(240/299): loss=0.5524237570608478, w0=-1.4270155725878245e-05, w1=-0.10931258943954202\n",
      "Regularized Logistic Regression(241/299): loss=0.552418732964085, w0=-1.4299668121210573e-05, w1=-0.10942941288568982\n",
      "Regularized Logistic Regression(242/299): loss=0.5524137932744214, w0=-1.4329028694449822e-05, w1=-0.10954542590454999\n",
      "Regularized Logistic Regression(243/299): loss=0.5524089364817845, w0=-1.4358238247574977e-05, w1=-0.10966063486549756\n",
      "Regularized Logistic Regression(244/299): loss=0.5524041611053557, w0=-1.4387297578139567e-05, w1=-0.10977504607311772\n",
      "Regularized Logistic Regression(245/299): loss=0.5523994656929393, w0=-1.4416207479297581e-05, w1=-0.10988866576815544\n",
      "Regularized Logistic Regression(246/299): loss=0.5523948488203432, w0=-1.4444968739829234e-05, w1=-0.11000150012844609\n",
      "Regularized Logistic Regression(247/299): loss=0.5523903090907789, w0=-1.4473582144166565e-05, w1=-0.11011355526982763\n",
      "Regularized Logistic Regression(248/299): loss=0.552385845134275, w0=-1.4502048472418893e-05, w1=-0.11022483724703516\n",
      "Regularized Logistic Regression(249/299): loss=0.5523814556071043, w0=-1.4530368500398113e-05, w1=-0.11033535205457738\n",
      "Regularized Logistic Regression(250/299): loss=0.5523771391912266, w0=-1.4558542999643838e-05, w1=-0.11044510562759659\n",
      "Regularized Logistic Regression(251/299): loss=0.5523728945937445, w0=-1.458657273744839e-05, w1=-0.11055410384271147\n",
      "Regularized Logistic Regression(252/299): loss=0.5523687205463732, w0=-1.461445847688164e-05, w1=-0.11066235251884328\n",
      "Regularized Logistic Regression(253/299): loss=0.5523646158049221, w0=-1.4642200976815695e-05, w1=-0.1107698574180268\n",
      "Regularized Logistic Regression(254/299): loss=0.5523605791487898, w0=-1.4669800991949432e-05, w1=-0.11087662424620527\n",
      "Regularized Logistic Regression(255/299): loss=0.5523566093804709, w0=-1.4697259272832895e-05, w1=-0.11098265865400986\n",
      "Regularized Logistic Regression(256/299): loss=0.552352705325075, w0=-1.4724576565891525e-05, w1=-0.11108796623752452\n",
      "Regularized Logistic Regression(257/299): loss=0.5523488658298569, w0=-1.4751753613450252e-05, w1=-0.11119255253903647\n",
      "Regularized Logistic Regression(258/299): loss=0.5523450897637582, w0=-1.4778791153757443e-05, w1=-0.11129642304777196\n",
      "Regularized Logistic Regression(259/299): loss=0.5523413760169593, w0=-1.4805689921008686e-05, w1=-0.11139958320061846\n",
      "Regularized Logistic Regression(260/299): loss=0.5523377235004423, w0=-1.4832450645370458e-05, w1=-0.11150203838283329\n",
      "Regularized Logistic Regression(261/299): loss=0.5523341311455646, w0=-1.4859074053003612e-05, w1=-0.11160379392873869\n",
      "Regularized Logistic Regression(262/299): loss=0.5523305979036408, w0=-1.488556086608675e-05, w1=-0.11170485512240419\n",
      "Regularized Logistic Regression(263/299): loss=0.5523271227455371, w0=-1.4911911802839432e-05, w1=-0.11180522719831622\n",
      "Regularized Logistic Regression(264/299): loss=0.5523237046612715, w0=-1.4938127577545255e-05, w1=-0.11190491534203552\n",
      "Regularized Logistic Regression(265/299): loss=0.5523203426596269, w0=-1.496420890057478e-05, w1=-0.11200392469084185\n",
      "Regularized Logistic Regression(266/299): loss=0.55231703576777, w0=-1.4990156478408329e-05, w1=-0.11210226033436743\n",
      "Regularized Logistic Regression(267/299): loss=0.5523137830308812, w0=-1.5015971013658622e-05, w1=-0.11219992731521856\n",
      "Regularized Logistic Regression(268/299): loss=0.5523105835117912, w0=-1.50416532050933e-05, w1=-0.11229693062958616\n",
      "Regularized Logistic Regression(269/299): loss=0.5523074362906267, w0=-1.506720374765729e-05, w1=-0.1123932752278446\n",
      "Regularized Logistic Regression(270/299): loss=0.5523043404644643, w0=-1.5092623332495038e-05, w1=-0.11248896601514059\n",
      "Regularized Logistic Regression(271/299): loss=0.5523012951469913, w0=-1.5117912646972599e-05, w1=-0.11258400785197067\n",
      "Regularized Logistic Regression(272/299): loss=0.5522982994681751, w0=-1.5143072374699604e-05, w1=-0.1126784055547491\n",
      "Regularized Logistic Regression(273/299): loss=0.5522953525739388, w0=-1.5168103195551077e-05, w1=-0.1127721638963649\n",
      "Regularized Logistic Regression(274/299): loss=0.5522924536258453, w0=-1.5193005785689124e-05, w1=-0.1128652876067295\n",
      "Regularized Logistic Regression(275/299): loss=0.5522896018007879, w0=-1.5217780817584484e-05, w1=-0.11295778137331464\n",
      "Regularized Logistic Regression(276/299): loss=0.5522867962906863, w0=-1.5242428960037949e-05, w1=-0.1130496498416805\n",
      "Regularized Logistic Regression(277/299): loss=0.5522840363021919, w0=-1.5266950878201654e-05, w1=-0.11314089761599501\n",
      "Regularized Logistic Regression(278/299): loss=0.5522813210563977, w0=-1.5291347233600224e-05, w1=-0.11323152925954351\n",
      "Regularized Logistic Regression(279/299): loss=0.5522786497885551, w0=-1.531561868415181e-05, w1=-0.11332154929523026\n",
      "Regularized Logistic Regression(280/299): loss=0.5522760217477961, w0=-1.533976588418897e-05, w1=-0.11341096220607079\n",
      "Regularized Logistic Regression(281/299): loss=0.5522734361968632, w0=-1.5363789484479434e-05, w1=-0.11349977243567576\n",
      "Regularized Logistic Regression(282/299): loss=0.5522708924118437, w0=-1.5387690132246747e-05, w1=-0.11358798438872678\n",
      "Regularized Logistic Regression(283/299): loss=0.5522683896819097, w0=-1.541146847119077e-05, w1=-0.1136756024314441\n",
      "Regularized Logistic Regression(284/299): loss=0.5522659273090645, w0=-1.5435125141508054e-05, w1=-0.11376263089204594\n",
      "Regularized Logistic Regression(285/299): loss=0.5522635046078941, w0=-1.5458660779912107e-05, w1=-0.11384907406120054\n",
      "Regularized Logistic Regression(286/299): loss=0.5522611209053234, w0=-1.5482076019653505e-05, w1=-0.11393493619247026\n",
      "Regularized Logistic Regression(287/299): loss=0.5522587755403787, w0=-1.550537149053991e-05, w1=-0.11402022150274842\n",
      "Regularized Logistic Regression(288/299): loss=0.552256467863954, w0=-1.5528547818955934e-05, w1=-0.11410493417268873\n",
      "Regularized Logistic Regression(289/299): loss=0.5522541972385828, w0=-1.55516056278829e-05, w1=-0.11418907834712783\n",
      "Regularized Logistic Regression(290/299): loss=0.552251963038216, w0=-1.5574545536918482e-05, w1=-0.11427265813550024\n",
      "Regularized Logistic Regression(291/299): loss=0.5522497646480014, w0=-1.5597368162296202e-05, w1=-0.11435567761224727\n",
      "Regularized Logistic Regression(292/299): loss=0.5522476014640705, w0=-1.5620074116904823e-05, w1=-0.11443814081721874\n",
      "Regularized Logistic Regression(293/299): loss=0.552245472893329, w0=-1.5642664010307625e-05, w1=-0.11452005175606803\n",
      "Regularized Logistic Regression(294/299): loss=0.5522433783532509, w0=-1.5665138448761546e-05, w1=-0.1146014144006411\n",
      "Regularized Logistic Regression(295/299): loss=0.5522413172716785, w0=-1.5687498035236215e-05, w1=-0.11468223268935905\n",
      "Regularized Logistic Regression(296/299): loss=0.5522392890866238, w0=-1.570974336943287e-05, w1=-0.11476251052759445\n",
      "Regularized Logistic Regression(297/299): loss=0.5522372932460777, w0=-1.573187504780314e-05, w1=-0.11484225178804167\n",
      "Regularized Logistic Regression(298/299): loss=0.55223532920782, w0=-1.5753893663567745e-05, w1=-0.11492146031108152\n",
      "Regularized Logistic Regression(299/299): loss=0.552233396439234, w0=-1.5775799806735036e-05, w1=-0.11500013990513978\n",
      "Regularized Logistic Regression(0/99): loss=0.693147180559945, w0=-1.0441465146389343e-07, w1=-0.0018294491082988765\n",
      "Regularized Logistic Regression(1/99): loss=0.6855109278971108, w0=-2.0344820679313237e-07, w1=-0.003525592705210975\n",
      "Regularized Logistic Regression(2/99): loss=0.6790094636482038, w0=-2.973992347005631e-07, w1=-0.005098920788683085\n",
      "Regularized Logistic Regression(3/99): loss=0.6734722049940254, w0=-3.865462349189329e-07, w1=-0.00655903555223109\n",
      "Regularized Logistic Regression(4/99): loss=0.668754459737611, w0=-4.711494797636086e-07, w1=-0.007914726471614484\n",
      "Regularized Logistic Regression(5/99): loss=0.6647334829006758, w0=-5.514526387148568e-07, w1=-0.009174041140295113\n",
      "Regularized Logistic Regression(6/99): loss=0.6613051257272127, w0=-6.276842122014097e-07, w1=-0.01034435117737273\n",
      "Regularized Logistic Regression(7/99): loss=0.658380993459813, w0=-7.000587986888713e-07, w1=-0.011432412948657019\n",
      "Regularized Logistic Regression(8/99): loss=0.6558860376149422, w0=-7.687782167799227e-07, w1=-0.012444423116756883\n",
      "Regularized Logistic Regression(9/99): loss=0.6537565178715707, w0=-8.340325015656089e-07, w1=-0.013386069209108925\n",
      "Regularized Logistic Regression(10/99): loss=0.6519382775264527, w0=-8.960007920809409e-07, w1=-0.014262575494170114\n",
      "Regularized Logistic Regression(11/99): loss=0.6503852844834055, w0=-9.548521245004876e-07, w1=-0.015078744508214167\n",
      "Regularized Logistic Regression(12/99): loss=0.6490583968371794, w0=-1.0107461437031831e-06, w1=-0.015838994594721486\n",
      "Regularized Logistic Regression(13/99): loss=0.6479243182907957, w0=-1.0638337440542881e-06, w1=-0.01654739381689451\n",
      "Regularized Logistic Regression(14/99): loss=0.646954713968031, w0=-1.1142576486927605e-06, w1=-0.01720769058948778\n",
      "Regularized Logistic Regression(15/99): loss=0.6461254617338402, w0=-1.1621529352605968e-06, w1=-0.01782334135445739\n",
      "Regularized Logistic Regression(16/99): loss=0.6454160180064545, w0=-1.2076475148481932e-06, w1=-0.018397535599615756\n",
      "Regularized Logistic Regression(17/99): loss=0.6448088803252755, w0=-1.250862569935668e-06, w1=-0.01893321849294144\n",
      "Regularized Logistic Regression(18/99): loss=0.6442891317117418, w0=-1.291912956263624e-06, w1=-0.019433111378968373\n",
      "Regularized Logistic Regression(19/99): loss=0.6438440542006197, w0=-1.3309075728482698e-06, w1=-0.01989973035866197\n",
      "Regularized Logistic Regression(20/99): loss=0.6434628008921405, w0=-1.3679497037471788e-06, w1=-0.02033540315090935\n",
      "Regularized Logistic Regression(21/99): loss=0.6431361175377305, w0=-1.4031373346671406e-06, w1=-0.020742284412415206\n",
      "Regularized Logistic Regression(22/99): loss=0.642856106072275, w0=-1.4365634470704348e-06, w1=-0.021122369673482768\n",
      "Regularized Logistic Regression(23/99): loss=0.6426160236852199, w0=-1.4683162920681836e-06, w1=-0.021477508029816766\n",
      "Regularized Logistic Regression(24/99): loss=0.6424101120163701, w0=-1.4984796460787012e-06, w1=-0.021809413715006755\n",
      "Regularized Logistic Regression(25/99): loss=0.6422334518994198, w0=-1.5271330499659707e-06, w1=-0.02211967666458812\n",
      "Regularized Logistic Regression(26/99): loss=0.6420818397820426, w0=-1.554352033150933e-06, w1=-0.022409772170389478\n",
      "Regularized Logistic Regression(27/99): loss=0.641951682546564, w0=-1.580208323999726e-06, w1=-0.022681069713089305\n",
      "Regularized Logistic Regression(28/99): loss=0.6418399079574882, w0=-1.6047700476329381e-06, w1=-0.022934841051376744\n",
      "Regularized Logistic Regression(29/99): loss=0.641743888386166, w0=-1.6281019121637858e-06, w1=-0.02317226763770369\n",
      "Regularized Logistic Regression(30/99): loss=0.6416613758210538, w0=-1.6502653842570522e-06, w1=-0.023394447423183893\n",
      "Regularized Logistic Regression(31/99): loss=0.6415904464747508, w0=-1.6713188548014302e-06, w1=-0.02360240110763889\n",
      "Regularized Logistic Regression(32/99): loss=0.6415294535549896, w0=-1.6913177954029137e-06, w1=-0.023797077884991157\n",
      "Regularized Logistic Regression(33/99): loss=0.6414769869833846, w0=-1.710314906333821e-06, w1=-0.023979360729077807\n",
      "Regularized Logistic Regression(34/99): loss=0.6414318390291269, w0=-1.7283602565090326e-06, w1=-0.024150071260416076\n",
      "Regularized Logistic Regression(35/99): loss=0.6413929749801622, w0=-1.7455014160065144e-06, w1=-0.024309974230424167\n",
      "Regularized Logistic Regression(36/99): loss=0.6413595081060502, w0=-1.7617835816018456e-06, w1=-0.024459781656026486\n",
      "Regularized Logistic Regression(37/99): loss=0.6413306782783439, w0=-1.7772496957451856e-06, w1=-0.02460015663439064\n",
      "Regularized Logistic Regression(38/99): loss=0.6413058337090459, w0=-1.791940559372973e-06, w1=-0.02473171686470995\n",
      "Regularized Logistic Regression(39/99): loss=0.6412844153480873, w0=-1.8058949389148553e-06, w1=-0.02485503790142018\n",
      "Regularized Logistic Regression(40/99): loss=0.6412659435490552, w0=-1.8191496678282952e-06, w1=-0.024970656160979068\n",
      "Regularized Logistic Regression(41/99): loss=0.6412500066703761, w0=-1.831739742968392e-06, w1=-0.025079071702314458\n",
      "Regularized Logistic Regression(42/99): loss=0.6412362513284766, w0=-1.8436984160782726e-06, w1=-0.025180750799235437\n",
      "Regularized Logistic Regression(43/99): loss=0.6412243740613408, w0=-1.8550572806655537e-06, w1=-0.025276128321471362\n",
      "Regularized Logistic Regression(44/99): loss=0.6412141141965455, w0=-1.8658463545125136e-06, w1=-0.02536560993953815\n",
      "Regularized Logistic Regression(45/99): loss=0.6412052477481859, w0=-1.876094158051494e-06, w1=-0.02544957416731269\n",
      "Regularized Logistic Regression(46/99): loss=0.6411975821929324, w0=-1.8858277888224252e-06, w1=-0.02552837425500273\n",
      "Regularized Logistic Regression(47/99): loss=0.6411909519974448, w0=-1.8950729922160446e-06, w1=-0.025602339944121984\n",
      "Regularized Logistic Regression(48/99): loss=0.6411852147881044, w0=-1.9038542286942046e-06, w1=-0.025671779095107646\n",
      "Regularized Logistic Regression(49/99): loss=0.6411802480699849, w0=-1.9121947376674784e-06, w1=-0.02573697919733046\n",
      "Regularized Logistic Regression(50/99): loss=0.6411759464155817, w0=-1.920116598199979e-06, w1=-0.025798208770446453\n",
      "Regularized Logistic Regression(51/99): loss=0.6411722190554249, w0=-1.9276407867017824e-06, w1=-0.025855718665307816\n",
      "Regularized Logistic Regression(52/99): loss=0.6411689878125825, w0=-1.934787231760523e-06, w1=-0.02590974327198851\n",
      "Regularized Logistic Regression(53/99): loss=0.6411661853315138, w0=-1.9415748662555217e-06, w1=-0.025960501641872857\n",
      "Regularized Logistic Regression(54/99): loss=0.6411637535589119, w0=-1.948021676890152e-06, w1=-0.026008198530202455\n",
      "Regularized Logistic Regression(55/99): loss=0.6411616424403408, w0=-1.9541447512709996e-06, w1=-0.02605302536497479\n",
      "Regularized Logistic Regression(56/99): loss=0.6411598088017018, w0=-1.959960322655671e-06, w1=-0.026095161147621133\n",
      "Regularized Logistic Regression(57/99): loss=0.641158215389055, w0=-1.9654838124848254e-06, w1=-0.02613477329047422\n",
      "Regularized Logistic Regression(58/99): loss=0.6411568300441436, w0=-1.9707298708080844e-06, w1=-0.026172018395643027\n",
      "Regularized Logistic Regression(59/99): loss=0.6411556249962368, w0=-1.975712414707913e-06, w1=-0.026207042979562903\n",
      "Regularized Logistic Regression(60/99): loss=0.641154576253704, w0=-1.9804446648203158e-06, w1=-0.026239984147158692\n",
      "Regularized Logistic Regression(61/99): loss=0.6411536630811181, w0=-1.9849391800462316e-06, w1=-0.026270970219263223\n",
      "Regularized Logistic Regression(62/99): loss=0.6411528675497274, w0=-1.9892078905428376e-06, w1=-0.026300121316655406\n",
      "Regularized Logistic Regression(63/99): loss=0.6411521741508809, w0=-1.993262129079518e-06, w1=-0.026327549903831154\n",
      "Regularized Logistic Regression(64/99): loss=0.6411515694634872, w0=-1.9971126608390743e-06, w1=-0.026353361295386256\n",
      "Regularized Logistic Regression(65/99): loss=0.6411510418678601, w0=-2.0007697117407687e-06, w1=-0.026377654127676776\n",
      "Regularized Logistic Regression(66/99): loss=0.6411505812993972, w0=-2.004242995358011e-06, w1=-0.02640052079822499\n",
      "Regularized Logistic Regression(67/99): loss=0.6411501790364793, w0=-2.0075417384999314e-06, w1=-0.026422047875154925\n",
      "Regularized Logistic Regression(68/99): loss=0.6411498275177687, w0=-2.0106747055226715e-06, w1=-0.026442316478775218\n",
      "Regularized Logistic Regression(69/99): loss=0.6411495201847791, w0=-2.0136502214330046e-06, w1=-0.026461402637272338\n",
      "Regularized Logistic Regression(70/99): loss=0.6411492513461712, w0=-2.016476193843822e-06, w1=-0.026479377618330628\n",
      "Regularized Logistic Regression(71/99): loss=0.6411490160607329, w0=-2.019160133838113e-06, w1=-0.02649630823836791\n",
      "Regularized Logistic Regression(72/99): loss=0.6411488100364344, w0=-2.0217091757952945e-06, w1=-0.02651225715094927\n",
      "Regularized Logistic Regression(73/99): loss=0.6411486295433144, w0=-2.0241300962310995e-06, w1=-0.02652728311582993\n",
      "Regularized Logistic Regression(74/99): loss=0.6411484713382777, w0=-2.0264293316997525e-06, w1=-0.026541441249973676\n",
      "Regularized Logistic Regression(75/99): loss=0.6411483326001438, w0=-2.028612995804744e-06, w1=-0.02655478326179723\n",
      "Regularized Logistic Regression(76/99): loss=0.6411482108735304, w0=-2.0306868953622793e-06, w1=-0.026567357669799303\n",
      "Regularized Logistic Regression(77/99): loss=0.6411481040203483, w0=-2.032656545759299e-06, w1=-0.02657921000665211\n",
      "Regularized Logistic Regression(78/99): loss=0.64114801017786, w0=-2.0345271855459276e-06, w1=-0.026590383009755487\n",
      "Regularized Logistic Regression(79/99): loss=0.6411479277223947, w0=-2.0363037903002493e-06, w1=-0.026600916799183225\n",
      "Regularized Logistic Regression(80/99): loss=0.6411478552379444, w0=-2.037991085801454e-06, w1=-0.026610849043883677\n",
      "Regularized Logistic Regression(81/99): loss=0.6411477914889774, w0=-2.0395935605456267e-06, w1=-0.026620215116938985\n",
      "Regularized Logistic Regression(82/99): loss=0.6411477353968824, w0=-2.0411154776367777e-06, w1=-0.02662904824062677\n",
      "Regularized Logistic Regression(83/99): loss=0.6411476860195595, w0=-2.042560886084102e-06, w1=-0.026637379621977753\n",
      "Regularized Logistic Regression(84/99): loss=0.6411476425337219, w0=-2.0439336315349383e-06, w1=-0.026645238579475795\n",
      "Regularized Logistic Regression(85/99): loss=0.6411476042195454, w0=-2.0452373664714553e-06, w1=-0.026652652661496124\n",
      "Regularized Logistic Regression(86/99): loss=0.6411475704473468, w0=-2.0464755598976976e-06, w1=-0.02665964775704306\n",
      "Regularized Logistic Regression(87/99): loss=0.6411475406660154, w0=-2.0476515065423297e-06, w1=-0.026666248199303202\n",
      "Regularized Logistic Regression(88/99): loss=0.6411475143929668, w0=-2.0487683356011594e-06, w1=-0.026672476862497903\n",
      "Regularized Logistic Regression(89/99): loss=0.6411474912054097, w0=-2.049829019042336e-06, w1=-0.026678355252484156\n",
      "Regularized Logistic Regression(90/99): loss=0.6411474707327522, w0=-2.050836379495991e-06, w1=-0.026683903591520904\n",
      "Regularized Logistic Regression(91/99): loss=0.6411474526499971, w0=-2.0517930977490103e-06, w1=-0.02668914089759103\n",
      "Regularized Logistic Regression(92/99): loss=0.6411474366719917, w0=-2.0527017198646134e-06, w1=-0.02669408505864099\n",
      "Regularized Logistic Regression(93/99): loss=0.6411474225484216, w0=-2.0535646639454314e-06, w1=-0.02669875290207493\n",
      "Regularized Logistic Regression(94/99): loss=0.6411474100594484, w0=-2.0543842265578643e-06, w1=-0.02670316025981833\n",
      "Regularized Logistic Regression(95/99): loss=0.6411473990119069, w0=-2.0551625888346036e-06, w1=-0.026707322029243686\n",
      "Regularized Logistic Regression(96/99): loss=0.6411473892359897, w0=-2.0559018222713823e-06, w1=-0.02671125223023016\n",
      "Regularized Logistic Regression(97/99): loss=0.6411473805823513, w0=-2.056603894233212e-06, w1=-0.026714964058613063\n",
      "Regularized Logistic Regression(98/99): loss=0.6411473729195851, w0=-2.0572706731846125e-06, w1=-0.026718469936257688\n",
      "Regularized Logistic Regression(99/99): loss=0.6411473661320144, w0=-2.0579039336576173e-06, w1=-0.02672178155798119\n",
      "Regularized Logistic Regression(0/199): loss=0.693147180559945, w0=-1.0441465146389343e-07, w1=-0.0018294491082988765\n",
      "Regularized Logistic Regression(1/199): loss=0.6855109278971108, w0=-2.0344820679313237e-07, w1=-0.003525592705210975\n",
      "Regularized Logistic Regression(2/199): loss=0.6790094636482038, w0=-2.973992347005631e-07, w1=-0.005098920788683085\n",
      "Regularized Logistic Regression(3/199): loss=0.6734722049940254, w0=-3.865462349189329e-07, w1=-0.00655903555223109\n",
      "Regularized Logistic Regression(4/199): loss=0.668754459737611, w0=-4.711494797636086e-07, w1=-0.007914726471614484\n",
      "Regularized Logistic Regression(5/199): loss=0.6647334829006758, w0=-5.514526387148568e-07, w1=-0.009174041140295113\n",
      "Regularized Logistic Regression(6/199): loss=0.6613051257272127, w0=-6.276842122014097e-07, w1=-0.01034435117737273\n",
      "Regularized Logistic Regression(7/199): loss=0.658380993459813, w0=-7.000587986888713e-07, w1=-0.011432412948657019\n",
      "Regularized Logistic Regression(8/199): loss=0.6558860376149422, w0=-7.687782167799227e-07, w1=-0.012444423116756883\n",
      "Regularized Logistic Regression(9/199): loss=0.6537565178715707, w0=-8.340325015656089e-07, w1=-0.013386069209108925\n",
      "Regularized Logistic Regression(10/199): loss=0.6519382775264527, w0=-8.960007920809409e-07, w1=-0.014262575494170114\n",
      "Regularized Logistic Regression(11/199): loss=0.6503852844834055, w0=-9.548521245004876e-07, w1=-0.015078744508214167\n",
      "Regularized Logistic Regression(12/199): loss=0.6490583968371794, w0=-1.0107461437031831e-06, w1=-0.015838994594721486\n",
      "Regularized Logistic Regression(13/199): loss=0.6479243182907957, w0=-1.0638337440542881e-06, w1=-0.01654739381689451\n",
      "Regularized Logistic Regression(14/199): loss=0.646954713968031, w0=-1.1142576486927605e-06, w1=-0.01720769058948778\n",
      "Regularized Logistic Regression(15/199): loss=0.6461254617338402, w0=-1.1621529352605968e-06, w1=-0.01782334135445739\n",
      "Regularized Logistic Regression(16/199): loss=0.6454160180064545, w0=-1.2076475148481932e-06, w1=-0.018397535599615756\n",
      "Regularized Logistic Regression(17/199): loss=0.6448088803252755, w0=-1.250862569935668e-06, w1=-0.01893321849294144\n",
      "Regularized Logistic Regression(18/199): loss=0.6442891317117418, w0=-1.291912956263624e-06, w1=-0.019433111378968373\n",
      "Regularized Logistic Regression(19/199): loss=0.6438440542006197, w0=-1.3309075728482698e-06, w1=-0.01989973035866197\n",
      "Regularized Logistic Regression(20/199): loss=0.6434628008921405, w0=-1.3679497037471788e-06, w1=-0.02033540315090935\n",
      "Regularized Logistic Regression(21/199): loss=0.6431361175377305, w0=-1.4031373346671406e-06, w1=-0.020742284412415206\n",
      "Regularized Logistic Regression(22/199): loss=0.642856106072275, w0=-1.4365634470704348e-06, w1=-0.021122369673482768\n",
      "Regularized Logistic Regression(23/199): loss=0.6426160236852199, w0=-1.4683162920681836e-06, w1=-0.021477508029816766\n",
      "Regularized Logistic Regression(24/199): loss=0.6424101120163701, w0=-1.4984796460787012e-06, w1=-0.021809413715006755\n",
      "Regularized Logistic Regression(25/199): loss=0.6422334518994198, w0=-1.5271330499659707e-06, w1=-0.02211967666458812\n",
      "Regularized Logistic Regression(26/199): loss=0.6420818397820426, w0=-1.554352033150933e-06, w1=-0.022409772170389478\n",
      "Regularized Logistic Regression(27/199): loss=0.641951682546564, w0=-1.580208323999726e-06, w1=-0.022681069713089305\n",
      "Regularized Logistic Regression(28/199): loss=0.6418399079574882, w0=-1.6047700476329381e-06, w1=-0.022934841051376744\n",
      "Regularized Logistic Regression(29/199): loss=0.641743888386166, w0=-1.6281019121637858e-06, w1=-0.02317226763770369\n",
      "Regularized Logistic Regression(30/199): loss=0.6416613758210538, w0=-1.6502653842570522e-06, w1=-0.023394447423183893\n",
      "Regularized Logistic Regression(31/199): loss=0.6415904464747508, w0=-1.6713188548014302e-06, w1=-0.02360240110763889\n",
      "Regularized Logistic Regression(32/199): loss=0.6415294535549896, w0=-1.6913177954029137e-06, w1=-0.023797077884991157\n",
      "Regularized Logistic Regression(33/199): loss=0.6414769869833846, w0=-1.710314906333821e-06, w1=-0.023979360729077807\n",
      "Regularized Logistic Regression(34/199): loss=0.6414318390291269, w0=-1.7283602565090326e-06, w1=-0.024150071260416076\n",
      "Regularized Logistic Regression(35/199): loss=0.6413929749801622, w0=-1.7455014160065144e-06, w1=-0.024309974230424167\n",
      "Regularized Logistic Regression(36/199): loss=0.6413595081060502, w0=-1.7617835816018456e-06, w1=-0.024459781656026486\n",
      "Regularized Logistic Regression(37/199): loss=0.6413306782783439, w0=-1.7772496957451856e-06, w1=-0.02460015663439064\n",
      "Regularized Logistic Regression(38/199): loss=0.6413058337090459, w0=-1.791940559372973e-06, w1=-0.02473171686470995\n",
      "Regularized Logistic Regression(39/199): loss=0.6412844153480873, w0=-1.8058949389148553e-06, w1=-0.02485503790142018\n",
      "Regularized Logistic Regression(40/199): loss=0.6412659435490552, w0=-1.8191496678282952e-06, w1=-0.024970656160979068\n",
      "Regularized Logistic Regression(41/199): loss=0.6412500066703761, w0=-1.831739742968392e-06, w1=-0.025079071702314458\n",
      "Regularized Logistic Regression(42/199): loss=0.6412362513284766, w0=-1.8436984160782726e-06, w1=-0.025180750799235437\n",
      "Regularized Logistic Regression(43/199): loss=0.6412243740613408, w0=-1.8550572806655537e-06, w1=-0.025276128321471362\n",
      "Regularized Logistic Regression(44/199): loss=0.6412141141965455, w0=-1.8658463545125136e-06, w1=-0.02536560993953815\n",
      "Regularized Logistic Regression(45/199): loss=0.6412052477481859, w0=-1.876094158051494e-06, w1=-0.02544957416731269\n",
      "Regularized Logistic Regression(46/199): loss=0.6411975821929324, w0=-1.8858277888224252e-06, w1=-0.02552837425500273\n",
      "Regularized Logistic Regression(47/199): loss=0.6411909519974448, w0=-1.8950729922160446e-06, w1=-0.025602339944121984\n",
      "Regularized Logistic Regression(48/199): loss=0.6411852147881044, w0=-1.9038542286942046e-06, w1=-0.025671779095107646\n",
      "Regularized Logistic Regression(49/199): loss=0.6411802480699849, w0=-1.9121947376674784e-06, w1=-0.02573697919733046\n",
      "Regularized Logistic Regression(50/199): loss=0.6411759464155817, w0=-1.920116598199979e-06, w1=-0.025798208770446453\n",
      "Regularized Logistic Regression(51/199): loss=0.6411722190554249, w0=-1.9276407867017824e-06, w1=-0.025855718665307816\n",
      "Regularized Logistic Regression(52/199): loss=0.6411689878125825, w0=-1.934787231760523e-06, w1=-0.02590974327198851\n",
      "Regularized Logistic Regression(53/199): loss=0.6411661853315138, w0=-1.9415748662555217e-06, w1=-0.025960501641872857\n",
      "Regularized Logistic Regression(54/199): loss=0.6411637535589119, w0=-1.948021676890152e-06, w1=-0.026008198530202455\n",
      "Regularized Logistic Regression(55/199): loss=0.6411616424403408, w0=-1.9541447512709996e-06, w1=-0.02605302536497479\n",
      "Regularized Logistic Regression(56/199): loss=0.6411598088017018, w0=-1.959960322655671e-06, w1=-0.026095161147621133\n",
      "Regularized Logistic Regression(57/199): loss=0.641158215389055, w0=-1.9654838124848254e-06, w1=-0.02613477329047422\n",
      "Regularized Logistic Regression(58/199): loss=0.6411568300441436, w0=-1.9707298708080844e-06, w1=-0.026172018395643027\n",
      "Regularized Logistic Regression(59/199): loss=0.6411556249962368, w0=-1.975712414707913e-06, w1=-0.026207042979562903\n",
      "Regularized Logistic Regression(60/199): loss=0.641154576253704, w0=-1.9804446648203158e-06, w1=-0.026239984147158692\n",
      "Regularized Logistic Regression(61/199): loss=0.6411536630811181, w0=-1.9849391800462316e-06, w1=-0.026270970219263223\n",
      "Regularized Logistic Regression(62/199): loss=0.6411528675497274, w0=-1.9892078905428376e-06, w1=-0.026300121316655406\n",
      "Regularized Logistic Regression(63/199): loss=0.6411521741508809, w0=-1.993262129079518e-06, w1=-0.026327549903831154\n",
      "Regularized Logistic Regression(64/199): loss=0.6411515694634872, w0=-1.9971126608390743e-06, w1=-0.026353361295386256\n",
      "Regularized Logistic Regression(65/199): loss=0.6411510418678601, w0=-2.0007697117407687e-06, w1=-0.026377654127676776\n",
      "Regularized Logistic Regression(66/199): loss=0.6411505812993972, w0=-2.004242995358011e-06, w1=-0.02640052079822499\n",
      "Regularized Logistic Regression(67/199): loss=0.6411501790364793, w0=-2.0075417384999314e-06, w1=-0.026422047875154925\n",
      "Regularized Logistic Regression(68/199): loss=0.6411498275177687, w0=-2.0106747055226715e-06, w1=-0.026442316478775218\n",
      "Regularized Logistic Regression(69/199): loss=0.6411495201847791, w0=-2.0136502214330046e-06, w1=-0.026461402637272338\n",
      "Regularized Logistic Regression(70/199): loss=0.6411492513461712, w0=-2.016476193843822e-06, w1=-0.026479377618330628\n",
      "Regularized Logistic Regression(71/199): loss=0.6411490160607329, w0=-2.019160133838113e-06, w1=-0.02649630823836791\n",
      "Regularized Logistic Regression(72/199): loss=0.6411488100364344, w0=-2.0217091757952945e-06, w1=-0.02651225715094927\n",
      "Regularized Logistic Regression(73/199): loss=0.6411486295433144, w0=-2.0241300962310995e-06, w1=-0.02652728311582993\n",
      "Regularized Logistic Regression(74/199): loss=0.6411484713382777, w0=-2.0264293316997525e-06, w1=-0.026541441249973676\n",
      "Regularized Logistic Regression(75/199): loss=0.6411483326001438, w0=-2.028612995804744e-06, w1=-0.02655478326179723\n",
      "Regularized Logistic Regression(76/199): loss=0.6411482108735304, w0=-2.0306868953622793e-06, w1=-0.026567357669799303\n",
      "Regularized Logistic Regression(77/199): loss=0.6411481040203483, w0=-2.032656545759299e-06, w1=-0.02657921000665211\n",
      "Regularized Logistic Regression(78/199): loss=0.64114801017786, w0=-2.0345271855459276e-06, w1=-0.026590383009755487\n",
      "Regularized Logistic Regression(79/199): loss=0.6411479277223947, w0=-2.0363037903002493e-06, w1=-0.026600916799183225\n",
      "Regularized Logistic Regression(80/199): loss=0.6411478552379444, w0=-2.037991085801454e-06, w1=-0.026610849043883677\n",
      "Regularized Logistic Regression(81/199): loss=0.6411477914889774, w0=-2.0395935605456267e-06, w1=-0.026620215116938985\n",
      "Regularized Logistic Regression(82/199): loss=0.6411477353968824, w0=-2.0411154776367777e-06, w1=-0.02662904824062677\n",
      "Regularized Logistic Regression(83/199): loss=0.6411476860195595, w0=-2.042560886084102e-06, w1=-0.026637379621977753\n",
      "Regularized Logistic Regression(84/199): loss=0.6411476425337219, w0=-2.0439336315349383e-06, w1=-0.026645238579475795\n",
      "Regularized Logistic Regression(85/199): loss=0.6411476042195454, w0=-2.0452373664714553e-06, w1=-0.026652652661496124\n",
      "Regularized Logistic Regression(86/199): loss=0.6411475704473468, w0=-2.0464755598976976e-06, w1=-0.02665964775704306\n",
      "Regularized Logistic Regression(87/199): loss=0.6411475406660154, w0=-2.0476515065423297e-06, w1=-0.026666248199303202\n",
      "Regularized Logistic Regression(88/199): loss=0.6411475143929668, w0=-2.0487683356011594e-06, w1=-0.026672476862497903\n",
      "Regularized Logistic Regression(89/199): loss=0.6411474912054097, w0=-2.049829019042336e-06, w1=-0.026678355252484156\n",
      "Regularized Logistic Regression(90/199): loss=0.6411474707327522, w0=-2.050836379495991e-06, w1=-0.026683903591520904\n",
      "Regularized Logistic Regression(91/199): loss=0.6411474526499971, w0=-2.0517930977490103e-06, w1=-0.02668914089759103\n",
      "Regularized Logistic Regression(92/199): loss=0.6411474366719917, w0=-2.0527017198646134e-06, w1=-0.02669408505864099\n",
      "Regularized Logistic Regression(93/199): loss=0.6411474225484216, w0=-2.0535646639454314e-06, w1=-0.02669875290207493\n",
      "Regularized Logistic Regression(94/199): loss=0.6411474100594484, w0=-2.0543842265578643e-06, w1=-0.02670316025981833\n",
      "Regularized Logistic Regression(95/199): loss=0.6411473990119069, w0=-2.0551625888346036e-06, w1=-0.026707322029243686\n",
      "Regularized Logistic Regression(96/199): loss=0.6411473892359897, w0=-2.0559018222713823e-06, w1=-0.02671125223023016\n",
      "Regularized Logistic Regression(97/199): loss=0.6411473805823513, w0=-2.056603894233212e-06, w1=-0.026714964058613063\n",
      "Regularized Logistic Regression(98/199): loss=0.6411473729195851, w0=-2.0572706731846125e-06, w1=-0.026718469936257688\n",
      "Regularized Logistic Regression(99/199): loss=0.6411473661320144, w0=-2.0579039336576173e-06, w1=-0.02672178155798119\n",
      "Regularized Logistic Regression(100/199): loss=0.6411473601177639, w0=-2.058505360970663e-06, w1=-0.026724909935525686\n",
      "Regularized Logistic Regression(101/199): loss=0.6411473547870749, w0=-2.059076555710802e-06, w1=-0.02672786543877579\n",
      "Regularized Logistic Regression(102/199): loss=0.6411473500608291, w0=-2.059619037991081e-06, w1=-0.026730657834399877\n",
      "Regularized Logistic Regression(103/199): loss=0.6411473458692584, w0=-2.0601342514943236e-06, w1=-0.02673329632208141\n",
      "Regularized Logistic Regression(104/199): loss=0.6411473421508135, w0=-2.060623567314002e-06, w1=-0.026735789568496215\n",
      "Regularized Logistic Regression(105/199): loss=0.641147338851174, w0=-2.0610882876023535e-06, w1=-0.026738145739181388\n",
      "Regularized Logistic Regression(106/199): loss=0.6411473359223809, w0=-2.061529649035386e-06, w1=-0.026740372528431554\n",
      "Regularized Logistic Regression(107/199): loss=0.6411473333220735, w0=-2.061948826103941e-06, w1=-0.026742477187348263\n",
      "Regularized Logistic Regression(108/199): loss=0.6411473310128223, w0=-2.0623469342395248e-06, w1=-0.026744466550161508\n",
      "Regularized Logistic Regression(109/199): loss=0.6411473289615435, w0=-2.062725032783183e-06, w1=-0.02674634705893352\n",
      "Regularized Logistic Regression(110/199): loss=0.6411473271389813, w0=-2.063084127805283e-06, w1=-0.02674812478674842\n",
      "Regularized Logistic Regression(111/199): loss=0.6411473255192581, w0=-2.063425174783672e-06, w1=-0.026749805459482495\n",
      "Regularized Logistic Regression(112/199): loss=0.6411473240794753, w0=-2.0637490811473154e-06, w1=-0.026751394476247067\n",
      "Regularized Logistic Regression(113/199): loss=0.6411473227993632, w0=-2.0640567086921516e-06, w1=-0.02675289692858568\n",
      "Regularized Logistic Regression(114/199): loss=0.641147321660974, w0=-2.064348875875576e-06, w1=-0.026754317618506225\n",
      "Regularized Logistic Regression(115/199): loss=0.6411473206484096, w0=-2.06462635999564e-06, w1=-0.026755661075420923\n",
      "Regularized Logistic Regression(116/199): loss=0.6411473197475849, w0=-2.064889899260741e-06, w1=-0.026756931572060737\n",
      "Regularized Logistic Regression(117/199): loss=0.6411473189460141, w0=-2.065140194755312e-06, w1=-0.0267581331394314\n",
      "Regularized Logistic Regression(118/199): loss=0.6411473182326293, w0=-2.0653779123067103e-06, w1=-0.026759269580868866\n",
      "Regularized Logistic Regression(119/199): loss=0.6411473175976137, w0=-2.0656036842582824e-06, w1=-0.026760344485251306\n",
      "Regularized Logistic Regression(120/199): loss=0.6411473170322601, w0=-2.0658181111532976e-06, w1=-0.02676136123941915\n",
      "Regularized Logistic Regression(121/199): loss=0.6411473165288414, w0=-2.066021763334238e-06, w1=-0.02676232303985362\n",
      "Regularized Logistic Regression(122/199): loss=0.641147316080499, w0=-2.0662151824616847e-06, w1=-0.026763232903658257\n",
      "Regularized Logistic Regression(123/199): loss=0.6411473156811448, w0=-2.0663988829568463e-06, w1=-0.02676409367888791\n",
      "Regularized Logistic Regression(124/199): loss=0.6411473153253715, w0=-2.066573353371555e-06, w1=-0.026764908054263768\n",
      "Regularized Logistic Regression(125/199): loss=0.6411473150083764, w0=-2.0667390576893825e-06, w1=-0.026765678568313057\n",
      "Regularized Logistic Regression(126/199): loss=0.6411473147258924, w0=-2.0668964365613277e-06, w1=-0.02676640761796908\n",
      "Regularized Logistic Regression(127/199): loss=0.6411473144741275, w0=-2.0670459084793706e-06, w1=-0.026767097466663524\n",
      "Regularized Logistic Regression(128/199): loss=0.6411473142497114, w0=-2.067187870891007e-06, w1=-0.0267677502519424\n",
      "Regularized Logistic Regression(129/199): loss=0.6411473140496469, w0=-2.067322701257735e-06, w1=-0.026768367992635735\n",
      "Regularized Logistic Regression(130/199): loss=0.6411473138712694, w0=-2.067450758060309e-06, w1=-0.026768952595605595\n",
      "Regularized Logistic Regression(131/199): loss=0.641147313712209, w0=-2.067572381753434e-06, w1=-0.026769505862101146\n",
      "Regularized Logistic Regression(132/199): loss=0.6411473135703567, w0=-2.0676878956724408e-06, w1=-0.026770029493740865\n",
      "Regularized Logistic Regression(133/199): loss=0.6411473134438371, w0=-2.0677976068943617e-06, w1=-0.026770525098148243\n",
      "Regularized Logistic Regression(134/199): loss=0.6411473133309802, w0=-2.0679018070556946e-06, w1=-0.026770994194258938\n",
      "Regularized Logistic Regression(135/199): loss=0.6411473132302996, w0=-2.0680007731290324e-06, w1=-0.02677143821732007\n",
      "Regularized Logistic Regression(136/199): loss=0.6411473131404729, w0=-2.0680947681606286e-06, w1=-0.02677185852360004\n",
      "Regularized Logistic Regression(137/199): loss=0.6411473130603219, w0=-2.068184041970858e-06, w1=-0.02677225639482719\n",
      "Regularized Logistic Regression(138/199): loss=0.6411473129887973, w0=-2.0682688318194483e-06, w1=-0.026772633042370833\n",
      "Regularized Logistic Regression(139/199): loss=0.641147312924965, w0=-2.068349363037242e-06, w1=-0.02677298961118264\n",
      "Regularized Logistic Regression(140/199): loss=0.6411473128679922, w0=-2.0684258496261814e-06, w1=-0.026773327183512084\n",
      "Regularized Logistic Regression(141/199): loss=0.6411473128171377, w0=-2.0684984948291085e-06, w1=-0.02677364678240761\n",
      "Regularized Logistic Regression(142/199): loss=0.6411473127717403, w0=-2.068567491670904e-06, w1=-0.026773949375017935\n",
      "Regularized Logistic Regression(143/199): loss=0.6411473127312112, w0=-2.0686330234724005e-06, w1=-0.026774235875705606\n",
      "Regularized Logistic Regression(144/199): loss=0.6411473126950258, w0=-2.068695264338443e-06, w1=-0.02677450714898076\n",
      "Regularized Logistic Regression(145/199): loss=0.6411473126627155, w0=-2.0687543796213967e-06, w1=-0.02677476401227072\n",
      "Regularized Logistic Regression(146/199): loss=0.6411473126338636, w0=-2.0688105263613386e-06, w1=-0.02677500723852984\n",
      "Regularized Logistic Regression(147/199): loss=0.6411473126080978, w0=-2.068863853704102e-06, w1=-0.026775237558702626\n",
      "Regularized Logistic Regression(148/199): loss=0.6411473125850865, w0=-2.068914503298291e-06, w1=-0.026775455664046287\n",
      "Regularized Logistic Regression(149/199): loss=0.6411473125645336, w0=-2.0689626096723233e-06, w1=-0.02677566220832355\n",
      "Regularized Logistic Regression(150/199): loss=0.6411473125461756, w0=-2.069008300592502e-06, w1=-0.026775857809869893\n",
      "Regularized Logistic Regression(151/199): loss=0.641147312529777, w0=-2.0690516974030787e-06, w1=-0.026776043053545636\n",
      "Regularized Logistic Regression(152/199): loss=0.6411473125151277, w0=-2.069092915349203e-06, w1=-0.02677621849257746\n",
      "Regularized Logistic Regression(153/199): loss=0.64114731250204, w0=-2.069132063883632e-06, w1=-0.026776384650296813\n",
      "Regularized Logistic Regression(154/199): loss=0.641147312490347, w0=-2.069169246958006e-06, w1=-0.02677654202178061\n",
      "Regularized Logistic Regression(155/199): loss=0.6411473124798994, w0=-2.069204563299473e-06, w1=-0.02677669107540015\n",
      "Regularized Logistic Regression(156/199): loss=0.6411473124705643, w0=-2.0692381066733954e-06, w1=-0.026776832254283445\n",
      "Regularized Logistic Regression(157/199): loss=0.6411473124622223, w0=-2.069269966132846e-06, w1=-0.026776965977696052\n",
      "Regularized Logistic Regression(158/199): loss=0.6411473124547681, w0=-2.0693002262555477e-06, w1=-0.02677709264234399\n",
      "Regularized Logistic Regression(159/199): loss=0.6411473124481061, w0=-2.069328967368898e-06, w1=-0.026777212623606113\n",
      "Regularized Logistic Regression(160/199): loss=0.641147312442152, w0=-2.069356265763674e-06, w1=-0.026777326276695487\n",
      "Regularized Logistic Regression(161/199): loss=0.6411473124368305, w0=-2.069382193896985e-06, w1=-0.026777433937758343\n",
      "Regularized Logistic Regression(162/199): loss=0.6411473124320743, w0=-2.069406820585019e-06, w1=-0.026777535924910827\n",
      "Regularized Logistic Regression(163/199): loss=0.6411473124278229, w0=-2.0694302111860928e-06, w1=-0.02677763253921938\n",
      "Regularized Logistic Regression(164/199): loss=0.6411473124240225, w0=-2.069452427774494e-06, w1=-0.026777724065625827\n",
      "Regularized Logistic Regression(165/199): loss=0.6411473124206253, w0=-2.0694735293055827e-06, w1=-0.026777810773821697\n",
      "Regularized Logistic Regression(166/199): loss=0.6411473124175882, w0=-2.0694935717725825e-06, w1=-0.02677789291907535\n",
      "Regularized Logistic Regression(167/199): loss=0.641147312414873, w0=-2.069512608355491e-06, w1=-0.026777970743012223\n",
      "Regularized Logistic Regression(168/199): loss=0.6411473124124456, w0=-2.0695306895624994e-06, w1=-0.026778044474352666\n",
      "Regularized Logistic Regression(169/199): loss=0.6411473124102752, w0=-2.069547863364297e-06, w1=-0.02677811432960928\n",
      "Regularized Logistic Regression(170/199): loss=0.6411473124083347, w0=-2.0695641753216233e-06, w1=-0.02677818051374619\n",
      "Regularized Logistic Regression(171/199): loss=0.6411473124065995, w0=-2.069579668706407e-06, w1=-0.02677824322080153\n",
      "Regularized Logistic Regression(172/199): loss=0.6411473124050479, w0=-2.069594384616807e-06, w1=-0.026778302634476524\n",
      "Regularized Logistic Regression(173/199): loss=0.6411473124036604, w0=-2.0696083620864737e-06, w1=-0.026778358928692115\n",
      "Regularized Logistic Regression(174/199): loss=0.6411473124024197, w0=-2.0696216381883128e-06, w1=-0.026778412268115226\n",
      "Regularized Logistic Regression(175/199): loss=0.6411473124013101, w0=-2.069634248133032e-06, w1=-0.02677846280865582\n",
      "Regularized Logistic Regression(176/199): loss=0.641147312400318, w0=-2.06964622536273e-06, w1=-0.026778510697937765\n",
      "Regularized Logistic Regression(177/199): loss=0.6411473123994307, w0=-2.069657601639784e-06, w1=-0.02677855607574352\n",
      "Regularized Logistic Regression(178/199): loss=0.6411473123986368, w0=-2.0696684071312623e-06, w1=-0.026778599074434212\n",
      "Regularized Logistic Regression(179/199): loss=0.6411473123979268, w0=-2.0696786704890957e-06, w1=-0.026778639819347944\n",
      "Regularized Logistic Regression(180/199): loss=0.6411473123972918, w0=-2.069688418926216e-06, w1=-0.0267786784291761\n",
      "Regularized Logistic Regression(181/199): loss=0.641147312396724, w0=-2.0696976782888684e-06, w1=-0.026778715016318032\n",
      "Regularized Logistic Regression(182/199): loss=0.6411473123962159, w0=-2.0697064731252863e-06, w1=-0.02677874968721868\n",
      "Regularized Logistic Regression(183/199): loss=0.6411473123957616, w0=-2.069714826750913e-06, w1=-0.02677878254268602\n",
      "Regularized Logistic Regression(184/199): loss=0.641147312395355, w0=-2.0697227613103467e-06, w1=-0.026778813678192705\n",
      "Regularized Logistic Regression(185/199): loss=0.6411473123949913, w0=-2.0697302978361676e-06, w1=-0.026778843184160364\n",
      "Regularized Logistic Regression(186/199): loss=0.6411473123946662, w0=-2.0697374563048114e-06, w1=-0.026778871146229004\n",
      "Regularized Logistic Regression(187/199): loss=0.641147312394375, w0=-2.0697442556896313e-06, w1=-0.026778897645512393\n",
      "Regularized Logistic Regression(188/199): loss=0.6411473123941147, w0=-2.0697507140112937e-06, w1=-0.026778922758838808\n",
      "Regularized Logistic Regression(189/199): loss=0.6411473123938818, w0=-2.0697568483856408e-06, w1=-0.026778946558978948\n",
      "Regularized Logistic Regression(190/199): loss=0.6411473123936733, w0=-2.0697626750691453e-06, w1=-0.026778969114862404\n",
      "Regularized Logistic Regression(191/199): loss=0.6411473123934867, w0=-2.0697682095020833e-06, w1=-0.026778990491781465\n",
      "Regularized Logistic Regression(192/199): loss=0.6411473123933198, w0=-2.0697734663495347e-06, w1=-0.026779010751584776\n",
      "Regularized Logistic Regression(193/199): loss=0.6411473123931706, w0=-2.069778459540325e-06, w1=-0.026779029952859836\n",
      "Regularized Logistic Regression(194/199): loss=0.6411473123930369, w0=-2.069783202304005e-06, w1=-0.026779048151106613\n",
      "Regularized Logistic Regression(195/199): loss=0.6411473123929176, w0=-2.069787707205981e-06, w1=-0.026779065398900917\n",
      "Regularized Logistic Regression(196/199): loss=0.6411473123928104, w0=-2.0697919861808695e-06, w1=-0.02677908174604976\n",
      "Regularized Logistic Regression(197/199): loss=0.6411473123927147, w0=-2.0697960505641806e-06, w1=-0.026779097239738113\n",
      "Regularized Logistic Regression(198/199): loss=0.6411473123926292, w0=-2.0697999111224092e-06, w1=-0.02677911192466766\n",
      "Regularized Logistic Regression(199/199): loss=0.6411473123925525, w0=-2.069803578081614e-06, w1=-0.026779125843188433\n",
      "Regularized Logistic Regression(0/299): loss=0.693147180559945, w0=-1.0441465146389343e-07, w1=-0.0018294491082988765\n",
      "Regularized Logistic Regression(1/299): loss=0.6855109278971108, w0=-2.0344820679313237e-07, w1=-0.003525592705210975\n",
      "Regularized Logistic Regression(2/299): loss=0.6790094636482038, w0=-2.973992347005631e-07, w1=-0.005098920788683085\n",
      "Regularized Logistic Regression(3/299): loss=0.6734722049940254, w0=-3.865462349189329e-07, w1=-0.00655903555223109\n",
      "Regularized Logistic Regression(4/299): loss=0.668754459737611, w0=-4.711494797636086e-07, w1=-0.007914726471614484\n",
      "Regularized Logistic Regression(5/299): loss=0.6647334829006758, w0=-5.514526387148568e-07, w1=-0.009174041140295113\n",
      "Regularized Logistic Regression(6/299): loss=0.6613051257272127, w0=-6.276842122014097e-07, w1=-0.01034435117737273\n",
      "Regularized Logistic Regression(7/299): loss=0.658380993459813, w0=-7.000587986888713e-07, w1=-0.011432412948657019\n",
      "Regularized Logistic Regression(8/299): loss=0.6558860376149422, w0=-7.687782167799227e-07, w1=-0.012444423116756883\n",
      "Regularized Logistic Regression(9/299): loss=0.6537565178715707, w0=-8.340325015656089e-07, w1=-0.013386069209108925\n",
      "Regularized Logistic Regression(10/299): loss=0.6519382775264527, w0=-8.960007920809409e-07, w1=-0.014262575494170114\n",
      "Regularized Logistic Regression(11/299): loss=0.6503852844834055, w0=-9.548521245004876e-07, w1=-0.015078744508214167\n",
      "Regularized Logistic Regression(12/299): loss=0.6490583968371794, w0=-1.0107461437031831e-06, w1=-0.015838994594721486\n",
      "Regularized Logistic Regression(13/299): loss=0.6479243182907957, w0=-1.0638337440542881e-06, w1=-0.01654739381689451\n",
      "Regularized Logistic Regression(14/299): loss=0.646954713968031, w0=-1.1142576486927605e-06, w1=-0.01720769058948778\n",
      "Regularized Logistic Regression(15/299): loss=0.6461254617338402, w0=-1.1621529352605968e-06, w1=-0.01782334135445739\n",
      "Regularized Logistic Regression(16/299): loss=0.6454160180064545, w0=-1.2076475148481932e-06, w1=-0.018397535599615756\n",
      "Regularized Logistic Regression(17/299): loss=0.6448088803252755, w0=-1.250862569935668e-06, w1=-0.01893321849294144\n",
      "Regularized Logistic Regression(18/299): loss=0.6442891317117418, w0=-1.291912956263624e-06, w1=-0.019433111378968373\n",
      "Regularized Logistic Regression(19/299): loss=0.6438440542006197, w0=-1.3309075728482698e-06, w1=-0.01989973035866197\n",
      "Regularized Logistic Regression(20/299): loss=0.6434628008921405, w0=-1.3679497037471788e-06, w1=-0.02033540315090935\n",
      "Regularized Logistic Regression(21/299): loss=0.6431361175377305, w0=-1.4031373346671406e-06, w1=-0.020742284412415206\n",
      "Regularized Logistic Regression(22/299): loss=0.642856106072275, w0=-1.4365634470704348e-06, w1=-0.021122369673482768\n",
      "Regularized Logistic Regression(23/299): loss=0.6426160236852199, w0=-1.4683162920681836e-06, w1=-0.021477508029816766\n",
      "Regularized Logistic Regression(24/299): loss=0.6424101120163701, w0=-1.4984796460787012e-06, w1=-0.021809413715006755\n",
      "Regularized Logistic Regression(25/299): loss=0.6422334518994198, w0=-1.5271330499659707e-06, w1=-0.02211967666458812\n",
      "Regularized Logistic Regression(26/299): loss=0.6420818397820426, w0=-1.554352033150933e-06, w1=-0.022409772170389478\n",
      "Regularized Logistic Regression(27/299): loss=0.641951682546564, w0=-1.580208323999726e-06, w1=-0.022681069713089305\n",
      "Regularized Logistic Regression(28/299): loss=0.6418399079574882, w0=-1.6047700476329381e-06, w1=-0.022934841051376744\n",
      "Regularized Logistic Regression(29/299): loss=0.641743888386166, w0=-1.6281019121637858e-06, w1=-0.02317226763770369\n",
      "Regularized Logistic Regression(30/299): loss=0.6416613758210538, w0=-1.6502653842570522e-06, w1=-0.023394447423183893\n",
      "Regularized Logistic Regression(31/299): loss=0.6415904464747508, w0=-1.6713188548014302e-06, w1=-0.02360240110763889\n",
      "Regularized Logistic Regression(32/299): loss=0.6415294535549896, w0=-1.6913177954029137e-06, w1=-0.023797077884991157\n",
      "Regularized Logistic Regression(33/299): loss=0.6414769869833846, w0=-1.710314906333821e-06, w1=-0.023979360729077807\n",
      "Regularized Logistic Regression(34/299): loss=0.6414318390291269, w0=-1.7283602565090326e-06, w1=-0.024150071260416076\n",
      "Regularized Logistic Regression(35/299): loss=0.6413929749801622, w0=-1.7455014160065144e-06, w1=-0.024309974230424167\n",
      "Regularized Logistic Regression(36/299): loss=0.6413595081060502, w0=-1.7617835816018456e-06, w1=-0.024459781656026486\n",
      "Regularized Logistic Regression(37/299): loss=0.6413306782783439, w0=-1.7772496957451856e-06, w1=-0.02460015663439064\n",
      "Regularized Logistic Regression(38/299): loss=0.6413058337090459, w0=-1.791940559372973e-06, w1=-0.02473171686470995\n",
      "Regularized Logistic Regression(39/299): loss=0.6412844153480873, w0=-1.8058949389148553e-06, w1=-0.02485503790142018\n",
      "Regularized Logistic Regression(40/299): loss=0.6412659435490552, w0=-1.8191496678282952e-06, w1=-0.024970656160979068\n",
      "Regularized Logistic Regression(41/299): loss=0.6412500066703761, w0=-1.831739742968392e-06, w1=-0.025079071702314458\n",
      "Regularized Logistic Regression(42/299): loss=0.6412362513284766, w0=-1.8436984160782726e-06, w1=-0.025180750799235437\n",
      "Regularized Logistic Regression(43/299): loss=0.6412243740613408, w0=-1.8550572806655537e-06, w1=-0.025276128321471362\n",
      "Regularized Logistic Regression(44/299): loss=0.6412141141965455, w0=-1.8658463545125136e-06, w1=-0.02536560993953815\n",
      "Regularized Logistic Regression(45/299): loss=0.6412052477481859, w0=-1.876094158051494e-06, w1=-0.02544957416731269\n",
      "Regularized Logistic Regression(46/299): loss=0.6411975821929324, w0=-1.8858277888224252e-06, w1=-0.02552837425500273\n",
      "Regularized Logistic Regression(47/299): loss=0.6411909519974448, w0=-1.8950729922160446e-06, w1=-0.025602339944121984\n",
      "Regularized Logistic Regression(48/299): loss=0.6411852147881044, w0=-1.9038542286942046e-06, w1=-0.025671779095107646\n",
      "Regularized Logistic Regression(49/299): loss=0.6411802480699849, w0=-1.9121947376674784e-06, w1=-0.02573697919733046\n",
      "Regularized Logistic Regression(50/299): loss=0.6411759464155817, w0=-1.920116598199979e-06, w1=-0.025798208770446453\n",
      "Regularized Logistic Regression(51/299): loss=0.6411722190554249, w0=-1.9276407867017824e-06, w1=-0.025855718665307816\n",
      "Regularized Logistic Regression(52/299): loss=0.6411689878125825, w0=-1.934787231760523e-06, w1=-0.02590974327198851\n",
      "Regularized Logistic Regression(53/299): loss=0.6411661853315138, w0=-1.9415748662555217e-06, w1=-0.025960501641872857\n",
      "Regularized Logistic Regression(54/299): loss=0.6411637535589119, w0=-1.948021676890152e-06, w1=-0.026008198530202455\n",
      "Regularized Logistic Regression(55/299): loss=0.6411616424403408, w0=-1.9541447512709996e-06, w1=-0.02605302536497479\n",
      "Regularized Logistic Regression(56/299): loss=0.6411598088017018, w0=-1.959960322655671e-06, w1=-0.026095161147621133\n",
      "Regularized Logistic Regression(57/299): loss=0.641158215389055, w0=-1.9654838124848254e-06, w1=-0.02613477329047422\n",
      "Regularized Logistic Regression(58/299): loss=0.6411568300441436, w0=-1.9707298708080844e-06, w1=-0.026172018395643027\n",
      "Regularized Logistic Regression(59/299): loss=0.6411556249962368, w0=-1.975712414707913e-06, w1=-0.026207042979562903\n",
      "Regularized Logistic Regression(60/299): loss=0.641154576253704, w0=-1.9804446648203158e-06, w1=-0.026239984147158692\n",
      "Regularized Logistic Regression(61/299): loss=0.6411536630811181, w0=-1.9849391800462316e-06, w1=-0.026270970219263223\n",
      "Regularized Logistic Regression(62/299): loss=0.6411528675497274, w0=-1.9892078905428376e-06, w1=-0.026300121316655406\n",
      "Regularized Logistic Regression(63/299): loss=0.6411521741508809, w0=-1.993262129079518e-06, w1=-0.026327549903831154\n",
      "Regularized Logistic Regression(64/299): loss=0.6411515694634872, w0=-1.9971126608390743e-06, w1=-0.026353361295386256\n",
      "Regularized Logistic Regression(65/299): loss=0.6411510418678601, w0=-2.0007697117407687e-06, w1=-0.026377654127676776\n",
      "Regularized Logistic Regression(66/299): loss=0.6411505812993972, w0=-2.004242995358011e-06, w1=-0.02640052079822499\n",
      "Regularized Logistic Regression(67/299): loss=0.6411501790364793, w0=-2.0075417384999314e-06, w1=-0.026422047875154925\n",
      "Regularized Logistic Regression(68/299): loss=0.6411498275177687, w0=-2.0106747055226715e-06, w1=-0.026442316478775218\n",
      "Regularized Logistic Regression(69/299): loss=0.6411495201847791, w0=-2.0136502214330046e-06, w1=-0.026461402637272338\n",
      "Regularized Logistic Regression(70/299): loss=0.6411492513461712, w0=-2.016476193843822e-06, w1=-0.026479377618330628\n",
      "Regularized Logistic Regression(71/299): loss=0.6411490160607329, w0=-2.019160133838113e-06, w1=-0.02649630823836791\n",
      "Regularized Logistic Regression(72/299): loss=0.6411488100364344, w0=-2.0217091757952945e-06, w1=-0.02651225715094927\n",
      "Regularized Logistic Regression(73/299): loss=0.6411486295433144, w0=-2.0241300962310995e-06, w1=-0.02652728311582993\n",
      "Regularized Logistic Regression(74/299): loss=0.6411484713382777, w0=-2.0264293316997525e-06, w1=-0.026541441249973676\n",
      "Regularized Logistic Regression(75/299): loss=0.6411483326001438, w0=-2.028612995804744e-06, w1=-0.02655478326179723\n",
      "Regularized Logistic Regression(76/299): loss=0.6411482108735304, w0=-2.0306868953622793e-06, w1=-0.026567357669799303\n",
      "Regularized Logistic Regression(77/299): loss=0.6411481040203483, w0=-2.032656545759299e-06, w1=-0.02657921000665211\n",
      "Regularized Logistic Regression(78/299): loss=0.64114801017786, w0=-2.0345271855459276e-06, w1=-0.026590383009755487\n",
      "Regularized Logistic Regression(79/299): loss=0.6411479277223947, w0=-2.0363037903002493e-06, w1=-0.026600916799183225\n",
      "Regularized Logistic Regression(80/299): loss=0.6411478552379444, w0=-2.037991085801454e-06, w1=-0.026610849043883677\n",
      "Regularized Logistic Regression(81/299): loss=0.6411477914889774, w0=-2.0395935605456267e-06, w1=-0.026620215116938985\n",
      "Regularized Logistic Regression(82/299): loss=0.6411477353968824, w0=-2.0411154776367777e-06, w1=-0.02662904824062677\n",
      "Regularized Logistic Regression(83/299): loss=0.6411476860195595, w0=-2.042560886084102e-06, w1=-0.026637379621977753\n",
      "Regularized Logistic Regression(84/299): loss=0.6411476425337219, w0=-2.0439336315349383e-06, w1=-0.026645238579475795\n",
      "Regularized Logistic Regression(85/299): loss=0.6411476042195454, w0=-2.0452373664714553e-06, w1=-0.026652652661496124\n",
      "Regularized Logistic Regression(86/299): loss=0.6411475704473468, w0=-2.0464755598976976e-06, w1=-0.02665964775704306\n",
      "Regularized Logistic Regression(87/299): loss=0.6411475406660154, w0=-2.0476515065423297e-06, w1=-0.026666248199303202\n",
      "Regularized Logistic Regression(88/299): loss=0.6411475143929668, w0=-2.0487683356011594e-06, w1=-0.026672476862497903\n",
      "Regularized Logistic Regression(89/299): loss=0.6411474912054097, w0=-2.049829019042336e-06, w1=-0.026678355252484156\n",
      "Regularized Logistic Regression(90/299): loss=0.6411474707327522, w0=-2.050836379495991e-06, w1=-0.026683903591520904\n",
      "Regularized Logistic Regression(91/299): loss=0.6411474526499971, w0=-2.0517930977490103e-06, w1=-0.02668914089759103\n",
      "Regularized Logistic Regression(92/299): loss=0.6411474366719917, w0=-2.0527017198646134e-06, w1=-0.02669408505864099\n",
      "Regularized Logistic Regression(93/299): loss=0.6411474225484216, w0=-2.0535646639454314e-06, w1=-0.02669875290207493\n",
      "Regularized Logistic Regression(94/299): loss=0.6411474100594484, w0=-2.0543842265578643e-06, w1=-0.02670316025981833\n",
      "Regularized Logistic Regression(95/299): loss=0.6411473990119069, w0=-2.0551625888346036e-06, w1=-0.026707322029243686\n",
      "Regularized Logistic Regression(96/299): loss=0.6411473892359897, w0=-2.0559018222713823e-06, w1=-0.02671125223023016\n",
      "Regularized Logistic Regression(97/299): loss=0.6411473805823513, w0=-2.056603894233212e-06, w1=-0.026714964058613063\n",
      "Regularized Logistic Regression(98/299): loss=0.6411473729195851, w0=-2.0572706731846125e-06, w1=-0.026718469936257688\n",
      "Regularized Logistic Regression(99/299): loss=0.6411473661320144, w0=-2.0579039336576173e-06, w1=-0.02672178155798119\n",
      "Regularized Logistic Regression(100/299): loss=0.6411473601177639, w0=-2.058505360970663e-06, w1=-0.026724909935525686\n",
      "Regularized Logistic Regression(101/299): loss=0.6411473547870749, w0=-2.059076555710802e-06, w1=-0.02672786543877579\n",
      "Regularized Logistic Regression(102/299): loss=0.6411473500608291, w0=-2.059619037991081e-06, w1=-0.026730657834399877\n",
      "Regularized Logistic Regression(103/299): loss=0.6411473458692584, w0=-2.0601342514943236e-06, w1=-0.02673329632208141\n",
      "Regularized Logistic Regression(104/299): loss=0.6411473421508135, w0=-2.060623567314002e-06, w1=-0.026735789568496215\n",
      "Regularized Logistic Regression(105/299): loss=0.641147338851174, w0=-2.0610882876023535e-06, w1=-0.026738145739181388\n",
      "Regularized Logistic Regression(106/299): loss=0.6411473359223809, w0=-2.061529649035386e-06, w1=-0.026740372528431554\n",
      "Regularized Logistic Regression(107/299): loss=0.6411473333220735, w0=-2.061948826103941e-06, w1=-0.026742477187348263\n",
      "Regularized Logistic Regression(108/299): loss=0.6411473310128223, w0=-2.0623469342395248e-06, w1=-0.026744466550161508\n",
      "Regularized Logistic Regression(109/299): loss=0.6411473289615435, w0=-2.062725032783183e-06, w1=-0.02674634705893352\n",
      "Regularized Logistic Regression(110/299): loss=0.6411473271389813, w0=-2.063084127805283e-06, w1=-0.02674812478674842\n",
      "Regularized Logistic Regression(111/299): loss=0.6411473255192581, w0=-2.063425174783672e-06, w1=-0.026749805459482495\n",
      "Regularized Logistic Regression(112/299): loss=0.6411473240794753, w0=-2.0637490811473154e-06, w1=-0.026751394476247067\n",
      "Regularized Logistic Regression(113/299): loss=0.6411473227993632, w0=-2.0640567086921516e-06, w1=-0.02675289692858568\n",
      "Regularized Logistic Regression(114/299): loss=0.641147321660974, w0=-2.064348875875576e-06, w1=-0.026754317618506225\n",
      "Regularized Logistic Regression(115/299): loss=0.6411473206484096, w0=-2.06462635999564e-06, w1=-0.026755661075420923\n",
      "Regularized Logistic Regression(116/299): loss=0.6411473197475849, w0=-2.064889899260741e-06, w1=-0.026756931572060737\n",
      "Regularized Logistic Regression(117/299): loss=0.6411473189460141, w0=-2.065140194755312e-06, w1=-0.0267581331394314\n",
      "Regularized Logistic Regression(118/299): loss=0.6411473182326293, w0=-2.0653779123067103e-06, w1=-0.026759269580868866\n",
      "Regularized Logistic Regression(119/299): loss=0.6411473175976137, w0=-2.0656036842582824e-06, w1=-0.026760344485251306\n",
      "Regularized Logistic Regression(120/299): loss=0.6411473170322601, w0=-2.0658181111532976e-06, w1=-0.02676136123941915\n",
      "Regularized Logistic Regression(121/299): loss=0.6411473165288414, w0=-2.066021763334238e-06, w1=-0.02676232303985362\n",
      "Regularized Logistic Regression(122/299): loss=0.641147316080499, w0=-2.0662151824616847e-06, w1=-0.026763232903658257\n",
      "Regularized Logistic Regression(123/299): loss=0.6411473156811448, w0=-2.0663988829568463e-06, w1=-0.02676409367888791\n",
      "Regularized Logistic Regression(124/299): loss=0.6411473153253715, w0=-2.066573353371555e-06, w1=-0.026764908054263768\n",
      "Regularized Logistic Regression(125/299): loss=0.6411473150083764, w0=-2.0667390576893825e-06, w1=-0.026765678568313057\n",
      "Regularized Logistic Regression(126/299): loss=0.6411473147258924, w0=-2.0668964365613277e-06, w1=-0.02676640761796908\n",
      "Regularized Logistic Regression(127/299): loss=0.6411473144741275, w0=-2.0670459084793706e-06, w1=-0.026767097466663524\n",
      "Regularized Logistic Regression(128/299): loss=0.6411473142497114, w0=-2.067187870891007e-06, w1=-0.0267677502519424\n",
      "Regularized Logistic Regression(129/299): loss=0.6411473140496469, w0=-2.067322701257735e-06, w1=-0.026768367992635735\n",
      "Regularized Logistic Regression(130/299): loss=0.6411473138712694, w0=-2.067450758060309e-06, w1=-0.026768952595605595\n",
      "Regularized Logistic Regression(131/299): loss=0.641147313712209, w0=-2.067572381753434e-06, w1=-0.026769505862101146\n",
      "Regularized Logistic Regression(132/299): loss=0.6411473135703567, w0=-2.0676878956724408e-06, w1=-0.026770029493740865\n",
      "Regularized Logistic Regression(133/299): loss=0.6411473134438371, w0=-2.0677976068943617e-06, w1=-0.026770525098148243\n",
      "Regularized Logistic Regression(134/299): loss=0.6411473133309802, w0=-2.0679018070556946e-06, w1=-0.026770994194258938\n",
      "Regularized Logistic Regression(135/299): loss=0.6411473132302996, w0=-2.0680007731290324e-06, w1=-0.02677143821732007\n",
      "Regularized Logistic Regression(136/299): loss=0.6411473131404729, w0=-2.0680947681606286e-06, w1=-0.02677185852360004\n",
      "Regularized Logistic Regression(137/299): loss=0.6411473130603219, w0=-2.068184041970858e-06, w1=-0.02677225639482719\n",
      "Regularized Logistic Regression(138/299): loss=0.6411473129887973, w0=-2.0682688318194483e-06, w1=-0.026772633042370833\n",
      "Regularized Logistic Regression(139/299): loss=0.641147312924965, w0=-2.068349363037242e-06, w1=-0.02677298961118264\n",
      "Regularized Logistic Regression(140/299): loss=0.6411473128679922, w0=-2.0684258496261814e-06, w1=-0.026773327183512084\n",
      "Regularized Logistic Regression(141/299): loss=0.6411473128171377, w0=-2.0684984948291085e-06, w1=-0.02677364678240761\n",
      "Regularized Logistic Regression(142/299): loss=0.6411473127717403, w0=-2.068567491670904e-06, w1=-0.026773949375017935\n",
      "Regularized Logistic Regression(143/299): loss=0.6411473127312112, w0=-2.0686330234724005e-06, w1=-0.026774235875705606\n",
      "Regularized Logistic Regression(144/299): loss=0.6411473126950258, w0=-2.068695264338443e-06, w1=-0.02677450714898076\n",
      "Regularized Logistic Regression(145/299): loss=0.6411473126627155, w0=-2.0687543796213967e-06, w1=-0.02677476401227072\n",
      "Regularized Logistic Regression(146/299): loss=0.6411473126338636, w0=-2.0688105263613386e-06, w1=-0.02677500723852984\n",
      "Regularized Logistic Regression(147/299): loss=0.6411473126080978, w0=-2.068863853704102e-06, w1=-0.026775237558702626\n",
      "Regularized Logistic Regression(148/299): loss=0.6411473125850865, w0=-2.068914503298291e-06, w1=-0.026775455664046287\n",
      "Regularized Logistic Regression(149/299): loss=0.6411473125645336, w0=-2.0689626096723233e-06, w1=-0.02677566220832355\n",
      "Regularized Logistic Regression(150/299): loss=0.6411473125461756, w0=-2.069008300592502e-06, w1=-0.026775857809869893\n",
      "Regularized Logistic Regression(151/299): loss=0.641147312529777, w0=-2.0690516974030787e-06, w1=-0.026776043053545636\n",
      "Regularized Logistic Regression(152/299): loss=0.6411473125151277, w0=-2.069092915349203e-06, w1=-0.02677621849257746\n",
      "Regularized Logistic Regression(153/299): loss=0.64114731250204, w0=-2.069132063883632e-06, w1=-0.026776384650296813\n",
      "Regularized Logistic Regression(154/299): loss=0.641147312490347, w0=-2.069169246958006e-06, w1=-0.02677654202178061\n",
      "Regularized Logistic Regression(155/299): loss=0.6411473124798994, w0=-2.069204563299473e-06, w1=-0.02677669107540015\n",
      "Regularized Logistic Regression(156/299): loss=0.6411473124705643, w0=-2.0692381066733954e-06, w1=-0.026776832254283445\n",
      "Regularized Logistic Regression(157/299): loss=0.6411473124622223, w0=-2.069269966132846e-06, w1=-0.026776965977696052\n",
      "Regularized Logistic Regression(158/299): loss=0.6411473124547681, w0=-2.0693002262555477e-06, w1=-0.02677709264234399\n",
      "Regularized Logistic Regression(159/299): loss=0.6411473124481061, w0=-2.069328967368898e-06, w1=-0.026777212623606113\n",
      "Regularized Logistic Regression(160/299): loss=0.641147312442152, w0=-2.069356265763674e-06, w1=-0.026777326276695487\n",
      "Regularized Logistic Regression(161/299): loss=0.6411473124368305, w0=-2.069382193896985e-06, w1=-0.026777433937758343\n",
      "Regularized Logistic Regression(162/299): loss=0.6411473124320743, w0=-2.069406820585019e-06, w1=-0.026777535924910827\n",
      "Regularized Logistic Regression(163/299): loss=0.6411473124278229, w0=-2.0694302111860928e-06, w1=-0.02677763253921938\n",
      "Regularized Logistic Regression(164/299): loss=0.6411473124240225, w0=-2.069452427774494e-06, w1=-0.026777724065625827\n",
      "Regularized Logistic Regression(165/299): loss=0.6411473124206253, w0=-2.0694735293055827e-06, w1=-0.026777810773821697\n",
      "Regularized Logistic Regression(166/299): loss=0.6411473124175882, w0=-2.0694935717725825e-06, w1=-0.02677789291907535\n",
      "Regularized Logistic Regression(167/299): loss=0.641147312414873, w0=-2.069512608355491e-06, w1=-0.026777970743012223\n",
      "Regularized Logistic Regression(168/299): loss=0.6411473124124456, w0=-2.0695306895624994e-06, w1=-0.026778044474352666\n",
      "Regularized Logistic Regression(169/299): loss=0.6411473124102752, w0=-2.069547863364297e-06, w1=-0.02677811432960928\n",
      "Regularized Logistic Regression(170/299): loss=0.6411473124083347, w0=-2.0695641753216233e-06, w1=-0.02677818051374619\n",
      "Regularized Logistic Regression(171/299): loss=0.6411473124065995, w0=-2.069579668706407e-06, w1=-0.02677824322080153\n",
      "Regularized Logistic Regression(172/299): loss=0.6411473124050479, w0=-2.069594384616807e-06, w1=-0.026778302634476524\n",
      "Regularized Logistic Regression(173/299): loss=0.6411473124036604, w0=-2.0696083620864737e-06, w1=-0.026778358928692115\n",
      "Regularized Logistic Regression(174/299): loss=0.6411473124024197, w0=-2.0696216381883128e-06, w1=-0.026778412268115226\n",
      "Regularized Logistic Regression(175/299): loss=0.6411473124013101, w0=-2.069634248133032e-06, w1=-0.02677846280865582\n",
      "Regularized Logistic Regression(176/299): loss=0.641147312400318, w0=-2.06964622536273e-06, w1=-0.026778510697937765\n",
      "Regularized Logistic Regression(177/299): loss=0.6411473123994307, w0=-2.069657601639784e-06, w1=-0.02677855607574352\n",
      "Regularized Logistic Regression(178/299): loss=0.6411473123986368, w0=-2.0696684071312623e-06, w1=-0.026778599074434212\n",
      "Regularized Logistic Regression(179/299): loss=0.6411473123979268, w0=-2.0696786704890957e-06, w1=-0.026778639819347944\n",
      "Regularized Logistic Regression(180/299): loss=0.6411473123972918, w0=-2.069688418926216e-06, w1=-0.0267786784291761\n",
      "Regularized Logistic Regression(181/299): loss=0.641147312396724, w0=-2.0696976782888684e-06, w1=-0.026778715016318032\n",
      "Regularized Logistic Regression(182/299): loss=0.6411473123962159, w0=-2.0697064731252863e-06, w1=-0.02677874968721868\n",
      "Regularized Logistic Regression(183/299): loss=0.6411473123957616, w0=-2.069714826750913e-06, w1=-0.02677878254268602\n",
      "Regularized Logistic Regression(184/299): loss=0.641147312395355, w0=-2.0697227613103467e-06, w1=-0.026778813678192705\n",
      "Regularized Logistic Regression(185/299): loss=0.6411473123949913, w0=-2.0697302978361676e-06, w1=-0.026778843184160364\n",
      "Regularized Logistic Regression(186/299): loss=0.6411473123946662, w0=-2.0697374563048114e-06, w1=-0.026778871146229004\n",
      "Regularized Logistic Regression(187/299): loss=0.641147312394375, w0=-2.0697442556896313e-06, w1=-0.026778897645512393\n",
      "Regularized Logistic Regression(188/299): loss=0.6411473123941147, w0=-2.0697507140112937e-06, w1=-0.026778922758838808\n",
      "Regularized Logistic Regression(189/299): loss=0.6411473123938818, w0=-2.0697568483856408e-06, w1=-0.026778946558978948\n",
      "Regularized Logistic Regression(190/299): loss=0.6411473123936733, w0=-2.0697626750691453e-06, w1=-0.026778969114862404\n",
      "Regularized Logistic Regression(191/299): loss=0.6411473123934867, w0=-2.0697682095020833e-06, w1=-0.026778990491781465\n",
      "Regularized Logistic Regression(192/299): loss=0.6411473123933198, w0=-2.0697734663495347e-06, w1=-0.026779010751584776\n",
      "Regularized Logistic Regression(193/299): loss=0.6411473123931706, w0=-2.069778459540325e-06, w1=-0.026779029952859836\n",
      "Regularized Logistic Regression(194/299): loss=0.6411473123930369, w0=-2.069783202304005e-06, w1=-0.026779048151106613\n",
      "Regularized Logistic Regression(195/299): loss=0.6411473123929176, w0=-2.069787707205981e-06, w1=-0.026779065398900917\n",
      "Regularized Logistic Regression(196/299): loss=0.6411473123928104, w0=-2.0697919861808695e-06, w1=-0.02677908174604976\n",
      "Regularized Logistic Regression(197/299): loss=0.6411473123927147, w0=-2.0697960505641806e-06, w1=-0.026779097239738113\n",
      "Regularized Logistic Regression(198/299): loss=0.6411473123926292, w0=-2.0697999111224092e-06, w1=-0.02677911192466766\n",
      "Regularized Logistic Regression(199/299): loss=0.6411473123925525, w0=-2.069803578081614e-06, w1=-0.026779125843188433\n",
      "Regularized Logistic Regression(200/299): loss=0.6411473123924837, w0=-2.0698070611545574e-06, w1=-0.026779139035423193\n",
      "Regularized Logistic Regression(201/299): loss=0.6411473123924225, w0=-2.0698103695664866e-06, w1=-0.026779151539385315\n",
      "Regularized Logistic Regression(202/299): loss=0.6411473123923676, w0=-2.069813512079614e-06, w1=-0.02677916339109044\n",
      "Regularized Logistic Regression(203/299): loss=0.6411473123923183, w0=-2.069816497016371e-06, w1=-0.026779174624661773\n",
      "Regularized Logistic Regression(204/299): loss=0.6411473123922743, w0=-2.069819332281493e-06, w1=-0.026779185272430374\n",
      "Regularized Logistic Regression(205/299): loss=0.6411473123922349, w0=-2.0698220253829925e-06, w1=-0.026779195365029547\n",
      "Regularized Logistic Regression(206/299): loss=0.6411473123921995, w0=-2.0698245834520805e-06, w1=-0.026779204931484772\n",
      "Regularized Logistic Regression(207/299): loss=0.641147312392168, w0=-2.0698270132620878e-06, w1=-0.02677921399929819\n",
      "Regularized Logistic Regression(208/299): loss=0.6411473123921396, w0=-2.0698293212464332e-06, w1=-0.026779222594529146\n",
      "Regularized Logistic Regression(209/299): loss=0.6411473123921146, w0=-2.069831513515691e-06, w1=-0.02677923074187028\n",
      "Regularized Logistic Regression(210/299): loss=0.6411473123920919, w0=-2.069833595873798e-06, w1=-0.02677923846471963\n",
      "Regularized Logistic Regression(211/299): loss=0.6411473123920717, w0=-2.0698355738334527e-06, w1=-0.026779245785248405\n",
      "Regularized Logistic Regression(212/299): loss=0.6411473123920537, w0=-2.0698374526307337e-06, w1=-0.02677925272446617\n",
      "Regularized Logistic Regression(213/299): loss=0.6411473123920374, w0=-2.0698392372389882e-06, w1=-0.026779259302281817\n",
      "Regularized Logistic Regression(214/299): loss=0.6411473123920227, w0=-2.069840932382022e-06, w1=-0.02677926553756133\n",
      "Regularized Logistic Regression(215/299): loss=0.6411473123920098, w0=-2.0698425425466247e-06, w1=-0.026779271448182585\n",
      "Regularized Logistic Regression(216/299): loss=0.6411473123919982, w0=-2.0698440719944704e-06, w1=-0.026779277051087597\n",
      "Regularized Logistic Regression(217/299): loss=0.6411473123919876, w0=-2.0698455247734157e-06, w1=-0.02677928236233161\n",
      "Regularized Logistic Regression(218/299): loss=0.6411473123919784, w0=-2.069846904728234e-06, w1=-0.026779287397129686\n",
      "Regularized Logistic Regression(219/299): loss=0.64114731239197, w0=-2.069848215510809e-06, w1=-0.02677929216990096\n",
      "Regularized Logistic Regression(220/299): loss=0.6411473123919625, w0=-2.0698494605898152e-06, w1=-0.02677929669431025\n",
      "Regularized Logistic Regression(221/299): loss=0.6411473123919557, w0=-2.069850643259916e-06, w1=-0.02677930098330767\n",
      "Regularized Logistic Regression(222/299): loss=0.6411473123919497, w0=-2.0698517666504955e-06, w1=-0.026779305049166594\n",
      "Regularized Logistic Regression(223/299): loss=0.6411473123919443, w0=-2.069852833733954e-06, w1=-0.02677930890351839\n",
      "Regularized Logistic Regression(224/299): loss=0.6411473123919396, w0=-2.069853847333587e-06, w1=-0.026779312557386685\n",
      "Regularized Logistic Regression(225/299): loss=0.6411473123919352, w0=-2.0698548101310687e-06, w1=-0.02677931602121913\n",
      "Regularized Logistic Regression(226/299): loss=0.6411473123919313, w0=-2.0698557246735587e-06, w1=-0.026779319304917493\n",
      "Regularized Logistic Regression(227/299): loss=0.6411473123919279, w0=-2.0698565933804527e-06, w1=-0.026779322417866517\n",
      "Regularized Logistic Regression(228/299): loss=0.6411473123919249, w0=-2.0698574185497936e-06, w1=-0.02677932536896065\n",
      "Regularized Logistic Regression(229/299): loss=0.6411473123919221, w0=-2.069858202364362e-06, w1=-0.026779328166630034\n",
      "Regularized Logistic Regression(230/299): loss=0.6411473123919196, w0=-2.0698589468974587e-06, w1=-0.026779330818864605\n",
      "Regularized Logistic Regression(231/299): loss=0.6411473123919175, w0=-2.0698596541183994e-06, w1=-0.026779333333237336\n",
      "Regularized Logistic Regression(232/299): loss=0.6411473123919156, w0=-2.069860325897732e-06, w1=-0.026779335716926013\n",
      "Regularized Logistic Regression(233/299): loss=0.6411473123919137, w0=-2.069860964012192e-06, w1=-0.0267793379767337\n",
      "Regularized Logistic Regression(234/299): loss=0.641147312391912, w0=-2.0698615701494103e-06, w1=-0.026779340119109\n",
      "Regularized Logistic Regression(235/299): loss=0.6411473123919106, w0=-2.069862145912383e-06, w1=-0.02677934215016357\n",
      "Regularized Logistic Regression(236/299): loss=0.6411473123919094, w0=-2.0698626928237176e-06, w1=-0.026779344075690673\n",
      "Regularized Logistic Regression(237/299): loss=0.6411473123919081, w0=-2.069863212329667e-06, w1=-0.026779345901181407\n",
      "Regularized Logistic Regression(238/299): loss=0.6411473123919073, w0=-2.0698637058039603e-06, w1=-0.026779347631840585\n",
      "Regularized Logistic Regression(239/299): loss=0.6411473123919061, w0=-2.0698641745514393e-06, w1=-0.026779349272601824\n",
      "Regularized Logistic Regression(240/299): loss=0.6411473123919054, w0=-2.069864619811517e-06, w1=-0.02677935082814115\n",
      "Regularized Logistic Regression(241/299): loss=0.6411473123919047, w0=-2.0698650427614575e-06, w1=-0.026779352302891385\n",
      "Regularized Logistic Regression(242/299): loss=0.641147312391904, w0=-2.069865444519495e-06, w1=-0.02677935370105409\n",
      "Regularized Logistic Regression(243/299): loss=0.6411473123919031, w0=-2.0698658261477947e-06, w1=-0.026779355026612092\n",
      "Regularized Logistic Regression(244/299): loss=0.6411473123919028, w0=-2.0698661886552656e-06, w1=-0.02677935628334055\n",
      "Regularized Logistic Regression(245/299): loss=0.6411473123919023, w0=-2.0698665330002303e-06, w1=-0.02677935747481805\n",
      "Regularized Logistic Regression(246/299): loss=0.6411473123919019, w0=-2.069866860092964e-06, w1=-0.026779358604436865\n",
      "Regularized Logistic Regression(247/299): loss=0.6411473123919017, w0=-2.0698671707981037e-06, w1=-0.026779359675412662\n",
      "Regularized Logistic Regression(248/299): loss=0.6411473123919011, w0=-2.0698674659369386e-06, w1=-0.026779360690793425\n",
      "Regularized Logistic Regression(249/299): loss=0.6411473123919009, w0=-2.0698677462895823e-06, w1=-0.026779361653468937\n",
      "Regularized Logistic Regression(250/299): loss=0.6411473123919006, w0=-2.0698680125970387e-06, w1=-0.026779362566177863\n",
      "Regularized Logistic Regression(251/299): loss=0.6411473123919004, w0=-2.0698682655631637e-06, w1=-0.026779363431516913\n",
      "Regularized Logistic Regression(252/299): loss=0.6411473123919001, w0=-2.069868505856529e-06, w1=-0.026779364251947404\n",
      "Regularized Logistic Regression(253/299): loss=0.6411473123919, w0=-2.0698687341121893e-06, w1=-0.026779365029802542\n",
      "Regularized Logistic Regression(254/299): loss=0.6411473123918998, w0=-2.0698689509333656e-06, w1=-0.026779365767294363\n",
      "Regularized Logistic Regression(255/299): loss=0.6411473123918997, w0=-2.06986915689304e-06, w1=-0.02677936646651949\n",
      "Regularized Logistic Regression(256/299): loss=0.6411473123918995, w0=-2.069869352535474e-06, w1=-0.026779367129465676\n",
      "Regularized Logistic Regression(257/299): loss=0.6411473123918995, w0=-2.069869538377646e-06, w1=-0.026779367758017127\n",
      "Regularized Logistic Regression(258/299): loss=0.6411473123918993, w0=-2.0698697149106213e-06, w1=-0.026779368353960442\n",
      "Regularized Logistic Regression(259/299): loss=0.641147312391899, w0=-2.0698698826008514e-06, w1=-0.026779368918988782\n",
      "Regularized Logistic Regression(260/299): loss=0.641147312391899, w0=-2.0698700418914077e-06, w1=-0.02677936945470764\n",
      "Regularized Logistic Regression(261/299): loss=0.6411473123918989, w0=-2.0698701932031537e-06, w1=-0.02677936996263881\n",
      "Regularized Logistic Regression(262/299): loss=0.6411473123918989, w0=-2.069870336935859e-06, w1=-0.02677937044422504\n",
      "Regularized Logistic Regression(263/299): loss=0.6411473123918988, w0=-2.0698704734692573e-06, w1=-0.02677937090083417\n",
      "Regularized Logistic Regression(264/299): loss=0.6411473123918987, w0=-2.069870603164049e-06, w1=-0.026779371333762764\n",
      "Regularized Logistic Regression(265/299): loss=0.6411473123918987, w0=-2.0698707263628577e-06, w1=-0.026779371744240204\n",
      "Regularized Logistic Regression(266/299): loss=0.6411473123918987, w0=-2.0698708433911353e-06, w1=-0.02677937213343178\n",
      "Regularized Logistic Regression(267/299): loss=0.6411473123918986, w0=-2.069870954558024e-06, w1=-0.026779372502442386\n",
      "Regularized Logistic Regression(268/299): loss=0.6411473123918986, w0=-2.0698710601571732e-06, w1=-0.026779372852319412\n",
      "Regularized Logistic Regression(269/299): loss=0.6411473123918985, w0=-2.069871160467516e-06, w1=-0.026779373184055714\n",
      "Regularized Logistic Regression(270/299): loss=0.6411473123918984, w0=-2.0698712557540066e-06, w1=-0.02677937349859275\n",
      "Regularized Logistic Regression(271/299): loss=0.6411473123918987, w0=-2.0698713462683226e-06, w1=-0.02677937379682298\n",
      "Regularized Logistic Regression(272/299): loss=0.6411473123918984, w0=-2.069871432249529e-06, w1=-0.026779374079592647\n",
      "Regularized Logistic Regression(273/299): loss=0.6411473123918985, w0=-2.0698715139247116e-06, w1=-0.026779374347703867\n",
      "Regularized Logistic Regression(274/299): loss=0.6411473123918984, w0=-2.069871591509576e-06, w1=-0.026779374601917094\n",
      "Regularized Logistic Regression(275/299): loss=0.6411473123918984, w0=-2.069871665209021e-06, w1=-0.026779374842953323\n",
      "Regularized Logistic Regression(276/299): loss=0.6411473123918985, w0=-2.0698717352176755e-06, w1=-0.026779375071496143\n",
      "Regularized Logistic Regression(277/299): loss=0.6411473123918985, w0=-2.0698718017204184e-06, w1=-0.02677937528819368\n",
      "Regularized Logistic Regression(278/299): loss=0.6411473123918984, w0=-2.069871864892864e-06, w1=-0.026779375493660315\n",
      "Regularized Logistic Regression(279/299): loss=0.6411473123918983, w0=-2.069871924901828e-06, w1=-0.02677937568847859\n",
      "Regularized Logistic Regression(280/299): loss=0.6411473123918984, w0=-2.0698719819057672e-06, w1=-0.026779375873200822\n",
      "Regularized Logistic Regression(281/299): loss=0.6411473123918985, w0=-2.0698720360552e-06, w1=-0.026779376048350418\n",
      "Regularized Logistic Regression(282/299): loss=0.6411473123918984, w0=-2.0698720874931023e-06, w1=-0.026779376214423854\n",
      "Regularized Logistic Regression(283/299): loss=0.6411473123918984, w0=-2.0698721363552867e-06, w1=-0.026779376371891973\n",
      "Regularized Logistic Regression(284/299): loss=0.6411473123918983, w0=-2.0698721827707623e-06, w1=-0.026779376521200934\n",
      "Regularized Logistic Regression(285/299): loss=0.6411473123918985, w0=-2.0698722268620747e-06, w1=-0.026779376662773757\n",
      "Regularized Logistic Regression(286/299): loss=0.6411473123918983, w0=-2.0698722687456296e-06, w1=-0.02677937679701166\n",
      "Regularized Logistic Regression(287/299): loss=0.6411473123918983, w0=-2.0698723085320013e-06, w1=-0.026779376924294777\n",
      "Regularized Logistic Regression(288/299): loss=0.6411473123918985, w0=-2.069872346326224e-06, w1=-0.02677937704498374\n",
      "Regularized Logistic Regression(289/299): loss=0.6411473123918983, w0=-2.0698723822280702e-06, w1=-0.0267793771594203\n",
      "Regularized Logistic Regression(290/299): loss=0.6411473123918981, w0=-2.0698724163323145e-06, w1=-0.026779377267928867\n",
      "Regularized Logistic Regression(291/299): loss=0.6411473123918984, w0=-2.069872448728983e-06, w1=-0.026779377370816346\n",
      "Regularized Logistic Regression(292/299): loss=0.6411473123918984, w0=-2.069872479503593e-06, w1=-0.026779377468374513\n",
      "Regularized Logistic Regression(293/299): loss=0.6411473123918983, w0=-2.0698725087373774e-06, w1=-0.026779377560879374\n",
      "Regularized Logistic Regression(294/299): loss=0.6411473123918984, w0=-2.069872536507499e-06, w1=-0.02677937764859285\n",
      "Regularized Logistic Regression(295/299): loss=0.6411473123918983, w0=-2.0698725628872573e-06, w1=-0.026779377731763233\n",
      "Regularized Logistic Regression(296/299): loss=0.6411473123918985, w0=-2.0698725879462786e-06, w1=-0.02677937781062608\n",
      "Regularized Logistic Regression(297/299): loss=0.6411473123918981, w0=-2.0698726117507026e-06, w1=-0.026779377885404625\n",
      "Regularized Logistic Regression(298/299): loss=0.6411473123918984, w0=-2.069872634363355e-06, w1=-0.02677937795631044\n",
      "Regularized Logistic Regression(299/299): loss=0.6411473123918981, w0=-2.069872655843915e-06, w1=-0.02677937802354433\n",
      "Regularized Logistic Regression(0/99): loss=0.693147180559945, w0=-2.0882930292778685e-07, w1=-0.003658898216597753\n",
      "Regularized Logistic Regression(1/99): loss=0.6776955923065077, w0=-4.169962701924912e-07, w1=-0.007150181685440866\n",
      "Regularized Logistic Regression(2/99): loss=0.6638822204426907, w0=-6.246728750319227e-07, w1=-0.010488031183482814\n",
      "Regularized Logistic Regression(3/99): loss=0.6515149783929285, w0=-8.319892409548812e-07, w1=-0.01368522358718803\n",
      "Regularized Logistic Regression(4/99): loss=0.640424722544377, w0=-1.039042448731666e-06, w1=-0.0167532354513428\n",
      "Regularized Logistic Regression(5/99): loss=0.6304626011750312, w0=-1.2459037871272066e-06, w1=-0.01970236388848945\n",
      "Regularized Logistic Regression(6/99): loss=0.6214977097265673, w0=-1.4526246175856188e-06, w1=-0.022541850158026053\n",
      "Regularized Logistic Regression(7/99): loss=0.6134150035278391, w0=-1.6592410512095323e-06, w1=-0.025279997708318735\n",
      "Regularized Logistic Regression(8/99): loss=0.60611343168562, w0=-1.865777636058614e-06, w1=-0.027924280554486376\n",
      "Regularized Logistic Regression(9/99): loss=0.5995042666930963, w0=-2.072250236838718e-06, w1=-0.03048144042067572\n",
      "Regularized Logistic Regression(10/99): loss=0.5935096113571188, w0=-2.278668265996055e-06, w1=-0.03295757254254673\n",
      "Regularized Logistic Regression(11/99): loss=0.5880610684112597, w0=-2.485036400328833e-06, w1=-0.035358200800256534\n",
      "Regularized Logistic Regression(12/99): loss=0.583098559820608, w0=-2.6913558934430878e-06, w1=-0.037688343204250775\n",
      "Regularized Logistic Regression(13/99): loss=0.5785692833348673, w0=-2.89762557316298e-06, w1=-0.03995256886353105\n",
      "Regularized Logistic Regression(14/99): loss=0.5744267940123832, w0=-3.10384259488553e-06, w1=-0.042155047540973906\n",
      "Regularized Logistic Regression(15/99): loss=0.570630198608567, w0=-3.3100030068454805e-06, w1=-0.04429959281057337\n",
      "Regularized Logistic Regression(16/99): loss=0.5671434510646289, w0=-3.516102171058805e-06, w1=-0.046389699716461584\n",
      "Regularized Logistic Regression(17/99): loss=0.563934737880546, w0=-3.722135073962921e-06, w1=-0.04842857771476157\n",
      "Regularized Logistic Regression(18/99): loss=0.5609759428778833, w0=-3.9280965530659485e-06, w1=-0.050419179567610696\n",
      "Regularized Logistic Regression(19/99): loss=0.5582421816969212, w0=-4.133981459879748e-06, w1=-0.05236422675881614\n",
      "Regularized Logistic Regression(20/99): loss=0.5557113972707043, w0=-4.33978477471136e-06, w1=-0.05426623191387268\n",
      "Regularized Logistic Regression(21/99): loss=0.5533640084270813, w0=-4.545501685246345e-06, w1=-0.056127518633122056\n",
      "Regularized Logistic Regression(22/99): loss=0.5511826046519578, w0=-4.751127638047214e-06, w1=-0.05795023908439821\n",
      "Regularized Logistic Regression(23/99): loss=0.5491516808781363, w0=-4.956658369926896e-06, w1=-0.05973638964913545\n",
      "Regularized Logistic Regression(24/99): loss=0.5472574069296435, w0=-5.1620899244956925e-06, w1=-0.061487824872087794\n",
      "Regularized Logistic Regression(25/99): loss=0.5454874269443699, w0=-5.367418657905939e-06, w1=-0.06320626992814232\n",
      "Regularized Logistic Regression(26/99): loss=0.5438306847167236, w0=-5.572641236842651e-06, w1=-0.06489333178902201\n",
      "Regularized Logistic Regression(27/99): loss=0.5422772714490199, w0=-5.777754631061811e-06, w1=-0.06655050924690431\n",
      "Regularized Logistic Regression(28/99): loss=0.540818292879958, w0=-5.982756102207219e-06, w1=-0.06817920193031383\n",
      "Regularized Logistic Regression(29/99): loss=0.5394457531764244, w0=-6.187643190201117e-06, w1=-0.06978071842933939\n",
      "Regularized Logistic Regression(30/99): loss=0.538152453337257, w0=-6.392413698171456e-06, w1=-0.07135628363172858\n",
      "Regularized Logistic Regression(31/99): loss=0.5369319021706687, w0=-6.597065676625606e-06, w1=-0.07290704535822672\n",
      "Regularized Logistic Regression(32/99): loss=0.5357782381767828, w0=-6.801597407387919e-06, w1=-0.07443408037427915\n",
      "Regularized Logistic Regression(33/99): loss=0.5346861608986765, w0=-7.006007387672619e-06, w1=-0.07593839984558773\n",
      "Regularized Logistic Regression(34/99): loss=0.5336508705045079, w0=-7.21029431455317e-06, w1=-0.07742095429673258\n",
      "Regularized Logistic Regression(35/99): loss=0.5326680145341901, w0=-7.414457070006136e-06, w1=-0.07888263812493898\n",
      "Regularized Logistic Regression(36/99): loss=0.5317336408906204, w0=-7.618494706645257e-06, w1=-0.08032429371490143\n",
      "Regularized Logistic Regression(37/99): loss=0.5308441562811397, w0=-7.822406434215057e-06, w1=-0.08174671519522728\n",
      "Regularized Logistic Regression(38/99): loss=0.5299962894226783, w0=-8.026191606879081e-06, w1=-0.08315065187241101\n",
      "Regularized Logistic Regression(39/99): loss=0.5291870584164988, w0=-8.229849711312829e-06, w1=-0.08453681137419451\n",
      "Regularized Logistic Regression(40/99): loss=0.528413741777859, w0=-8.433380355593496e-06, w1=-0.08590586253062395\n",
      "Regularized Logistic Regression(41/99): loss=0.5276738526741019, w0=-8.636783258866013e-06, w1=-0.08725843801801056\n",
      "Regularized Logistic Regression(42/99): loss=0.5269651159833556, w0=-8.840058241756263e-06, w1=-0.088595136788272\n",
      "Regularized Logistic Regression(43/99): loss=0.526285447836504, w0=-9.043205217496735e-06, w1=-0.08991652630373848\n",
      "Regularized Logistic Regression(44/99): loss=0.5256329373486096, w0=-9.246224183726595e-06, w1=-0.09122314459538634\n",
      "Regularized Logistic Regression(45/99): loss=0.5250058302835086, w0=-9.449115214926455e-06, w1=-0.09251550216060082\n",
      "Regularized Logistic Regression(46/99): loss=0.5244025144277193, w0=-9.651878455447684e-06, w1=-0.0937940837149127\n",
      "Regularized Logistic Regression(47/99): loss=0.5238215064778662, w0=-9.854514113096553e-06, w1=-0.09505934981069299\n",
      "Regularized Logistic Regression(48/99): loss=0.5232614402701007, w0=-1.0057022453234534e-05, w1=-0.09631173833448517\n",
      "Regularized Logistic Regression(49/99): loss=0.5227210562010687, w0=-1.0259403793357586e-05, w1=-0.09755166589350546\n",
      "Regularized Logistic Regression(50/99): loss=0.5221991917082711, w0=-1.0461658498118943e-05, w1=-0.09877952910080529\n",
      "Regularized Logistic Regression(51/99): loss=0.52169477269356, w0=-1.0663786974761886e-05, w1=-0.099995705767677\n",
      "Regularized Logistic Regression(52/99): loss=0.5212068057873711, w0=-1.0865789668930943e-05, w1=-0.10120055601106369\n",
      "Regularized Logistic Regression(53/99): loss=0.5207343713633689, w0=-1.1067667060831952e-05, w1=-0.10239442328299951\n",
      "Regularized Logistic Regression(54/99): loss=0.5202766172237164, w0=-1.1269419661713447e-05, w1=-0.10357763532845222\n",
      "Regularized Logistic Regression(55/99): loss=0.5198327528844165, w0=-1.1471048010643713e-05, w1=-0.10475050507735029\n",
      "Regularized Logistic Regression(56/99): loss=0.519402044398224, w0=-1.167255267155973e-05, w1=-0.10591333147605002\n",
      "Regularized Logistic Regression(57/99): loss=0.5189838096597216, w0=-1.187393423056601e-05, w1=-0.10706640026302254\n",
      "Regularized Logistic Regression(58/99): loss=0.518577414143354, w0=-1.2075193293462976e-05, w1=-0.10820998469311265\n",
      "Regularized Logistic Regression(59/99): loss=0.5181822670306818, w0=-1.2276330483486098e-05, w1=-0.1093443462143397\n",
      "Regularized Logistic Regression(60/99): loss=0.5177978176879156, w0=-1.247734643923853e-05, w1=-0.11046973510085907\n",
      "Regularized Logistic Regression(61/99): loss=0.517423552459039, w0=-1.2678241812801273e-05, w1=-0.1115863910453946\n",
      "Regularized Logistic Regression(62/99): loss=0.5170589917435577, w0=-1.2879017268006239e-05, w1=-0.1126945437141639\n",
      "Regularized Logistic Regression(63/99): loss=0.5167036873312162, w0=-1.3079673478858742e-05, w1=-0.11379441326706731\n",
      "Regularized Logistic Regression(64/99): loss=0.5163572199689528, w0=-1.328021112809704e-05, w1=-0.11488621084567383\n",
      "Regularized Logistic Regression(65/99): loss=0.5160191971379497, w0=-1.3480630905877544e-05, w1=-0.11597013903133249\n",
      "Regularized Logistic Regression(66/99): loss=0.5156892510209207, w0=-1.3680933508575278e-05, w1=-0.11704639227554088\n",
      "Regularized Logistic Regression(67/99): loss=0.5153670366418351, w0=-1.3881119637690001e-05, w1=-0.11811515730453338\n",
      "Regularized Logistic Regression(68/99): loss=0.5150522301620764, w0=-1.4081189998849203e-05, w1=-0.11917661349989152\n",
      "Regularized Logistic Regression(69/99): loss=0.5147445273186496, w0=-1.4281145300899902e-05, w1=-0.1202309332568364\n",
      "Regularized Logistic Regression(70/99): loss=0.5144436419914895, w0=-1.4480986255081855e-05, w1=-0.1212782823217303\n",
      "Regularized Logistic Regression(71/99): loss=0.5141493048882048, w0=-1.4680713574275394e-05, w1=-0.1223188201101976\n",
      "Regularized Logistic Regression(72/99): loss=0.5138612623357344, w0=-1.488032797231765e-05, w1=-0.1233527000071645\n",
      "Regularized Logistic Regression(73/99): loss=0.5135792751694174, w0=-1.5079830163381474e-05, w1=-0.12438006965001729\n",
      "Regularized Logistic Regression(74/99): loss=0.5133031177108979, w0=-1.52792208614118e-05, w1=-0.12540107119599042\n",
      "Regularized Logistic Regression(75/99): loss=0.5130325768270945, w0=-1.5478500779614662e-05, w1=-0.12641584157480729\n",
      "Regularized Logistic Regression(76/99): loss=0.5127674510632166, w0=-1.5677670629994445e-05, w1=-0.12742451272752617\n",
      "Regularized Logistic Regression(77/99): loss=0.512507549843452, w0=-1.5876731122935348e-05, w1=-0.1284272118324696\n",
      "Regularized Logistic Regression(78/99): loss=0.5122526927335589, w0=-1.6075682966823318e-05, w1=-0.12942406151905286\n",
      "Regularized Logistic Regression(79/99): loss=0.5120027087601196, w0=-1.62745268677051e-05, w1=-0.1304151800702676\n",
      "Regularized Logistic Regression(80/99): loss=0.5117574357816949, w0=-1.6473263528981236e-05, w1=-0.13140068161452398\n",
      "Regularized Logistic Regression(81/99): loss=0.5115167199075565, w0=-1.667189365113018e-05, w1=-0.13238067630750222\n",
      "Regularized Logistic Regression(82/99): loss=0.5112804149600572, w0=-1.6870417931460906e-05, w1=-0.13335527050462184\n",
      "Regularized Logistic Regression(83/99): loss=0.5110483819770552, w0=-1.706883706389156e-05, w1=-0.13432456692469216\n",
      "Regularized Logistic Regression(84/99): loss=0.5108204887511293, w0=-1.7267151738752005e-05, w1=-0.13528866480526794\n",
      "Regularized Logistic Regression(85/99): loss=0.5105966094026039, w0=-1.7465362642608166e-05, w1=-0.1362476600502034\n",
      "Regularized Logistic Regression(86/99): loss=0.5103766239836677, w0=-1.7663470458106357e-05, w1=-0.13720164536985657\n",
      "Regularized Logistic Regression(87/99): loss=0.5101604181111054, w0=-1.786147586383587e-05, w1=-0.13815071041437305\n",
      "Regularized Logistic Regression(88/99): loss=0.5099478826253742, w0=-1.8059379534208238e-05, w1=-0.13909494190044466\n",
      "Regularized Logistic Regression(89/99): loss=0.5097389132739484, w0=-1.8257182139351763e-05, w1=-0.1400344237319149\n",
      "Regularized Logistic Regression(90/99): loss=0.509533410417043, w0=-1.845488434501996e-05, w1=-0.14096923711457768\n",
      "Regularized Logistic Regression(91/99): loss=0.5093312787539684, w0=-1.865248681251272e-05, w1=-0.14189946066549458\n",
      "Regularized Logistic Regression(92/99): loss=0.5091324270685328, w0=-1.884999019860907e-05, w1=-0.142825170517133\n",
      "Regularized Logistic Regression(93/99): loss=0.5089367679920294, w0=-1.9047395155510526e-05, w1=-0.14374644041661086\n",
      "Regularized Logistic Regression(94/99): loss=0.5087442177824703, w0=-1.924470233079407e-05, w1=-0.14466334182031326\n",
      "Regularized Logistic Regression(95/99): loss=0.5085546961188386, w0=-1.9441912367373914e-05, w1=-0.14557594398413085\n",
      "Regularized Logistic Regression(96/99): loss=0.5083681259092293, w0=-1.963902590347127e-05, w1=-0.14648431404955511\n",
      "Regularized Logistic Regression(97/99): loss=0.5081844331118431, w0=-1.9836043572591357e-05, w1=-0.14738851712584927\n",
      "Regularized Logistic Regression(98/99): loss=0.5080035465678797, w0=-2.003296600350704e-05, w1=-0.14828861636850119\n",
      "Regularized Logistic Regression(99/99): loss=0.507825397845451, w0=-2.0229793820248452e-05, w1=-0.14918467305415437\n",
      "Regularized Logistic Regression(0/199): loss=0.693147180559945, w0=-2.0882930292778685e-07, w1=-0.003658898216597753\n",
      "Regularized Logistic Regression(1/199): loss=0.6776955923065077, w0=-4.169962701924912e-07, w1=-0.007150181685440866\n",
      "Regularized Logistic Regression(2/199): loss=0.6638822204426907, w0=-6.246728750319227e-07, w1=-0.010488031183482814\n",
      "Regularized Logistic Regression(3/199): loss=0.6515149783929285, w0=-8.319892409548812e-07, w1=-0.01368522358718803\n",
      "Regularized Logistic Regression(4/199): loss=0.640424722544377, w0=-1.039042448731666e-06, w1=-0.0167532354513428\n",
      "Regularized Logistic Regression(5/199): loss=0.6304626011750312, w0=-1.2459037871272066e-06, w1=-0.01970236388848945\n",
      "Regularized Logistic Regression(6/199): loss=0.6214977097265673, w0=-1.4526246175856188e-06, w1=-0.022541850158026053\n",
      "Regularized Logistic Regression(7/199): loss=0.6134150035278391, w0=-1.6592410512095323e-06, w1=-0.025279997708318735\n",
      "Regularized Logistic Regression(8/199): loss=0.60611343168562, w0=-1.865777636058614e-06, w1=-0.027924280554486376\n",
      "Regularized Logistic Regression(9/199): loss=0.5995042666930963, w0=-2.072250236838718e-06, w1=-0.03048144042067572\n",
      "Regularized Logistic Regression(10/199): loss=0.5935096113571188, w0=-2.278668265996055e-06, w1=-0.03295757254254673\n",
      "Regularized Logistic Regression(11/199): loss=0.5880610684112597, w0=-2.485036400328833e-06, w1=-0.035358200800256534\n",
      "Regularized Logistic Regression(12/199): loss=0.583098559820608, w0=-2.6913558934430878e-06, w1=-0.037688343204250775\n",
      "Regularized Logistic Regression(13/199): loss=0.5785692833348673, w0=-2.89762557316298e-06, w1=-0.03995256886353105\n",
      "Regularized Logistic Regression(14/199): loss=0.5744267940123832, w0=-3.10384259488553e-06, w1=-0.042155047540973906\n",
      "Regularized Logistic Regression(15/199): loss=0.570630198608567, w0=-3.3100030068454805e-06, w1=-0.04429959281057337\n",
      "Regularized Logistic Regression(16/199): loss=0.5671434510646289, w0=-3.516102171058805e-06, w1=-0.046389699716461584\n",
      "Regularized Logistic Regression(17/199): loss=0.563934737880546, w0=-3.722135073962921e-06, w1=-0.04842857771476157\n",
      "Regularized Logistic Regression(18/199): loss=0.5609759428778833, w0=-3.9280965530659485e-06, w1=-0.050419179567610696\n",
      "Regularized Logistic Regression(19/199): loss=0.5582421816969212, w0=-4.133981459879748e-06, w1=-0.05236422675881614\n",
      "Regularized Logistic Regression(20/199): loss=0.5557113972707043, w0=-4.33978477471136e-06, w1=-0.05426623191387268\n",
      "Regularized Logistic Regression(21/199): loss=0.5533640084270813, w0=-4.545501685246345e-06, w1=-0.056127518633122056\n",
      "Regularized Logistic Regression(22/199): loss=0.5511826046519578, w0=-4.751127638047214e-06, w1=-0.05795023908439821\n",
      "Regularized Logistic Regression(23/199): loss=0.5491516808781363, w0=-4.956658369926896e-06, w1=-0.05973638964913545\n",
      "Regularized Logistic Regression(24/199): loss=0.5472574069296435, w0=-5.1620899244956925e-06, w1=-0.061487824872087794\n",
      "Regularized Logistic Regression(25/199): loss=0.5454874269443699, w0=-5.367418657905939e-06, w1=-0.06320626992814232\n",
      "Regularized Logistic Regression(26/199): loss=0.5438306847167236, w0=-5.572641236842651e-06, w1=-0.06489333178902201\n",
      "Regularized Logistic Regression(27/199): loss=0.5422772714490199, w0=-5.777754631061811e-06, w1=-0.06655050924690431\n",
      "Regularized Logistic Regression(28/199): loss=0.540818292879958, w0=-5.982756102207219e-06, w1=-0.06817920193031383\n",
      "Regularized Logistic Regression(29/199): loss=0.5394457531764244, w0=-6.187643190201117e-06, w1=-0.06978071842933939\n",
      "Regularized Logistic Regression(30/199): loss=0.538152453337257, w0=-6.392413698171456e-06, w1=-0.07135628363172858\n",
      "Regularized Logistic Regression(31/199): loss=0.5369319021706687, w0=-6.597065676625606e-06, w1=-0.07290704535822672\n",
      "Regularized Logistic Regression(32/199): loss=0.5357782381767828, w0=-6.801597407387919e-06, w1=-0.07443408037427915\n",
      "Regularized Logistic Regression(33/199): loss=0.5346861608986765, w0=-7.006007387672619e-06, w1=-0.07593839984558773\n",
      "Regularized Logistic Regression(34/199): loss=0.5336508705045079, w0=-7.21029431455317e-06, w1=-0.07742095429673258\n",
      "Regularized Logistic Regression(35/199): loss=0.5326680145341901, w0=-7.414457070006136e-06, w1=-0.07888263812493898\n",
      "Regularized Logistic Regression(36/199): loss=0.5317336408906204, w0=-7.618494706645257e-06, w1=-0.08032429371490143\n",
      "Regularized Logistic Regression(37/199): loss=0.5308441562811397, w0=-7.822406434215057e-06, w1=-0.08174671519522728\n",
      "Regularized Logistic Regression(38/199): loss=0.5299962894226783, w0=-8.026191606879081e-06, w1=-0.08315065187241101\n",
      "Regularized Logistic Regression(39/199): loss=0.5291870584164988, w0=-8.229849711312829e-06, w1=-0.08453681137419451\n",
      "Regularized Logistic Regression(40/199): loss=0.528413741777859, w0=-8.433380355593496e-06, w1=-0.08590586253062395\n",
      "Regularized Logistic Regression(41/199): loss=0.5276738526741019, w0=-8.636783258866013e-06, w1=-0.08725843801801056\n",
      "Regularized Logistic Regression(42/199): loss=0.5269651159833556, w0=-8.840058241756263e-06, w1=-0.088595136788272\n",
      "Regularized Logistic Regression(43/199): loss=0.526285447836504, w0=-9.043205217496735e-06, w1=-0.08991652630373848\n",
      "Regularized Logistic Regression(44/199): loss=0.5256329373486096, w0=-9.246224183726595e-06, w1=-0.09122314459538634\n",
      "Regularized Logistic Regression(45/199): loss=0.5250058302835086, w0=-9.449115214926455e-06, w1=-0.09251550216060082\n",
      "Regularized Logistic Regression(46/199): loss=0.5244025144277193, w0=-9.651878455447684e-06, w1=-0.0937940837149127\n",
      "Regularized Logistic Regression(47/199): loss=0.5238215064778662, w0=-9.854514113096553e-06, w1=-0.09505934981069299\n",
      "Regularized Logistic Regression(48/199): loss=0.5232614402701007, w0=-1.0057022453234534e-05, w1=-0.09631173833448517\n",
      "Regularized Logistic Regression(49/199): loss=0.5227210562010687, w0=-1.0259403793357586e-05, w1=-0.09755166589350546\n",
      "Regularized Logistic Regression(50/199): loss=0.5221991917082711, w0=-1.0461658498118943e-05, w1=-0.09877952910080529\n",
      "Regularized Logistic Regression(51/199): loss=0.52169477269356, w0=-1.0663786974761886e-05, w1=-0.099995705767677\n",
      "Regularized Logistic Regression(52/199): loss=0.5212068057873711, w0=-1.0865789668930943e-05, w1=-0.10120055601106369\n",
      "Regularized Logistic Regression(53/199): loss=0.5207343713633689, w0=-1.1067667060831952e-05, w1=-0.10239442328299951\n",
      "Regularized Logistic Regression(54/199): loss=0.5202766172237164, w0=-1.1269419661713447e-05, w1=-0.10357763532845222\n",
      "Regularized Logistic Regression(55/199): loss=0.5198327528844165, w0=-1.1471048010643713e-05, w1=-0.10475050507735029\n",
      "Regularized Logistic Regression(56/199): loss=0.519402044398224, w0=-1.167255267155973e-05, w1=-0.10591333147605002\n",
      "Regularized Logistic Regression(57/199): loss=0.5189838096597216, w0=-1.187393423056601e-05, w1=-0.10706640026302254\n",
      "Regularized Logistic Regression(58/199): loss=0.518577414143354, w0=-1.2075193293462976e-05, w1=-0.10820998469311265\n",
      "Regularized Logistic Regression(59/199): loss=0.5181822670306818, w0=-1.2276330483486098e-05, w1=-0.1093443462143397\n",
      "Regularized Logistic Regression(60/199): loss=0.5177978176879156, w0=-1.247734643923853e-05, w1=-0.11046973510085907\n",
      "Regularized Logistic Regression(61/199): loss=0.517423552459039, w0=-1.2678241812801273e-05, w1=-0.1115863910453946\n",
      "Regularized Logistic Regression(62/199): loss=0.5170589917435577, w0=-1.2879017268006239e-05, w1=-0.1126945437141639\n",
      "Regularized Logistic Regression(63/199): loss=0.5167036873312162, w0=-1.3079673478858742e-05, w1=-0.11379441326706731\n",
      "Regularized Logistic Regression(64/199): loss=0.5163572199689528, w0=-1.328021112809704e-05, w1=-0.11488621084567383\n",
      "Regularized Logistic Regression(65/199): loss=0.5160191971379497, w0=-1.3480630905877544e-05, w1=-0.11597013903133249\n",
      "Regularized Logistic Regression(66/199): loss=0.5156892510209207, w0=-1.3680933508575278e-05, w1=-0.11704639227554088\n",
      "Regularized Logistic Regression(67/199): loss=0.5153670366418351, w0=-1.3881119637690001e-05, w1=-0.11811515730453338\n",
      "Regularized Logistic Regression(68/199): loss=0.5150522301620764, w0=-1.4081189998849203e-05, w1=-0.11917661349989152\n",
      "Regularized Logistic Regression(69/199): loss=0.5147445273186496, w0=-1.4281145300899902e-05, w1=-0.1202309332568364\n",
      "Regularized Logistic Regression(70/199): loss=0.5144436419914895, w0=-1.4480986255081855e-05, w1=-0.1212782823217303\n",
      "Regularized Logistic Regression(71/199): loss=0.5141493048882048, w0=-1.4680713574275394e-05, w1=-0.1223188201101976\n",
      "Regularized Logistic Regression(72/199): loss=0.5138612623357344, w0=-1.488032797231765e-05, w1=-0.1233527000071645\n",
      "Regularized Logistic Regression(73/199): loss=0.5135792751694174, w0=-1.5079830163381474e-05, w1=-0.12438006965001729\n",
      "Regularized Logistic Regression(74/199): loss=0.5133031177108979, w0=-1.52792208614118e-05, w1=-0.12540107119599042\n",
      "Regularized Logistic Regression(75/199): loss=0.5130325768270945, w0=-1.5478500779614662e-05, w1=-0.12641584157480729\n",
      "Regularized Logistic Regression(76/199): loss=0.5127674510632166, w0=-1.5677670629994445e-05, w1=-0.12742451272752617\n",
      "Regularized Logistic Regression(77/199): loss=0.512507549843452, w0=-1.5876731122935348e-05, w1=-0.1284272118324696\n",
      "Regularized Logistic Regression(78/199): loss=0.5122526927335589, w0=-1.6075682966823318e-05, w1=-0.12942406151905286\n",
      "Regularized Logistic Regression(79/199): loss=0.5120027087601196, w0=-1.62745268677051e-05, w1=-0.1304151800702676\n",
      "Regularized Logistic Regression(80/199): loss=0.5117574357816949, w0=-1.6473263528981236e-05, w1=-0.13140068161452398\n",
      "Regularized Logistic Regression(81/199): loss=0.5115167199075565, w0=-1.667189365113018e-05, w1=-0.13238067630750222\n",
      "Regularized Logistic Regression(82/199): loss=0.5112804149600572, w0=-1.6870417931460906e-05, w1=-0.13335527050462184\n",
      "Regularized Logistic Regression(83/199): loss=0.5110483819770552, w0=-1.706883706389156e-05, w1=-0.13432456692469216\n",
      "Regularized Logistic Regression(84/199): loss=0.5108204887511293, w0=-1.7267151738752005e-05, w1=-0.13528866480526794\n",
      "Regularized Logistic Regression(85/199): loss=0.5105966094026039, w0=-1.7465362642608166e-05, w1=-0.1362476600502034\n",
      "Regularized Logistic Regression(86/199): loss=0.5103766239836677, w0=-1.7663470458106357e-05, w1=-0.13720164536985657\n",
      "Regularized Logistic Regression(87/199): loss=0.5101604181111054, w0=-1.786147586383587e-05, w1=-0.13815071041437305\n",
      "Regularized Logistic Regression(88/199): loss=0.5099478826253742, w0=-1.8059379534208238e-05, w1=-0.13909494190044466\n",
      "Regularized Logistic Regression(89/199): loss=0.5097389132739484, w0=-1.8257182139351763e-05, w1=-0.1400344237319149\n",
      "Regularized Logistic Regression(90/199): loss=0.509533410417043, w0=-1.845488434501996e-05, w1=-0.14096923711457768\n",
      "Regularized Logistic Regression(91/199): loss=0.5093312787539684, w0=-1.865248681251272e-05, w1=-0.14189946066549458\n",
      "Regularized Logistic Regression(92/199): loss=0.5091324270685328, w0=-1.884999019860907e-05, w1=-0.142825170517133\n",
      "Regularized Logistic Regression(93/199): loss=0.5089367679920294, w0=-1.9047395155510526e-05, w1=-0.14374644041661086\n",
      "Regularized Logistic Regression(94/199): loss=0.5087442177824703, w0=-1.924470233079407e-05, w1=-0.14466334182031326\n",
      "Regularized Logistic Regression(95/199): loss=0.5085546961188386, w0=-1.9441912367373914e-05, w1=-0.14557594398413085\n",
      "Regularized Logistic Regression(96/199): loss=0.5083681259092293, w0=-1.963902590347127e-05, w1=-0.14648431404955511\n",
      "Regularized Logistic Regression(97/199): loss=0.5081844331118431, w0=-1.9836043572591357e-05, w1=-0.14738851712584927\n",
      "Regularized Logistic Regression(98/199): loss=0.5080035465678797, w0=-2.003296600350704e-05, w1=-0.14828861636850119\n",
      "Regularized Logistic Regression(99/199): loss=0.507825397845451, w0=-2.0229793820248452e-05, w1=-0.14918467305415437\n",
      "Regularized Logistic Regression(100/199): loss=0.5076499210937109, w0=-2.0426527642098024e-05, w1=-0.15007674665219556\n",
      "Regularized Logistic Regression(101/199): loss=0.5074770529064524, w0=-2.062316808359047e-05, w1=-0.1509648948931749\n",
      "Regularized Logistic Regression(102/199): loss=0.5073067321944925, w0=-2.0819715754517218e-05, w1=-0.151849173834216\n",
      "Regularized Logistic Regression(103/199): loss=0.5071389000662081, w0=-2.1016171259934816e-05, w1=-0.15272963792157157\n",
      "Regularized Logistic Regression(104/199): loss=0.5069734997156448, w0=-2.121253520017702e-05, w1=-0.15360634005046422\n",
      "Regularized Logistic Regression(105/199): loss=0.5068104763176549, w0=-2.1408808170870096e-05, w1=-0.15447933162235053\n",
      "Regularized Logistic Regression(106/199): loss=0.5066497769295766, w0=-2.1604990762951073e-05, w1=-0.15534866259973354\n",
      "Regularized Logistic Regression(107/199): loss=0.5064913503989846, w0=-2.1801083562688605e-05, w1=-0.1562143815586449\n",
      "Regularized Logistic Regression(108/199): loss=0.5063351472771006, w0=-2.1997087151706197e-05, w1=-0.15707653573890956\n",
      "Regularized Logistic Regression(109/199): loss=0.5061811197374599, w0=-2.2193002107007503e-05, w1=-0.1579351710922997\n",
      "Regularized Logistic Regression(110/199): loss=0.506029221499481, w0=-2.2388829001003488e-05, w1=-0.15879033232867995\n",
      "Regularized Logistic Regression(111/199): loss=0.5058794077565973, w0=-2.2584568401541237e-05, w1=-0.15964206296023867\n",
      "Regularized Logistic Regression(112/199): loss=0.505731635108642, w0=-2.278022087193422e-05, w1=-0.1604904053438958\n",
      "Regularized Logistic Regression(113/199): loss=0.505585861498199, w0=-2.29757869709938e-05, w1=-0.16133540072197314\n",
      "Regularized Logistic Regression(114/199): loss=0.5054420461506507, w0=-2.3171267253061887e-05, w1=-0.16217708926120755\n",
      "Regularized Logistic Regression(115/199): loss=0.5053001495176814, w0=-2.3366662268044525e-05, w1=-0.16301551009018506\n",
      "Regularized Logistic Regression(116/199): loss=0.5051601332239994, w0=-2.3561972561446315e-05, w1=-0.1638507013352656\n",
      "Regularized Logistic Regression(117/199): loss=0.5050219600170704, w0=-2.3757198674405533e-05, w1=-0.1646827001550705\n",
      "Regularized Logistic Regression(118/199): loss=0.504885593719665, w0=-2.395234114372985e-05, w1=-0.16551154277359567\n",
      "Regularized Logistic Regression(119/199): loss=0.5047509991850332, w0=-2.4147400501932523e-05, w1=-0.16633726451201328\n",
      "Regularized Logistic Regression(120/199): loss=0.5046181422545396, w0=-2.434237727726899e-05, w1=-0.16715989981922091\n",
      "Regularized Logistic Regression(121/199): loss=0.5044869897176002, w0=-2.453727199377378e-05, w1=-0.16797948230119308\n",
      "Regularized Logistic Regression(122/199): loss=0.5043575092737712, w0=-2.4732085171297627e-05, w1=-0.16879604474918838\n",
      "Regularized Logistic Regression(123/199): loss=0.5042296694968581, w0=-2.492681732554479e-05, w1=-0.1696096191668629\n",
      "Regularized Logistic Regression(124/199): loss=0.5041034398009118, w0=-2.5121468968110445e-05, w1=-0.1704202367963376\n",
      "Regularized Logistic Regression(125/199): loss=0.5039787904079991, w0=-2.5316040606518127e-05, w1=-0.171227928143264\n",
      "Regularized Logistic Regression(126/199): loss=0.5038556923176338, w0=-2.5510532744257168e-05, w1=-0.1720327230009335\n",
      "Regularized Logistic Regression(127/199): loss=0.5037341172777671, w0=-2.5704945880820083e-05, w1=-0.17283465047346855\n",
      "Regularized Logistic Regression(128/199): loss=0.5036140377572402, w0=-2.589928051173986e-05, w1=-0.1736337389981379\n",
      "Regularized Logistic Regression(129/199): loss=0.5034954269196136, w0=-2.6093537128627115e-05, w1=-0.17443001636683078\n",
      "Regularized Logistic Regression(130/199): loss=0.5033782585982857, w0=-2.6287716219207085e-05, w1=-0.1752235097467259\n",
      "Regularized Logistic Regression(131/199): loss=0.5032625072728244, w0=-2.6481818267356433e-05, w1=-0.17601424570019075\n",
      "Regularized Logistic Regression(132/199): loss=0.5031481480464437, w0=-2.6675843753139817e-05, w1=-0.17680225020394028\n",
      "Regularized Logistic Regression(133/199): loss=0.5030351566245495, w0=-2.686979315284623e-05, w1=-0.17758754866748883\n",
      "Regularized Logistic Regression(134/199): loss=0.5029235092942995, w0=-2.706366693902507e-05, w1=-0.17837016595092256\n",
      "Regularized Logistic Regression(135/199): loss=0.502813182905114, w0=-2.725746558052191e-05, w1=-0.17915012638202008\n",
      "Regularized Logistic Regression(136/199): loss=0.502704154850084, w0=-2.7451189542514007e-05, w1=-0.1799274537727501\n",
      "Regularized Logistic Regression(137/199): loss=0.502596403048224, w0=-2.7644839286545456e-05, w1=-0.1807021714351687\n",
      "Regularized Logistic Regression(138/199): loss=0.5024899059275225, w0=-2.7838415270562054e-05, w1=-0.18147430219674224\n",
      "Regularized Logistic Regression(139/199): loss=0.5023846424087469, w0=-2.8031917948945805e-05, w1=-0.182243868415119\n",
      "Regularized Logistic Regression(140/199): loss=0.5022805918899562, w0=-2.822534777254909e-05, w1=-0.18301089199236978\n",
      "Regularized Logistic Regression(141/199): loss=0.5021777342316848, w0=-2.841870518872849e-05, w1=-0.18377539438872226\n",
      "Regularized Logistic Regression(142/199): loss=0.5020760497427633, w0=-2.8611990641378256e-05, w1=-0.18453739663580485\n",
      "Regularized Logistic Regression(143/199): loss=0.501975519166735, w0=-2.8805204570963393e-05, w1=-0.1852969193494232\n",
      "Regularized Logistic Regression(144/199): loss=0.5018761236688428, w0=-2.899834741455241e-05, w1=-0.18605398274188506\n",
      "Regularized Logistic Regression(145/199): loss=0.5017778448235499, w0=-2.9191419605849692e-05, w1=-0.18680860663389295\n",
      "Regularized Logistic Regression(146/199): loss=0.5016806646025714, w0=-2.938442157522749e-05, w1=-0.18756081046602077\n",
      "Regularized Logistic Regression(147/199): loss=0.5015845653633855, w0=-2.957735374975756e-05, w1=-0.18831061330979032\n",
      "Regularized Logistic Regression(148/199): loss=0.5014895298382018, w0=-2.9770216553242414e-05, w1=-0.18905803387836312\n",
      "Regularized Logistic Regression(149/199): loss=0.5013955411233635, w0=-2.996301040624622e-05, w1=-0.18980309053686348\n",
      "Regularized Logistic Regression(150/199): loss=0.5013025826691578, w0=-3.0155735726125302e-05, w1=-0.19054580131234522\n",
      "Regularized Logistic Regression(151/199): loss=0.5012106382700191, w0=-3.0348392927058313e-05, w1=-0.19128618390341764\n",
      "Regularized Logistic Regression(152/199): loss=0.5011196920551009, w0=-3.054098242007601e-05, w1=-0.19202425568954157\n",
      "Regularized Logistic Regression(153/199): loss=0.5010297284791995, w0=-3.073350461309067e-05, w1=-0.19276003374000958\n",
      "Regularized Logistic Regression(154/199): loss=0.5009407323140143, w0=-3.092595991092516e-05, w1=-0.19349353482262177\n",
      "Regularized Logistic Regression(155/199): loss=0.500852688639724, w0=-3.111834871534164e-05, w1=-0.19422477541206815\n",
      "Regularized Logistic Regression(156/199): loss=0.5007655828368678, w0=-3.1310671425069884e-05, w1=-0.1949537716980297\n",
      "Regularized Logistic Regression(157/199): loss=0.5006794005785146, w0=-3.150292843583532e-05, w1=-0.1956805395930074\n",
      "Regularized Logistic Regression(158/199): loss=0.5005941278227072, w0=-3.169512014038662e-05, w1=-0.19640509473989015\n",
      "Regularized Logistic Regression(159/199): loss=0.5005097508051685, w0=-3.188724692852306e-05, w1=-0.1971274525192703\n",
      "Regularized Logistic Regression(160/199): loss=0.5004262560322585, w0=-3.2079309187121444e-05, w1=-0.19784762805651787\n",
      "Regularized Logistic Regression(161/199): loss=0.5003436302741702, w0=-3.2271307300162737e-05, w1=-0.19856563622862033\n",
      "Regularized Logistic Regression(162/199): loss=0.5002618605583508, w0=-3.2463241648758374e-05, w1=-0.199281491670798\n",
      "Regularized Logistic Regression(163/199): loss=0.500180934163145, w0=-3.265511261117621e-05, w1=-0.19999520878290228\n",
      "Regularized Logistic Regression(164/199): loss=0.5001008386116428, w0=-3.284692056286617e-05, w1=-0.2007068017356053\n",
      "Regularized Logistic Regression(165/199): loss=0.500021561665728, w0=-3.3038665876485574e-05, w1=-0.20141628447638818\n",
      "Regularized Logistic Regression(166/199): loss=0.49994309132031645, w0=-3.323034892192416e-05, w1=-0.2021236707353354\n",
      "Regularized Logistic Regression(167/199): loss=0.4998654157977771, w0=-3.3421970066328776e-05, w1=-0.20282897403074251\n",
      "Regularized Logistic Regression(168/199): loss=0.4997885235425275, w0=-3.3613529674127765e-05, w1=-0.20353220767454225\n",
      "Regularized Logistic Regression(169/199): loss=0.4997124032157964, w0=-3.3805028107055074e-05, w1=-0.20423338477755898\n",
      "Regularized Logistic Regression(170/199): loss=0.49963704369054657, w0=-3.399646572417406e-05, w1=-0.20493251825459327\n",
      "Regularized Logistic Regression(171/199): loss=0.4995624340465523, w0=-3.4187842881900984e-05, w1=-0.20562962082934666\n",
      "Regularized Logistic Regression(172/199): loss=0.49948856356562343, w0=-3.4379159934028244e-05, w1=-0.20632470503918884\n",
      "Regularized Logistic Regression(173/199): loss=0.49941542172697245, w0=-3.457041723174732e-05, w1=-0.20701778323977593\n",
      "Regularized Logistic Regression(174/199): loss=0.49934299820271516, w0=-3.476161512367142e-05, w1=-0.20770886760952303\n",
      "Regularized Logistic Regression(175/199): loss=0.49927128285350636, w0=-3.495275395585787e-05, w1=-0.2083979701539362\n",
      "Regularized Logistic Regression(176/199): loss=0.4992002657242971, w0=-3.514383407183025e-05, w1=-0.2090851027098103\n",
      "Regularized Logistic Regression(177/199): loss=0.49912993704021563, w0=-3.533485581260023e-05, w1=-0.20977027694929612\n",
      "Regularized Logistic Regression(178/199): loss=0.499060287202563, w0=-3.552581951668916e-05, w1=-0.21045350438384106\n",
      "Regularized Logistic Regression(179/199): loss=0.498991306784923, w0=-3.571672552014944e-05, w1=-0.21113479636800958\n",
      "Regularized Logistic Regression(180/199): loss=0.49892298652937683, w0=-3.5907574156585564e-05, w1=-0.2118141641031852\n",
      "Regularized Logistic Regression(181/199): loss=0.49885531734282595, w0=-3.609836575717499e-05, w1=-0.2124916186411595\n",
      "Regularized Logistic Regression(182/199): loss=0.4987882902934105, w0=-3.628910065068869e-05, w1=-0.21316717088761306\n",
      "Regularized Logistic Regression(183/199): loss=0.49872189660702887, w0=-3.647977916351156e-05, w1=-0.21384083160548892\n",
      "Regularized Logistic Regression(184/199): loss=0.49865612766394696, w0=-3.667040161966248e-05, w1=-0.2145126114182655\n",
      "Regularized Logistic Regression(185/199): loss=0.49859097499550026, w0=-3.686096834081425e-05, w1=-0.21518252081313052\n",
      "Regularized Logistic Regression(186/199): loss=0.4985264302808801, w0=-3.705147964631319e-05, w1=-0.21585057014405962\n",
      "Regularized Logistic Regression(187/199): loss=0.49846248534400756, w0=-3.724193585319861e-05, w1=-0.2165167696348039\n",
      "Regularized Logistic Regression(188/199): loss=0.4983991321504844, w0=-3.743233727622202e-05, w1=-0.21718112938178774\n",
      "Regularized Logistic Regression(189/199): loss=0.49833636280462545, w0=-3.7622684227866084e-05, w1=-0.2178436593569212\n",
      "Regularized Logistic Regression(190/199): loss=0.4982741695465653, w0=-3.781297701836344e-05, w1=-0.2185043694103298\n",
      "Regularized Logistic Regression(191/199): loss=0.49821254474943916, w0=-3.800321595571524e-05, w1=-0.21916326927300409\n",
      "Regularized Logistic Regression(192/199): loss=0.4981514809166342, w0=-3.819340134570952e-05, w1=-0.21982036855937126\n",
      "Regularized Logistic Regression(193/199): loss=0.4980909706791097, w0=-3.838353349193934e-05, w1=-0.22047567676979263\n",
      "Regularized Logistic Regression(194/199): loss=0.49803100679278384, w0=-3.857361269582077e-05, w1=-0.22112920329298863\n",
      "Regularized Logistic Regression(195/199): loss=0.4979715821359847, w0=-3.87636392566106e-05, w1=-0.22178095740839338\n",
      "Regularized Logistic Regression(196/199): loss=0.49791268970696456, w0=-3.8953613471423954e-05, w1=-0.22243094828844226\n",
      "Regularized Logistic Regression(197/199): loss=0.4978543226214732, w0=-3.914353563525163e-05, w1=-0.22307918500079382\n",
      "Regularized Logistic Regression(198/199): loss=0.49779647411039113, w0=-3.933340604097727e-05, w1=-0.22372567651048808\n",
      "Regularized Logistic Regression(199/199): loss=0.49773913751741894, w0=-3.952322497939439e-05, w1=-0.22437043168204437\n",
      "Regularized Logistic Regression(0/299): loss=0.693147180559945, w0=-2.0882930292778685e-07, w1=-0.003658898216597753\n",
      "Regularized Logistic Regression(1/299): loss=0.6776955923065077, w0=-4.169962701924912e-07, w1=-0.007150181685440866\n",
      "Regularized Logistic Regression(2/299): loss=0.6638822204426907, w0=-6.246728750319227e-07, w1=-0.010488031183482814\n",
      "Regularized Logistic Regression(3/299): loss=0.6515149783929285, w0=-8.319892409548812e-07, w1=-0.01368522358718803\n",
      "Regularized Logistic Regression(4/299): loss=0.640424722544377, w0=-1.039042448731666e-06, w1=-0.0167532354513428\n",
      "Regularized Logistic Regression(5/299): loss=0.6304626011750312, w0=-1.2459037871272066e-06, w1=-0.01970236388848945\n",
      "Regularized Logistic Regression(6/299): loss=0.6214977097265673, w0=-1.4526246175856188e-06, w1=-0.022541850158026053\n",
      "Regularized Logistic Regression(7/299): loss=0.6134150035278391, w0=-1.6592410512095323e-06, w1=-0.025279997708318735\n",
      "Regularized Logistic Regression(8/299): loss=0.60611343168562, w0=-1.865777636058614e-06, w1=-0.027924280554486376\n",
      "Regularized Logistic Regression(9/299): loss=0.5995042666930963, w0=-2.072250236838718e-06, w1=-0.03048144042067572\n",
      "Regularized Logistic Regression(10/299): loss=0.5935096113571188, w0=-2.278668265996055e-06, w1=-0.03295757254254673\n",
      "Regularized Logistic Regression(11/299): loss=0.5880610684112597, w0=-2.485036400328833e-06, w1=-0.035358200800256534\n",
      "Regularized Logistic Regression(12/299): loss=0.583098559820608, w0=-2.6913558934430878e-06, w1=-0.037688343204250775\n",
      "Regularized Logistic Regression(13/299): loss=0.5785692833348673, w0=-2.89762557316298e-06, w1=-0.03995256886353105\n",
      "Regularized Logistic Regression(14/299): loss=0.5744267940123832, w0=-3.10384259488553e-06, w1=-0.042155047540973906\n",
      "Regularized Logistic Regression(15/299): loss=0.570630198608567, w0=-3.3100030068454805e-06, w1=-0.04429959281057337\n",
      "Regularized Logistic Regression(16/299): loss=0.5671434510646289, w0=-3.516102171058805e-06, w1=-0.046389699716461584\n",
      "Regularized Logistic Regression(17/299): loss=0.563934737880546, w0=-3.722135073962921e-06, w1=-0.04842857771476157\n",
      "Regularized Logistic Regression(18/299): loss=0.5609759428778833, w0=-3.9280965530659485e-06, w1=-0.050419179567610696\n",
      "Regularized Logistic Regression(19/299): loss=0.5582421816969212, w0=-4.133981459879748e-06, w1=-0.05236422675881614\n",
      "Regularized Logistic Regression(20/299): loss=0.5557113972707043, w0=-4.33978477471136e-06, w1=-0.05426623191387268\n",
      "Regularized Logistic Regression(21/299): loss=0.5533640084270813, w0=-4.545501685246345e-06, w1=-0.056127518633122056\n",
      "Regularized Logistic Regression(22/299): loss=0.5511826046519578, w0=-4.751127638047214e-06, w1=-0.05795023908439821\n",
      "Regularized Logistic Regression(23/299): loss=0.5491516808781363, w0=-4.956658369926896e-06, w1=-0.05973638964913545\n",
      "Regularized Logistic Regression(24/299): loss=0.5472574069296435, w0=-5.1620899244956925e-06, w1=-0.061487824872087794\n",
      "Regularized Logistic Regression(25/299): loss=0.5454874269443699, w0=-5.367418657905939e-06, w1=-0.06320626992814232\n",
      "Regularized Logistic Regression(26/299): loss=0.5438306847167236, w0=-5.572641236842651e-06, w1=-0.06489333178902201\n",
      "Regularized Logistic Regression(27/299): loss=0.5422772714490199, w0=-5.777754631061811e-06, w1=-0.06655050924690431\n",
      "Regularized Logistic Regression(28/299): loss=0.540818292879958, w0=-5.982756102207219e-06, w1=-0.06817920193031383\n",
      "Regularized Logistic Regression(29/299): loss=0.5394457531764244, w0=-6.187643190201117e-06, w1=-0.06978071842933939\n",
      "Regularized Logistic Regression(30/299): loss=0.538152453337257, w0=-6.392413698171456e-06, w1=-0.07135628363172858\n",
      "Regularized Logistic Regression(31/299): loss=0.5369319021706687, w0=-6.597065676625606e-06, w1=-0.07290704535822672\n",
      "Regularized Logistic Regression(32/299): loss=0.5357782381767828, w0=-6.801597407387919e-06, w1=-0.07443408037427915\n",
      "Regularized Logistic Regression(33/299): loss=0.5346861608986765, w0=-7.006007387672619e-06, w1=-0.07593839984558773\n",
      "Regularized Logistic Regression(34/299): loss=0.5336508705045079, w0=-7.21029431455317e-06, w1=-0.07742095429673258\n",
      "Regularized Logistic Regression(35/299): loss=0.5326680145341901, w0=-7.414457070006136e-06, w1=-0.07888263812493898\n",
      "Regularized Logistic Regression(36/299): loss=0.5317336408906204, w0=-7.618494706645257e-06, w1=-0.08032429371490143\n",
      "Regularized Logistic Regression(37/299): loss=0.5308441562811397, w0=-7.822406434215057e-06, w1=-0.08174671519522728\n",
      "Regularized Logistic Regression(38/299): loss=0.5299962894226783, w0=-8.026191606879081e-06, w1=-0.08315065187241101\n",
      "Regularized Logistic Regression(39/299): loss=0.5291870584164988, w0=-8.229849711312829e-06, w1=-0.08453681137419451\n",
      "Regularized Logistic Regression(40/299): loss=0.528413741777859, w0=-8.433380355593496e-06, w1=-0.08590586253062395\n",
      "Regularized Logistic Regression(41/299): loss=0.5276738526741019, w0=-8.636783258866013e-06, w1=-0.08725843801801056\n",
      "Regularized Logistic Regression(42/299): loss=0.5269651159833556, w0=-8.840058241756263e-06, w1=-0.088595136788272\n",
      "Regularized Logistic Regression(43/299): loss=0.526285447836504, w0=-9.043205217496735e-06, w1=-0.08991652630373848\n",
      "Regularized Logistic Regression(44/299): loss=0.5256329373486096, w0=-9.246224183726595e-06, w1=-0.09122314459538634\n",
      "Regularized Logistic Regression(45/299): loss=0.5250058302835086, w0=-9.449115214926455e-06, w1=-0.09251550216060082\n",
      "Regularized Logistic Regression(46/299): loss=0.5244025144277193, w0=-9.651878455447684e-06, w1=-0.0937940837149127\n",
      "Regularized Logistic Regression(47/299): loss=0.5238215064778662, w0=-9.854514113096553e-06, w1=-0.09505934981069299\n",
      "Regularized Logistic Regression(48/299): loss=0.5232614402701007, w0=-1.0057022453234534e-05, w1=-0.09631173833448517\n",
      "Regularized Logistic Regression(49/299): loss=0.5227210562010687, w0=-1.0259403793357586e-05, w1=-0.09755166589350546\n",
      "Regularized Logistic Regression(50/299): loss=0.5221991917082711, w0=-1.0461658498118943e-05, w1=-0.09877952910080529\n",
      "Regularized Logistic Regression(51/299): loss=0.52169477269356, w0=-1.0663786974761886e-05, w1=-0.099995705767677\n",
      "Regularized Logistic Regression(52/299): loss=0.5212068057873711, w0=-1.0865789668930943e-05, w1=-0.10120055601106369\n",
      "Regularized Logistic Regression(53/299): loss=0.5207343713633689, w0=-1.1067667060831952e-05, w1=-0.10239442328299951\n",
      "Regularized Logistic Regression(54/299): loss=0.5202766172237164, w0=-1.1269419661713447e-05, w1=-0.10357763532845222\n",
      "Regularized Logistic Regression(55/299): loss=0.5198327528844165, w0=-1.1471048010643713e-05, w1=-0.10475050507735029\n",
      "Regularized Logistic Regression(56/299): loss=0.519402044398224, w0=-1.167255267155973e-05, w1=-0.10591333147605002\n",
      "Regularized Logistic Regression(57/299): loss=0.5189838096597216, w0=-1.187393423056601e-05, w1=-0.10706640026302254\n",
      "Regularized Logistic Regression(58/299): loss=0.518577414143354, w0=-1.2075193293462976e-05, w1=-0.10820998469311265\n",
      "Regularized Logistic Regression(59/299): loss=0.5181822670306818, w0=-1.2276330483486098e-05, w1=-0.1093443462143397\n",
      "Regularized Logistic Regression(60/299): loss=0.5177978176879156, w0=-1.247734643923853e-05, w1=-0.11046973510085907\n",
      "Regularized Logistic Regression(61/299): loss=0.517423552459039, w0=-1.2678241812801273e-05, w1=-0.1115863910453946\n",
      "Regularized Logistic Regression(62/299): loss=0.5170589917435577, w0=-1.2879017268006239e-05, w1=-0.1126945437141639\n",
      "Regularized Logistic Regression(63/299): loss=0.5167036873312162, w0=-1.3079673478858742e-05, w1=-0.11379441326706731\n",
      "Regularized Logistic Regression(64/299): loss=0.5163572199689528, w0=-1.328021112809704e-05, w1=-0.11488621084567383\n",
      "Regularized Logistic Regression(65/299): loss=0.5160191971379497, w0=-1.3480630905877544e-05, w1=-0.11597013903133249\n",
      "Regularized Logistic Regression(66/299): loss=0.5156892510209207, w0=-1.3680933508575278e-05, w1=-0.11704639227554088\n",
      "Regularized Logistic Regression(67/299): loss=0.5153670366418351, w0=-1.3881119637690001e-05, w1=-0.11811515730453338\n",
      "Regularized Logistic Regression(68/299): loss=0.5150522301620764, w0=-1.4081189998849203e-05, w1=-0.11917661349989152\n",
      "Regularized Logistic Regression(69/299): loss=0.5147445273186496, w0=-1.4281145300899902e-05, w1=-0.1202309332568364\n",
      "Regularized Logistic Regression(70/299): loss=0.5144436419914895, w0=-1.4480986255081855e-05, w1=-0.1212782823217303\n",
      "Regularized Logistic Regression(71/299): loss=0.5141493048882048, w0=-1.4680713574275394e-05, w1=-0.1223188201101976\n",
      "Regularized Logistic Regression(72/299): loss=0.5138612623357344, w0=-1.488032797231765e-05, w1=-0.1233527000071645\n",
      "Regularized Logistic Regression(73/299): loss=0.5135792751694174, w0=-1.5079830163381474e-05, w1=-0.12438006965001729\n",
      "Regularized Logistic Regression(74/299): loss=0.5133031177108979, w0=-1.52792208614118e-05, w1=-0.12540107119599042\n",
      "Regularized Logistic Regression(75/299): loss=0.5130325768270945, w0=-1.5478500779614662e-05, w1=-0.12641584157480729\n",
      "Regularized Logistic Regression(76/299): loss=0.5127674510632166, w0=-1.5677670629994445e-05, w1=-0.12742451272752617\n",
      "Regularized Logistic Regression(77/299): loss=0.512507549843452, w0=-1.5876731122935348e-05, w1=-0.1284272118324696\n",
      "Regularized Logistic Regression(78/299): loss=0.5122526927335589, w0=-1.6075682966823318e-05, w1=-0.12942406151905286\n",
      "Regularized Logistic Regression(79/299): loss=0.5120027087601196, w0=-1.62745268677051e-05, w1=-0.1304151800702676\n",
      "Regularized Logistic Regression(80/299): loss=0.5117574357816949, w0=-1.6473263528981236e-05, w1=-0.13140068161452398\n",
      "Regularized Logistic Regression(81/299): loss=0.5115167199075565, w0=-1.667189365113018e-05, w1=-0.13238067630750222\n",
      "Regularized Logistic Regression(82/299): loss=0.5112804149600572, w0=-1.6870417931460906e-05, w1=-0.13335527050462184\n",
      "Regularized Logistic Regression(83/299): loss=0.5110483819770552, w0=-1.706883706389156e-05, w1=-0.13432456692469216\n",
      "Regularized Logistic Regression(84/299): loss=0.5108204887511293, w0=-1.7267151738752005e-05, w1=-0.13528866480526794\n",
      "Regularized Logistic Regression(85/299): loss=0.5105966094026039, w0=-1.7465362642608166e-05, w1=-0.1362476600502034\n",
      "Regularized Logistic Regression(86/299): loss=0.5103766239836677, w0=-1.7663470458106357e-05, w1=-0.13720164536985657\n",
      "Regularized Logistic Regression(87/299): loss=0.5101604181111054, w0=-1.786147586383587e-05, w1=-0.13815071041437305\n",
      "Regularized Logistic Regression(88/299): loss=0.5099478826253742, w0=-1.8059379534208238e-05, w1=-0.13909494190044466\n",
      "Regularized Logistic Regression(89/299): loss=0.5097389132739484, w0=-1.8257182139351763e-05, w1=-0.1400344237319149\n",
      "Regularized Logistic Regression(90/299): loss=0.509533410417043, w0=-1.845488434501996e-05, w1=-0.14096923711457768\n",
      "Regularized Logistic Regression(91/299): loss=0.5093312787539684, w0=-1.865248681251272e-05, w1=-0.14189946066549458\n",
      "Regularized Logistic Regression(92/299): loss=0.5091324270685328, w0=-1.884999019860907e-05, w1=-0.142825170517133\n",
      "Regularized Logistic Regression(93/299): loss=0.5089367679920294, w0=-1.9047395155510526e-05, w1=-0.14374644041661086\n",
      "Regularized Logistic Regression(94/299): loss=0.5087442177824703, w0=-1.924470233079407e-05, w1=-0.14466334182031326\n",
      "Regularized Logistic Regression(95/299): loss=0.5085546961188386, w0=-1.9441912367373914e-05, w1=-0.14557594398413085\n",
      "Regularized Logistic Regression(96/299): loss=0.5083681259092293, w0=-1.963902590347127e-05, w1=-0.14648431404955511\n",
      "Regularized Logistic Regression(97/299): loss=0.5081844331118431, w0=-1.9836043572591357e-05, w1=-0.14738851712584927\n",
      "Regularized Logistic Regression(98/299): loss=0.5080035465678797, w0=-2.003296600350704e-05, w1=-0.14828861636850119\n",
      "Regularized Logistic Regression(99/299): loss=0.507825397845451, w0=-2.0229793820248452e-05, w1=-0.14918467305415437\n",
      "Regularized Logistic Regression(100/299): loss=0.5076499210937109, w0=-2.0426527642098024e-05, w1=-0.15007674665219556\n",
      "Regularized Logistic Regression(101/299): loss=0.5074770529064524, w0=-2.062316808359047e-05, w1=-0.1509648948931749\n",
      "Regularized Logistic Regression(102/299): loss=0.5073067321944925, w0=-2.0819715754517218e-05, w1=-0.151849173834216\n",
      "Regularized Logistic Regression(103/299): loss=0.5071389000662081, w0=-2.1016171259934816e-05, w1=-0.15272963792157157\n",
      "Regularized Logistic Regression(104/299): loss=0.5069734997156448, w0=-2.121253520017702e-05, w1=-0.15360634005046422\n",
      "Regularized Logistic Regression(105/299): loss=0.5068104763176549, w0=-2.1408808170870096e-05, w1=-0.15447933162235053\n",
      "Regularized Logistic Regression(106/299): loss=0.5066497769295766, w0=-2.1604990762951073e-05, w1=-0.15534866259973354\n",
      "Regularized Logistic Regression(107/299): loss=0.5064913503989846, w0=-2.1801083562688605e-05, w1=-0.1562143815586449\n",
      "Regularized Logistic Regression(108/299): loss=0.5063351472771006, w0=-2.1997087151706197e-05, w1=-0.15707653573890956\n",
      "Regularized Logistic Regression(109/299): loss=0.5061811197374599, w0=-2.2193002107007503e-05, w1=-0.1579351710922997\n",
      "Regularized Logistic Regression(110/299): loss=0.506029221499481, w0=-2.2388829001003488e-05, w1=-0.15879033232867995\n",
      "Regularized Logistic Regression(111/299): loss=0.5058794077565973, w0=-2.2584568401541237e-05, w1=-0.15964206296023867\n",
      "Regularized Logistic Regression(112/299): loss=0.505731635108642, w0=-2.278022087193422e-05, w1=-0.1604904053438958\n",
      "Regularized Logistic Regression(113/299): loss=0.505585861498199, w0=-2.29757869709938e-05, w1=-0.16133540072197314\n",
      "Regularized Logistic Regression(114/299): loss=0.5054420461506507, w0=-2.3171267253061887e-05, w1=-0.16217708926120755\n",
      "Regularized Logistic Regression(115/299): loss=0.5053001495176814, w0=-2.3366662268044525e-05, w1=-0.16301551009018506\n",
      "Regularized Logistic Regression(116/299): loss=0.5051601332239994, w0=-2.3561972561446315e-05, w1=-0.1638507013352656\n",
      "Regularized Logistic Regression(117/299): loss=0.5050219600170704, w0=-2.3757198674405533e-05, w1=-0.1646827001550705\n",
      "Regularized Logistic Regression(118/299): loss=0.504885593719665, w0=-2.395234114372985e-05, w1=-0.16551154277359567\n",
      "Regularized Logistic Regression(119/299): loss=0.5047509991850332, w0=-2.4147400501932523e-05, w1=-0.16633726451201328\n",
      "Regularized Logistic Regression(120/299): loss=0.5046181422545396, w0=-2.434237727726899e-05, w1=-0.16715989981922091\n",
      "Regularized Logistic Regression(121/299): loss=0.5044869897176002, w0=-2.453727199377378e-05, w1=-0.16797948230119308\n",
      "Regularized Logistic Regression(122/299): loss=0.5043575092737712, w0=-2.4732085171297627e-05, w1=-0.16879604474918838\n",
      "Regularized Logistic Regression(123/299): loss=0.5042296694968581, w0=-2.492681732554479e-05, w1=-0.1696096191668629\n",
      "Regularized Logistic Regression(124/299): loss=0.5041034398009118, w0=-2.5121468968110445e-05, w1=-0.1704202367963376\n",
      "Regularized Logistic Regression(125/299): loss=0.5039787904079991, w0=-2.5316040606518127e-05, w1=-0.171227928143264\n",
      "Regularized Logistic Regression(126/299): loss=0.5038556923176338, w0=-2.5510532744257168e-05, w1=-0.1720327230009335\n",
      "Regularized Logistic Regression(127/299): loss=0.5037341172777671, w0=-2.5704945880820083e-05, w1=-0.17283465047346855\n",
      "Regularized Logistic Regression(128/299): loss=0.5036140377572402, w0=-2.589928051173986e-05, w1=-0.1736337389981379\n",
      "Regularized Logistic Regression(129/299): loss=0.5034954269196136, w0=-2.6093537128627115e-05, w1=-0.17443001636683078\n",
      "Regularized Logistic Regression(130/299): loss=0.5033782585982857, w0=-2.6287716219207085e-05, w1=-0.1752235097467259\n",
      "Regularized Logistic Regression(131/299): loss=0.5032625072728244, w0=-2.6481818267356433e-05, w1=-0.17601424570019075\n",
      "Regularized Logistic Regression(132/299): loss=0.5031481480464437, w0=-2.6675843753139817e-05, w1=-0.17680225020394028\n",
      "Regularized Logistic Regression(133/299): loss=0.5030351566245495, w0=-2.686979315284623e-05, w1=-0.17758754866748883\n",
      "Regularized Logistic Regression(134/299): loss=0.5029235092942995, w0=-2.706366693902507e-05, w1=-0.17837016595092256\n",
      "Regularized Logistic Regression(135/299): loss=0.502813182905114, w0=-2.725746558052191e-05, w1=-0.17915012638202008\n",
      "Regularized Logistic Regression(136/299): loss=0.502704154850084, w0=-2.7451189542514007e-05, w1=-0.1799274537727501\n",
      "Regularized Logistic Regression(137/299): loss=0.502596403048224, w0=-2.7644839286545456e-05, w1=-0.1807021714351687\n",
      "Regularized Logistic Regression(138/299): loss=0.5024899059275225, w0=-2.7838415270562054e-05, w1=-0.18147430219674224\n",
      "Regularized Logistic Regression(139/299): loss=0.5023846424087469, w0=-2.8031917948945805e-05, w1=-0.182243868415119\n",
      "Regularized Logistic Regression(140/299): loss=0.5022805918899562, w0=-2.822534777254909e-05, w1=-0.18301089199236978\n",
      "Regularized Logistic Regression(141/299): loss=0.5021777342316848, w0=-2.841870518872849e-05, w1=-0.18377539438872226\n",
      "Regularized Logistic Regression(142/299): loss=0.5020760497427633, w0=-2.8611990641378256e-05, w1=-0.18453739663580485\n",
      "Regularized Logistic Regression(143/299): loss=0.501975519166735, w0=-2.8805204570963393e-05, w1=-0.1852969193494232\n",
      "Regularized Logistic Regression(144/299): loss=0.5018761236688428, w0=-2.899834741455241e-05, w1=-0.18605398274188506\n",
      "Regularized Logistic Regression(145/299): loss=0.5017778448235499, w0=-2.9191419605849692e-05, w1=-0.18680860663389295\n",
      "Regularized Logistic Regression(146/299): loss=0.5016806646025714, w0=-2.938442157522749e-05, w1=-0.18756081046602077\n",
      "Regularized Logistic Regression(147/299): loss=0.5015845653633855, w0=-2.957735374975756e-05, w1=-0.18831061330979032\n",
      "Regularized Logistic Regression(148/299): loss=0.5014895298382018, w0=-2.9770216553242414e-05, w1=-0.18905803387836312\n",
      "Regularized Logistic Regression(149/299): loss=0.5013955411233635, w0=-2.996301040624622e-05, w1=-0.18980309053686348\n",
      "Regularized Logistic Regression(150/299): loss=0.5013025826691578, w0=-3.0155735726125302e-05, w1=-0.19054580131234522\n",
      "Regularized Logistic Regression(151/299): loss=0.5012106382700191, w0=-3.0348392927058313e-05, w1=-0.19128618390341764\n",
      "Regularized Logistic Regression(152/299): loss=0.5011196920551009, w0=-3.054098242007601e-05, w1=-0.19202425568954157\n",
      "Regularized Logistic Regression(153/299): loss=0.5010297284791995, w0=-3.073350461309067e-05, w1=-0.19276003374000958\n",
      "Regularized Logistic Regression(154/299): loss=0.5009407323140143, w0=-3.092595991092516e-05, w1=-0.19349353482262177\n",
      "Regularized Logistic Regression(155/299): loss=0.500852688639724, w0=-3.111834871534164e-05, w1=-0.19422477541206815\n",
      "Regularized Logistic Regression(156/299): loss=0.5007655828368678, w0=-3.1310671425069884e-05, w1=-0.1949537716980297\n",
      "Regularized Logistic Regression(157/299): loss=0.5006794005785146, w0=-3.150292843583532e-05, w1=-0.1956805395930074\n",
      "Regularized Logistic Regression(158/299): loss=0.5005941278227072, w0=-3.169512014038662e-05, w1=-0.19640509473989015\n",
      "Regularized Logistic Regression(159/299): loss=0.5005097508051685, w0=-3.188724692852306e-05, w1=-0.1971274525192703\n",
      "Regularized Logistic Regression(160/299): loss=0.5004262560322585, w0=-3.2079309187121444e-05, w1=-0.19784762805651787\n",
      "Regularized Logistic Regression(161/299): loss=0.5003436302741702, w0=-3.2271307300162737e-05, w1=-0.19856563622862033\n",
      "Regularized Logistic Regression(162/299): loss=0.5002618605583508, w0=-3.2463241648758374e-05, w1=-0.199281491670798\n",
      "Regularized Logistic Regression(163/299): loss=0.500180934163145, w0=-3.265511261117621e-05, w1=-0.19999520878290228\n",
      "Regularized Logistic Regression(164/299): loss=0.5001008386116428, w0=-3.284692056286617e-05, w1=-0.2007068017356053\n",
      "Regularized Logistic Regression(165/299): loss=0.500021561665728, w0=-3.3038665876485574e-05, w1=-0.20141628447638818\n",
      "Regularized Logistic Regression(166/299): loss=0.49994309132031645, w0=-3.323034892192416e-05, w1=-0.2021236707353354\n",
      "Regularized Logistic Regression(167/299): loss=0.4998654157977771, w0=-3.3421970066328776e-05, w1=-0.20282897403074251\n",
      "Regularized Logistic Regression(168/299): loss=0.4997885235425275, w0=-3.3613529674127765e-05, w1=-0.20353220767454225\n",
      "Regularized Logistic Regression(169/299): loss=0.4997124032157964, w0=-3.3805028107055074e-05, w1=-0.20423338477755898\n",
      "Regularized Logistic Regression(170/299): loss=0.49963704369054657, w0=-3.399646572417406e-05, w1=-0.20493251825459327\n",
      "Regularized Logistic Regression(171/299): loss=0.4995624340465523, w0=-3.4187842881900984e-05, w1=-0.20562962082934666\n",
      "Regularized Logistic Regression(172/299): loss=0.49948856356562343, w0=-3.4379159934028244e-05, w1=-0.20632470503918884\n",
      "Regularized Logistic Regression(173/299): loss=0.49941542172697245, w0=-3.457041723174732e-05, w1=-0.20701778323977593\n",
      "Regularized Logistic Regression(174/299): loss=0.49934299820271516, w0=-3.476161512367142e-05, w1=-0.20770886760952303\n",
      "Regularized Logistic Regression(175/299): loss=0.49927128285350636, w0=-3.495275395585787e-05, w1=-0.2083979701539362\n",
      "Regularized Logistic Regression(176/299): loss=0.4992002657242971, w0=-3.514383407183025e-05, w1=-0.2090851027098103\n",
      "Regularized Logistic Regression(177/299): loss=0.49912993704021563, w0=-3.533485581260023e-05, w1=-0.20977027694929612\n",
      "Regularized Logistic Regression(178/299): loss=0.499060287202563, w0=-3.552581951668916e-05, w1=-0.21045350438384106\n",
      "Regularized Logistic Regression(179/299): loss=0.498991306784923, w0=-3.571672552014944e-05, w1=-0.21113479636800958\n",
      "Regularized Logistic Regression(180/299): loss=0.49892298652937683, w0=-3.5907574156585564e-05, w1=-0.2118141641031852\n",
      "Regularized Logistic Regression(181/299): loss=0.49885531734282595, w0=-3.609836575717499e-05, w1=-0.2124916186411595\n",
      "Regularized Logistic Regression(182/299): loss=0.4987882902934105, w0=-3.628910065068869e-05, w1=-0.21316717088761306\n",
      "Regularized Logistic Regression(183/299): loss=0.49872189660702887, w0=-3.647977916351156e-05, w1=-0.21384083160548892\n",
      "Regularized Logistic Regression(184/299): loss=0.49865612766394696, w0=-3.667040161966248e-05, w1=-0.2145126114182655\n",
      "Regularized Logistic Regression(185/299): loss=0.49859097499550026, w0=-3.686096834081425e-05, w1=-0.21518252081313052\n",
      "Regularized Logistic Regression(186/299): loss=0.4985264302808801, w0=-3.705147964631319e-05, w1=-0.21585057014405962\n",
      "Regularized Logistic Regression(187/299): loss=0.49846248534400756, w0=-3.724193585319861e-05, w1=-0.2165167696348039\n",
      "Regularized Logistic Regression(188/299): loss=0.4983991321504844, w0=-3.743233727622202e-05, w1=-0.21718112938178774\n",
      "Regularized Logistic Regression(189/299): loss=0.49833636280462545, w0=-3.7622684227866084e-05, w1=-0.2178436593569212\n",
      "Regularized Logistic Regression(190/299): loss=0.4982741695465653, w0=-3.781297701836344e-05, w1=-0.2185043694103298\n",
      "Regularized Logistic Regression(191/299): loss=0.49821254474943916, w0=-3.800321595571524e-05, w1=-0.21916326927300409\n",
      "Regularized Logistic Regression(192/299): loss=0.4981514809166342, w0=-3.819340134570952e-05, w1=-0.21982036855937126\n",
      "Regularized Logistic Regression(193/299): loss=0.4980909706791097, w0=-3.838353349193934e-05, w1=-0.22047567676979263\n",
      "Regularized Logistic Regression(194/299): loss=0.49803100679278384, w0=-3.857361269582077e-05, w1=-0.22112920329298863\n",
      "Regularized Logistic Regression(195/299): loss=0.4979715821359847, w0=-3.87636392566106e-05, w1=-0.22178095740839338\n",
      "Regularized Logistic Regression(196/299): loss=0.49791268970696456, w0=-3.8953613471423954e-05, w1=-0.22243094828844226\n",
      "Regularized Logistic Regression(197/299): loss=0.4978543226214732, w0=-3.914353563525163e-05, w1=-0.22307918500079382\n",
      "Regularized Logistic Regression(198/299): loss=0.49779647411039113, w0=-3.933340604097727e-05, w1=-0.22372567651048808\n",
      "Regularized Logistic Regression(199/299): loss=0.49773913751741894, w0=-3.952322497939439e-05, w1=-0.22437043168204437\n",
      "Regularized Logistic Regression(200/299): loss=0.49768230629682275, w0=-3.971299273922317e-05, w1=-0.22501345928149974\n",
      "Regularized Logistic Regression(201/299): loss=0.4976259740112325, w0=-3.990270960712712e-05, w1=-0.22565476797838974\n",
      "Regularized Logistic Regression(202/299): loss=0.4975701343294921, w0=-4.009237586772948e-05, w1=-0.2262943663476744\n",
      "Regularized Logistic Regression(203/299): loss=0.4975147810245604, w0=-4.028199180362957e-05, w1=-0.22693226287161053\n",
      "Regularized Logistic Regression(204/299): loss=0.49745990797146133, w0=-4.047155769541886e-05, w1=-0.22756846594157168\n",
      "Regularized Logistic Regression(205/299): loss=0.4974055091452813, w0=-4.066107382169692e-05, w1=-0.2282029838598188\n",
      "Regularized Logistic Regression(206/299): loss=0.4973515786192133, w0=-4.0850540459087216e-05, w1=-0.22883582484122167\n",
      "Regularized Logistic Regression(207/299): loss=0.49729811056264456, w0=-4.10399578822527e-05, w1=-0.2294669970149339\n",
      "Regularized Logistic Regression(208/299): loss=0.4972450992392907, w0=-4.122932636391127e-05, w1=-0.23009650842602158\n",
      "Regularized Logistic Regression(209/299): loss=0.49719253900536925, w0=-4.1418646174851076e-05, w1=-0.2307243670370492\n",
      "Regularized Logistic Regression(210/299): loss=0.4971404243078162, w0=-4.1607917583945623e-05, w1=-0.23135058072962195\n",
      "Regularized Logistic Regression(211/299): loss=0.4970887496825427, w0=-4.1797140858168784e-05, w1=-0.23197515730588683\n",
      "Regularized Logistic Regression(212/299): loss=0.497037509752731, w0=-4.198631626260962e-05, w1=-0.23259810448999443\n",
      "Regularized Logistic Regression(213/299): loss=0.4969866992271667, w0=-4.2175444060487066e-05, w1=-0.23321942992952147\n",
      "Regularized Logistic Regression(214/299): loss=0.4969363128986108, w0=-4.236452451316444e-05, w1=-0.23383914119685587\n",
      "Regularized Logistic Regression(215/299): loss=0.49688634564220513, w0=-4.2553557880163856e-05, w1=-0.23445724579054614\n",
      "Regularized Logistic Regression(216/299): loss=0.4968367924139156, w0=-4.274254441918046e-05, w1=-0.23507375113661472\n",
      "Regularized Logistic Regression(217/299): loss=0.4967876482490068, w0=-4.2931484386096506e-05, w1=-0.23568866458983778\n",
      "Regularized Logistic Regression(218/299): loss=0.49673890826055167, w0=-4.312037803499537e-05, w1=-0.23630199343499245\n",
      "Regularized Logistic Regression(219/299): loss=0.4966905676379737, w0=-4.330922561817533e-05, w1=-0.23691374488807063\n",
      "Regularized Logistic Regression(220/299): loss=0.49664262164561956, w0=-4.3498027386163266e-05, w1=-0.23752392609746356\n",
      "Regularized Logistic Regression(221/299): loss=0.4965950656213639, w0=-4.368678358772824e-05, w1=-0.2381325441451155\n",
      "Regularized Logistic Regression(222/299): loss=0.49654789497524277, w0=-4.3875494469894876e-05, w1=-0.23873960604764785\n",
      "Regularized Logistic Regression(223/299): loss=0.49650110518811735, w0=-4.4064160277956695e-05, w1=-0.23934511875745618\n",
      "Regularized Logistic Regression(224/299): loss=0.49645469181036644, w0=-4.425278125548926e-05, w1=-0.23994908916377883\n",
      "Regularized Logistic Regression(225/299): loss=0.49640865046060517, w0=-4.444135764436322e-05, w1=-0.24055152409373992\n",
      "Regularized Logistic Regression(226/299): loss=0.4963629768244324, w0=-4.462988968475722e-05, w1=-0.24115243031336572\n",
      "Regularized Logistic Regression(227/299): loss=0.4963176666532048, w0=-4.481837761517069e-05, w1=-0.24175181452857739\n",
      "Regularized Logistic Regression(228/299): loss=0.49627271576283444, w0=-4.500682167243654e-05, w1=-0.2423496833861577\n",
      "Regularized Logistic Regression(229/299): loss=0.49622812003261524, w0=-4.519522209173368e-05, w1=-0.24294604347469625\n",
      "Regularized Logistic Regression(230/299): loss=0.4961838754040699, w0=-4.538357910659946e-05, w1=-0.2435409013255105\n",
      "Regularized Logistic Regression(231/299): loss=0.49613997787982417, w0=-4.557189294894201e-05, w1=-0.24413426341354577\n",
      "Regularized Logistic Regression(232/299): loss=0.496096423522503, w0=-4.576016384905239e-05, w1=-0.2447261361582527\n",
      "Regularized Logistic Regression(233/299): loss=0.49605320845364903, w0=-4.594839203561673e-05, w1=-0.2453165259244452\n",
      "Regularized Logistic Regression(234/299): loss=0.49601032885266555, w0=-4.613657773572818e-05, w1=-0.24590543902313766\n",
      "Regularized Logistic Regression(235/299): loss=0.49596778095577876, w0=-4.632472117489876e-05, w1=-0.24649288171236172\n",
      "Regularized Logistic Regression(236/299): loss=0.49592556105502167, w0=-4.6512822577071146e-05, w1=-0.2470788601979663\n",
      "Regularized Logistic Regression(237/299): loss=0.49588366549724205, w0=-4.6700882164630295e-05, w1=-0.2476633806343967\n",
      "Regularized Logistic Regression(238/299): loss=0.4958420906831253, w0=-4.688890015841499e-05, w1=-0.24824644912545735\n",
      "Regularized Logistic Regression(239/299): loss=0.49580083306624134, w0=-4.707687677772929e-05, w1=-0.24882807172505697\n",
      "Regularized Logistic Regression(240/299): loss=0.4957598891521096, w0=-4.726481224035383e-05, w1=-0.2494082544379363\n",
      "Regularized Logistic Regression(241/299): loss=0.4957192554972818, w0=-4.745270676255709e-05, w1=-0.2499870032203805\n",
      "Regularized Logistic Regression(242/299): loss=0.4956789287084455, w0=-4.764056055910647e-05, w1=-0.2505643239809141\n",
      "Regularized Logistic Regression(243/299): loss=0.4956389054415431, w0=-4.782837384327938e-05, w1=-0.2511402225809821\n",
      "Regularized Logistic Regression(244/299): loss=0.4955991824009105, w0=-4.801614682687413e-05, w1=-0.25171470483561525\n",
      "Regularized Logistic Regression(245/299): loss=0.4955597563384325, w0=-4.820387972022075e-05, w1=-0.2522877765140807\n",
      "Regularized Logistic Regression(246/299): loss=0.495520624052714, w0=-4.839157273219177e-05, w1=-0.25285944334051863\n",
      "Regularized Logistic Regression(247/299): loss=0.4954817823882693, w0=-4.857922607021282e-05, w1=-0.2534297109945654\n",
      "Regularized Logistic Regression(248/299): loss=0.4954432282347258, w0=-4.876683994027319e-05, w1=-0.2539985851119628\n",
      "Regularized Logistic Regression(249/299): loss=0.49540495852604605, w0=-4.895441454693629e-05, w1=-0.25456607128515485\n",
      "Regularized Logistic Regression(250/299): loss=0.4953669702397615, w0=-4.914195009334999e-05, w1=-0.25513217506387154\n",
      "Regularized Logistic Regression(251/299): loss=0.49532926039622505, w0=-4.932944678125692e-05, w1=-0.2556969019557006\n",
      "Regularized Logistic Regression(252/299): loss=0.49529182605787475, w0=-4.9516904811004615e-05, w1=-0.2562602574266468\n",
      "Regularized Logistic Regression(253/299): loss=0.4952546643285158, w0=-4.9704324381555644e-05, w1=-0.25682224690168043\n",
      "Regularized Logistic Regression(254/299): loss=0.495217772352613, w0=-4.989170569049759e-05, w1=-0.2573828757652738\n",
      "Regularized Logistic Regression(255/299): loss=0.49518114731459895, w0=-5.007904893405297e-05, w1=-0.25794214936192705\n",
      "Regularized Logistic Regression(256/299): loss=0.4951447864381959, w0=-5.0266354307089076e-05, w1=-0.2585000729966836\n",
      "Regularized Logistic Regression(257/299): loss=0.4951086869857489, w0=-5.045362200312772e-05, w1=-0.25905665193563415\n",
      "Regularized Logistic Regression(258/299): loss=0.49507284625757436, w0=-5.06408522143549e-05, w1=-0.259611891406412\n",
      "Regularized Logistic Regression(259/299): loss=0.4950372615913194, w0=-5.0828045131630375e-05, w1=-0.2601657965986772\n",
      "Regularized Logistic Regression(260/299): loss=0.49500193036133416, w0=-5.1015200944497165e-05, w1=-0.26071837266459247\n",
      "Regularized Logistic Regression(261/299): loss=0.4949668499780557, w0=-5.120231984119098e-05, w1=-0.26126962471928794\n",
      "Regularized Logistic Regression(262/299): loss=0.494932017887405, w0=-5.138940200864956e-05, w1=-0.26181955784131894\n",
      "Regularized Logistic Regression(263/299): loss=0.49489743157019406, w0=-5.157644763252192e-05, w1=-0.2623681770731137\n",
      "Regularized Logistic Regression(264/299): loss=0.4948630885415451, w0=-5.176345689717755e-05, w1=-0.2629154874214124\n",
      "Regularized Logistic Regression(265/299): loss=0.49482898635032, w0=-5.195042998571551e-05, w1=-0.2634614938576983\n",
      "Regularized Logistic Regression(266/299): loss=0.4947951225785626, w0=-5.213736707997347e-05, w1=-0.2640062013186207\n",
      "Regularized Logistic Regression(267/299): loss=0.4947614948409489, w0=-5.232426836053664e-05, w1=-0.2645496147064096\n",
      "Regularized Logistic Regression(268/299): loss=0.4947281007842489, w0=-5.2511134006746684e-05, w1=-0.26509173888928306\n",
      "Regularized Logistic Regression(269/299): loss=0.4946949380868001, w0=-5.269796419671049e-05, w1=-0.26563257870184653\n",
      "Regularized Logistic Regression(270/299): loss=0.49466200445798797, w0=-5.288475910730893e-05, w1=-0.26617213894548536\n",
      "Regularized Logistic Regression(271/299): loss=0.4946292976377382, w0=-5.307151891420552e-05, w1=-0.2667104243887494\n",
      "Regularized Logistic Regression(272/299): loss=0.4945968153960186, w0=-5.325824379185497e-05, w1=-0.267247439767732\n",
      "Regularized Logistic Regression(273/299): loss=0.4945645555323481, w0=-5.3444933913511756e-05, w1=-0.26778318978644056\n",
      "Regularized Logistic Regression(274/299): loss=0.49453251587531843, w0=-5.363158945123854e-05, w1=-0.2683176791171611\n",
      "Regularized Logistic Regression(275/299): loss=0.4945006942821221, w0=-5.3818210575914546e-05, w1=-0.2688509124008173\n",
      "Regularized Logistic Regression(276/299): loss=0.49446908863808836, w0=-5.4004797457243876e-05, w1=-0.26938289424732165\n",
      "Regularized Logistic Regression(277/299): loss=0.4944376968562318, w0=-5.4191350263763763e-05, w1=-0.2699136292359215\n",
      "Regularized Logistic Regression(278/299): loss=0.4944065168768052, w0=-5.437786916285273e-05, w1=-0.27044312191553876\n",
      "Regularized Logistic Regression(279/299): loss=0.49437554666686223, w0=-5.4564354320738725e-05, w1=-0.2709713768051039\n",
      "Regularized Logistic Regression(280/299): loss=0.49434478421982897, w0=-5.475080590250713e-05, w1=-0.2714983983938843\n",
      "Regularized Logistic Regression(281/299): loss=0.49431422755508203, w0=-5.493722407210878e-05, w1=-0.27202419114180604\n",
      "Regularized Logistic Regression(282/299): loss=0.494283874717535, w0=-5.512360899236785e-05, w1=-0.2725487594797722\n",
      "Regularized Logistic Regression(283/299): loss=0.4942537237772324, w0=-5.530996082498971e-05, w1=-0.27307210780997415\n",
      "Regularized Logistic Regression(284/299): loss=0.4942237728289512, w0=-5.549627973056874e-05, w1=-0.2735942405061983\n",
      "Regularized Logistic Regression(285/299): loss=0.4941940199918095, w0=-5.568256586859601e-05, w1=-0.27411516191412805\n",
      "Regularized Logistic Regression(286/299): loss=0.4941644634088823, w0=-5.5868819397467014e-05, w1=-0.27463487635164\n",
      "Regularized Logistic Regression(287/299): loss=0.4941351012468236, w0=-5.60550404744892e-05, w1=-0.27515338810909623\n",
      "Regularized Logistic Regression(288/299): loss=0.4941059316954977, w0=-5.624122925588957e-05, w1=-0.2756707014496311\n",
      "Regularized Logistic Regression(289/299): loss=0.4940769529676135, w0=-5.642738589682212e-05, w1=-0.2761868206094344\n",
      "Regularized Logistic Regression(290/299): loss=0.49404816329836887, w0=-5.6613510551375314e-05, w1=-0.2767017497980291\n",
      "Regularized Logistic Regression(291/299): loss=0.494019560945099, w0=-5.679960337257942e-05, w1=-0.2772154931985448\n",
      "Regularized Logistic Regression(292/299): loss=0.49399114418693313, w0=-5.698566451241383e-05, w1=-0.2777280549679877\n",
      "Regularized Logistic Regression(293/299): loss=0.4939629113244551, w0=-5.717169412181429e-05, w1=-0.27823943923750527\n",
      "Regularized Logistic Regression(294/299): loss=0.49393486067937187, w0=-5.7357692350680144e-05, w1=-0.27874965011264746\n",
      "Regularized Logistic Regression(295/299): loss=0.4939069905941877, w0=-5.754365934788142e-05, w1=-0.27925869167362416\n",
      "Regularized Logistic Regression(296/299): loss=0.493879299431883, w0=-5.7729595261265954e-05, w1=-0.2797665679755581\n",
      "Regularized Logistic Regression(297/299): loss=0.49385178557560006, w0=-5.7915500237666384e-05, w1=-0.28027328304873383\n",
      "Regularized Logistic Regression(298/299): loss=0.49382444742833403, w0=-5.810137442290716e-05, w1=-0.2807788408988441\n",
      "Regularized Logistic Regression(299/299): loss=0.49379728341262913, w0=-5.8287217961811434e-05, w1=-0.2812832455072309\n",
      "Regularized Logistic Regression(0/99): loss=0.693147180559945, w0=-2.0882930292778685e-07, w1=-0.003658898216597753\n",
      "Regularized Logistic Regression(1/99): loss=0.6777027414889991, w0=-4.168083238198562e-07, w1=-0.007146888677045928\n",
      "Regularized Logistic Regression(2/99): loss=0.6639220287926192, w0=-6.241103972554306e-07, w1=-0.010478455819701962\n",
      "Regularized Logistic Regression(3/99): loss=0.6516074780558584, w0=-8.308665434388834e-07, w1=-0.013666639343024482\n",
      "Regularized Logistic Regression(4/99): loss=0.6405854572249043, w0=-1.0371744290199916e-06, w1=-0.016723143045150855\n",
      "Regularized Logistic Regression(5/99): loss=0.6307034550896028, w0=-1.2431057303520428e-06, w1=-0.019658460743975262\n",
      "Regularized Logistic Regression(6/99): loss=0.6218275976937215, w0=-1.4487120780475333e-06, w1=-0.022482004410888365\n",
      "Regularized Logistic Regression(7/99): loss=0.6138404421241037, w0=-1.654029789004874e-06, w1=-0.025202226168933934\n",
      "Regularized Logistic Regression(8/99): loss=0.6066390094895175, w0=-1.8590835904639685e-06, w1=-0.02782673004384163\n",
      "Regularized Logistic Regression(9/99): loss=0.6001330296528367, w0=-2.0638895233270064e-06, w1=-0.03036237194877745\n",
      "Regularized Logistic Regression(10/99): loss=0.5942433773351784, w0=-2.2684571876861515e-06, w1=-0.032815347867214864\n",
      "Regularized Logistic Regression(11/99): loss=0.5889006830457942, w0=-2.472791467569436e-06, w1=-0.03519127097420068\n",
      "Regularized Logistic Regression(12/99): loss=0.5840441040530545, w0=-2.6768938473209933e-06, w1=-0.03749523878251754\n",
      "Regularized Logistic Regression(13/99): loss=0.5796202413240482, w0=-2.8807634102018747e-06, w1=-0.03973189149922548\n",
      "Regularized Logistic Regression(14/99): loss=0.5755821887097818, w0=-3.0843975912252882e-06, w1=-0.04190546274427294\n",
      "Regularized Logistic Regression(15/99): loss=0.5718887010136365, w0=-3.287792740889839e-06, w1=-0.04401982368517599\n",
      "Regularized Logistic Regression(16/99): loss=0.5685034681069756, w0=-3.4909445440461804e-06, w1=-0.046078521519826644\n",
      "Regularized Logistic Regression(17/99): loss=0.5653944829737665, w0=-3.693848328220759e-06, w1=-0.048084813114880616\n",
      "Regularized Logistic Regression(18/99): loss=0.5625334924393118, w0=-3.896499287903624e-06, w1=-0.05004169449068526\n",
      "Regularized Logistic Regression(19/99): loss=0.5598955203086917, w0=-4.098892645194189e-06, w1=-0.051951926739912946\n",
      "Regularized Logistic Regression(20/99): loss=0.5574584536507287, w0=-4.301023762448226e-06, w1=-0.05381805887717278\n",
      "Regularized Logistic Regression(21/99): loss=0.5552026839656389, w0=-4.502888218894906e-06, w1=-0.05564244804032322\n",
      "Regularized Logistic Regression(22/99): loss=0.5531107959344795, w0=-4.704481860360919e-06, w1=-0.05742727739969828\n",
      "Regularized Logistic Regression(23/99): loss=0.5511672973434852, w0=-4.905800829061995e-06, w1=-0.059174572077367205\n",
      "Regularized Logistic Regression(24/99): loss=0.5493583845939481, w0=-5.106841578752527e-06, w1=-0.06088621333333345\n",
      "Regularized Logistic Regression(25/99): loss=0.5476717389434784, w0=-5.3076008792452365e-06, w1=-0.06256395123777692\n",
      "Regularized Logistic Regression(26/99): loss=0.546096349277477, w0=-5.508075813334671e-06, w1=-0.06420941601681088\n",
      "Regularized Logistic Regression(27/99): loss=0.5446223577842261, w0=-5.708263768410941e-06, w1=-0.0658241282326961\n",
      "Regularized Logistic Regression(28/99): loss=0.5432409254087941, w0=-5.908162424479578e-06, w1=-0.06740950793714375\n",
      "Regularized Logistic Regression(29/99): loss=0.5419441143967121, w0=-6.1077697398683915e-06, w1=-0.0689668829175159\n",
      "Regularized Logistic Regression(30/99): loss=0.5407247856151154, w0=-6.307083935570893e-06, w1=-0.0704974961397879\n",
      "Regularized Logistic Regression(31/99): loss=0.5395765086636842, w0=-6.506103478923923e-06, w1=-0.07200251247859983\n",
      "Regularized Logistic Regression(32/99): loss=0.5384934830667778, w0=-6.70482706712594e-06, w1=-0.07348302481317033\n",
      "Regularized Logistic Regression(33/99): loss=0.5374704690776083, w0=-6.903253610957618e-06, w1=-0.07494005955796679\n",
      "Regularized Logistic Regression(34/99): loss=0.5365027268305491, w0=-7.101382218957205e-06, w1=-0.07637458168853567\n",
      "Regularized Logistic Regression(35/99): loss=0.5355859627534498, w0=-7.299212182220944e-06, w1=-0.07778749931559369\n",
      "Regularized Logistic Regression(36/99): loss=0.5347162823023525, w0=-7.496742959937498e-06, w1=-0.079179667854157\n",
      "Regularized Logistic Regression(37/99): loss=0.5338901482098877, w0=-7.693974165719746e-06, w1=-0.08055189382901766\n",
      "Regularized Logistic Regression(38/99): loss=0.5331043435490107, w0=-7.890905554763824e-06, w1=-0.08190493835311619\n",
      "Regularized Logistic Regression(39/99): loss=0.5323559390083464, w0=-8.08753701184091e-06, w1=-0.08323952031121813\n",
      "Regularized Logistic Regression(40/99): loss=0.5316422638565264, w0=-8.283868540109933e-06, w1=-0.08455631927767604\n",
      "Regularized Logistic Regression(41/99): loss=0.5309608801425283, w0=-8.479900250727155e-06, w1=-0.08585597819389897\n",
      "Regularized Logistic Regression(42/99): loss=0.5303095597388479, w0=-8.675632353220541e-06, w1=-0.08713910582835908\n",
      "Regularized Logistic Regression(43/99): loss=0.5296862638857623, w0=-8.871065146591547e-06, w1=-0.08840627903953008\n",
      "Regularized Logistic Regression(44/99): loss=0.5290891249392529, w0=-9.066199011104001e-06, w1=-0.08965804485998935\n",
      "Regularized Logistic Regression(45/99): loss=0.5285164300633132, w0=-9.261034400718404e-06, w1=-0.09089492241802105\n",
      "Regularized Logistic Regression(46/99): loss=0.5279666066403503, w0=-9.455571836129762e-06, w1=-0.09211740471137227\n",
      "Regularized Logistic Regression(47/99): loss=0.5274382092018465, w0=-9.649811898367776e-06, w1=-0.09332596024632447\n",
      "Regularized Logistic Regression(48/99): loss=0.5269299077061106, w0=-9.843755222919411e-06, w1=-0.09452103455392236\n",
      "Regularized Logistic Regression(49/99): loss=0.5264404770113099, w0=-1.0037402494335562e-05, w1=-0.09570305159402721\n",
      "Regularized Logistic Regression(50/99): loss=0.5259687874105015, w0=-1.023075444128539e-05, w1=-0.09687241505681586\n",
      "Regularized Logistic Regression(51/99): loss=0.5255137961115031, w0=-1.0423811832023988e-05, w1=-0.0980295095704166\n",
      "Regularized Logistic Regression(52/99): loss=0.5250745395584564, w0=-1.0616575470241094e-05, w1=-0.09917470182253654\n",
      "Regularized Logistic Regression(53/99): loss=0.5246501265041453, w0=-1.0809046191260702e-05, w1=-0.10030834160319839\n",
      "Regularized Logistic Regression(54/99): loss=0.5242397317528056, w0=-1.100122485856349e-05, w1=-0.10143076277503302\n",
      "Regularized Logistic Regression(55/99): loss=0.523842590502468, w0=-1.1193112360605984e-05, w1=-0.10254228417698036\n",
      "Regularized Logistic Regression(56/99): loss=0.523457993224028, w0=-1.1384709607912305e-05, w1=-0.10364321046671672\n",
      "Regularized Logistic Regression(57/99): loss=0.5230852810213876, w0=-1.1576017530416164e-05, w1=-0.10473383290664268\n",
      "Regularized Logistic Regression(58/99): loss=0.5227238414232696, w0=-1.1767037075032508e-05, w1=-0.10581443009783588\n",
      "Regularized Logistic Regression(59/99): loss=0.5223731045628198, w0=-1.1957769203439805e-05, w1=-0.10688526866598153\n",
      "Regularized Logistic Regression(60/99): loss=0.522032539705954, w0=-1.2148214890055492e-05, w1=-0.10794660390294356\n",
      "Regularized Logistic Regression(61/99): loss=0.5217016520936757, w0=-1.2338375120188522e-05, w1=-0.10899868036732013\n",
      "Regularized Logistic Regression(62/99): loss=0.5213799800673514, w0=-1.2528250888354194e-05, w1=-0.11004173244704302\n",
      "Regularized Logistic Regression(63/99): loss=0.5210670924492621, w0=-1.271784319673773e-05, w1=-0.11107598488681822\n",
      "Regularized Logistic Regression(64/99): loss=0.520762586153676, w0=-1.290715305379413e-05, w1=-0.11210165328297311\n",
      "Regularized Logistic Regression(65/99): loss=0.520466084006311, w0=-1.3096181472972854e-05, w1=-0.11311894454806062\n",
      "Regularized Logistic Regression(66/99): loss=0.5201772327523375, w0=-1.3284929471556886e-05, w1=-0.1141280573473784\n",
      "Regularized Logistic Regression(67/99): loss=0.5198957012351461, w0=-1.347339806960654e-05, w1=-0.1151291825093848\n",
      "Regularized Logistic Regression(68/99): loss=0.5196211787299001, w0=-1.3661588288999195e-05, w1=-0.11612250341183541\n",
      "Regularized Logistic Regression(69/99): loss=0.5193533734175325, w0=-1.3849501152556903e-05, w1=-0.11710819634531666\n",
      "Regularized Logistic Regression(70/99): loss=0.5190920109862688, w0=-1.403713768325444e-05, w1=-0.11808643085572179\n",
      "Regularized Logistic Regression(71/99): loss=0.5188368333490617, w0=-1.4224498903501036e-05, w1=-0.11905737006709396\n",
      "Regularized Logistic Regression(72/99): loss=0.5185875974664577, w0=-1.4411585834489565e-05, w1=-0.12002117098615009\n",
      "Regularized Logistic Regression(73/99): loss=0.5183440742654377, w0=-1.4598399495607481e-05, w1=-0.12097798478969957\n",
      "Regularized Logistic Regression(74/99): loss=0.5181060476457129, w0=-1.47849409039043e-05, w1=-0.12192795709608001\n",
      "Regularized Logistic Regression(75/99): loss=0.5178733135657436, w0=-1.4971211073610841e-05, w1=-0.12287122822164531\n",
      "Regularized Logistic Regression(76/99): loss=0.517645679201519, w0=-1.5157211015705823e-05, w1=-0.1238079334232691\n",
      "Regularized Logistic Regression(77/99): loss=0.5174229621717703, w0=-1.5342941737525835e-05, w1=-0.12473820312775077\n",
      "Regularized Logistic Regression(78/99): loss=0.5172049898238955, w0=-1.552840424241495e-05, w1=-0.1256621631489509\n",
      "Regularized Logistic Regression(79/99): loss=0.516991598575405, w0=-1.571359952941068e-05, w1=-0.12657993489341937\n",
      "Regularized Logistic Regression(80/99): loss=0.5167826333061731, w0=-1.5898528592963087e-05, w1=-0.12749163555522774\n",
      "Regularized Logistic Regression(81/99): loss=0.5165779467972155, w0=-1.608319242268429e-05, w1=-0.12839737830066583\n",
      "Regularized Logistic Regression(82/99): loss=0.5163773992121018, w0=-1.6267592003125753e-05, w1=-0.129297272443416\n",
      "Regularized Logistic Regression(83/99): loss=0.5161808576174606, w0=-1.6451728313580928e-05, w1=-0.13019142361077687\n",
      "Regularized Logistic Regression(84/99): loss=0.5159881955393557, w0=-1.663560232791115e-05, w1=-0.1310799339014673\n",
      "Regularized Logistic Regression(85/99): loss=0.5157992925525964, w0=-1.6819215014392676e-05, w1=-0.131962902035507\n",
      "Regularized Logistic Regression(86/99): loss=0.5156140339003, w0=-1.7002567335583153e-05, w1=-0.13284042349663525\n",
      "Regularized Logistic Regression(87/99): loss=0.5154323101412716, w0=-1.718566024820572e-05, w1=-0.13371259066769894\n",
      "Regularized Logistic Regression(88/99): loss=0.5152540168229613, w0=-1.7368494703049274e-05, w1=-0.13457949295941346\n",
      "Regularized Logistic Regression(89/99): loss=0.515079054177969, w0=-1.7551071644883465e-05, w1=-0.1354412169328698\n",
      "Regularized Logistic Regression(90/99): loss=0.5149073268422245, w0=-1.7733392012387098e-05, w1=-0.13629784641614212\n",
      "Regularized Logistic Regression(91/99): loss=0.5147387435931504, w0=-1.7915456738088797e-05, w1=-0.13714946261532265\n",
      "Regularized Logistic Regression(92/99): loss=0.5145732171062355, w0=-1.8097266748318797e-05, w1=-0.13799614422029308\n",
      "Regularized Logistic Regression(93/99): loss=0.5144106637285991, w0=-1.8278822963170887e-05, w1=-0.13883796750552005\n",
      "Regularized Logistic Regression(94/99): loss=0.5142510032682306, w0=-1.846012629647359e-05, w1=-0.13967500642614392\n",
      "Regularized Logistic Regression(95/99): loss=0.5140941587977074, w0=-1.864117765576972e-05, w1=-0.1405073327096158\n",
      "Regularized Logistic Regression(96/99): loss=0.5139400564712865, w0=-1.8821977942303565e-05, w1=-0.14133501594311937\n",
      "Regularized Logistic Regression(97/99): loss=0.5137886253543598, w0=-1.9002528051014964e-05, w1=-0.1421581236570009\n",
      "Regularized Logistic Regression(98/99): loss=0.5136397972643428, w0=-1.9182828870539672e-05, w1=-0.1429767214044161\n",
      "Regularized Logistic Regression(99/99): loss=0.5134935066221458, w0=-1.9362881283215375e-05, w1=-0.14379087283739303\n",
      "Regularized Logistic Regression(0/199): loss=0.693147180559945, w0=-2.0882930292778685e-07, w1=-0.003658898216597753\n",
      "Regularized Logistic Regression(1/199): loss=0.6777027414889991, w0=-4.168083238198562e-07, w1=-0.007146888677045928\n",
      "Regularized Logistic Regression(2/199): loss=0.6639220287926192, w0=-6.241103972554306e-07, w1=-0.010478455819701962\n",
      "Regularized Logistic Regression(3/199): loss=0.6516074780558584, w0=-8.308665434388834e-07, w1=-0.013666639343024482\n",
      "Regularized Logistic Regression(4/199): loss=0.6405854572249043, w0=-1.0371744290199916e-06, w1=-0.016723143045150855\n",
      "Regularized Logistic Regression(5/199): loss=0.6307034550896028, w0=-1.2431057303520428e-06, w1=-0.019658460743975262\n",
      "Regularized Logistic Regression(6/199): loss=0.6218275976937215, w0=-1.4487120780475333e-06, w1=-0.022482004410888365\n",
      "Regularized Logistic Regression(7/199): loss=0.6138404421241037, w0=-1.654029789004874e-06, w1=-0.025202226168933934\n",
      "Regularized Logistic Regression(8/199): loss=0.6066390094895175, w0=-1.8590835904639685e-06, w1=-0.02782673004384163\n",
      "Regularized Logistic Regression(9/199): loss=0.6001330296528367, w0=-2.0638895233270064e-06, w1=-0.03036237194877745\n",
      "Regularized Logistic Regression(10/199): loss=0.5942433773351784, w0=-2.2684571876861515e-06, w1=-0.032815347867214864\n",
      "Regularized Logistic Regression(11/199): loss=0.5889006830457942, w0=-2.472791467569436e-06, w1=-0.03519127097420068\n",
      "Regularized Logistic Regression(12/199): loss=0.5840441040530545, w0=-2.6768938473209933e-06, w1=-0.03749523878251754\n",
      "Regularized Logistic Regression(13/199): loss=0.5796202413240482, w0=-2.8807634102018747e-06, w1=-0.03973189149922548\n",
      "Regularized Logistic Regression(14/199): loss=0.5755821887097818, w0=-3.0843975912252882e-06, w1=-0.04190546274427294\n",
      "Regularized Logistic Regression(15/199): loss=0.5718887010136365, w0=-3.287792740889839e-06, w1=-0.04401982368517599\n",
      "Regularized Logistic Regression(16/199): loss=0.5685034681069756, w0=-3.4909445440461804e-06, w1=-0.046078521519826644\n",
      "Regularized Logistic Regression(17/199): loss=0.5653944829737665, w0=-3.693848328220759e-06, w1=-0.048084813114880616\n",
      "Regularized Logistic Regression(18/199): loss=0.5625334924393118, w0=-3.896499287903624e-06, w1=-0.05004169449068526\n",
      "Regularized Logistic Regression(19/199): loss=0.5598955203086917, w0=-4.098892645194189e-06, w1=-0.051951926739912946\n",
      "Regularized Logistic Regression(20/199): loss=0.5574584536507287, w0=-4.301023762448226e-06, w1=-0.05381805887717278\n",
      "Regularized Logistic Regression(21/199): loss=0.5552026839656389, w0=-4.502888218894906e-06, w1=-0.05564244804032322\n",
      "Regularized Logistic Regression(22/199): loss=0.5531107959344795, w0=-4.704481860360919e-06, w1=-0.05742727739969828\n",
      "Regularized Logistic Regression(23/199): loss=0.5511672973434852, w0=-4.905800829061995e-06, w1=-0.059174572077367205\n",
      "Regularized Logistic Regression(24/199): loss=0.5493583845939481, w0=-5.106841578752527e-06, w1=-0.06088621333333345\n",
      "Regularized Logistic Regression(25/199): loss=0.5476717389434784, w0=-5.3076008792452365e-06, w1=-0.06256395123777692\n",
      "Regularized Logistic Regression(26/199): loss=0.546096349277477, w0=-5.508075813334671e-06, w1=-0.06420941601681088\n",
      "Regularized Logistic Regression(27/199): loss=0.5446223577842261, w0=-5.708263768410941e-06, w1=-0.0658241282326961\n",
      "Regularized Logistic Regression(28/199): loss=0.5432409254087941, w0=-5.908162424479578e-06, w1=-0.06740950793714375\n",
      "Regularized Logistic Regression(29/199): loss=0.5419441143967121, w0=-6.1077697398683915e-06, w1=-0.0689668829175159\n",
      "Regularized Logistic Regression(30/199): loss=0.5407247856151154, w0=-6.307083935570893e-06, w1=-0.0704974961397879\n",
      "Regularized Logistic Regression(31/199): loss=0.5395765086636842, w0=-6.506103478923923e-06, w1=-0.07200251247859983\n",
      "Regularized Logistic Regression(32/199): loss=0.5384934830667778, w0=-6.70482706712594e-06, w1=-0.07348302481317033\n",
      "Regularized Logistic Regression(33/199): loss=0.5374704690776083, w0=-6.903253610957618e-06, w1=-0.07494005955796679\n",
      "Regularized Logistic Regression(34/199): loss=0.5365027268305491, w0=-7.101382218957205e-06, w1=-0.07637458168853567\n",
      "Regularized Logistic Regression(35/199): loss=0.5355859627534498, w0=-7.299212182220944e-06, w1=-0.07778749931559369\n",
      "Regularized Logistic Regression(36/199): loss=0.5347162823023525, w0=-7.496742959937498e-06, w1=-0.079179667854157\n",
      "Regularized Logistic Regression(37/199): loss=0.5338901482098877, w0=-7.693974165719746e-06, w1=-0.08055189382901766\n",
      "Regularized Logistic Regression(38/199): loss=0.5331043435490107, w0=-7.890905554763824e-06, w1=-0.08190493835311619\n",
      "Regularized Logistic Regression(39/199): loss=0.5323559390083464, w0=-8.08753701184091e-06, w1=-0.08323952031121813\n",
      "Regularized Logistic Regression(40/199): loss=0.5316422638565264, w0=-8.283868540109933e-06, w1=-0.08455631927767604\n",
      "Regularized Logistic Regression(41/199): loss=0.5309608801425283, w0=-8.479900250727155e-06, w1=-0.08585597819389897\n",
      "Regularized Logistic Regression(42/199): loss=0.5303095597388479, w0=-8.675632353220541e-06, w1=-0.08713910582835908\n",
      "Regularized Logistic Regression(43/199): loss=0.5296862638857623, w0=-8.871065146591547e-06, w1=-0.08840627903953008\n",
      "Regularized Logistic Regression(44/199): loss=0.5290891249392529, w0=-9.066199011104001e-06, w1=-0.08965804485998935\n",
      "Regularized Logistic Regression(45/199): loss=0.5285164300633132, w0=-9.261034400718404e-06, w1=-0.09089492241802105\n",
      "Regularized Logistic Regression(46/199): loss=0.5279666066403503, w0=-9.455571836129762e-06, w1=-0.09211740471137227\n",
      "Regularized Logistic Regression(47/199): loss=0.5274382092018465, w0=-9.649811898367776e-06, w1=-0.09332596024632447\n",
      "Regularized Logistic Regression(48/199): loss=0.5269299077061106, w0=-9.843755222919411e-06, w1=-0.09452103455392236\n",
      "Regularized Logistic Regression(49/199): loss=0.5264404770113099, w0=-1.0037402494335562e-05, w1=-0.09570305159402721\n",
      "Regularized Logistic Regression(50/199): loss=0.5259687874105015, w0=-1.023075444128539e-05, w1=-0.09687241505681586\n",
      "Regularized Logistic Regression(51/199): loss=0.5255137961115031, w0=-1.0423811832023988e-05, w1=-0.0980295095704166\n",
      "Regularized Logistic Regression(52/199): loss=0.5250745395584564, w0=-1.0616575470241094e-05, w1=-0.09917470182253654\n",
      "Regularized Logistic Regression(53/199): loss=0.5246501265041453, w0=-1.0809046191260702e-05, w1=-0.10030834160319839\n",
      "Regularized Logistic Regression(54/199): loss=0.5242397317528056, w0=-1.100122485856349e-05, w1=-0.10143076277503302\n",
      "Regularized Logistic Regression(55/199): loss=0.523842590502468, w0=-1.1193112360605984e-05, w1=-0.10254228417698036\n",
      "Regularized Logistic Regression(56/199): loss=0.523457993224028, w0=-1.1384709607912305e-05, w1=-0.10364321046671672\n",
      "Regularized Logistic Regression(57/199): loss=0.5230852810213876, w0=-1.1576017530416164e-05, w1=-0.10473383290664268\n",
      "Regularized Logistic Regression(58/199): loss=0.5227238414232696, w0=-1.1767037075032508e-05, w1=-0.10581443009783588\n",
      "Regularized Logistic Regression(59/199): loss=0.5223731045628198, w0=-1.1957769203439805e-05, w1=-0.10688526866598153\n",
      "Regularized Logistic Regression(60/199): loss=0.522032539705954, w0=-1.2148214890055492e-05, w1=-0.10794660390294356\n",
      "Regularized Logistic Regression(61/199): loss=0.5217016520936757, w0=-1.2338375120188522e-05, w1=-0.10899868036732013\n",
      "Regularized Logistic Regression(62/199): loss=0.5213799800673514, w0=-1.2528250888354194e-05, w1=-0.11004173244704302\n",
      "Regularized Logistic Regression(63/199): loss=0.5210670924492621, w0=-1.271784319673773e-05, w1=-0.11107598488681822\n",
      "Regularized Logistic Regression(64/199): loss=0.520762586153676, w0=-1.290715305379413e-05, w1=-0.11210165328297311\n",
      "Regularized Logistic Regression(65/199): loss=0.520466084006311, w0=-1.3096181472972854e-05, w1=-0.11311894454806062\n",
      "Regularized Logistic Regression(66/199): loss=0.5201772327523375, w0=-1.3284929471556886e-05, w1=-0.1141280573473784\n",
      "Regularized Logistic Regression(67/199): loss=0.5198957012351461, w0=-1.347339806960654e-05, w1=-0.1151291825093848\n",
      "Regularized Logistic Regression(68/199): loss=0.5196211787299001, w0=-1.3661588288999195e-05, w1=-0.11612250341183541\n",
      "Regularized Logistic Regression(69/199): loss=0.5193533734175325, w0=-1.3849501152556903e-05, w1=-0.11710819634531666\n",
      "Regularized Logistic Regression(70/199): loss=0.5190920109862688, w0=-1.403713768325444e-05, w1=-0.11808643085572179\n",
      "Regularized Logistic Regression(71/199): loss=0.5188368333490617, w0=-1.4224498903501036e-05, w1=-0.11905737006709396\n",
      "Regularized Logistic Regression(72/199): loss=0.5185875974664577, w0=-1.4411585834489565e-05, w1=-0.12002117098615009\n",
      "Regularized Logistic Regression(73/199): loss=0.5183440742654377, w0=-1.4598399495607481e-05, w1=-0.12097798478969957\n",
      "Regularized Logistic Regression(74/199): loss=0.5181060476457129, w0=-1.47849409039043e-05, w1=-0.12192795709608001\n",
      "Regularized Logistic Regression(75/199): loss=0.5178733135657436, w0=-1.4971211073610841e-05, w1=-0.12287122822164531\n",
      "Regularized Logistic Regression(76/199): loss=0.517645679201519, w0=-1.5157211015705823e-05, w1=-0.1238079334232691\n",
      "Regularized Logistic Regression(77/199): loss=0.5174229621717703, w0=-1.5342941737525835e-05, w1=-0.12473820312775077\n",
      "Regularized Logistic Regression(78/199): loss=0.5172049898238955, w0=-1.552840424241495e-05, w1=-0.1256621631489509\n",
      "Regularized Logistic Regression(79/199): loss=0.516991598575405, w0=-1.571359952941068e-05, w1=-0.12657993489341937\n",
      "Regularized Logistic Regression(80/199): loss=0.5167826333061731, w0=-1.5898528592963087e-05, w1=-0.12749163555522774\n",
      "Regularized Logistic Regression(81/199): loss=0.5165779467972155, w0=-1.608319242268429e-05, w1=-0.12839737830066583\n",
      "Regularized Logistic Regression(82/199): loss=0.5163773992121018, w0=-1.6267592003125753e-05, w1=-0.129297272443416\n",
      "Regularized Logistic Regression(83/199): loss=0.5161808576174606, w0=-1.6451728313580928e-05, w1=-0.13019142361077687\n",
      "Regularized Logistic Regression(84/199): loss=0.5159881955393557, w0=-1.663560232791115e-05, w1=-0.1310799339014673\n",
      "Regularized Logistic Regression(85/199): loss=0.5157992925525964, w0=-1.6819215014392676e-05, w1=-0.131962902035507\n",
      "Regularized Logistic Regression(86/199): loss=0.5156140339003, w0=-1.7002567335583153e-05, w1=-0.13284042349663525\n",
      "Regularized Logistic Regression(87/199): loss=0.5154323101412716, w0=-1.718566024820572e-05, w1=-0.13371259066769894\n",
      "Regularized Logistic Regression(88/199): loss=0.5152540168229613, w0=-1.7368494703049274e-05, w1=-0.13457949295941346\n",
      "Regularized Logistic Regression(89/199): loss=0.515079054177969, w0=-1.7551071644883465e-05, w1=-0.1354412169328698\n",
      "Regularized Logistic Regression(90/199): loss=0.5149073268422245, w0=-1.7733392012387098e-05, w1=-0.13629784641614212\n",
      "Regularized Logistic Regression(91/199): loss=0.5147387435931504, w0=-1.7915456738088797e-05, w1=-0.13714946261532265\n",
      "Regularized Logistic Regression(92/199): loss=0.5145732171062355, w0=-1.8097266748318797e-05, w1=-0.13799614422029308\n",
      "Regularized Logistic Regression(93/199): loss=0.5144106637285991, w0=-1.8278822963170887e-05, w1=-0.13883796750552005\n",
      "Regularized Logistic Regression(94/199): loss=0.5142510032682306, w0=-1.846012629647359e-05, w1=-0.13967500642614392\n",
      "Regularized Logistic Regression(95/199): loss=0.5140941587977074, w0=-1.864117765576972e-05, w1=-0.1405073327096158\n",
      "Regularized Logistic Regression(96/199): loss=0.5139400564712865, w0=-1.8821977942303565e-05, w1=-0.14133501594311937\n",
      "Regularized Logistic Regression(97/199): loss=0.5137886253543598, w0=-1.9002528051014964e-05, w1=-0.1421581236570009\n",
      "Regularized Logistic Regression(98/199): loss=0.5136397972643428, w0=-1.9182828870539672e-05, w1=-0.1429767214044161\n",
      "Regularized Logistic Regression(99/199): loss=0.5134935066221458, w0=-1.9362881283215375e-05, w1=-0.14379087283739303\n",
      "Regularized Logistic Regression(100/199): loss=0.5133496903134359, w0=-1.9542686165092827e-05, w1=-0.14460063977949283\n",
      "Regularized Logistic Regression(101/199): loss=0.5132082875589759, w0=-1.972224438595163e-05, w1=-0.14540608229524596\n",
      "Regularized Logistic Regression(102/199): loss=0.513069239793367, w0=-1.9901556809320163e-05, w1=-0.14620725875652535\n",
      "Regularized Logistic Regression(103/199): loss=0.512932490551589, w0=-2.0080624292499282e-05, w1=-0.1470042259060123\n",
      "Regularized Logistic Regression(104/199): loss=0.5127979853627692, w0=-2.0259447686589362e-05, w1=-0.14779703891789964\n",
      "Regularized Logistic Regression(105/199): loss=0.5126656716506648, w0=-2.0438027836520384e-05, w1=-0.14858575145597036\n",
      "Regularized Logistic Regression(106/199): loss=0.5125354986403731, w0=-2.0616365581084696e-05, w1=-0.14937041572918058\n",
      "Regularized Logistic Regression(107/199): loss=0.5124074172708312, w0=-2.0794461752972187e-05, w1=-0.15015108254486922\n",
      "Regularized Logistic Regression(108/199): loss=0.5122813801126956, w0=-2.0972317178807587e-05, w1=-0.15092780135970876\n",
      "Regularized Logistic Regression(109/199): loss=0.5121573412912237, w0=-2.1149932679189662e-05, w1=-0.15170062032850729\n",
      "Regularized Logistic Regression(110/199): loss=0.5120352564138078, w0=-2.1327309068732068e-05, w1=-0.15246958635096453\n",
      "Regularized Logistic Regression(111/199): loss=0.5119150825018424, w0=-2.1504447156105667e-05, w1=-0.15323474511647814\n",
      "Regularized Logistic Regression(112/199): loss=0.5117967779266216, w0=-2.16813477440821e-05, w1=-0.15399614114709334\n",
      "Regularized Logistic Regression(113/199): loss=0.5116803023489955, w0=-2.1858011629578478e-05, w1=-0.15475381783868256\n",
      "Regularized Logistic Regression(114/199): loss=0.5115656166625291, w0=-2.2034439603703008e-05, w1=-0.155507817500438\n",
      "Regularized Logistic Regression(115/199): loss=0.5114526829399251, w0=-2.221063245180141e-05, w1=-0.15625818139275402\n",
      "Regularized Logistic Regression(116/199): loss=0.5113414643824941, w0=-2.238659095350402e-05, w1=-0.1570049497635753\n",
      "Regularized Logistic Regression(117/199): loss=0.5112319252724691, w0=-2.2562315882773452e-05, w1=-0.1577481618832789\n",
      "Regularized Logistic Regression(118/199): loss=0.5111240309279751, w0=-2.273780800795268e-05, w1=-0.1584878560781577\n",
      "Regularized Logistic Regression(119/199): loss=0.5110177476604787, w0=-2.2913068091813498e-05, w1=-0.1592240697625694\n",
      "Regularized Logistic Regression(120/199): loss=0.5109130427345597, w0=-2.308809689160522e-05, w1=-0.15995683946980785\n",
      "Regularized Logistic Regression(121/199): loss=0.5108098843298484, w0=-2.326289515910357e-05, w1=-0.16068620088175758\n",
      "Regularized Logistic Regression(122/199): loss=0.5107082415049957, w0=-2.3437463640659674e-05, w1=-0.1614121888573817\n",
      "Regularized Logistic Regression(123/199): loss=0.5106080841635393, w0=-2.3611803077249105e-05, w1=-0.16213483746009785\n",
      "Regularized Logistic Regression(124/199): loss=0.5105093830215532, w0=-2.378591420452089e-05, w1=-0.162854179984088\n",
      "Regularized Logistic Regression(125/199): loss=0.5104121095769609, w0=-2.3959797752846464e-05, w1=-0.16357024897959083\n",
      "Regularized Logistic Regression(126/199): loss=0.5103162360804152, w0=-2.4133454447368494e-05, w1=-0.16428307627721928\n",
      "Regularized Logistic Regression(127/199): loss=0.5102217355076402, w0=-2.430688500804954e-05, w1=-0.16499269301134556\n",
      "Regularized Logistic Regression(128/199): loss=0.5101285815331547, w0=-2.4480090149720516e-05, w1=-0.16569912964259464\n",
      "Regularized Logistic Regression(129/199): loss=0.5100367485052822, w0=-2.4653070582128916e-05, w1=-0.16640241597948174\n",
      "Regularized Logistic Regression(130/199): loss=0.5099462114223792, w0=-2.4825827009986765e-05, w1=-0.16710258119923344\n",
      "Regularized Logistic Regression(131/199): loss=0.509856945910201, w0=-2.4998360133018276e-05, w1=-0.16779965386782336\n",
      "Regularized Logistic Regression(132/199): loss=0.5097689282003427, w0=-2.517067064600719e-05, w1=-0.16849366195925816\n",
      "Regularized Logistic Regression(133/199): loss=0.50968213510969, w0=-2.534275923884377e-05, w1=-0.1691846328741436\n",
      "Regularized Logistic Regression(134/199): loss=0.5095965440208176, w0=-2.5514626596571436e-05, w1=-0.16987259345756087\n",
      "Regularized Logistic Regression(135/199): loss=0.5095121328632861, w0=-2.5686273399433014e-05, w1=-0.17055757001628122\n",
      "Regularized Logistic Regression(136/199): loss=0.5094288800957821, w0=-2.58577003229166e-05, w1=-0.17123958833534855\n",
      "Regularized Logistic Regression(137/199): loss=0.5093467646890509, w0=-2.6028908037801023e-05, w1=-0.1719186736940515\n",
      "Regularized Logistic Regression(138/199): loss=0.5092657661095834, w0=-2.6199897210200877e-05, w1=-0.17259485088131402\n",
      "Regularized Logistic Regression(139/199): loss=0.5091858643040107, w0=-2.6370668501611137e-05, w1=-0.17326814421052553\n",
      "Regularized Logistic Regression(140/199): loss=0.5091070396841656, w0=-2.6541222568951346e-05, w1=-0.17393857753383477\n",
      "Regularized Logistic Regression(141/199): loss=0.5090292731127793, w0=-2.6711560064609353e-05, w1=-0.1746061742559275\n",
      "Regularized Logistic Regression(142/199): loss=0.5089525458897755, w0=-2.6881681636484614e-05, w1=-0.17527095734731024\n",
      "Regularized Logistic Regression(143/199): loss=0.5088768397391278, w0=-2.7051587928031047e-05, w1=-0.17593294935711765\n",
      "Regularized Logistic Regression(144/199): loss=0.508802136796257, w0=-2.7221279578299428e-05, w1=-0.17659217242546532\n",
      "Regularized Logistic Regression(145/199): loss=0.5087284195959304, w0=-2.7390757221979337e-05, w1=-0.1772486482953637\n",
      "Regularized Logistic Regression(146/199): loss=0.5086556710606469, w0=-2.7560021489440663e-05, w1=-0.1779023983242105\n",
      "Regularized Logistic Regression(147/199): loss=0.5085838744894737, w0=-2.772907300677463e-05, w1=-0.17855344349487956\n",
      "Regularized Logistic Regression(148/199): loss=0.508513013547318, w0=-2.7897912395834385e-05, w1=-0.17920180442642072\n",
      "Regularized Logistic Regression(149/199): loss=0.5084430722546069, w0=-2.8066540274275153e-05, w1=-0.1798475013843859\n",
      "Regularized Logistic Regression(150/199): loss=0.5083740349773603, w0=-2.8234957255593885e-05, w1=-0.18049055429079666\n",
      "Regularized Logistic Regression(151/199): loss=0.5083058864176302, w0=-2.8403163949168517e-05, w1=-0.18113098273376668\n",
      "Regularized Logistic Regression(152/199): loss=0.508238611604296, w0=-2.8571160960296745e-05, w1=-0.1817688059767915\n",
      "Regularized Logistic Regression(153/199): loss=0.5081721958841928, w0=-2.873894889023437e-05, w1=-0.18240404296771967\n",
      "Regularized Logistic Regression(154/199): loss=0.5081066249135593, w0=-2.8906528336233193e-05, w1=-0.18303671234741695\n",
      "Regularized Logistic Regression(155/199): loss=0.5080418846497894, w0=-2.9073899891578485e-05, w1=-0.18366683245813523\n",
      "Regularized Logistic Regression(156/199): loss=0.5079779613434747, w0=-2.9241064145626013e-05, w1=-0.18429442135159615\n",
      "Regularized Logistic Regression(157/199): loss=0.5079148415307198, w0=-2.9408021683838637e-05, w1=-0.1849194967968035\n",
      "Regularized Logistic Regression(158/199): loss=0.5078525120257259, w0=-2.9574773087822486e-05, w1=-0.18554207628759056\n",
      "Regularized Logistic Regression(159/199): loss=0.5077909599136216, w0=-2.9741318935362704e-05, w1=-0.1861621770499147\n",
      "Regularized Logistic Regression(160/199): loss=0.5077301725435359, w0=-2.990765980045879e-05, w1=-0.18677981604890903\n",
      "Regularized Logistic Regression(161/199): loss=0.5076701375219013, w0=-3.0073796253359517e-05, w1=-0.18739500999569844\n",
      "Regularized Logistic Regression(162/199): loss=0.5076108427059738, w0=-3.0239728860597446e-05, w1=-0.18800777535398946\n",
      "Regularized Logistic Regression(163/199): loss=0.5075522761975663, w0=-3.040545818502304e-05, w1=-0.18861812834644404\n",
      "Regularized Logistic Regression(164/199): loss=0.5074944263369808, w0=-3.057098478583836e-05, w1=-0.18922608496084142\n",
      "Regularized Logistic Regression(165/199): loss=0.5074372816971326, w0=-3.0736309218630415e-05, w1=-0.1898316609560404\n",
      "Regularized Logistic Regression(166/199): loss=0.507380831077861, w0=-3.090143203540407e-05, w1=-0.19043487186774535\n",
      "Regularized Logistic Regression(167/199): loss=0.5073250635004143, w0=-3.106635378461459e-05, w1=-0.1910357330140862\n",
      "Regularized Logistic Regression(168/199): loss=0.5072699682021072, w0=-3.123107501119983e-05, w1=-0.1916342595010169\n",
      "Regularized Logistic Regression(169/199): loss=0.5072155346311376, w0=-3.1395596256611996e-05, w1=-0.19223046622754025\n",
      "Regularized Logistic Regression(170/199): loss=0.5071617524415628, w0=-3.155991805884912e-05, w1=-0.1928243678907651\n",
      "Regularized Logistic Regression(171/199): loss=0.5071086114884226, w0=-3.172404095248607e-05, w1=-0.19341597899080212\n",
      "Regularized Logistic Regression(172/199): loss=0.5070561018230095, w0=-3.18879654687053e-05, w1=-0.19400531383550318\n",
      "Regularized Logistic Regression(173/199): loss=0.5070042136882761, w0=-3.2051692135327176e-05, w1=-0.19459238654505087\n",
      "Regularized Logistic Regression(174/199): loss=0.5069529375143748, w0=-3.2215221476839976e-05, w1=-0.19517721105640315\n",
      "Regularized Logistic Regression(175/199): loss=0.5069022639143279, w0=-3.2378554014429575e-05, w1=-0.1957598011275984\n",
      "Regularized Logistic Regression(176/199): loss=0.5068521836798201, w0=-3.2541690266008725e-05, w1=-0.1963401703419254\n",
      "Regularized Logistic Regression(177/199): loss=0.5068026877771097, w0=-3.270463074624608e-05, w1=-0.19691833211196366\n",
      "Regularized Logistic Regression(178/199): loss=0.5067537673430551, w0=-3.2867375966594834e-05, w1=-0.19749429968349821\n",
      "Regularized Logistic Regression(179/199): loss=0.5067054136812515, w0=-3.3029926435321035e-05, w1=-0.19806808613931398\n",
      "Regularized Logistic Regression(180/199): loss=0.5066576182582742, w0=-3.319228265753163e-05, w1=-0.1986397044028725\n",
      "Regularized Logistic Regression(181/199): loss=0.5066103727000245, w0=-3.335444513520213e-05, w1=-0.1992091672418772\n",
      "Regularized Logistic Regression(182/199): loss=0.5065636687881752, w0=-3.3516414367204024e-05, w1=-0.19977648727172903\n",
      "Regularized Logistic Regression(183/199): loss=0.5065174984567118, w0=-3.367819084933184e-05, w1=-0.2003416769588779\n",
      "Regularized Logistic Regression(184/199): loss=0.5064718537885655, w0=-3.3839775074329905e-05, w1=-0.2009047486240724\n",
      "Regularized Logistic Regression(185/199): loss=0.5064267270123375, w0=-3.4001167531918854e-05, w1=-0.2014657144455123\n",
      "Regularized Logistic Regression(186/199): loss=0.5063821104991069, w0=-3.416236870882179e-05, w1=-0.20202458646190652\n",
      "Regularized Logistic Regression(187/199): loss=0.5063379967593246, w0=-3.4323379088790195e-05, w1=-0.20258137657543976\n",
      "Regularized Logistic Regression(188/199): loss=0.506294378439788, w0=-3.448419915262953e-05, w1=-0.20313609655465129\n",
      "Regularized Logistic Regression(189/199): loss=0.5062512483206907, w0=-3.4644829378224575e-05, w1=-0.2036887580372294\n",
      "Regularized Logistic Regression(190/199): loss=0.5062085993127527, w0=-3.480527024056445e-05, w1=-0.2042393725327226\n",
      "Regularized Logistic Regression(191/199): loss=0.5061664244544196, w0=-3.496552221176744e-05, w1=-0.20478795142517278\n",
      "Regularized Logistic Regression(192/199): loss=0.5061247169091356, w0=-3.512558576110546e-05, w1=-0.20533450597567138\n",
      "Regularized Logistic Regression(193/199): loss=0.5060834699626836, w0=-3.528546135502832e-05, w1=-0.2058790473248411\n",
      "Regularized Logistic Regression(194/199): loss=0.506042677020593, w0=-3.544514945718771e-05, w1=-0.20642158649524683\n",
      "Regularized Logistic Regression(195/199): loss=0.5060023316056115, w0=-3.560465052846092e-05, w1=-0.2069621343937376\n",
      "Regularized Logistic Regression(196/199): loss=0.5059624273552397, w0=-3.5763965026974296e-05, w1=-0.20750070181372057\n",
      "Regularized Logistic Regression(197/199): loss=0.5059229580193257, w0=-3.592309340812648e-05, w1=-0.2080372994373725\n",
      "Regularized Logistic Regression(198/199): loss=0.5058839174577208, w0=-3.6082036124611363e-05, w1=-0.20857193783778713\n",
      "Regularized Logistic Regression(199/199): loss=0.5058452996379895, w0=-3.624079362644084e-05, w1=-0.20910462748106334\n",
      "Regularized Logistic Regression(0/299): loss=0.693147180559945, w0=-2.0882930292778685e-07, w1=-0.003658898216597753\n",
      "Regularized Logistic Regression(1/299): loss=0.6777027414889991, w0=-4.168083238198562e-07, w1=-0.007146888677045928\n",
      "Regularized Logistic Regression(2/299): loss=0.6639220287926192, w0=-6.241103972554306e-07, w1=-0.010478455819701962\n",
      "Regularized Logistic Regression(3/299): loss=0.6516074780558584, w0=-8.308665434388834e-07, w1=-0.013666639343024482\n",
      "Regularized Logistic Regression(4/299): loss=0.6405854572249043, w0=-1.0371744290199916e-06, w1=-0.016723143045150855\n",
      "Regularized Logistic Regression(5/299): loss=0.6307034550896028, w0=-1.2431057303520428e-06, w1=-0.019658460743975262\n",
      "Regularized Logistic Regression(6/299): loss=0.6218275976937215, w0=-1.4487120780475333e-06, w1=-0.022482004410888365\n",
      "Regularized Logistic Regression(7/299): loss=0.6138404421241037, w0=-1.654029789004874e-06, w1=-0.025202226168933934\n",
      "Regularized Logistic Regression(8/299): loss=0.6066390094895175, w0=-1.8590835904639685e-06, w1=-0.02782673004384163\n",
      "Regularized Logistic Regression(9/299): loss=0.6001330296528367, w0=-2.0638895233270064e-06, w1=-0.03036237194877745\n",
      "Regularized Logistic Regression(10/299): loss=0.5942433773351784, w0=-2.2684571876861515e-06, w1=-0.032815347867214864\n",
      "Regularized Logistic Regression(11/299): loss=0.5889006830457942, w0=-2.472791467569436e-06, w1=-0.03519127097420068\n",
      "Regularized Logistic Regression(12/299): loss=0.5840441040530545, w0=-2.6768938473209933e-06, w1=-0.03749523878251754\n",
      "Regularized Logistic Regression(13/299): loss=0.5796202413240482, w0=-2.8807634102018747e-06, w1=-0.03973189149922548\n",
      "Regularized Logistic Regression(14/299): loss=0.5755821887097818, w0=-3.0843975912252882e-06, w1=-0.04190546274427294\n",
      "Regularized Logistic Regression(15/299): loss=0.5718887010136365, w0=-3.287792740889839e-06, w1=-0.04401982368517599\n",
      "Regularized Logistic Regression(16/299): loss=0.5685034681069756, w0=-3.4909445440461804e-06, w1=-0.046078521519826644\n",
      "Regularized Logistic Regression(17/299): loss=0.5653944829737665, w0=-3.693848328220759e-06, w1=-0.048084813114880616\n",
      "Regularized Logistic Regression(18/299): loss=0.5625334924393118, w0=-3.896499287903624e-06, w1=-0.05004169449068526\n",
      "Regularized Logistic Regression(19/299): loss=0.5598955203086917, w0=-4.098892645194189e-06, w1=-0.051951926739912946\n",
      "Regularized Logistic Regression(20/299): loss=0.5574584536507287, w0=-4.301023762448226e-06, w1=-0.05381805887717278\n",
      "Regularized Logistic Regression(21/299): loss=0.5552026839656389, w0=-4.502888218894906e-06, w1=-0.05564244804032322\n",
      "Regularized Logistic Regression(22/299): loss=0.5531107959344795, w0=-4.704481860360919e-06, w1=-0.05742727739969828\n",
      "Regularized Logistic Regression(23/299): loss=0.5511672973434852, w0=-4.905800829061995e-06, w1=-0.059174572077367205\n",
      "Regularized Logistic Regression(24/299): loss=0.5493583845939481, w0=-5.106841578752527e-06, w1=-0.06088621333333345\n",
      "Regularized Logistic Regression(25/299): loss=0.5476717389434784, w0=-5.3076008792452365e-06, w1=-0.06256395123777692\n",
      "Regularized Logistic Regression(26/299): loss=0.546096349277477, w0=-5.508075813334671e-06, w1=-0.06420941601681088\n",
      "Regularized Logistic Regression(27/299): loss=0.5446223577842261, w0=-5.708263768410941e-06, w1=-0.0658241282326961\n",
      "Regularized Logistic Regression(28/299): loss=0.5432409254087941, w0=-5.908162424479578e-06, w1=-0.06740950793714375\n",
      "Regularized Logistic Regression(29/299): loss=0.5419441143967121, w0=-6.1077697398683915e-06, w1=-0.0689668829175159\n",
      "Regularized Logistic Regression(30/299): loss=0.5407247856151154, w0=-6.307083935570893e-06, w1=-0.0704974961397879\n",
      "Regularized Logistic Regression(31/299): loss=0.5395765086636842, w0=-6.506103478923923e-06, w1=-0.07200251247859983\n",
      "Regularized Logistic Regression(32/299): loss=0.5384934830667778, w0=-6.70482706712594e-06, w1=-0.07348302481317033\n",
      "Regularized Logistic Regression(33/299): loss=0.5374704690776083, w0=-6.903253610957618e-06, w1=-0.07494005955796679\n",
      "Regularized Logistic Regression(34/299): loss=0.5365027268305491, w0=-7.101382218957205e-06, w1=-0.07637458168853567\n",
      "Regularized Logistic Regression(35/299): loss=0.5355859627534498, w0=-7.299212182220944e-06, w1=-0.07778749931559369\n",
      "Regularized Logistic Regression(36/299): loss=0.5347162823023525, w0=-7.496742959937498e-06, w1=-0.079179667854157\n",
      "Regularized Logistic Regression(37/299): loss=0.5338901482098877, w0=-7.693974165719746e-06, w1=-0.08055189382901766\n",
      "Regularized Logistic Regression(38/299): loss=0.5331043435490107, w0=-7.890905554763824e-06, w1=-0.08190493835311619\n",
      "Regularized Logistic Regression(39/299): loss=0.5323559390083464, w0=-8.08753701184091e-06, w1=-0.08323952031121813\n",
      "Regularized Logistic Regression(40/299): loss=0.5316422638565264, w0=-8.283868540109933e-06, w1=-0.08455631927767604\n",
      "Regularized Logistic Regression(41/299): loss=0.5309608801425283, w0=-8.479900250727155e-06, w1=-0.08585597819389897\n",
      "Regularized Logistic Regression(42/299): loss=0.5303095597388479, w0=-8.675632353220541e-06, w1=-0.08713910582835908\n",
      "Regularized Logistic Regression(43/299): loss=0.5296862638857623, w0=-8.871065146591547e-06, w1=-0.08840627903953008\n",
      "Regularized Logistic Regression(44/299): loss=0.5290891249392529, w0=-9.066199011104001e-06, w1=-0.08965804485998935\n",
      "Regularized Logistic Regression(45/299): loss=0.5285164300633132, w0=-9.261034400718404e-06, w1=-0.09089492241802105\n",
      "Regularized Logistic Regression(46/299): loss=0.5279666066403503, w0=-9.455571836129762e-06, w1=-0.09211740471137227\n",
      "Regularized Logistic Regression(47/299): loss=0.5274382092018465, w0=-9.649811898367776e-06, w1=-0.09332596024632447\n",
      "Regularized Logistic Regression(48/299): loss=0.5269299077061106, w0=-9.843755222919411e-06, w1=-0.09452103455392236\n",
      "Regularized Logistic Regression(49/299): loss=0.5264404770113099, w0=-1.0037402494335562e-05, w1=-0.09570305159402721\n",
      "Regularized Logistic Regression(50/299): loss=0.5259687874105015, w0=-1.023075444128539e-05, w1=-0.09687241505681586\n",
      "Regularized Logistic Regression(51/299): loss=0.5255137961115031, w0=-1.0423811832023988e-05, w1=-0.0980295095704166\n",
      "Regularized Logistic Regression(52/299): loss=0.5250745395584564, w0=-1.0616575470241094e-05, w1=-0.09917470182253654\n",
      "Regularized Logistic Regression(53/299): loss=0.5246501265041453, w0=-1.0809046191260702e-05, w1=-0.10030834160319839\n",
      "Regularized Logistic Regression(54/299): loss=0.5242397317528056, w0=-1.100122485856349e-05, w1=-0.10143076277503302\n",
      "Regularized Logistic Regression(55/299): loss=0.523842590502468, w0=-1.1193112360605984e-05, w1=-0.10254228417698036\n",
      "Regularized Logistic Regression(56/299): loss=0.523457993224028, w0=-1.1384709607912305e-05, w1=-0.10364321046671672\n",
      "Regularized Logistic Regression(57/299): loss=0.5230852810213876, w0=-1.1576017530416164e-05, w1=-0.10473383290664268\n",
      "Regularized Logistic Regression(58/299): loss=0.5227238414232696, w0=-1.1767037075032508e-05, w1=-0.10581443009783588\n",
      "Regularized Logistic Regression(59/299): loss=0.5223731045628198, w0=-1.1957769203439805e-05, w1=-0.10688526866598153\n",
      "Regularized Logistic Regression(60/299): loss=0.522032539705954, w0=-1.2148214890055492e-05, w1=-0.10794660390294356\n",
      "Regularized Logistic Regression(61/299): loss=0.5217016520936757, w0=-1.2338375120188522e-05, w1=-0.10899868036732013\n",
      "Regularized Logistic Regression(62/299): loss=0.5213799800673514, w0=-1.2528250888354194e-05, w1=-0.11004173244704302\n",
      "Regularized Logistic Regression(63/299): loss=0.5210670924492621, w0=-1.271784319673773e-05, w1=-0.11107598488681822\n",
      "Regularized Logistic Regression(64/299): loss=0.520762586153676, w0=-1.290715305379413e-05, w1=-0.11210165328297311\n",
      "Regularized Logistic Regression(65/299): loss=0.520466084006311, w0=-1.3096181472972854e-05, w1=-0.11311894454806062\n",
      "Regularized Logistic Regression(66/299): loss=0.5201772327523375, w0=-1.3284929471556886e-05, w1=-0.1141280573473784\n",
      "Regularized Logistic Regression(67/299): loss=0.5198957012351461, w0=-1.347339806960654e-05, w1=-0.1151291825093848\n",
      "Regularized Logistic Regression(68/299): loss=0.5196211787299001, w0=-1.3661588288999195e-05, w1=-0.11612250341183541\n",
      "Regularized Logistic Regression(69/299): loss=0.5193533734175325, w0=-1.3849501152556903e-05, w1=-0.11710819634531666\n",
      "Regularized Logistic Regression(70/299): loss=0.5190920109862688, w0=-1.403713768325444e-05, w1=-0.11808643085572179\n",
      "Regularized Logistic Regression(71/299): loss=0.5188368333490617, w0=-1.4224498903501036e-05, w1=-0.11905737006709396\n",
      "Regularized Logistic Regression(72/299): loss=0.5185875974664577, w0=-1.4411585834489565e-05, w1=-0.12002117098615009\n",
      "Regularized Logistic Regression(73/299): loss=0.5183440742654377, w0=-1.4598399495607481e-05, w1=-0.12097798478969957\n",
      "Regularized Logistic Regression(74/299): loss=0.5181060476457129, w0=-1.47849409039043e-05, w1=-0.12192795709608001\n",
      "Regularized Logistic Regression(75/299): loss=0.5178733135657436, w0=-1.4971211073610841e-05, w1=-0.12287122822164531\n",
      "Regularized Logistic Regression(76/299): loss=0.517645679201519, w0=-1.5157211015705823e-05, w1=-0.1238079334232691\n",
      "Regularized Logistic Regression(77/299): loss=0.5174229621717703, w0=-1.5342941737525835e-05, w1=-0.12473820312775077\n",
      "Regularized Logistic Regression(78/299): loss=0.5172049898238955, w0=-1.552840424241495e-05, w1=-0.1256621631489509\n",
      "Regularized Logistic Regression(79/299): loss=0.516991598575405, w0=-1.571359952941068e-05, w1=-0.12657993489341937\n",
      "Regularized Logistic Regression(80/299): loss=0.5167826333061731, w0=-1.5898528592963087e-05, w1=-0.12749163555522774\n",
      "Regularized Logistic Regression(81/299): loss=0.5165779467972155, w0=-1.608319242268429e-05, w1=-0.12839737830066583\n",
      "Regularized Logistic Regression(82/299): loss=0.5163773992121018, w0=-1.6267592003125753e-05, w1=-0.129297272443416\n",
      "Regularized Logistic Regression(83/299): loss=0.5161808576174606, w0=-1.6451728313580928e-05, w1=-0.13019142361077687\n",
      "Regularized Logistic Regression(84/299): loss=0.5159881955393557, w0=-1.663560232791115e-05, w1=-0.1310799339014673\n",
      "Regularized Logistic Regression(85/299): loss=0.5157992925525964, w0=-1.6819215014392676e-05, w1=-0.131962902035507\n",
      "Regularized Logistic Regression(86/299): loss=0.5156140339003, w0=-1.7002567335583153e-05, w1=-0.13284042349663525\n",
      "Regularized Logistic Regression(87/299): loss=0.5154323101412716, w0=-1.718566024820572e-05, w1=-0.13371259066769894\n",
      "Regularized Logistic Regression(88/299): loss=0.5152540168229613, w0=-1.7368494703049274e-05, w1=-0.13457949295941346\n",
      "Regularized Logistic Regression(89/299): loss=0.515079054177969, w0=-1.7551071644883465e-05, w1=-0.1354412169328698\n",
      "Regularized Logistic Regression(90/299): loss=0.5149073268422245, w0=-1.7733392012387098e-05, w1=-0.13629784641614212\n",
      "Regularized Logistic Regression(91/299): loss=0.5147387435931504, w0=-1.7915456738088797e-05, w1=-0.13714946261532265\n",
      "Regularized Logistic Regression(92/299): loss=0.5145732171062355, w0=-1.8097266748318797e-05, w1=-0.13799614422029308\n",
      "Regularized Logistic Regression(93/299): loss=0.5144106637285991, w0=-1.8278822963170887e-05, w1=-0.13883796750552005\n",
      "Regularized Logistic Regression(94/299): loss=0.5142510032682306, w0=-1.846012629647359e-05, w1=-0.13967500642614392\n",
      "Regularized Logistic Regression(95/299): loss=0.5140941587977074, w0=-1.864117765576972e-05, w1=-0.1405073327096158\n",
      "Regularized Logistic Regression(96/299): loss=0.5139400564712865, w0=-1.8821977942303565e-05, w1=-0.14133501594311937\n",
      "Regularized Logistic Regression(97/299): loss=0.5137886253543598, w0=-1.9002528051014964e-05, w1=-0.1421581236570009\n",
      "Regularized Logistic Regression(98/299): loss=0.5136397972643428, w0=-1.9182828870539672e-05, w1=-0.1429767214044161\n",
      "Regularized Logistic Regression(99/299): loss=0.5134935066221458, w0=-1.9362881283215375e-05, w1=-0.14379087283739303\n",
      "Regularized Logistic Regression(100/299): loss=0.5133496903134359, w0=-1.9542686165092827e-05, w1=-0.14460063977949283\n",
      "Regularized Logistic Regression(101/299): loss=0.5132082875589759, w0=-1.972224438595163e-05, w1=-0.14540608229524596\n",
      "Regularized Logistic Regression(102/299): loss=0.513069239793367, w0=-1.9901556809320163e-05, w1=-0.14620725875652535\n",
      "Regularized Logistic Regression(103/299): loss=0.512932490551589, w0=-2.0080624292499282e-05, w1=-0.1470042259060123\n",
      "Regularized Logistic Regression(104/299): loss=0.5127979853627692, w0=-2.0259447686589362e-05, w1=-0.14779703891789964\n",
      "Regularized Logistic Regression(105/299): loss=0.5126656716506648, w0=-2.0438027836520384e-05, w1=-0.14858575145597036\n",
      "Regularized Logistic Regression(106/299): loss=0.5125354986403731, w0=-2.0616365581084696e-05, w1=-0.14937041572918058\n",
      "Regularized Logistic Regression(107/299): loss=0.5124074172708312, w0=-2.0794461752972187e-05, w1=-0.15015108254486922\n",
      "Regularized Logistic Regression(108/299): loss=0.5122813801126956, w0=-2.0972317178807587e-05, w1=-0.15092780135970876\n",
      "Regularized Logistic Regression(109/299): loss=0.5121573412912237, w0=-2.1149932679189662e-05, w1=-0.15170062032850729\n",
      "Regularized Logistic Regression(110/299): loss=0.5120352564138078, w0=-2.1327309068732068e-05, w1=-0.15246958635096453\n",
      "Regularized Logistic Regression(111/299): loss=0.5119150825018424, w0=-2.1504447156105667e-05, w1=-0.15323474511647814\n",
      "Regularized Logistic Regression(112/299): loss=0.5117967779266216, w0=-2.16813477440821e-05, w1=-0.15399614114709334\n",
      "Regularized Logistic Regression(113/299): loss=0.5116803023489955, w0=-2.1858011629578478e-05, w1=-0.15475381783868256\n",
      "Regularized Logistic Regression(114/299): loss=0.5115656166625291, w0=-2.2034439603703008e-05, w1=-0.155507817500438\n",
      "Regularized Logistic Regression(115/299): loss=0.5114526829399251, w0=-2.221063245180141e-05, w1=-0.15625818139275402\n",
      "Regularized Logistic Regression(116/299): loss=0.5113414643824941, w0=-2.238659095350402e-05, w1=-0.1570049497635753\n",
      "Regularized Logistic Regression(117/299): loss=0.5112319252724691, w0=-2.2562315882773452e-05, w1=-0.1577481618832789\n",
      "Regularized Logistic Regression(118/299): loss=0.5111240309279751, w0=-2.273780800795268e-05, w1=-0.1584878560781577\n",
      "Regularized Logistic Regression(119/299): loss=0.5110177476604787, w0=-2.2913068091813498e-05, w1=-0.1592240697625694\n",
      "Regularized Logistic Regression(120/299): loss=0.5109130427345597, w0=-2.308809689160522e-05, w1=-0.15995683946980785\n",
      "Regularized Logistic Regression(121/299): loss=0.5108098843298484, w0=-2.326289515910357e-05, w1=-0.16068620088175758\n",
      "Regularized Logistic Regression(122/299): loss=0.5107082415049957, w0=-2.3437463640659674e-05, w1=-0.1614121888573817\n",
      "Regularized Logistic Regression(123/299): loss=0.5106080841635393, w0=-2.3611803077249105e-05, w1=-0.16213483746009785\n",
      "Regularized Logistic Regression(124/299): loss=0.5105093830215532, w0=-2.378591420452089e-05, w1=-0.162854179984088\n",
      "Regularized Logistic Regression(125/299): loss=0.5104121095769609, w0=-2.3959797752846464e-05, w1=-0.16357024897959083\n",
      "Regularized Logistic Regression(126/299): loss=0.5103162360804152, w0=-2.4133454447368494e-05, w1=-0.16428307627721928\n",
      "Regularized Logistic Regression(127/299): loss=0.5102217355076402, w0=-2.430688500804954e-05, w1=-0.16499269301134556\n",
      "Regularized Logistic Regression(128/299): loss=0.5101285815331547, w0=-2.4480090149720516e-05, w1=-0.16569912964259464\n",
      "Regularized Logistic Regression(129/299): loss=0.5100367485052822, w0=-2.4653070582128916e-05, w1=-0.16640241597948174\n",
      "Regularized Logistic Regression(130/299): loss=0.5099462114223792, w0=-2.4825827009986765e-05, w1=-0.16710258119923344\n",
      "Regularized Logistic Regression(131/299): loss=0.509856945910201, w0=-2.4998360133018276e-05, w1=-0.16779965386782336\n",
      "Regularized Logistic Regression(132/299): loss=0.5097689282003427, w0=-2.517067064600719e-05, w1=-0.16849366195925816\n",
      "Regularized Logistic Regression(133/299): loss=0.50968213510969, w0=-2.534275923884377e-05, w1=-0.1691846328741436\n",
      "Regularized Logistic Regression(134/299): loss=0.5095965440208176, w0=-2.5514626596571436e-05, w1=-0.16987259345756087\n",
      "Regularized Logistic Regression(135/299): loss=0.5095121328632861, w0=-2.5686273399433014e-05, w1=-0.17055757001628122\n",
      "Regularized Logistic Regression(136/299): loss=0.5094288800957821, w0=-2.58577003229166e-05, w1=-0.17123958833534855\n",
      "Regularized Logistic Regression(137/299): loss=0.5093467646890509, w0=-2.6028908037801023e-05, w1=-0.1719186736940515\n",
      "Regularized Logistic Regression(138/299): loss=0.5092657661095834, w0=-2.6199897210200877e-05, w1=-0.17259485088131402\n",
      "Regularized Logistic Regression(139/299): loss=0.5091858643040107, w0=-2.6370668501611137e-05, w1=-0.17326814421052553\n",
      "Regularized Logistic Regression(140/299): loss=0.5091070396841656, w0=-2.6541222568951346e-05, w1=-0.17393857753383477\n",
      "Regularized Logistic Regression(141/299): loss=0.5090292731127793, w0=-2.6711560064609353e-05, w1=-0.1746061742559275\n",
      "Regularized Logistic Regression(142/299): loss=0.5089525458897755, w0=-2.6881681636484614e-05, w1=-0.17527095734731024\n",
      "Regularized Logistic Regression(143/299): loss=0.5088768397391278, w0=-2.7051587928031047e-05, w1=-0.17593294935711765\n",
      "Regularized Logistic Regression(144/299): loss=0.508802136796257, w0=-2.7221279578299428e-05, w1=-0.17659217242546532\n",
      "Regularized Logistic Regression(145/299): loss=0.5087284195959304, w0=-2.7390757221979337e-05, w1=-0.1772486482953637\n",
      "Regularized Logistic Regression(146/299): loss=0.5086556710606469, w0=-2.7560021489440663e-05, w1=-0.1779023983242105\n",
      "Regularized Logistic Regression(147/299): loss=0.5085838744894737, w0=-2.772907300677463e-05, w1=-0.17855344349487956\n",
      "Regularized Logistic Regression(148/299): loss=0.508513013547318, w0=-2.7897912395834385e-05, w1=-0.17920180442642072\n",
      "Regularized Logistic Regression(149/299): loss=0.5084430722546069, w0=-2.8066540274275153e-05, w1=-0.1798475013843859\n",
      "Regularized Logistic Regression(150/299): loss=0.5083740349773603, w0=-2.8234957255593885e-05, w1=-0.18049055429079666\n",
      "Regularized Logistic Regression(151/299): loss=0.5083058864176302, w0=-2.8403163949168517e-05, w1=-0.18113098273376668\n",
      "Regularized Logistic Regression(152/299): loss=0.508238611604296, w0=-2.8571160960296745e-05, w1=-0.1817688059767915\n",
      "Regularized Logistic Regression(153/299): loss=0.5081721958841928, w0=-2.873894889023437e-05, w1=-0.18240404296771967\n",
      "Regularized Logistic Regression(154/299): loss=0.5081066249135593, w0=-2.8906528336233193e-05, w1=-0.18303671234741695\n",
      "Regularized Logistic Regression(155/299): loss=0.5080418846497894, w0=-2.9073899891578485e-05, w1=-0.18366683245813523\n",
      "Regularized Logistic Regression(156/299): loss=0.5079779613434747, w0=-2.9241064145626013e-05, w1=-0.18429442135159615\n",
      "Regularized Logistic Regression(157/299): loss=0.5079148415307198, w0=-2.9408021683838637e-05, w1=-0.1849194967968035\n",
      "Regularized Logistic Regression(158/299): loss=0.5078525120257259, w0=-2.9574773087822486e-05, w1=-0.18554207628759056\n",
      "Regularized Logistic Regression(159/299): loss=0.5077909599136216, w0=-2.9741318935362704e-05, w1=-0.1861621770499147\n",
      "Regularized Logistic Regression(160/299): loss=0.5077301725435359, w0=-2.990765980045879e-05, w1=-0.18677981604890903\n",
      "Regularized Logistic Regression(161/299): loss=0.5076701375219013, w0=-3.0073796253359517e-05, w1=-0.18739500999569844\n",
      "Regularized Logistic Regression(162/299): loss=0.5076108427059738, w0=-3.0239728860597446e-05, w1=-0.18800777535398946\n",
      "Regularized Logistic Regression(163/299): loss=0.5075522761975663, w0=-3.040545818502304e-05, w1=-0.18861812834644404\n",
      "Regularized Logistic Regression(164/299): loss=0.5074944263369808, w0=-3.057098478583836e-05, w1=-0.18922608496084142\n",
      "Regularized Logistic Regression(165/299): loss=0.5074372816971326, w0=-3.0736309218630415e-05, w1=-0.1898316609560404\n",
      "Regularized Logistic Regression(166/299): loss=0.507380831077861, w0=-3.090143203540407e-05, w1=-0.19043487186774535\n",
      "Regularized Logistic Regression(167/299): loss=0.5073250635004143, w0=-3.106635378461459e-05, w1=-0.1910357330140862\n",
      "Regularized Logistic Regression(168/299): loss=0.5072699682021072, w0=-3.123107501119983e-05, w1=-0.1916342595010169\n",
      "Regularized Logistic Regression(169/299): loss=0.5072155346311376, w0=-3.1395596256611996e-05, w1=-0.19223046622754025\n",
      "Regularized Logistic Regression(170/299): loss=0.5071617524415628, w0=-3.155991805884912e-05, w1=-0.1928243678907651\n",
      "Regularized Logistic Regression(171/299): loss=0.5071086114884226, w0=-3.172404095248607e-05, w1=-0.19341597899080212\n",
      "Regularized Logistic Regression(172/299): loss=0.5070561018230095, w0=-3.18879654687053e-05, w1=-0.19400531383550318\n",
      "Regularized Logistic Regression(173/299): loss=0.5070042136882761, w0=-3.2051692135327176e-05, w1=-0.19459238654505087\n",
      "Regularized Logistic Regression(174/299): loss=0.5069529375143748, w0=-3.2215221476839976e-05, w1=-0.19517721105640315\n",
      "Regularized Logistic Regression(175/299): loss=0.5069022639143279, w0=-3.2378554014429575e-05, w1=-0.1957598011275984\n",
      "Regularized Logistic Regression(176/299): loss=0.5068521836798201, w0=-3.2541690266008725e-05, w1=-0.1963401703419254\n",
      "Regularized Logistic Regression(177/299): loss=0.5068026877771097, w0=-3.270463074624608e-05, w1=-0.19691833211196366\n",
      "Regularized Logistic Regression(178/299): loss=0.5067537673430551, w0=-3.2867375966594834e-05, w1=-0.19749429968349821\n",
      "Regularized Logistic Regression(179/299): loss=0.5067054136812515, w0=-3.3029926435321035e-05, w1=-0.19806808613931398\n",
      "Regularized Logistic Regression(180/299): loss=0.5066576182582742, w0=-3.319228265753163e-05, w1=-0.1986397044028725\n",
      "Regularized Logistic Regression(181/299): loss=0.5066103727000245, w0=-3.335444513520213e-05, w1=-0.1992091672418772\n",
      "Regularized Logistic Regression(182/299): loss=0.5065636687881752, w0=-3.3516414367204024e-05, w1=-0.19977648727172903\n",
      "Regularized Logistic Regression(183/299): loss=0.5065174984567118, w0=-3.367819084933184e-05, w1=-0.2003416769588779\n",
      "Regularized Logistic Regression(184/299): loss=0.5064718537885655, w0=-3.3839775074329905e-05, w1=-0.2009047486240724\n",
      "Regularized Logistic Regression(185/299): loss=0.5064267270123375, w0=-3.4001167531918854e-05, w1=-0.2014657144455123\n",
      "Regularized Logistic Regression(186/299): loss=0.5063821104991069, w0=-3.416236870882179e-05, w1=-0.20202458646190652\n",
      "Regularized Logistic Regression(187/299): loss=0.5063379967593246, w0=-3.4323379088790195e-05, w1=-0.20258137657543976\n",
      "Regularized Logistic Regression(188/299): loss=0.506294378439788, w0=-3.448419915262953e-05, w1=-0.20313609655465129\n",
      "Regularized Logistic Regression(189/299): loss=0.5062512483206907, w0=-3.4644829378224575e-05, w1=-0.2036887580372294\n",
      "Regularized Logistic Regression(190/299): loss=0.5062085993127527, w0=-3.480527024056445e-05, w1=-0.2042393725327226\n",
      "Regularized Logistic Regression(191/299): loss=0.5061664244544196, w0=-3.496552221176744e-05, w1=-0.20478795142517278\n",
      "Regularized Logistic Regression(192/299): loss=0.5061247169091356, w0=-3.512558576110546e-05, w1=-0.20533450597567138\n",
      "Regularized Logistic Regression(193/299): loss=0.5060834699626836, w0=-3.528546135502832e-05, w1=-0.2058790473248411\n",
      "Regularized Logistic Regression(194/299): loss=0.506042677020593, w0=-3.544514945718771e-05, w1=-0.20642158649524683\n",
      "Regularized Logistic Regression(195/299): loss=0.5060023316056115, w0=-3.560465052846092e-05, w1=-0.2069621343937376\n",
      "Regularized Logistic Regression(196/299): loss=0.5059624273552397, w0=-3.5763965026974296e-05, w1=-0.20750070181372057\n",
      "Regularized Logistic Regression(197/299): loss=0.5059229580193257, w0=-3.592309340812648e-05, w1=-0.2080372994373725\n",
      "Regularized Logistic Regression(198/299): loss=0.5058839174577208, w0=-3.6082036124611363e-05, w1=-0.20857193783778713\n",
      "Regularized Logistic Regression(199/299): loss=0.5058452996379895, w0=-3.624079362644084e-05, w1=-0.20910462748106334\n",
      "Regularized Logistic Regression(200/299): loss=0.5058070986331766, w0=-3.639936636096727e-05, w1=-0.20963537872833454\n",
      "Regularized Logistic Regression(201/299): loss=0.5057693086196294, w0=-3.655775477290572e-05, w1=-0.21016420183774162\n",
      "Regularized Logistic Regression(202/299): loss=0.5057319238748683, w0=-3.671595930435602e-05, w1=-0.21069110696635213\n",
      "Regularized Logistic Regression(203/299): loss=0.5056949387755127, w0=-3.6873980394824525e-05, w1=-0.21121610417202594\n",
      "Regularized Logistic Regression(204/299): loss=0.5056583477952544, w0=-3.7031818481245675e-05, w1=-0.21173920341523048\n",
      "Regularized Logistic Regression(205/299): loss=0.5056221455028769, w0=-3.7189473998003336e-05, w1=-0.2122604145608066\n",
      "Regularized Logistic Regression(206/299): loss=0.5055863265603251, w0=-3.73469473769519e-05, w1=-0.21277974737968633\n",
      "Regularized Logistic Regression(207/299): loss=0.5055508857208181, w0=-3.750423904743721e-05, w1=-0.21329721155056525\n",
      "Regularized Logistic Regression(208/299): loss=0.5055158178270063, w0=-3.7661349436317214e-05, w1=-0.21381281666152976\n",
      "Regularized Logistic Regression(209/299): loss=0.5054811178091716, w0=-3.781827896798244e-05, w1=-0.214326572211641\n",
      "Regularized Logistic Regression(210/299): loss=0.5054467806834695, w0=-3.7975028064376264e-05, w1=-0.21483848761247726\n",
      "Regularized Logistic Regression(211/299): loss=0.5054128015502114, w0=-3.813159714501496e-05, w1=-0.21534857218963613\n",
      "Regularized Logistic Regression(212/299): loss=0.5053791755921866, w0=-3.8287986627007545e-05, w1=-0.21585683518419688\n",
      "Regularized Logistic Regression(213/299): loss=0.5053458980730227, w0=-3.8444196925075435e-05, w1=-0.21636328575414565\n",
      "Regularized Logistic Regression(214/299): loss=0.505312964335583, w0=-3.860022845157188e-05, w1=-0.21686793297576296\n",
      "Regularized Logistic Regression(215/299): loss=0.5052803698004009, w0=-3.8756081616501236e-05, w1=-0.21737078584497704\n",
      "Regularized Logistic Regression(216/299): loss=0.5052481099641498, w0=-3.891175682753802e-05, w1=-0.21787185327868144\n",
      "Regularized Logistic Regression(217/299): loss=0.5052161803981468, w0=-3.9067254490045774e-05, w1=-0.21837114411601985\n",
      "Regularized Logistic Regression(218/299): loss=0.50518457674689, w0=-3.922257500709576e-05, w1=-0.2188686671196392\n",
      "Regularized Logistic Regression(219/299): loss=0.5051532947266307, w0=-3.937771877948545e-05, w1=-0.21936443097691005\n",
      "Regularized Logistic Regression(220/299): loss=0.5051223301239738, w0=-3.953268620575683e-05, w1=-0.21985844430111817\n",
      "Regularized Logistic Regression(221/299): loss=0.5050916787945119, w0=-3.968747768221455e-05, w1=-0.22035071563262598\n",
      "Regularized Logistic Regression(222/299): loss=0.5050613366614903, w0=-3.984209360294387e-05, w1=-0.2208412534400056\n",
      "Regularized Logistic Regression(223/299): loss=0.5050312997144983, w0=-3.999653435982844e-05, w1=-0.22133006612114453\n",
      "Regularized Logistic Regression(224/299): loss=0.5050015640081934, w0=-4.0150800342567895e-05, w1=-0.22181716200432428\n",
      "Regularized Logistic Regression(225/299): loss=0.5049721256610498, w0=-4.03048919386953e-05, w1=-0.22230254934927365\n",
      "Regularized Logistic Regression(226/299): loss=0.5049429808541366, w0=-4.04588095335944e-05, w1=-0.22278623634819647\n",
      "Regularized Logistic Regression(227/299): loss=0.5049141258299222, w0=-4.061255351051674e-05, w1=-0.22326823112677485\n",
      "Regularized Logistic Regression(228/299): loss=0.5048855568911047, w0=-4.076612425059856e-05, w1=-0.22374854174514938\n",
      "Regularized Logistic Regression(229/299): loss=0.5048572703994667, w0=-4.091952213287759e-05, w1=-0.2242271761988758\n",
      "Regularized Logistic Regression(230/299): loss=0.5048292627747566, w0=-4.107274753430963e-05, w1=-0.22470414241986006\n",
      "Regularized Logistic Regression(231/299): loss=0.5048015304935927, w0=-4.1225800829785026e-05, w1=-0.2251794482772713\n",
      "Regularized Logistic Regression(232/299): loss=0.504774070088391, w0=-4.137868239214493e-05, w1=-0.2256531015784345\n",
      "Regularized Logistic Regression(233/299): loss=0.5047468781463174, w0=-4.1531392592197445e-05, w1=-0.226125110069702\n",
      "Regularized Logistic Regression(234/299): loss=0.5047199513082595, w0=-4.168393179873361e-05, w1=-0.22659548143730648\n",
      "Regularized Logistic Regression(235/299): loss=0.5046932862678241, w0=-4.1836300378543217e-05, w1=-0.22706422330819412\n",
      "Regularized Logistic Regression(236/299): loss=0.504666879770352, w0=-4.1988498696430504e-05, w1=-0.22753134325083893\n",
      "Regularized Logistic Regression(237/299): loss=0.5046407286119571, w0=-4.2140527115229666e-05, w1=-0.22799684877604007\n",
      "Regularized Logistic Regression(238/299): loss=0.5046148296385842, w0=-4.229238599582026e-05, w1=-0.22846074733770064\n",
      "Regularized Logistic Regression(239/299): loss=0.5045891797450857, w0=-4.244407569714244e-05, w1=-0.22892304633359042\n",
      "Regularized Logistic Regression(240/299): loss=0.504563775874321, w0=-4.259559657621202e-05, w1=-0.22938375310609152\n",
      "Regularized Logistic Regression(241/299): loss=0.5045386150162705, w0=-4.2746948988135504e-05, w1=-0.22984287494292863\n",
      "Regularized Logistic Regression(242/299): loss=0.5045136942071724, w0=-4.289813328612483e-05, w1=-0.23030041907788254\n",
      "Regularized Logistic Regression(243/299): loss=0.504489010528674, w0=-4.3049149821512077e-05, w1=-0.23075639269149048\n",
      "Regularized Logistic Regression(244/299): loss=0.5044645611070033, w0=-4.319999894376403e-05, w1=-0.23121080291172966\n",
      "Regularized Logistic Regression(245/299): loss=0.5044403431121559, w0=-4.335068100049656e-05, w1=-0.23166365681468845\n",
      "Regularized Logistic Regression(246/299): loss=0.5044163537571006, w0=-4.35011963374889e-05, w1=-0.23211496142522225\n",
      "Regularized Logistic Regression(247/299): loss=0.5043925902969991, w0=-4.365154529869782e-05, w1=-0.23256472371759693\n",
      "Regularized Logistic Regression(248/299): loss=0.5043690500284446, w0=-4.3801728226271605e-05, w1=-0.23301295061611882\n",
      "Regularized Logistic Regression(249/299): loss=0.5043457302887129, w0=-4.395174546056397e-05, w1=-0.23345964899575156\n",
      "Regularized Logistic Regression(250/299): loss=0.5043226284550322, w0=-4.4101597340147815e-05, w1=-0.23390482568272125\n",
      "Regularized Logistic Regression(251/299): loss=0.5042997419438664, w0=-4.4251284201828846e-05, w1=-0.23434848745510917\n",
      "Regularized Logistic Regression(252/299): loss=0.5042770682102099, w0=-4.44008063806591e-05, w1=-0.23479064104343308\n",
      "Regularized Logistic Regression(253/299): loss=0.5042546047469041, w0=-4.455016420995034e-05, w1=-0.23523129313121674\n",
      "Regularized Logistic Regression(254/299): loss=0.5042323490839603, w0=-4.4699358021287286e-05, w1=-0.23567045035554907\n",
      "Regularized Logistic Regression(255/299): loss=0.5042102987879008, w0=-4.484838814454082e-05, w1=-0.23610811930763137\n",
      "Regularized Logistic Regression(256/299): loss=0.5041884514611114, w0=-4.499725490788098e-05, w1=-0.2365443065333153\n",
      "Regularized Logistic Regression(257/299): loss=0.5041668047412085, w0=-4.514595863778986e-05, w1=-0.23697901853363007\n",
      "Regularized Logistic Regression(258/299): loss=0.5041453563004178, w0=-4.529449965907445e-05, w1=-0.2374122617652994\n",
      "Regularized Logistic Regression(259/299): loss=0.5041241038449656, w0=-4.5442878294879286e-05, w1=-0.2378440426412495\n",
      "Regularized Logistic Regression(260/299): loss=0.5041030451144818, w0=-4.559109486669903e-05, w1=-0.23827436753110753\n",
      "Regularized Logistic Regression(261/299): loss=0.5040821778814174, w0=-4.573914969439093e-05, w1=-0.23870324276168983\n",
      "Regularized Logistic Regression(262/299): loss=0.5040614999504699, w0=-4.588704309618716e-05, w1=-0.2391306746174828\n",
      "Regularized Logistic Regression(263/299): loss=0.5040410091580237, w0=-4.6034775388707065e-05, w1=-0.23955666934111416\n",
      "Regularized Logistic Regression(264/299): loss=0.5040207033715985, w0=-4.61823468869693e-05, w1=-0.23998123313381586\n",
      "Regularized Logistic Regression(265/299): loss=0.5040005804893111, w0=-4.632975790440383e-05, w1=-0.24040437215587845\n",
      "Regularized Logistic Regression(266/299): loss=0.5039806384393464, w0=-4.647700875286386e-05, w1=-0.2408260925270984\n",
      "Regularized Logistic Regression(267/299): loss=0.5039608751794399, w0=-4.662409974263765e-05, w1=-0.2412464003272164\n",
      "Regularized Logistic Regression(268/299): loss=0.5039412886963691, w0=-4.677103118246022e-05, w1=-0.24166530159634872\n",
      "Regularized Logistic Regression(269/299): loss=0.5039218770054564, w0=-4.691780337952496e-05, w1=-0.2420828023354108\n",
      "Regularized Logistic Regression(270/299): loss=0.5039026381500803, w0=-4.7064416639495116e-05, w1=-0.2424989085065342\n",
      "Regularized Logistic Regression(271/299): loss=0.5038835702011971, w0=-4.721087126651524e-05, w1=-0.24291362603347522\n",
      "Regularized Logistic Regression(272/299): loss=0.5038646712568722, w0=-4.735716756322245e-05, w1=-0.2433269608020177\n",
      "Regularized Logistic Regression(273/299): loss=0.5038459394418195, w0=-4.7503305830757646e-05, w1=-0.24373891866036895\n",
      "Regularized Logistic Regression(274/299): loss=0.5038273729069505, w0=-4.764928636877666e-05, w1=-0.24414950541954838\n",
      "Regularized Logistic Regression(275/299): loss=0.5038089698289322, w0=-4.779510947546121e-05, w1=-0.24455872685377078\n",
      "Regularized Logistic Regression(276/299): loss=0.5037907284097525, w0=-4.7940775447529856e-05, w1=-0.24496658870082208\n",
      "Regularized Logistic Regression(277/299): loss=0.5037726468762962, w0=-4.8086284580248845e-05, w1=-0.24537309666243023\n",
      "Regularized Logistic Regression(278/299): loss=0.5037547234799268, w0=-4.823163716744279e-05, w1=-0.24577825640462958\n",
      "Regularized Logistic Regression(279/299): loss=0.5037369564960775, w0=-4.837683350150535e-05, w1=-0.24618207355811852\n",
      "Regularized Logistic Regression(280/299): loss=0.5037193442238505, w0=-4.852187387340978e-05, w1=-0.24658455371861346\n",
      "Regularized Logistic Regression(281/299): loss=0.5037018849856226, w0=-4.8666758572719353e-05, w1=-0.24698570244719556\n",
      "Regularized Logistic Regression(282/299): loss=0.5036845771266604, w0=-4.881148788759779e-05, w1=-0.24738552527065233\n",
      "Regularized Logistic Regression(283/299): loss=0.5036674190147395, w0=-4.8956062104819494e-05, w1=-0.24778402768181462\n",
      "Regularized Logistic Regression(284/299): loss=0.5036504090397759, w0=-4.9100481509779756e-05, w1=-0.2481812151398873\n",
      "Regularized Logistic Regression(285/299): loss=0.5036335456134584, w0=-4.924474638650487e-05, w1=-0.2485770930707765\n",
      "Regularized Logistic Regression(286/299): loss=0.503616827168894, w0=-4.938885701766216e-05, w1=-0.24897166686740985\n",
      "Regularized Logistic Regression(287/299): loss=0.5036002521602552, w0=-4.953281368456991e-05, w1=-0.24936494189005348\n",
      "Regularized Logistic Regression(288/299): loss=0.503583819062437, w0=-4.967661666720722e-05, w1=-0.2497569234666241\n",
      "Regularized Logistic Regression(289/299): loss=0.5035675263707182, w0=-4.9820266244223765e-05, w1=-0.25014761689299525\n",
      "Regularized Logistic Regression(290/299): loss=0.5035513726004319, w0=-4.9963762692949516e-05, w1=-0.2505370274333006\n",
      "Regularized Logistic Regression(291/299): loss=0.5035353562866385, w0=-5.0107106289404314e-05, w1=-0.25092516032023177\n",
      "Regularized Logistic Regression(292/299): loss=0.5035194759838073, w0=-5.025029730830742e-05, w1=-0.25131202075533177\n",
      "Regularized Logistic Regression(293/299): loss=0.5035037302655049, w0=-5.039333602308694e-05, w1=-0.25169761390928547\n",
      "Regularized Logistic Regression(294/299): loss=0.5034881177240857, w0=-5.053622270588924e-05, w1=-0.2520819449222042\n",
      "Regularized Logistic Regression(295/299): loss=0.5034726369703917, w0=-5.067895762758816e-05, w1=-0.25246501890390743\n",
      "Regularized Logistic Regression(296/299): loss=0.5034572866334578, w0=-5.082154105779431e-05, w1=-0.25284684093420007\n",
      "Regularized Logistic Regression(297/299): loss=0.5034420653602186, w0=-5.0963973264864155e-05, w1=-0.253227416063146\n",
      "Regularized Logistic Regression(298/299): loss=0.5034269718152259, w0=-5.110625451590908e-05, w1=-0.253606749311338\n",
      "Regularized Logistic Regression(299/299): loss=0.5034120046803673, w0=-5.124838507680441e-05, w1=-0.2539848456701629\n",
      "Regularized Logistic Regression(0/99): loss=0.693147180559945, w0=-2.0882930292778685e-07, w1=-0.003658898216597753\n",
      "Regularized Logistic Regression(1/99): loss=0.6777742333139127, w0=-4.149288600935061e-07, w1=-0.007113958593096548\n",
      "Regularized Logistic Regression(2/99): loss=0.664317405013279, w0=-6.185042261823338e-07, w1=-0.010383028246313281\n",
      "Regularized Logistic Regression(3/99): loss=0.6525200807328003, w0=-8.197136169767935e-07, w1=-0.013482053631316214\n",
      "Regularized Logistic Regression(4/99): loss=0.6421609728339082, w0=-1.0186785037541348e-06, w1=-0.016425251453574055\n",
      "Regularized Logistic Regression(5/99): loss=0.633049312764378, w0=-1.2154921568242191e-06, w1=-0.01922529276773128\n",
      "Regularized Logistic Regression(6/99): loss=0.625020674237878, w0=-1.410226417394125e-06, w1=-0.02189348270574105\n",
      "Regularized Logistic Regression(7/99): loss=0.6179333451255659, w0=-1.6029369924152095e-06, w1=-0.02443992679417306\n",
      "Regularized Logistic Regression(8/99): loss=0.6116651764789414, w0=-1.7936675492723607e-06, w1=-0.026873680078337014\n",
      "Regularized Logistic Regression(9/99): loss=0.6061108489182427, w0=-1.9824528536886615e-06, w1=-0.02920287830341961\n",
      "Regularized Logistic Regression(10/99): loss=0.6011795057644772, w0=-2.1693211557921663e-06, w1=-0.0314348520112233\n",
      "Regularized Logistic Regression(11/99): loss=0.5967927087927455, w0=-2.3542959919102614e-06, w1=-0.03357622515644646\n",
      "Regularized Logistic Regression(12/99): loss=0.5928826772439377, w0=-2.537397536230927e-06, w1=-0.03563300009665769\n",
      "Regularized Logistic Regression(13/99): loss=0.5893907745700047, w0=-2.7186436080626062e-06, w1=-0.03761063079445074\n",
      "Regularized Logistic Regression(14/99): loss=0.5862662108108244, w0=-2.8980504170776973e-06, w1=-0.039514085925553256\n",
      "Regularized Logistic Regression(15/99): loss=0.5834649317431692, w0=-3.0756331101781525e-06, w1=-0.041347903390844196\n",
      "Regularized Logistic Regression(16/99): loss=0.5809486690704063, w0=-3.2514061688170914e-06, w1=-0.04311623752545021\n",
      "Regularized Logistic Regression(17/99): loss=0.5786841289189789, w0=-3.42538369405937e-06, w1=-0.044822900105258676\n",
      "Regularized Logistic Regression(18/99): loss=0.5766422987312941, w0=-3.5975796077316767e-06, w1=-0.04647139607931341\n",
      "Regularized Logistic Regression(19/99): loss=0.574797855253334, w0=-3.7680077911505493e-06, w1=-0.04806495480810373\n",
      "Regularized Logistic Regression(20/99): loss=0.5731286586818782, w0=-3.936682177670724e-06, w1=-0.049606557461977056\n",
      "Regularized Logistic Regression(21/99): loss=0.571615320149334, w0=-4.103616811300207e-06, w1=-0.051098961128490834\n",
      "Regularized Logistic Regression(22/99): loss=0.570240831586111, w0=-4.268825880592912e-06, w1=-0.052544720089787174\n",
      "Regularized Logistic Regression(23/99): loss=0.5689902486234303, w0=-4.432323734728441e-06, w1=-0.05394620465824972\n",
      "Regularized Logistic Regression(24/99): loss=0.5678504186018712, w0=-4.594124886946871e-06, w1=-0.055305617898333395\n",
      "Regularized Logistic Regression(25/99): loss=0.5668097469545121, w0=-4.754244009189885e-06, w1=-0.05662501051234887\n",
      "Regularized Logistic Regression(26/99): loss=0.5658579962608894, w0=-4.912695920805833e-06, w1=-0.05790629412634531\n",
      "Regularized Logistic Regression(27/99): loss=0.5649861131414311, w0=-5.069495573426978e-06, w1=-0.05915125317752716\n",
      "Regularized Logistic Regression(28/99): loss=0.5641860789023044, w0=-5.2246580335631505e-06, w1=-0.060361555575633795\n",
      "Regularized Logistic Regression(29/99): loss=0.5634507804667009, w0=-5.378198464032077e-06, w1=-0.061538762286379285\n",
      "Regularized Logistic Regression(30/99): loss=0.5627738986573186, w0=-5.530132105028767e-06, w1=-0.06268433596457637\n",
      "Regularized Logistic Regression(31/99): loss=0.562149811340927, w0=-5.680474255398652e-06, w1=-0.06379964874727558\n",
      "Regularized Logistic Regression(32/99): loss=0.5615735093221828, w0=-5.829240254502107e-06, w1=-0.06488598930259767\n",
      "Regularized Logistic Regression(33/99): loss=0.5610405231912345, w0=-5.976445464926639e-06, w1=-0.06594456921746678\n",
      "Regularized Logistic Regression(34/99): loss=0.560546859597443, w0=-6.122105256206355e-06, w1=-0.06697652879682096\n",
      "Regularized Logistic Regression(35/99): loss=0.5600889456476088, w0=-6.266234989637598e-06, w1=-0.06798294233776374\n",
      "Regularized Logistic Regression(36/99): loss=0.5596635803181291, w0=-6.408850004228558e-06, w1=-0.06896482293430214\n",
      "Regularized Logistic Regression(37/99): loss=0.5592678919320634, w0=-6.549965603784305e-06, w1=-0.06992312686157705\n",
      "Regularized Logistic Regression(38/99): loss=0.5588993008888978, w0=-6.689597045103335e-06, w1=-0.07085875758266642\n",
      "Regularized Logistic Regression(39/99): loss=0.558555486950791, w0=-6.827759527244554e-06, w1=-0.07177256941600392\n",
      "Regularized Logistic Regression(40/99): loss=0.5582343604875635, w0=-6.964468181812553e-06, w1=-0.07266537089707208\n",
      "Regularized Logistic Regression(41/99): loss=0.5579340371664094, w0=-7.099738064202445e-06, w1=-0.0735379278642111\n",
      "Regularized Logistic Regression(42/99): loss=0.5576528156436151, w0=-7.23358414574218e-06, w1=-0.07439096629505967\n",
      "Regularized Logistic Regression(43/99): loss=0.5573891578763449, w0=-7.366021306669274e-06, w1=-0.07522517491722527\n",
      "Regularized Logistic Regression(44/99): loss=0.5571416717244991, w0=-7.4970643298795745e-06, w1=-0.07604120761423193\n",
      "Regularized Logistic Regression(45/99): loss=0.5569090955570468, w0=-7.626727895387453e-06, w1=-0.07683968564554366\n",
      "Regularized Logistic Regression(46/99): loss=0.5566902846153126, w0=-7.75502657543944e-06, w1=-0.07762119969748896\n",
      "Regularized Logistic Regression(47/99): loss=0.5564841989183392, w0=-7.881974830226342e-06, w1=-0.07838631178016765\n",
      "Regularized Logistic Regression(48/99): loss=0.5562898925235331, w0=-8.007587004142203e-06, w1=-0.07913555698387797\n",
      "Regularized Logistic Regression(49/99): loss=0.5561065039799519, w0=-8.131877322541896e-06, w1=-0.07986944510724121\n",
      "Regularized Logistic Regression(50/99): loss=0.555933247832437, w0=-8.254859888952592e-06, w1=-0.08058846216798622\n",
      "Regularized Logistic Regression(51/99): loss=0.5557694070527828, w0=-8.376548682697701e-06, w1=-0.08129307180628824\n",
      "Regularized Logistic Regression(52/99): loss=0.5556143262896946, w0=-8.49695755689516e-06, w1=-0.08198371658959029\n",
      "Regularized Logistic Regression(53/99): loss=0.5554674058427641, w0=-8.616100236794976e-06, w1=-0.08266081922699031\n",
      "Regularized Logistic Regression(54/99): loss=0.5553280962773831, w0=-8.733990318423962e-06, w1=-0.08332478370051215\n",
      "Regularized Logistic Regression(55/99): loss=0.5551958936076695, w0=-8.850641267508192e-06, w1=-0.08397599631989788\n",
      "Regularized Logistic Regression(56/99): loss=0.5550703349833173, w0=-8.966066418646408e-06, w1=-0.08461482670695254\n",
      "Regularized Logistic Regression(57/99): loss=0.554950994823972, w0=-9.080278974709823e-06, w1=-0.0852416287149195\n",
      "Regularized Logistic Regression(58/99): loss=0.5548374813514553, w0=-9.193292006446058e-06, w1=-0.08585674128788183\n",
      "Regularized Logistic Regression(59/99): loss=0.5547294334760138, w0=-9.305118452266882e-06, w1=-0.08646048926473905\n",
      "Regularized Logistic Regression(60/99): loss=0.5546265179979099, w0=-9.415771118201298e-06, w1=-0.08705318413191093\n",
      "Regularized Logistic Regression(61/99): loss=0.554528427090149, w0=-9.525262677997188e-06, w1=-0.08763512472856533\n",
      "Regularized Logistic Regression(62/99): loss=0.5544348760320892, w0=-9.633605673356283e-06, w1=-0.08820659790784316\n",
      "Regularized Logistic Regression(63/99): loss=0.554345601167125, w0=-9.740812514288598e-06, w1=-0.08876787915725605\n",
      "Regularized Logistic Regression(64/99): loss=0.5542603580606809, w0=-9.846895479573805e-06, w1=-0.08931923318117854\n",
      "Regularized Logistic Regression(65/99): loss=0.5541789198374264, w0=-9.951866717318116e-06, w1=-0.08986091444810704\n",
      "Regularized Logistic Regression(66/99): loss=0.5541010756789686, w0=-1.0055738245596363e-05, w1=-0.09039316770514555\n",
      "Regularized Logistic Regression(67/99): loss=0.5540266294653717, w0=-1.0158521953169918e-05, w1=-0.09091622846198111\n",
      "Regularized Logistic Regression(68/99): loss=0.5539553985456671, w0=-1.0260229600271927e-05, w1=-0.09143032344643157\n",
      "Regularized Logistic Regression(69/99): loss=0.5538872126241551, w0=-1.0360872819452195e-05, w1=-0.09193567103348486\n",
      "Regularized Logistic Regression(70/99): loss=0.5538219127507231, w0=-1.0460463116474726e-05, w1=-0.09243248164960147\n",
      "Regularized Logistic Regression(71/99): loss=0.5537593504046724, w0=-1.0559011871261617e-05, w1=-0.09292095815391521\n",
      "Regularized Logistic Regression(72/99): loss=0.5536993866626804, w0=-1.0656530338877563e-05, w1=-0.0934012961978437\n",
      "Regularized Logistic Regression(73/99): loss=0.5536418914425104, w0=-1.0753029650549809e-05, w1=-0.09387368456450826\n",
      "Regularized Logistic Regression(74/99): loss=0.5535867428149762, w0=-1.0848520814718834e-05, w1=-0.09433830548925713\n",
      "Regularized Logistic Regression(75/99): loss=0.5535338263774532, w0=-1.0943014718115539e-05, w1=-0.09479533496249362\n",
      "Regularized Logistic Regression(76/99): loss=0.5534830346829233, w0=-1.1036522126861073e-05, w1=-0.09524494301592071\n",
      "Regularized Logistic Regression(77/99): loss=0.5534342667191747, w0=-1.1129053687585834e-05, w1=-0.09568729399323703\n",
      "Regularized Logistic Regression(78/99): loss=0.5533874274333193, w0=-1.1220619928564472e-05, w1=-0.09612254680624456\n",
      "Regularized Logistic Regression(79/99): loss=0.5533424272972954, w0=-1.1311231260864068e-05, w1=-0.09655085517726095\n",
      "Regularized Logistic Regression(80/99): loss=0.5532991819104656, w0=-1.14008979795029e-05, w1=-0.09697236786866567\n",
      "Regularized Logistic Regression(81/99): loss=0.5532576116358032, w0=-1.1489630264617466e-05, w1=-0.09738722890035761\n",
      "Regularized Logistic Regression(82/99): loss=0.5532176412665256, w0=-1.1577438182635685e-05, w1=-0.09779557775584027\n",
      "Regularized Logistic Regression(83/99): loss=0.5531791997203397, w0=-1.1664331687454324e-05, w1=-0.09819754957761301\n",
      "Regularized Logistic Regression(84/99): loss=0.5531422197587516, w0=-1.1750320621619002e-05, w1=-0.09859327535249021\n",
      "Regularized Logistic Regression(85/99): loss=0.5531066377291395, w0=-1.183541471750518e-05, w1=-0.09898288208743872\n",
      "Regularized Logistic Regression(86/99): loss=0.5530723933275199, w0=-1.1919623598498752e-05, w1=-0.09936649297648012\n",
      "Regularized Logistic Regression(87/99): loss=0.5530394293801384, w0=-1.2002956780174998e-05, w1=-0.09974422755916916\n",
      "Regularized Logistic Regression(88/99): loss=0.5530076916421962, w0=-1.2085423671474763e-05, w1=-0.10011620187112998\n",
      "Regularized Logistic Regression(89/99): loss=0.5529771286121873, w0=-1.2167033575876843e-05, w1=-0.10048252858709597\n",
      "Regularized Logistic Regression(90/99): loss=0.5529476913604733, w0=-1.2247795692565665e-05, w1=-0.1008433171568771\n",
      "Regularized Logistic Regression(91/99): loss=0.5529193333708433, w0=-1.232771911759347e-05, w1=-0.10119867393464571\n",
      "Regularized Logistic Regression(92/99): loss=0.5528920103939372, w0=-1.2406812845036239e-05, w1=-0.10154870230191336\n",
      "Regularized Logistic Regression(93/99): loss=0.5528656803115068, w0=-1.2485085768142737e-05, w1=-0.10189350278454301\n",
      "Regularized Logistic Regression(94/99): loss=0.5528403030105933, w0=-1.2562546680476076e-05, w1=-0.10223317316412352\n",
      "Regularized Logistic Regression(95/99): loss=0.5528158402667788, w0=-1.2639204277047301e-05, w1=-0.10256780858401386\n",
      "Regularized Logistic Regression(96/99): loss=0.5527922556357551, w0=-1.2715067155440527e-05, w1=-0.10289750165034144\n",
      "Regularized Logistic Regression(97/99): loss=0.552769514352514, w0=-1.2790143816929224e-05, w1=-0.1032223425282277\n",
      "Regularized Logistic Regression(98/99): loss=0.5527475832375369, w0=-1.2864442667583318e-05, w1=-0.10354241903349427\n",
      "Regularized Logistic Regression(99/99): loss=0.5527264306094076, w0=-1.2937972019366769e-05, w1=-0.10385781672008912\n",
      "Regularized Logistic Regression(0/199): loss=0.693147180559945, w0=-2.0882930292778685e-07, w1=-0.003658898216597753\n",
      "Regularized Logistic Regression(1/199): loss=0.6777742333139127, w0=-4.149288600935061e-07, w1=-0.007113958593096548\n",
      "Regularized Logistic Regression(2/199): loss=0.664317405013279, w0=-6.185042261823338e-07, w1=-0.010383028246313281\n",
      "Regularized Logistic Regression(3/199): loss=0.6525200807328003, w0=-8.197136169767935e-07, w1=-0.013482053631316214\n",
      "Regularized Logistic Regression(4/199): loss=0.6421609728339082, w0=-1.0186785037541348e-06, w1=-0.016425251453574055\n",
      "Regularized Logistic Regression(5/199): loss=0.633049312764378, w0=-1.2154921568242191e-06, w1=-0.01922529276773128\n",
      "Regularized Logistic Regression(6/199): loss=0.625020674237878, w0=-1.410226417394125e-06, w1=-0.02189348270574105\n",
      "Regularized Logistic Regression(7/199): loss=0.6179333451255659, w0=-1.6029369924152095e-06, w1=-0.02443992679417306\n",
      "Regularized Logistic Regression(8/199): loss=0.6116651764789414, w0=-1.7936675492723607e-06, w1=-0.026873680078337014\n",
      "Regularized Logistic Regression(9/199): loss=0.6061108489182427, w0=-1.9824528536886615e-06, w1=-0.02920287830341961\n",
      "Regularized Logistic Regression(10/199): loss=0.6011795057644772, w0=-2.1693211557921663e-06, w1=-0.0314348520112233\n",
      "Regularized Logistic Regression(11/199): loss=0.5967927087927455, w0=-2.3542959919102614e-06, w1=-0.03357622515644646\n",
      "Regularized Logistic Regression(12/199): loss=0.5928826772439377, w0=-2.537397536230927e-06, w1=-0.03563300009665769\n",
      "Regularized Logistic Regression(13/199): loss=0.5893907745700047, w0=-2.7186436080626062e-06, w1=-0.03761063079445074\n",
      "Regularized Logistic Regression(14/199): loss=0.5862662108108244, w0=-2.8980504170776973e-06, w1=-0.039514085925553256\n",
      "Regularized Logistic Regression(15/199): loss=0.5834649317431692, w0=-3.0756331101781525e-06, w1=-0.041347903390844196\n",
      "Regularized Logistic Regression(16/199): loss=0.5809486690704063, w0=-3.2514061688170914e-06, w1=-0.04311623752545021\n",
      "Regularized Logistic Regression(17/199): loss=0.5786841289189789, w0=-3.42538369405937e-06, w1=-0.044822900105258676\n",
      "Regularized Logistic Regression(18/199): loss=0.5766422987312941, w0=-3.5975796077316767e-06, w1=-0.04647139607931341\n",
      "Regularized Logistic Regression(19/199): loss=0.574797855253334, w0=-3.7680077911505493e-06, w1=-0.04806495480810373\n",
      "Regularized Logistic Regression(20/199): loss=0.5731286586818782, w0=-3.936682177670724e-06, w1=-0.049606557461977056\n",
      "Regularized Logistic Regression(21/199): loss=0.571615320149334, w0=-4.103616811300207e-06, w1=-0.051098961128490834\n",
      "Regularized Logistic Regression(22/199): loss=0.570240831586111, w0=-4.268825880592912e-06, w1=-0.052544720089787174\n",
      "Regularized Logistic Regression(23/199): loss=0.5689902486234303, w0=-4.432323734728441e-06, w1=-0.05394620465824972\n",
      "Regularized Logistic Regression(24/199): loss=0.5678504186018712, w0=-4.594124886946871e-06, w1=-0.055305617898333395\n",
      "Regularized Logistic Regression(25/199): loss=0.5668097469545121, w0=-4.754244009189885e-06, w1=-0.05662501051234887\n",
      "Regularized Logistic Regression(26/199): loss=0.5658579962608894, w0=-4.912695920805833e-06, w1=-0.05790629412634531\n",
      "Regularized Logistic Regression(27/199): loss=0.5649861131414311, w0=-5.069495573426978e-06, w1=-0.05915125317752716\n",
      "Regularized Logistic Regression(28/199): loss=0.5641860789023044, w0=-5.2246580335631505e-06, w1=-0.060361555575633795\n",
      "Regularized Logistic Regression(29/199): loss=0.5634507804667009, w0=-5.378198464032077e-06, w1=-0.061538762286379285\n",
      "Regularized Logistic Regression(30/199): loss=0.5627738986573186, w0=-5.530132105028767e-06, w1=-0.06268433596457637\n",
      "Regularized Logistic Regression(31/199): loss=0.562149811340927, w0=-5.680474255398652e-06, w1=-0.06379964874727558\n",
      "Regularized Logistic Regression(32/199): loss=0.5615735093221828, w0=-5.829240254502107e-06, w1=-0.06488598930259767\n",
      "Regularized Logistic Regression(33/199): loss=0.5610405231912345, w0=-5.976445464926639e-06, w1=-0.06594456921746678\n",
      "Regularized Logistic Regression(34/199): loss=0.560546859597443, w0=-6.122105256206355e-06, w1=-0.06697652879682096\n",
      "Regularized Logistic Regression(35/199): loss=0.5600889456476088, w0=-6.266234989637598e-06, w1=-0.06798294233776374\n",
      "Regularized Logistic Regression(36/199): loss=0.5596635803181291, w0=-6.408850004228558e-06, w1=-0.06896482293430214\n",
      "Regularized Logistic Regression(37/199): loss=0.5592678919320634, w0=-6.549965603784305e-06, w1=-0.06992312686157705\n",
      "Regularized Logistic Regression(38/199): loss=0.5588993008888978, w0=-6.689597045103335e-06, w1=-0.07085875758266642\n",
      "Regularized Logistic Regression(39/199): loss=0.558555486950791, w0=-6.827759527244554e-06, w1=-0.07177256941600392\n",
      "Regularized Logistic Regression(40/199): loss=0.5582343604875635, w0=-6.964468181812553e-06, w1=-0.07266537089707208\n",
      "Regularized Logistic Regression(41/199): loss=0.5579340371664094, w0=-7.099738064202445e-06, w1=-0.0735379278642111\n",
      "Regularized Logistic Regression(42/199): loss=0.5576528156436151, w0=-7.23358414574218e-06, w1=-0.07439096629505967\n",
      "Regularized Logistic Regression(43/199): loss=0.5573891578763449, w0=-7.366021306669274e-06, w1=-0.07522517491722527\n",
      "Regularized Logistic Regression(44/199): loss=0.5571416717244991, w0=-7.4970643298795745e-06, w1=-0.07604120761423193\n",
      "Regularized Logistic Regression(45/199): loss=0.5569090955570468, w0=-7.626727895387453e-06, w1=-0.07683968564554366\n",
      "Regularized Logistic Regression(46/199): loss=0.5566902846153126, w0=-7.75502657543944e-06, w1=-0.07762119969748896\n",
      "Regularized Logistic Regression(47/199): loss=0.5564841989183392, w0=-7.881974830226342e-06, w1=-0.07838631178016765\n",
      "Regularized Logistic Regression(48/199): loss=0.5562898925235331, w0=-8.007587004142203e-06, w1=-0.07913555698387797\n",
      "Regularized Logistic Regression(49/199): loss=0.5561065039799519, w0=-8.131877322541896e-06, w1=-0.07986944510724121\n",
      "Regularized Logistic Regression(50/199): loss=0.555933247832437, w0=-8.254859888952592e-06, w1=-0.08058846216798622\n",
      "Regularized Logistic Regression(51/199): loss=0.5557694070527828, w0=-8.376548682697701e-06, w1=-0.08129307180628824\n",
      "Regularized Logistic Regression(52/199): loss=0.5556143262896946, w0=-8.49695755689516e-06, w1=-0.08198371658959029\n",
      "Regularized Logistic Regression(53/199): loss=0.5554674058427641, w0=-8.616100236794976e-06, w1=-0.08266081922699031\n",
      "Regularized Logistic Regression(54/199): loss=0.5553280962773831, w0=-8.733990318423962e-06, w1=-0.08332478370051215\n",
      "Regularized Logistic Regression(55/199): loss=0.5551958936076695, w0=-8.850641267508192e-06, w1=-0.08397599631989788\n",
      "Regularized Logistic Regression(56/199): loss=0.5550703349833173, w0=-8.966066418646408e-06, w1=-0.08461482670695254\n",
      "Regularized Logistic Regression(57/199): loss=0.554950994823972, w0=-9.080278974709823e-06, w1=-0.0852416287149195\n",
      "Regularized Logistic Regression(58/199): loss=0.5548374813514553, w0=-9.193292006446058e-06, w1=-0.08585674128788183\n",
      "Regularized Logistic Regression(59/199): loss=0.5547294334760138, w0=-9.305118452266882e-06, w1=-0.08646048926473905\n",
      "Regularized Logistic Regression(60/199): loss=0.5546265179979099, w0=-9.415771118201298e-06, w1=-0.08705318413191093\n",
      "Regularized Logistic Regression(61/199): loss=0.554528427090149, w0=-9.525262677997188e-06, w1=-0.08763512472856533\n",
      "Regularized Logistic Regression(62/199): loss=0.5544348760320892, w0=-9.633605673356283e-06, w1=-0.08820659790784316\n",
      "Regularized Logistic Regression(63/199): loss=0.554345601167125, w0=-9.740812514288598e-06, w1=-0.08876787915725605\n",
      "Regularized Logistic Regression(64/199): loss=0.5542603580606809, w0=-9.846895479573805e-06, w1=-0.08931923318117854\n",
      "Regularized Logistic Regression(65/199): loss=0.5541789198374264, w0=-9.951866717318116e-06, w1=-0.08986091444810704\n",
      "Regularized Logistic Regression(66/199): loss=0.5541010756789686, w0=-1.0055738245596363e-05, w1=-0.09039316770514555\n",
      "Regularized Logistic Regression(67/199): loss=0.5540266294653717, w0=-1.0158521953169918e-05, w1=-0.09091622846198111\n",
      "Regularized Logistic Regression(68/199): loss=0.5539553985456671, w0=-1.0260229600271927e-05, w1=-0.09143032344643157\n",
      "Regularized Logistic Regression(69/199): loss=0.5538872126241551, w0=-1.0360872819452195e-05, w1=-0.09193567103348486\n",
      "Regularized Logistic Regression(70/199): loss=0.5538219127507231, w0=-1.0460463116474726e-05, w1=-0.09243248164960147\n",
      "Regularized Logistic Regression(71/199): loss=0.5537593504046724, w0=-1.0559011871261617e-05, w1=-0.09292095815391521\n",
      "Regularized Logistic Regression(72/199): loss=0.5536993866626804, w0=-1.0656530338877563e-05, w1=-0.0934012961978437\n",
      "Regularized Logistic Regression(73/199): loss=0.5536418914425104, w0=-1.0753029650549809e-05, w1=-0.09387368456450826\n",
      "Regularized Logistic Regression(74/199): loss=0.5535867428149762, w0=-1.0848520814718834e-05, w1=-0.09433830548925713\n",
      "Regularized Logistic Regression(75/199): loss=0.5535338263774532, w0=-1.0943014718115539e-05, w1=-0.09479533496249362\n",
      "Regularized Logistic Regression(76/199): loss=0.5534830346829233, w0=-1.1036522126861073e-05, w1=-0.09524494301592071\n",
      "Regularized Logistic Regression(77/199): loss=0.5534342667191747, w0=-1.1129053687585834e-05, w1=-0.09568729399323703\n",
      "Regularized Logistic Regression(78/199): loss=0.5533874274333193, w0=-1.1220619928564472e-05, w1=-0.09612254680624456\n",
      "Regularized Logistic Regression(79/199): loss=0.5533424272972954, w0=-1.1311231260864068e-05, w1=-0.09655085517726095\n",
      "Regularized Logistic Regression(80/199): loss=0.5532991819104656, w0=-1.14008979795029e-05, w1=-0.09697236786866567\n",
      "Regularized Logistic Regression(81/199): loss=0.5532576116358032, w0=-1.1489630264617466e-05, w1=-0.09738722890035761\n",
      "Regularized Logistic Regression(82/199): loss=0.5532176412665256, w0=-1.1577438182635685e-05, w1=-0.09779557775584027\n",
      "Regularized Logistic Regression(83/199): loss=0.5531791997203397, w0=-1.1664331687454324e-05, w1=-0.09819754957761301\n",
      "Regularized Logistic Regression(84/199): loss=0.5531422197587516, w0=-1.1750320621619002e-05, w1=-0.09859327535249021\n",
      "Regularized Logistic Regression(85/199): loss=0.5531066377291395, w0=-1.183541471750518e-05, w1=-0.09898288208743872\n",
      "Regularized Logistic Regression(86/199): loss=0.5530723933275199, w0=-1.1919623598498752e-05, w1=-0.09936649297648012\n",
      "Regularized Logistic Regression(87/199): loss=0.5530394293801384, w0=-1.2002956780174998e-05, w1=-0.09974422755916916\n",
      "Regularized Logistic Regression(88/199): loss=0.5530076916421962, w0=-1.2085423671474763e-05, w1=-0.10011620187112998\n",
      "Regularized Logistic Regression(89/199): loss=0.5529771286121873, w0=-1.2167033575876843e-05, w1=-0.10048252858709597\n",
      "Regularized Logistic Regression(90/199): loss=0.5529476913604733, w0=-1.2247795692565665e-05, w1=-0.1008433171568771\n",
      "Regularized Logistic Regression(91/199): loss=0.5529193333708433, w0=-1.232771911759347e-05, w1=-0.10119867393464571\n",
      "Regularized Logistic Regression(92/199): loss=0.5528920103939372, w0=-1.2406812845036239e-05, w1=-0.10154870230191336\n",
      "Regularized Logistic Regression(93/199): loss=0.5528656803115068, w0=-1.2485085768142737e-05, w1=-0.10189350278454301\n",
      "Regularized Logistic Regression(94/199): loss=0.5528403030105933, w0=-1.2562546680476076e-05, w1=-0.10223317316412352\n",
      "Regularized Logistic Regression(95/199): loss=0.5528158402667788, w0=-1.2639204277047301e-05, w1=-0.10256780858401386\n",
      "Regularized Logistic Regression(96/199): loss=0.5527922556357551, w0=-1.2715067155440527e-05, w1=-0.10289750165034144\n",
      "Regularized Logistic Regression(97/199): loss=0.552769514352514, w0=-1.2790143816929224e-05, w1=-0.1032223425282277\n",
      "Regularized Logistic Regression(98/199): loss=0.5527475832375369, w0=-1.2864442667583318e-05, w1=-0.10354241903349427\n",
      "Regularized Logistic Regression(99/199): loss=0.5527264306094076, w0=-1.2937972019366769e-05, w1=-0.10385781672008912\n",
      "Regularized Logistic Regression(100/199): loss=0.5527060262033362, w0=-1.3010740091225372e-05, w1=-0.10416861896345782\n",
      "Regularized Logistic Regression(101/199): loss=0.5526863410951147, w0=-1.3082755010164558e-05, w1=-0.10447490704007409\n",
      "Regularized Logistic Regression(102/199): loss=0.5526673476300816, w0=-1.3154024812316965e-05, w1=-0.10477676020332717\n",
      "Regularized Logistic Regression(103/199): loss=0.552649019356697, w0=-1.3224557443999647e-05, w1=-0.10507425575595651\n",
      "Regularized Logistic Regression(104/199): loss=0.5526313309643761, w0=-1.3294360762760746e-05, w1=-0.10536746911921104\n",
      "Regularized Logistic Regression(105/199): loss=0.5526142582252503, w0=-1.3363442538415529e-05, w1=-0.10565647389890162\n",
      "Regularized Logistic Regression(106/199): loss=0.5525977779395639, w0=-1.3431810454071698e-05, w1=-0.10594134194850396\n",
      "Regularized Logistic Regression(107/199): loss=0.5525818678844266, w0=-1.3499472107143888e-05, w1=-0.10622214342946273\n",
      "Regularized Logistic Regression(108/199): loss=0.5525665067656805, w0=-1.3566435010357295e-05, w1=-0.10649894686883782\n",
      "Regularized Logistic Regression(109/199): loss=0.5525516741726485, w0=-1.3632706592740428e-05, w1=-0.10677181921442605\n",
      "Regularized Logistic Regression(110/199): loss=0.5525373505355564, w0=-1.3698294200606912e-05, w1=-0.10704082588748494\n",
      "Regularized Logistic Regression(111/199): loss=0.5525235170854402, w0=-1.3763205098526384e-05, w1=-0.1073060308331771\n",
      "Regularized Logistic Regression(112/199): loss=0.5525101558163604, w0=-1.3827446470284454e-05, w1=-0.10756749656884934\n",
      "Regularized Logistic Regression(113/199): loss=0.5524972494497602, w0=-1.3891025419831755e-05, w1=-0.10782528423025242\n",
      "Regularized Logistic Regression(114/199): loss=0.5524847814008266, w0=-1.3953948972222115e-05, w1=-0.10807945361580164\n",
      "Regularized Logistic Regression(115/199): loss=0.5524727357467114, w0=-1.401622407453989e-05, w1=-0.10833006322897627\n",
      "Regularized Logistic Regression(116/199): loss=0.5524610971964886, w0=-1.407785759681648e-05, w1=-0.10857717031894537\n",
      "Regularized Logistic Regression(117/199): loss=0.5524498510627365, w0=-1.4138856332936103e-05, w1=-0.10882083091950848\n",
      "Regularized Logistic Regression(118/199): loss=0.5524389832346323, w0=-1.419922700153088e-05, w1=-0.10906109988643026\n",
      "Regularized Logistic Regression(119/199): loss=0.5524284801524662, w0=-1.4258976246865292e-05, w1=-0.10929803093324732\n",
      "Regularized Logistic Regression(120/199): loss=0.5524183287834821, w0=-1.431811063971008e-05, w1=-0.1095316766656196\n",
      "Regularized Logistic Regression(121/199): loss=0.5524085165989601, w0=-1.4376636678205665e-05, w1=-0.10976208861429529\n",
      "Regularized Logistic Regression(122/199): loss=0.5523990315524661, w0=-1.4434560788715169e-05, w1=-0.10998931726675598\n",
      "Regularized Logistic Regression(123/199): loss=0.5523898620591963, w0=-1.4491889326667125e-05, w1=-0.11021341209760224\n",
      "Regularized Logistic Regression(124/199): loss=0.5523809969763466, w0=-1.4548628577387952e-05, w1=-0.11043442159774078\n",
      "Regularized Logistic Regression(125/199): loss=0.5523724255844509, w0=-1.4604784756924289e-05, w1=-0.11065239330242671\n",
      "Regularized Logistic Regression(126/199): loss=0.5523641375696264, w0=-1.4660364012855298e-05, w1=-0.11086737381821746\n",
      "Regularized Logistic Regression(127/199): loss=0.5523561230066779, w0=-1.4715372425095004e-05, w1=-0.11107940884888448\n",
      "Regularized Logistic Regression(128/199): loss=0.552348372343003, w0=-1.4769816006684783e-05, w1=-0.1112885432203342\n",
      "Regularized Logistic Regression(129/199): loss=0.5523408763832652, w0=-1.482370070457611e-05, w1=-0.11149482090458186\n",
      "Regularized Logistic Regression(130/199): loss=0.5523336262747777, w0=-1.4877032400403646e-05, w1=-0.11169828504282177\n",
      "Regularized Logistic Regression(131/199): loss=0.5523266134935706, w0=-1.4929816911248767e-05, w1=-0.1118989779676353\n",
      "Regularized Logistic Regression(132/199): loss=0.5523198298310961, w0=-1.4982059990393671e-05, w1=-0.11209694122437441\n",
      "Regularized Logistic Regression(133/199): loss=0.5523132673815402, w0=-1.5033767328066111e-05, w1=-0.1122922155917606\n",
      "Regularized Logistic Regression(134/199): loss=0.5523069185297071, w0=-1.5084944552174909e-05, w1=-0.11248484110173006\n",
      "Regularized Logistic Regression(135/199): loss=0.5523007759394493, w0=-1.5135597229036335e-05, w1=-0.11267485705856271\n",
      "Regularized Logistic Regression(136/199): loss=0.5522948325426082, w0=-1.5185730864091449e-05, w1=-0.11286230205732464\n",
      "Regularized Logistic Regression(137/199): loss=0.5522890815284467, w0=-1.5235350902614524e-05, w1=-0.11304721400165464\n",
      "Regularized Logistic Regression(138/199): loss=0.5522835163335429, w0=-1.5284462730412653e-05, w1=-0.11322963012092328\n",
      "Regularized Logistic Regression(139/199): loss=0.5522781306321224, w0=-1.533307167451664e-05, w1=-0.11340958698679383\n",
      "Regularized Logistic Regression(140/199): loss=0.5522729183268099, w0=-1.538118300386328e-05, w1=-0.11358712052920784\n",
      "Regularized Logistic Regression(141/199): loss=0.5522678735397761, w0=-1.542880192996915e-05, w1=-0.11376226605182202\n",
      "Regularized Logistic Regression(142/199): loss=0.5522629906042631, w0=-1.5475933607595983e-05, w1=-0.11393505824692167\n",
      "Regularized Logistic Regression(143/199): loss=0.5522582640564678, w0=-1.552258313540775e-05, w1=-0.11410553120982982\n",
      "Regularized Logistic Regression(144/199): loss=0.5522536886277657, w0=-1.5568755556619576e-05, w1=-0.11427371845283467\n",
      "Regularized Logistic Regression(145/199): loss=0.5522492592372632, w0=-1.5614455859638527e-05, w1=-0.11443965291865701\n",
      "Regularized Logistic Regression(146/199): loss=0.5522449709846565, w0=-1.565968897869645e-05, w1=-0.11460336699347602\n",
      "Regularized Logistic Regression(147/199): loss=0.5522408191433881, w0=-1.5704459794474895e-05, w1=-0.1147648925195311\n",
      "Regularized Logistic Regression(148/199): loss=0.5522367991540822, w0=-1.5748773134722274e-05, w1=-0.11492426080731864\n",
      "Regularized Logistic Regression(149/199): loss=0.5522329066182523, w0=-1.5792633774863333e-05, w1=-0.11508150264740018\n",
      "Regularized Logistic Regression(150/199): loss=0.5522291372922622, w0=-1.583604643860103e-05, w1=-0.11523664832183778\n",
      "Regularized Logistic Regression(151/199): loss=0.5522254870815327, w0=-1.5879015798510932e-05, w1=-0.11538972761527105\n",
      "Regularized Logistic Regression(152/199): loss=0.5522219520349826, w0=-1.592154647662822e-05, w1=-0.11554076982565274\n",
      "Regularized Logistic Regression(153/199): loss=0.5522185283396916, w0=-1.596364304502741e-05, w1=-0.11568980377465338\n",
      "Regularized Logistic Regression(154/199): loss=0.5522152123157772, w0=-1.6005310026394856e-05, w1=-0.11583685781775115\n",
      "Regularized Logistic Regression(155/199): loss=0.5522120004114756, w0=-1.6046551894594168e-05, w1=-0.11598195985401827\n",
      "Regularized Logistic Regression(156/199): loss=0.5522088891984177, w0=-1.6087373075224607e-05, w1=-0.11612513733561568\n",
      "Regularized Logistic Regression(157/199): loss=0.5522058753670912, w0=-1.6127777946172572e-05, w1=-0.11626641727700811\n",
      "Regularized Logistic Regression(158/199): loss=0.5522029557224815, w0=-1.616777083815625e-05, w1=-0.11640582626391027\n",
      "Regularized Logistic Regression(159/199): loss=0.5522001271798815, w0=-1.6207356035263543e-05, w1=-0.11654339046197433\n",
      "Regularized Logistic Regression(160/199): loss=0.5521973867608699, w0=-1.6246537775483328e-05, w1=-0.1166791356252295\n",
      "Regularized Logistic Regression(161/199): loss=0.5521947315894394, w0=-1.6285320251230175e-05, w1=-0.11681308710428184\n",
      "Regularized Logistic Regression(162/199): loss=0.5521921588882797, w0=-1.6323707609862594e-05, w1=-0.11694526985428494\n",
      "Regularized Logistic Regression(163/199): loss=0.5521896659752027, w0=-1.6361703954194874e-05, w1=-0.11707570844268908\n",
      "Regularized Logistic Regression(164/199): loss=0.5521872502597039, w0=-1.6399313343002658e-05, w1=-0.11720442705677693\n",
      "Regularized Logistic Regression(165/199): loss=0.5521849092396586, w0=-1.6436539791522258e-05, w1=-0.1173314495109964\n",
      "Regularized Logistic Regression(166/199): loss=0.5521826404981423, w0=-1.6473387271943877e-05, w1=-0.11745679925409278\n",
      "Regularized Logistic Regression(167/199): loss=0.5521804417003718, w0=-1.6509859713898745e-05, w1=-0.11758049937605439\n",
      "Regularized Logistic Regression(168/199): loss=0.552178310590766, w0=-1.654596100494031e-05, w1=-0.11770257261487177\n",
      "Regularized Logistic Regression(169/199): loss=0.5521762449901145, w0=-1.6581694991019523e-05, w1=-0.11782304136312295\n",
      "Regularized Logistic Regression(170/199): loss=0.552174242792855, w0=-1.661706547695431e-05, w1=-0.11794192767438663\n",
      "Regularized Logistic Regression(171/199): loss=0.552172301964453, w0=-1.6652076226893317e-05, w1=-0.11805925326949206\n",
      "Regularized Logistic Regression(172/199): loss=0.5521704205388809, w0=-1.668673096477399e-05, w1=-0.11817503954261036\n",
      "Regularized Logistic Regression(173/199): loss=0.5521685966161898, w0=-1.672103337477507e-05, w1=-0.11828930756719273\n",
      "Regularized Logistic Regression(174/199): loss=0.5521668283601742, w0=-1.6754987101763588e-05, w1=-0.11840207810176173\n",
      "Regularized Logistic Regression(175/199): loss=0.5521651139961219, w0=-1.678859575173642e-05, w1=-0.11851337159555947\n",
      "Regularized Logistic Regression(176/199): loss=0.5521634518086495, w0=-1.6821862892256482e-05, w1=-0.11862320819405832\n",
      "Regularized Logistic Regression(177/199): loss=0.5521618401396172, w0=-1.6854792052883616e-05, w1=-0.1187316077443391\n",
      "Regularized Logistic Regression(178/199): loss=0.5521602773861208, w0=-1.688738672560029e-05, w1=-0.11883858980034054\n",
      "Regularized Logistic Regression(179/199): loss=0.5521587619985585, w0=-1.691965036523212e-05, w1=-0.118944173627984\n",
      "Regularized Logistic Regression(180/199): loss=0.5521572924787681, w0=-1.695158638986332e-05, w1=-0.11904837821017954\n",
      "Regularized Logistic Regression(181/199): loss=0.5521558673782341, w0=-1.698319818124713e-05, w1=-0.11915122225171389\n",
      "Regularized Logistic Regression(182/199): loss=0.5521544852963587, w0=-1.7014489085211306e-05, w1=-0.11925272418402776\n",
      "Regularized Logistic Regression(183/199): loss=0.5521531448787991, w0=-1.7045462412058727e-05, w1=-0.11935290216988355\n",
      "Regularized Logistic Regression(184/199): loss=0.5521518448158607, w0=-1.707612143696317e-05, w1=-0.11945177410792746\n",
      "Regularized Logistic Regression(185/199): loss=0.5521505838409553, w0=-1.7106469400360353e-05, w1=-0.11954935763715044\n",
      "Regularized Logistic Regression(186/199): loss=0.5521493607291096, w0=-1.7136509508334262e-05, w1=-0.1196456701412499\n",
      "Regularized Logistic Regression(187/199): loss=0.5521481742955313, w0=-1.7166244932998882e-05, w1=-0.11974072875289493\n",
      "Regularized Logistic Regression(188/199): loss=0.5521470233942256, w0=-1.7195678812875322e-05, w1=-0.11983455035790004\n",
      "Regularized Logistic Regression(189/199): loss=0.5521459069166628, w0=-1.7224814253264474e-05, w1=-0.1199271515993084\n",
      "Regularized Logistic Regression(190/199): loss=0.5521448237904929, w0=-1.7253654326615184e-05, w1=-0.12001854888138744\n",
      "Regularized Logistic Regression(191/199): loss=0.5521437729783075, w0=-1.7282202072888067e-05, w1=-0.12010875837354022\n",
      "Regularized Logistic Regression(192/199): loss=0.5521427534764459, w0=-1.731046049991497e-05, w1=-0.12019779601413419\n",
      "Regularized Logistic Regression(193/199): loss=0.5521417643138435, w0=-1.7338432583754166e-05, w1=-0.12028567751425105\n",
      "Regularized Logistic Regression(194/199): loss=0.552140804550922, w0=-1.736612126904134e-05, w1=-0.12037241836135801\n",
      "Regularized Logistic Regression(195/199): loss=0.5521398732785188, w0=-1.7393529469336394e-05, w1=-0.1204580338229047\n",
      "Regularized Logistic Regression(196/199): loss=0.5521389696168563, w0=-1.7420660067466155e-05, w1=-0.12054253894984689\n",
      "Regularized Logistic Regression(197/199): loss=0.5521380927145446, w0=-1.744751591586303e-05, w1=-0.12062594858009844\n",
      "Regularized Logistic Regression(198/199): loss=0.552137241747623, w0=-1.7474099836899655e-05, w1=-0.12070827734191558\n",
      "Regularized Logistic Regression(199/199): loss=0.5521364159186335, w0=-1.7500414623219586e-05, w1=-0.12078953965721306\n",
      "Regularized Logistic Regression(0/299): loss=0.693147180559945, w0=-2.0882930292778685e-07, w1=-0.003658898216597753\n",
      "Regularized Logistic Regression(1/299): loss=0.6777742333139127, w0=-4.149288600935061e-07, w1=-0.007113958593096548\n",
      "Regularized Logistic Regression(2/299): loss=0.664317405013279, w0=-6.185042261823338e-07, w1=-0.010383028246313281\n",
      "Regularized Logistic Regression(3/299): loss=0.6525200807328003, w0=-8.197136169767935e-07, w1=-0.013482053631316214\n",
      "Regularized Logistic Regression(4/299): loss=0.6421609728339082, w0=-1.0186785037541348e-06, w1=-0.016425251453574055\n",
      "Regularized Logistic Regression(5/299): loss=0.633049312764378, w0=-1.2154921568242191e-06, w1=-0.01922529276773128\n",
      "Regularized Logistic Regression(6/299): loss=0.625020674237878, w0=-1.410226417394125e-06, w1=-0.02189348270574105\n",
      "Regularized Logistic Regression(7/299): loss=0.6179333451255659, w0=-1.6029369924152095e-06, w1=-0.02443992679417306\n",
      "Regularized Logistic Regression(8/299): loss=0.6116651764789414, w0=-1.7936675492723607e-06, w1=-0.026873680078337014\n",
      "Regularized Logistic Regression(9/299): loss=0.6061108489182427, w0=-1.9824528536886615e-06, w1=-0.02920287830341961\n",
      "Regularized Logistic Regression(10/299): loss=0.6011795057644772, w0=-2.1693211557921663e-06, w1=-0.0314348520112233\n",
      "Regularized Logistic Regression(11/299): loss=0.5967927087927455, w0=-2.3542959919102614e-06, w1=-0.03357622515644646\n",
      "Regularized Logistic Regression(12/299): loss=0.5928826772439377, w0=-2.537397536230927e-06, w1=-0.03563300009665769\n",
      "Regularized Logistic Regression(13/299): loss=0.5893907745700047, w0=-2.7186436080626062e-06, w1=-0.03761063079445074\n",
      "Regularized Logistic Regression(14/299): loss=0.5862662108108244, w0=-2.8980504170776973e-06, w1=-0.039514085925553256\n",
      "Regularized Logistic Regression(15/299): loss=0.5834649317431692, w0=-3.0756331101781525e-06, w1=-0.041347903390844196\n",
      "Regularized Logistic Regression(16/299): loss=0.5809486690704063, w0=-3.2514061688170914e-06, w1=-0.04311623752545021\n",
      "Regularized Logistic Regression(17/299): loss=0.5786841289189789, w0=-3.42538369405937e-06, w1=-0.044822900105258676\n",
      "Regularized Logistic Regression(18/299): loss=0.5766422987312941, w0=-3.5975796077316767e-06, w1=-0.04647139607931341\n",
      "Regularized Logistic Regression(19/299): loss=0.574797855253334, w0=-3.7680077911505493e-06, w1=-0.04806495480810373\n",
      "Regularized Logistic Regression(20/299): loss=0.5731286586818782, w0=-3.936682177670724e-06, w1=-0.049606557461977056\n",
      "Regularized Logistic Regression(21/299): loss=0.571615320149334, w0=-4.103616811300207e-06, w1=-0.051098961128490834\n",
      "Regularized Logistic Regression(22/299): loss=0.570240831586111, w0=-4.268825880592912e-06, w1=-0.052544720089787174\n",
      "Regularized Logistic Regression(23/299): loss=0.5689902486234303, w0=-4.432323734728441e-06, w1=-0.05394620465824972\n",
      "Regularized Logistic Regression(24/299): loss=0.5678504186018712, w0=-4.594124886946871e-06, w1=-0.055305617898333395\n",
      "Regularized Logistic Regression(25/299): loss=0.5668097469545121, w0=-4.754244009189885e-06, w1=-0.05662501051234887\n",
      "Regularized Logistic Regression(26/299): loss=0.5658579962608894, w0=-4.912695920805833e-06, w1=-0.05790629412634531\n",
      "Regularized Logistic Regression(27/299): loss=0.5649861131414311, w0=-5.069495573426978e-06, w1=-0.05915125317752716\n",
      "Regularized Logistic Regression(28/299): loss=0.5641860789023044, w0=-5.2246580335631505e-06, w1=-0.060361555575633795\n",
      "Regularized Logistic Regression(29/299): loss=0.5634507804667009, w0=-5.378198464032077e-06, w1=-0.061538762286379285\n",
      "Regularized Logistic Regression(30/299): loss=0.5627738986573186, w0=-5.530132105028767e-06, w1=-0.06268433596457637\n",
      "Regularized Logistic Regression(31/299): loss=0.562149811340927, w0=-5.680474255398652e-06, w1=-0.06379964874727558\n",
      "Regularized Logistic Regression(32/299): loss=0.5615735093221828, w0=-5.829240254502107e-06, w1=-0.06488598930259767\n",
      "Regularized Logistic Regression(33/299): loss=0.5610405231912345, w0=-5.976445464926639e-06, w1=-0.06594456921746678\n",
      "Regularized Logistic Regression(34/299): loss=0.560546859597443, w0=-6.122105256206355e-06, w1=-0.06697652879682096\n",
      "Regularized Logistic Regression(35/299): loss=0.5600889456476088, w0=-6.266234989637598e-06, w1=-0.06798294233776374\n",
      "Regularized Logistic Regression(36/299): loss=0.5596635803181291, w0=-6.408850004228558e-06, w1=-0.06896482293430214\n",
      "Regularized Logistic Regression(37/299): loss=0.5592678919320634, w0=-6.549965603784305e-06, w1=-0.06992312686157705\n",
      "Regularized Logistic Regression(38/299): loss=0.5588993008888978, w0=-6.689597045103335e-06, w1=-0.07085875758266642\n",
      "Regularized Logistic Regression(39/299): loss=0.558555486950791, w0=-6.827759527244554e-06, w1=-0.07177256941600392\n",
      "Regularized Logistic Regression(40/299): loss=0.5582343604875635, w0=-6.964468181812553e-06, w1=-0.07266537089707208\n",
      "Regularized Logistic Regression(41/299): loss=0.5579340371664094, w0=-7.099738064202445e-06, w1=-0.0735379278642111\n",
      "Regularized Logistic Regression(42/299): loss=0.5576528156436151, w0=-7.23358414574218e-06, w1=-0.07439096629505967\n",
      "Regularized Logistic Regression(43/299): loss=0.5573891578763449, w0=-7.366021306669274e-06, w1=-0.07522517491722527\n",
      "Regularized Logistic Regression(44/299): loss=0.5571416717244991, w0=-7.4970643298795745e-06, w1=-0.07604120761423193\n",
      "Regularized Logistic Regression(45/299): loss=0.5569090955570468, w0=-7.626727895387453e-06, w1=-0.07683968564554366\n",
      "Regularized Logistic Regression(46/299): loss=0.5566902846153126, w0=-7.75502657543944e-06, w1=-0.07762119969748896\n",
      "Regularized Logistic Regression(47/299): loss=0.5564841989183392, w0=-7.881974830226342e-06, w1=-0.07838631178016765\n",
      "Regularized Logistic Regression(48/299): loss=0.5562898925235331, w0=-8.007587004142203e-06, w1=-0.07913555698387797\n",
      "Regularized Logistic Regression(49/299): loss=0.5561065039799519, w0=-8.131877322541896e-06, w1=-0.07986944510724121\n",
      "Regularized Logistic Regression(50/299): loss=0.555933247832437, w0=-8.254859888952592e-06, w1=-0.08058846216798622\n",
      "Regularized Logistic Regression(51/299): loss=0.5557694070527828, w0=-8.376548682697701e-06, w1=-0.08129307180628824\n",
      "Regularized Logistic Regression(52/299): loss=0.5556143262896946, w0=-8.49695755689516e-06, w1=-0.08198371658959029\n",
      "Regularized Logistic Regression(53/299): loss=0.5554674058427641, w0=-8.616100236794976e-06, w1=-0.08266081922699031\n",
      "Regularized Logistic Regression(54/299): loss=0.5553280962773831, w0=-8.733990318423962e-06, w1=-0.08332478370051215\n",
      "Regularized Logistic Regression(55/299): loss=0.5551958936076695, w0=-8.850641267508192e-06, w1=-0.08397599631989788\n",
      "Regularized Logistic Regression(56/299): loss=0.5550703349833173, w0=-8.966066418646408e-06, w1=-0.08461482670695254\n",
      "Regularized Logistic Regression(57/299): loss=0.554950994823972, w0=-9.080278974709823e-06, w1=-0.0852416287149195\n",
      "Regularized Logistic Regression(58/299): loss=0.5548374813514553, w0=-9.193292006446058e-06, w1=-0.08585674128788183\n",
      "Regularized Logistic Regression(59/299): loss=0.5547294334760138, w0=-9.305118452266882e-06, w1=-0.08646048926473905\n",
      "Regularized Logistic Regression(60/299): loss=0.5546265179979099, w0=-9.415771118201298e-06, w1=-0.08705318413191093\n",
      "Regularized Logistic Regression(61/299): loss=0.554528427090149, w0=-9.525262677997188e-06, w1=-0.08763512472856533\n",
      "Regularized Logistic Regression(62/299): loss=0.5544348760320892, w0=-9.633605673356283e-06, w1=-0.08820659790784316\n",
      "Regularized Logistic Regression(63/299): loss=0.554345601167125, w0=-9.740812514288598e-06, w1=-0.08876787915725605\n",
      "Regularized Logistic Regression(64/299): loss=0.5542603580606809, w0=-9.846895479573805e-06, w1=-0.08931923318117854\n",
      "Regularized Logistic Regression(65/299): loss=0.5541789198374264, w0=-9.951866717318116e-06, w1=-0.08986091444810704\n",
      "Regularized Logistic Regression(66/299): loss=0.5541010756789686, w0=-1.0055738245596363e-05, w1=-0.09039316770514555\n",
      "Regularized Logistic Regression(67/299): loss=0.5540266294653717, w0=-1.0158521953169918e-05, w1=-0.09091622846198111\n",
      "Regularized Logistic Regression(68/299): loss=0.5539553985456671, w0=-1.0260229600271927e-05, w1=-0.09143032344643157\n",
      "Regularized Logistic Regression(69/299): loss=0.5538872126241551, w0=-1.0360872819452195e-05, w1=-0.09193567103348486\n",
      "Regularized Logistic Regression(70/299): loss=0.5538219127507231, w0=-1.0460463116474726e-05, w1=-0.09243248164960147\n",
      "Regularized Logistic Regression(71/299): loss=0.5537593504046724, w0=-1.0559011871261617e-05, w1=-0.09292095815391521\n",
      "Regularized Logistic Regression(72/299): loss=0.5536993866626804, w0=-1.0656530338877563e-05, w1=-0.0934012961978437\n",
      "Regularized Logistic Regression(73/299): loss=0.5536418914425104, w0=-1.0753029650549809e-05, w1=-0.09387368456450826\n",
      "Regularized Logistic Regression(74/299): loss=0.5535867428149762, w0=-1.0848520814718834e-05, w1=-0.09433830548925713\n",
      "Regularized Logistic Regression(75/299): loss=0.5535338263774532, w0=-1.0943014718115539e-05, w1=-0.09479533496249362\n",
      "Regularized Logistic Regression(76/299): loss=0.5534830346829233, w0=-1.1036522126861073e-05, w1=-0.09524494301592071\n",
      "Regularized Logistic Regression(77/299): loss=0.5534342667191747, w0=-1.1129053687585834e-05, w1=-0.09568729399323703\n",
      "Regularized Logistic Regression(78/299): loss=0.5533874274333193, w0=-1.1220619928564472e-05, w1=-0.09612254680624456\n",
      "Regularized Logistic Regression(79/299): loss=0.5533424272972954, w0=-1.1311231260864068e-05, w1=-0.09655085517726095\n",
      "Regularized Logistic Regression(80/299): loss=0.5532991819104656, w0=-1.14008979795029e-05, w1=-0.09697236786866567\n",
      "Regularized Logistic Regression(81/299): loss=0.5532576116358032, w0=-1.1489630264617466e-05, w1=-0.09738722890035761\n",
      "Regularized Logistic Regression(82/299): loss=0.5532176412665256, w0=-1.1577438182635685e-05, w1=-0.09779557775584027\n",
      "Regularized Logistic Regression(83/299): loss=0.5531791997203397, w0=-1.1664331687454324e-05, w1=-0.09819754957761301\n",
      "Regularized Logistic Regression(84/299): loss=0.5531422197587516, w0=-1.1750320621619002e-05, w1=-0.09859327535249021\n",
      "Regularized Logistic Regression(85/299): loss=0.5531066377291395, w0=-1.183541471750518e-05, w1=-0.09898288208743872\n",
      "Regularized Logistic Regression(86/299): loss=0.5530723933275199, w0=-1.1919623598498752e-05, w1=-0.09936649297648012\n",
      "Regularized Logistic Regression(87/299): loss=0.5530394293801384, w0=-1.2002956780174998e-05, w1=-0.09974422755916916\n",
      "Regularized Logistic Regression(88/299): loss=0.5530076916421962, w0=-1.2085423671474763e-05, w1=-0.10011620187112998\n",
      "Regularized Logistic Regression(89/299): loss=0.5529771286121873, w0=-1.2167033575876843e-05, w1=-0.10048252858709597\n",
      "Regularized Logistic Regression(90/299): loss=0.5529476913604733, w0=-1.2247795692565665e-05, w1=-0.1008433171568771\n",
      "Regularized Logistic Regression(91/299): loss=0.5529193333708433, w0=-1.232771911759347e-05, w1=-0.10119867393464571\n",
      "Regularized Logistic Regression(92/299): loss=0.5528920103939372, w0=-1.2406812845036239e-05, w1=-0.10154870230191336\n",
      "Regularized Logistic Regression(93/299): loss=0.5528656803115068, w0=-1.2485085768142737e-05, w1=-0.10189350278454301\n",
      "Regularized Logistic Regression(94/299): loss=0.5528403030105933, w0=-1.2562546680476076e-05, w1=-0.10223317316412352\n",
      "Regularized Logistic Regression(95/299): loss=0.5528158402667788, w0=-1.2639204277047301e-05, w1=-0.10256780858401386\n",
      "Regularized Logistic Regression(96/299): loss=0.5527922556357551, w0=-1.2715067155440527e-05, w1=-0.10289750165034144\n",
      "Regularized Logistic Regression(97/299): loss=0.552769514352514, w0=-1.2790143816929224e-05, w1=-0.1032223425282277\n",
      "Regularized Logistic Regression(98/299): loss=0.5527475832375369, w0=-1.2864442667583318e-05, w1=-0.10354241903349427\n",
      "Regularized Logistic Regression(99/299): loss=0.5527264306094076, w0=-1.2937972019366769e-05, w1=-0.10385781672008912\n",
      "Regularized Logistic Regression(100/299): loss=0.5527060262033362, w0=-1.3010740091225372e-05, w1=-0.10416861896345782\n",
      "Regularized Logistic Regression(101/299): loss=0.5526863410951147, w0=-1.3082755010164558e-05, w1=-0.10447490704007409\n",
      "Regularized Logistic Regression(102/299): loss=0.5526673476300816, w0=-1.3154024812316965e-05, w1=-0.10477676020332717\n",
      "Regularized Logistic Regression(103/299): loss=0.552649019356697, w0=-1.3224557443999647e-05, w1=-0.10507425575595651\n",
      "Regularized Logistic Regression(104/299): loss=0.5526313309643761, w0=-1.3294360762760746e-05, w1=-0.10536746911921104\n",
      "Regularized Logistic Regression(105/299): loss=0.5526142582252503, w0=-1.3363442538415529e-05, w1=-0.10565647389890162\n",
      "Regularized Logistic Regression(106/299): loss=0.5525977779395639, w0=-1.3431810454071698e-05, w1=-0.10594134194850396\n",
      "Regularized Logistic Regression(107/299): loss=0.5525818678844266, w0=-1.3499472107143888e-05, w1=-0.10622214342946273\n",
      "Regularized Logistic Regression(108/299): loss=0.5525665067656805, w0=-1.3566435010357295e-05, w1=-0.10649894686883782\n",
      "Regularized Logistic Regression(109/299): loss=0.5525516741726485, w0=-1.3632706592740428e-05, w1=-0.10677181921442605\n",
      "Regularized Logistic Regression(110/299): loss=0.5525373505355564, w0=-1.3698294200606912e-05, w1=-0.10704082588748494\n",
      "Regularized Logistic Regression(111/299): loss=0.5525235170854402, w0=-1.3763205098526384e-05, w1=-0.1073060308331771\n",
      "Regularized Logistic Regression(112/299): loss=0.5525101558163604, w0=-1.3827446470284454e-05, w1=-0.10756749656884934\n",
      "Regularized Logistic Regression(113/299): loss=0.5524972494497602, w0=-1.3891025419831755e-05, w1=-0.10782528423025242\n",
      "Regularized Logistic Regression(114/299): loss=0.5524847814008266, w0=-1.3953948972222115e-05, w1=-0.10807945361580164\n",
      "Regularized Logistic Regression(115/299): loss=0.5524727357467114, w0=-1.401622407453989e-05, w1=-0.10833006322897627\n",
      "Regularized Logistic Regression(116/299): loss=0.5524610971964886, w0=-1.407785759681648e-05, w1=-0.10857717031894537\n",
      "Regularized Logistic Regression(117/299): loss=0.5524498510627365, w0=-1.4138856332936103e-05, w1=-0.10882083091950848\n",
      "Regularized Logistic Regression(118/299): loss=0.5524389832346323, w0=-1.419922700153088e-05, w1=-0.10906109988643026\n",
      "Regularized Logistic Regression(119/299): loss=0.5524284801524662, w0=-1.4258976246865292e-05, w1=-0.10929803093324732\n",
      "Regularized Logistic Regression(120/299): loss=0.5524183287834821, w0=-1.431811063971008e-05, w1=-0.1095316766656196\n",
      "Regularized Logistic Regression(121/299): loss=0.5524085165989601, w0=-1.4376636678205665e-05, w1=-0.10976208861429529\n",
      "Regularized Logistic Regression(122/299): loss=0.5523990315524661, w0=-1.4434560788715169e-05, w1=-0.10998931726675598\n",
      "Regularized Logistic Regression(123/299): loss=0.5523898620591963, w0=-1.4491889326667125e-05, w1=-0.11021341209760224\n",
      "Regularized Logistic Regression(124/299): loss=0.5523809969763466, w0=-1.4548628577387952e-05, w1=-0.11043442159774078\n",
      "Regularized Logistic Regression(125/299): loss=0.5523724255844509, w0=-1.4604784756924289e-05, w1=-0.11065239330242671\n",
      "Regularized Logistic Regression(126/299): loss=0.5523641375696264, w0=-1.4660364012855298e-05, w1=-0.11086737381821746\n",
      "Regularized Logistic Regression(127/299): loss=0.5523561230066779, w0=-1.4715372425095004e-05, w1=-0.11107940884888448\n",
      "Regularized Logistic Regression(128/299): loss=0.552348372343003, w0=-1.4769816006684783e-05, w1=-0.1112885432203342\n",
      "Regularized Logistic Regression(129/299): loss=0.5523408763832652, w0=-1.482370070457611e-05, w1=-0.11149482090458186\n",
      "Regularized Logistic Regression(130/299): loss=0.5523336262747777, w0=-1.4877032400403646e-05, w1=-0.11169828504282177\n",
      "Regularized Logistic Regression(131/299): loss=0.5523266134935706, w0=-1.4929816911248767e-05, w1=-0.1118989779676353\n",
      "Regularized Logistic Regression(132/299): loss=0.5523198298310961, w0=-1.4982059990393671e-05, w1=-0.11209694122437441\n",
      "Regularized Logistic Regression(133/299): loss=0.5523132673815402, w0=-1.5033767328066111e-05, w1=-0.1122922155917606\n",
      "Regularized Logistic Regression(134/299): loss=0.5523069185297071, w0=-1.5084944552174909e-05, w1=-0.11248484110173006\n",
      "Regularized Logistic Regression(135/299): loss=0.5523007759394493, w0=-1.5135597229036335e-05, w1=-0.11267485705856271\n",
      "Regularized Logistic Regression(136/299): loss=0.5522948325426082, w0=-1.5185730864091449e-05, w1=-0.11286230205732464\n",
      "Regularized Logistic Regression(137/299): loss=0.5522890815284467, w0=-1.5235350902614524e-05, w1=-0.11304721400165464\n",
      "Regularized Logistic Regression(138/299): loss=0.5522835163335429, w0=-1.5284462730412653e-05, w1=-0.11322963012092328\n",
      "Regularized Logistic Regression(139/299): loss=0.5522781306321224, w0=-1.533307167451664e-05, w1=-0.11340958698679383\n",
      "Regularized Logistic Regression(140/299): loss=0.5522729183268099, w0=-1.538118300386328e-05, w1=-0.11358712052920784\n",
      "Regularized Logistic Regression(141/299): loss=0.5522678735397761, w0=-1.542880192996915e-05, w1=-0.11376226605182202\n",
      "Regularized Logistic Regression(142/299): loss=0.5522629906042631, w0=-1.5475933607595983e-05, w1=-0.11393505824692167\n",
      "Regularized Logistic Regression(143/299): loss=0.5522582640564678, w0=-1.552258313540775e-05, w1=-0.11410553120982982\n",
      "Regularized Logistic Regression(144/299): loss=0.5522536886277657, w0=-1.5568755556619576e-05, w1=-0.11427371845283467\n",
      "Regularized Logistic Regression(145/299): loss=0.5522492592372632, w0=-1.5614455859638527e-05, w1=-0.11443965291865701\n",
      "Regularized Logistic Regression(146/299): loss=0.5522449709846565, w0=-1.565968897869645e-05, w1=-0.11460336699347602\n",
      "Regularized Logistic Regression(147/299): loss=0.5522408191433881, w0=-1.5704459794474895e-05, w1=-0.1147648925195311\n",
      "Regularized Logistic Regression(148/299): loss=0.5522367991540822, w0=-1.5748773134722274e-05, w1=-0.11492426080731864\n",
      "Regularized Logistic Regression(149/299): loss=0.5522329066182523, w0=-1.5792633774863333e-05, w1=-0.11508150264740018\n",
      "Regularized Logistic Regression(150/299): loss=0.5522291372922622, w0=-1.583604643860103e-05, w1=-0.11523664832183778\n",
      "Regularized Logistic Regression(151/299): loss=0.5522254870815327, w0=-1.5879015798510932e-05, w1=-0.11538972761527105\n",
      "Regularized Logistic Regression(152/299): loss=0.5522219520349826, w0=-1.592154647662822e-05, w1=-0.11554076982565274\n",
      "Regularized Logistic Regression(153/299): loss=0.5522185283396916, w0=-1.596364304502741e-05, w1=-0.11568980377465338\n",
      "Regularized Logistic Regression(154/299): loss=0.5522152123157772, w0=-1.6005310026394856e-05, w1=-0.11583685781775115\n",
      "Regularized Logistic Regression(155/299): loss=0.5522120004114756, w0=-1.6046551894594168e-05, w1=-0.11598195985401827\n",
      "Regularized Logistic Regression(156/299): loss=0.5522088891984177, w0=-1.6087373075224607e-05, w1=-0.11612513733561568\n",
      "Regularized Logistic Regression(157/299): loss=0.5522058753670912, w0=-1.6127777946172572e-05, w1=-0.11626641727700811\n",
      "Regularized Logistic Regression(158/299): loss=0.5522029557224815, w0=-1.616777083815625e-05, w1=-0.11640582626391027\n",
      "Regularized Logistic Regression(159/299): loss=0.5522001271798815, w0=-1.6207356035263543e-05, w1=-0.11654339046197433\n",
      "Regularized Logistic Regression(160/299): loss=0.5521973867608699, w0=-1.6246537775483328e-05, w1=-0.1166791356252295\n",
      "Regularized Logistic Regression(161/299): loss=0.5521947315894394, w0=-1.6285320251230175e-05, w1=-0.11681308710428184\n",
      "Regularized Logistic Regression(162/299): loss=0.5521921588882797, w0=-1.6323707609862594e-05, w1=-0.11694526985428494\n",
      "Regularized Logistic Regression(163/299): loss=0.5521896659752027, w0=-1.6361703954194874e-05, w1=-0.11707570844268908\n",
      "Regularized Logistic Regression(164/299): loss=0.5521872502597039, w0=-1.6399313343002658e-05, w1=-0.11720442705677693\n",
      "Regularized Logistic Regression(165/299): loss=0.5521849092396586, w0=-1.6436539791522258e-05, w1=-0.1173314495109964\n",
      "Regularized Logistic Regression(166/299): loss=0.5521826404981423, w0=-1.6473387271943877e-05, w1=-0.11745679925409278\n",
      "Regularized Logistic Regression(167/299): loss=0.5521804417003718, w0=-1.6509859713898745e-05, w1=-0.11758049937605439\n",
      "Regularized Logistic Regression(168/299): loss=0.552178310590766, w0=-1.654596100494031e-05, w1=-0.11770257261487177\n",
      "Regularized Logistic Regression(169/299): loss=0.5521762449901145, w0=-1.6581694991019523e-05, w1=-0.11782304136312295\n",
      "Regularized Logistic Regression(170/299): loss=0.552174242792855, w0=-1.661706547695431e-05, w1=-0.11794192767438663\n",
      "Regularized Logistic Regression(171/299): loss=0.552172301964453, w0=-1.6652076226893317e-05, w1=-0.11805925326949206\n",
      "Regularized Logistic Regression(172/299): loss=0.5521704205388809, w0=-1.668673096477399e-05, w1=-0.11817503954261036\n",
      "Regularized Logistic Regression(173/299): loss=0.5521685966161898, w0=-1.672103337477507e-05, w1=-0.11828930756719273\n",
      "Regularized Logistic Regression(174/299): loss=0.5521668283601742, w0=-1.6754987101763588e-05, w1=-0.11840207810176173\n",
      "Regularized Logistic Regression(175/299): loss=0.5521651139961219, w0=-1.678859575173642e-05, w1=-0.11851337159555947\n",
      "Regularized Logistic Regression(176/299): loss=0.5521634518086495, w0=-1.6821862892256482e-05, w1=-0.11862320819405832\n",
      "Regularized Logistic Regression(177/299): loss=0.5521618401396172, w0=-1.6854792052883616e-05, w1=-0.1187316077443391\n",
      "Regularized Logistic Regression(178/299): loss=0.5521602773861208, w0=-1.688738672560029e-05, w1=-0.11883858980034054\n",
      "Regularized Logistic Regression(179/299): loss=0.5521587619985585, w0=-1.691965036523212e-05, w1=-0.118944173627984\n",
      "Regularized Logistic Regression(180/299): loss=0.5521572924787681, w0=-1.695158638986332e-05, w1=-0.11904837821017954\n",
      "Regularized Logistic Regression(181/299): loss=0.5521558673782341, w0=-1.698319818124713e-05, w1=-0.11915122225171389\n",
      "Regularized Logistic Regression(182/299): loss=0.5521544852963587, w0=-1.7014489085211306e-05, w1=-0.11925272418402776\n",
      "Regularized Logistic Regression(183/299): loss=0.5521531448787991, w0=-1.7045462412058727e-05, w1=-0.11935290216988355\n",
      "Regularized Logistic Regression(184/299): loss=0.5521518448158607, w0=-1.707612143696317e-05, w1=-0.11945177410792746\n",
      "Regularized Logistic Regression(185/299): loss=0.5521505838409553, w0=-1.7106469400360353e-05, w1=-0.11954935763715044\n",
      "Regularized Logistic Regression(186/299): loss=0.5521493607291096, w0=-1.7136509508334262e-05, w1=-0.1196456701412499\n",
      "Regularized Logistic Regression(187/299): loss=0.5521481742955313, w0=-1.7166244932998882e-05, w1=-0.11974072875289493\n",
      "Regularized Logistic Regression(188/299): loss=0.5521470233942256, w0=-1.7195678812875322e-05, w1=-0.11983455035790004\n",
      "Regularized Logistic Regression(189/299): loss=0.5521459069166628, w0=-1.7224814253264474e-05, w1=-0.1199271515993084\n",
      "Regularized Logistic Regression(190/299): loss=0.5521448237904929, w0=-1.7253654326615184e-05, w1=-0.12001854888138744\n",
      "Regularized Logistic Regression(191/299): loss=0.5521437729783075, w0=-1.7282202072888067e-05, w1=-0.12010875837354022\n",
      "Regularized Logistic Regression(192/299): loss=0.5521427534764459, w0=-1.731046049991497e-05, w1=-0.12019779601413419\n",
      "Regularized Logistic Regression(193/299): loss=0.5521417643138435, w0=-1.7338432583754166e-05, w1=-0.12028567751425105\n",
      "Regularized Logistic Regression(194/299): loss=0.552140804550922, w0=-1.736612126904134e-05, w1=-0.12037241836135801\n",
      "Regularized Logistic Regression(195/299): loss=0.5521398732785188, w0=-1.7393529469336394e-05, w1=-0.1204580338229047\n",
      "Regularized Logistic Regression(196/299): loss=0.5521389696168563, w0=-1.7420660067466155e-05, w1=-0.12054253894984689\n",
      "Regularized Logistic Regression(197/299): loss=0.5521380927145446, w0=-1.744751591586303e-05, w1=-0.12062594858009844\n",
      "Regularized Logistic Regression(198/299): loss=0.552137241747623, w0=-1.7474099836899655e-05, w1=-0.12070827734191558\n",
      "Regularized Logistic Regression(199/299): loss=0.5521364159186335, w0=-1.7500414623219586e-05, w1=-0.12078953965721306\n",
      "Regularized Logistic Regression(200/299): loss=0.5521356144557276, w0=-1.7526463038064107e-05, w1=-0.12086974974481471\n",
      "Regularized Logistic Regression(201/299): loss=0.5521348366118053, w0=-1.7552247815595184e-05, w1=-0.12094892162364168\n",
      "Regularized Logistic Regression(202/299): loss=0.5521340816636838, w0=-1.757777166121462e-05, w1=-0.12102706911583841\n",
      "Regularized Logistic Regression(203/299): loss=0.5521333489112947, w0=-1.760303725187946e-05, w1=-0.12110420584983722\n",
      "Regularized Logistic Regression(204/299): loss=0.5521326376769117, w0=-1.7628047236413695e-05, w1=-0.12118034526336646\n",
      "Regularized Logistic Regression(205/299): loss=0.5521319473044035, w0=-1.7652804235816333e-05, w1=-0.12125550060639971\n",
      "Regularized Logistic Regression(206/299): loss=0.5521312771585128, w0=-1.767731084356583e-05, w1=-0.12132968494405097\n",
      "Regularized Logistic Regression(207/299): loss=0.5521306266241626, w0=-1.7701569625920995e-05, w1=-0.12140291115941418\n",
      "Regularized Logistic Regression(208/299): loss=0.5521299951057833, w0=-1.7725583122218366e-05, w1=-0.1214751919563512\n",
      "Regularized Logistic Regression(209/299): loss=0.5521293820266675, w0=-1.7749353845166115e-05, w1=-0.12154653986222744\n",
      "Regularized Logistic Regression(210/299): loss=0.5521287868283424, w0=-1.777288428113455e-05, w1=-0.1216169672305983\n",
      "Regularized Logistic Regression(211/299): loss=0.5521282089699691, w0=-1.779617689044321e-05, w1=-0.12168648624384555\n",
      "Regularized Logistic Regression(212/299): loss=0.5521276479277579, w0=-1.781923410764466e-05, w1=-0.12175510891576714\n",
      "Regularized Logistic Regression(213/299): loss=0.5521271031944082, w0=-1.784205834180494e-05, w1=-0.12182284709411936\n",
      "Regularized Logistic Regression(214/299): loss=0.552126574278563, w0=-1.7864651976780822e-05, w1=-0.12188971246311393\n",
      "Regularized Logistic Regression(215/299): loss=0.552126060704288, w0=-1.7887017371493804e-05, w1=-0.1219557165458712\n",
      "Regularized Logistic Regression(216/299): loss=0.5521255620105633, w0=-1.7909156860200983e-05, w1=-0.12202087070682907\n",
      "Regularized Logistic Regression(217/299): loss=0.5521250777507954, w0=-1.7931072752762763e-05, w1=-0.12208518615411047\n",
      "Regularized Logistic Regression(218/299): loss=0.5521246074923452, w0=-1.795276733490748e-05, w1=-0.12214867394184913\n",
      "Regularized Logistic Regression(219/299): loss=0.5521241508160727, w0=-1.7974242868492998e-05, w1=-0.12221134497247442\n",
      "Regularized Logistic Regression(220/299): loss=0.5521237073158961, w0=-1.7995501591765257e-05, w1=-0.12227320999895777\n",
      "Regularized Logistic Regression(221/299): loss=0.5521232765983676, w0=-1.8016545719613892e-05, w1=-0.1223342796270196\n",
      "Regularized Logistic Regression(222/299): loss=0.5521228582822623, w0=-1.8037377443824873e-05, w1=-0.12239456431729905\n",
      "Regularized Logistic Regression(223/299): loss=0.552122451998181, w0=-1.805799893333027e-05, w1=-0.12245407438748614\n",
      "Regularized Logistic Regression(224/299): loss=0.5521220573881682, w0=-1.807841233445515e-05, w1=-0.1225128200144189\n",
      "Regularized Logistic Regression(225/299): loss=0.5521216741053414, w0=-1.8098619771161647e-05, w1=-0.12257081123614416\n",
      "Regularized Logistic Regression(226/299): loss=0.5521213018135328, w0=-1.8118623345290232e-05, w1=-0.12262805795394417\n",
      "Regularized Logistic Regression(227/299): loss=0.5521209401869447, w0=-1.8138425136798244e-05, w1=-0.12268456993432965\n",
      "Regularized Logistic Regression(228/299): loss=0.552120588909815, w0=-1.8158027203995673e-05, w1=-0.12274035681099936\n",
      "Regularized Logistic Regression(229/299): loss=0.5521202476760952, w0=-1.8177431583778283e-05, w1=-0.12279542808676785\n",
      "Regularized Logistic Regression(230/299): loss=0.5521199161891377, w0=-1.819664029185806e-05, w1=-0.12284979313546023\n",
      "Regularized Logistic Regression(231/299): loss=0.5521195941613956, w0=-1.8215655322991036e-05, w1=-0.12290346120377771\n",
      "Regularized Logistic Regression(232/299): loss=0.5521192813141298, w0=-1.8234478651202544e-05, w1=-0.12295644141313118\n",
      "Regularized Logistic Regression(233/299): loss=0.5521189773771298, w0=-1.8253112230009892e-05, w1=-0.1230087427614454\n",
      "Regularized Logistic Regression(234/299): loss=0.5521186820884394, w0=-1.8271557992642528e-05, w1=-0.12306037412493483\n",
      "Regularized Logistic Regression(235/299): loss=0.5521183951940954, w0=-1.8289817852259695e-05, w1=-0.1231113442598493\n",
      "Regularized Logistic Regression(236/299): loss=0.5521181164478729, w0=-1.8307893702165636e-05, w1=-0.12316166180419312\n",
      "Regularized Logistic Regression(237/299): loss=0.5521178456110382, w0=-1.8325787416022354e-05, w1=-0.12321133527941584\n",
      "Regularized Logistic Regression(238/299): loss=0.5521175824521133, w0=-1.834350084805998e-05, w1=-0.12326037309207576\n",
      "Regularized Logistic Regression(239/299): loss=0.5521173267466449, w0=-1.836103583328475e-05, w1=-0.12330878353547843\n",
      "Regularized Logistic Regression(240/299): loss=0.5521170782769831, w0=-1.8378394187684656e-05, w1=-0.12335657479128785\n",
      "Regularized Logistic Regression(241/299): loss=0.5521168368320656, w0=-1.839557770843276e-05, w1=-0.12340375493111241\n",
      "Regularized Logistic Regression(242/299): loss=0.5521166022072099, w0=-1.8412588174088243e-05, w1=-0.12345033191806681\n",
      "Regularized Logistic Regression(243/299): loss=0.5521163742039142, w0=-1.842942734479517e-05, w1=-0.12349631360830896\n",
      "Regularized Logistic Regression(244/299): loss=0.5521161526296614, w0=-1.8446096962479034e-05, w1=-0.12354170775255223\n",
      "Regularized Logistic Regression(245/299): loss=0.5521159372977319, w0=-1.8462598751041095e-05, w1=-0.12358652199755515\n",
      "Regularized Logistic Regression(246/299): loss=0.5521157280270216, w0=-1.8478934416550542e-05, w1=-0.12363076388758787\n",
      "Regularized Logistic Regression(247/299): loss=0.5521155246418668, w0=-1.8495105647434494e-05, w1=-0.12367444086587541\n",
      "Regularized Logistic Regression(248/299): loss=0.5521153269718732, w0=-1.851111411466587e-05, w1=-0.12371756027601895\n",
      "Regularized Logistic Regression(249/299): loss=0.5521151348517523, w0=-1.852696147194918e-05, w1=-0.12376012936339535\n",
      "Regularized Logistic Regression(250/299): loss=0.5521149481211621, w0=-1.8542649355904222e-05, w1=-0.12380215527653493\n",
      "Regularized Logistic Regression(251/299): loss=0.5521147666245526, w0=-1.8558179386247726e-05, w1=-0.12384364506847878\n",
      "Regularized Logistic Regression(252/299): loss=0.5521145902110176, w0=-1.8573553165973e-05, w1=-0.12388460569811437\n",
      "Regularized Logistic Regression(253/299): loss=0.55211441873415, w0=-1.8588772281527528e-05, w1=-0.12392504403149214\n",
      "Regularized Logistic Regression(254/299): loss=0.552114252051903, w0=-1.8603838302988642e-05, w1=-0.12396496684312094\n",
      "Regularized Logistic Regression(255/299): loss=0.5521140900264546, w0=-1.8618752784237198e-05, w1=-0.12400438081724424\n",
      "Regularized Logistic Regression(256/299): loss=0.5521139325240774, w0=-1.8633517263129345e-05, w1=-0.1240432925490976\n",
      "Regularized Logistic Regression(257/299): loss=0.5521137794150119, w0=-1.8648133261666382e-05, w1=-0.12408170854614652\n",
      "Regularized Logistic Regression(258/299): loss=0.5521136305733447, w0=-1.8662602286162724e-05, w1=-0.12411963522930587\n",
      "Regularized Logistic Regression(259/299): loss=0.5521134858768894, w0=-1.867692582741202e-05, w1=-0.12415707893414188\n",
      "Regularized Logistic Regression(260/299): loss=0.5521133452070727, w0=-1.8691105360851416e-05, w1=-0.12419404591205407\n",
      "Regularized Logistic Regression(261/299): loss=0.5521132084488238, w0=-1.8705142346724002e-05, w1=-0.1242305423314422\n",
      "Regularized Logistic Regression(262/299): loss=0.5521130754904661, w0=-1.871903823023948e-05, w1=-0.12426657427885385\n",
      "Regularized Logistic Regression(263/299): loss=0.552112946223613, w0=-1.8732794441733037e-05, w1=-0.12430214776011587\n",
      "Regularized Logistic Regression(264/299): loss=0.5521128205430694, w0=-1.874641239682246e-05, w1=-0.12433726870144877\n",
      "Regularized Logistic Regression(265/299): loss=0.5521126983467319, w0=-1.8759893496563552e-05, w1=-0.12437194295056468\n",
      "Regularized Logistic Regression(266/299): loss=0.5521125795354962, w0=-1.8773239127603794e-05, w1=-0.12440617627774946\n",
      "Regularized Logistic Regression(267/299): loss=0.5521124640131647, w0=-1.8786450662334337e-05, w1=-0.124439974376928\n",
      "Regularized Logistic Regression(268/299): loss=0.5521123516863587, w0=-1.879952945904032e-05, w1=-0.12447334286671463\n",
      "Regularized Logistic Regression(269/299): loss=0.5521122424644327, w0=-1.8812476862049524e-05, w1=-0.124506287291448\n",
      "Regularized Logistic Regression(270/299): loss=0.552112136259391, w0=-1.882529420187941e-05, w1=-0.12453881312221052\n",
      "Regularized Logistic Regression(271/299): loss=0.5521120329858086, w0=-1.8837982795382527e-05, w1=-0.12457092575783298\n",
      "Regularized Logistic Regression(272/299): loss=0.5521119325607521, w0=-1.885054394589033e-05, w1=-0.12460263052588477\n",
      "Regularized Logistic Regression(273/299): loss=0.5521118349037051, w0=-1.8862978943355425e-05, w1=-0.12463393268364936\n",
      "Regularized Logistic Regression(274/299): loss=0.5521117399364949, w0=-1.887528906449225e-05, w1=-0.12466483741908557\n",
      "Regularized Logistic Regression(275/299): loss=0.5521116475832228, w0=-1.888747557291622e-05, w1=-0.12469534985177555\n",
      "Regularized Logistic Regression(276/299): loss=0.5521115577701945, w0=-1.8899539719281345e-05, w1=-0.12472547503385778\n",
      "Regularized Logistic Regression(277/299): loss=0.5521114704258541, w0=-1.8911482741416356e-05, w1=-0.12475521795094763\n",
      "Regularized Logistic Regression(278/299): loss=0.5521113854807207, w0=-1.892330586445932e-05, w1=-0.12478458352304414\n",
      "Regularized Logistic Regression(279/299): loss=0.552111302867325, w0=-1.8935010300990818e-05, w1=-0.1248135766054241\n",
      "Regularized Logistic Regression(280/299): loss=0.5521112225201495, w0=-1.8946597251165636e-05, w1=-0.12484220198952262\n",
      "Regularized Logistic Regression(281/299): loss=0.552111144375571, w0=-1.8958067902843054e-05, w1=-0.12487046440380176\n",
      "Regularized Logistic Regression(282/299): loss=0.552111068371802, w0=-1.8969423431715686e-05, w1=-0.12489836851460603\n",
      "Regularized Logistic Regression(283/299): loss=0.5521109944488384, w0=-1.898066500143694e-05, w1=-0.12492591892700557\n",
      "Regularized Logistic Regression(284/299): loss=0.5521109225484042, w0=-1.8991793763747082e-05, w1=-0.12495312018562849\n",
      "Regularized Logistic Regression(285/299): loss=0.5521108526139008, w0=-1.900281085859792e-05, w1=-0.12497997677547981\n",
      "Regularized Logistic Regression(286/299): loss=0.5521107845903579, w0=-1.9013717414276158e-05, w1=-0.12500649312274933\n",
      "Regularized Logistic Regression(287/299): loss=0.552110718424384, w0=-1.902451454752539e-05, w1=-0.1250326735956079\n",
      "Regularized Logistic Regression(288/299): loss=0.55211065406412, w0=-1.9035203363666777e-05, w1=-0.12505852250499283\n",
      "Regularized Logistic Regression(289/299): loss=0.5521105914591936, w0=-1.904578495671842e-05, w1=-0.12508404410538132\n",
      "Regularized Logistic Regression(290/299): loss=0.5521105305606763, w0=-1.9056260409513436e-05, w1=-0.1251092425955528\n",
      "Regularized Logistic Regression(291/299): loss=0.5521104713210385, w0=-1.906663079381675e-05, w1=-0.1251341221193421\n",
      "Regularized Logistic Regression(292/299): loss=0.5521104136941112, w0=-1.9076897170440632e-05, w1=-0.1251586867663798\n",
      "Regularized Logistic Regression(293/299): loss=0.5521103576350427, w0=-1.908706058935899e-05, w1=-0.1251829405728235\n",
      "Regularized Logistic Regression(294/299): loss=0.5521103031002627, w0=-1.90971220898204e-05, w1=-0.12520688752207795\n",
      "Regularized Logistic Regression(295/299): loss=0.5521102500474417, w0=-1.9107082700459967e-05, w1=-0.12523053154550567\n",
      "Regularized Logistic Regression(296/299): loss=0.5521101984354576, w0=-1.911694343940991e-05, w1=-0.12525387652312722\n",
      "Regularized Logistic Regression(297/299): loss=0.5521101482243577, w0=-1.912670531440902e-05, w1=-0.1252769262843118\n",
      "Regularized Logistic Regression(298/299): loss=0.5521100993753251, w0=-1.913636932291089e-05, w1=-0.12529968460845722\n",
      "Regularized Logistic Regression(299/299): loss=0.5521100518506465, w0=-1.914593645219102e-05, w1=-0.12532215522566229\n",
      "Regularized Logistic Regression(0/99): loss=0.693147180559945, w0=-2.0882930292778685e-07, w1=-0.003658898216597753\n",
      "Regularized Logistic Regression(1/99): loss=0.6784891515630495, w0=-3.961342228300053e-07, w1=-0.00678465775360275\n",
      "Regularized Logistic Regression(2/99): loss=0.6680075107837707, w0=-5.643031846318581e-07, w1=-0.009461358858280929\n",
      "Regularized Logistic Regression(3/99): loss=0.6604993716762532, w0=-7.154033413537734e-07, w1=-0.01175860559620912\n",
      "Regularized Logistic Regression(4/99): loss=0.6551120805505041, w0=-8.512399773915193e-07, w1=-0.013734185240072208\n",
      "Regularized Logistic Regression(5/99): loss=0.6512402094345634, w0=-9.734014242524225e-07, w1=-0.015436260305958866\n",
      "Regularized Logistic Regression(6/99): loss=0.6484530624535548, w0=-1.0832934134486223e-06, w1=-0.016905138715240988\n",
      "Regularized Logistic Regression(7/99): loss=0.6464436633095114, w0=-1.182165791675792e-06, w1=-0.018174692270827065\n",
      "Regularized Logistic Regression(8/99): loss=0.6449928115023803, w0=-1.2711337033217505e-06, w1=-0.019273492569536678\n",
      "Regularized Logistic Regression(9/99): loss=0.6439437073746082, w0=-1.3511947283307386e-06, w1=-0.020225723575334052\n",
      "Regularized Logistic Regression(10/99): loss=0.6431840000183142, w0=-1.42324301978774e-06, w1=-0.021051918612953524\n",
      "Regularized Logistic Regression(11/99): loss=0.6426330591202727, w0=-1.4880811743823323e-06, w1=-0.02176955921612948\n",
      "Regularized Logistic Regression(12/99): loss=0.642232931598934, w0=-1.546430353448919e-06, w1=-0.022393564813049646\n",
      "Regularized Logistic Regression(13/99): loss=0.6419419034077747, w0=-1.5989390239529494e-06, w1=-0.022936695613834587\n",
      "Regularized Logistic Regression(14/99): loss=0.6417299073780774, w0=-1.6461905867502848e-06, w1=-0.023409885991633423\n",
      "Regularized Logistic Regression(15/99): loss=0.6415752420036795, w0=-1.6887100890253016e-06, w1=-0.023822521794305027\n",
      "Regularized Logistic Regression(16/99): loss=0.6414622230652695, w0=-1.7269701689049833e-06, w1=-0.024182672100073552\n",
      "Regularized Logistic Regression(17/99): loss=0.6413795003014919, w0=-1.7613963459619677e-06, w1=-0.024497283707295245\n",
      "Regularized Logistic Regression(18/99): loss=0.6413188490443191, w0=-1.7923717469921945e-06, w1=-0.02477234494867808\n",
      "Regularized Logistic Regression(19/99): loss=0.641274301619076, w0=-1.8202413389288517e-06, w1=-0.025013024111463875\n",
      "Regularized Logistic Regression(20/99): loss=0.6412415221632937, w0=-1.8453157279043325e-06, w1=-0.02522378672911242\n",
      "Regularized Logistic Regression(21/99): loss=0.6412173560860205, w0=-1.867874573856507e-06, w1=-0.02540849521461615\n",
      "Regularized Logistic Regression(22/99): loss=0.6411995049893102, w0=-1.8881696627198177e-06, w1=-0.02557049367738958\n",
      "Regularized Logistic Regression(23/99): loss=0.6411862918355424, w0=-1.906427672483322e-06, w1=-0.025712680265342312\n",
      "Regularized Logistic Regression(24/99): loss=0.6411764911072533, w0=-1.92285266478488e-06, w1=-0.025837568971963435\n",
      "Regularized Logistic Regression(25/99): loss=0.6411692058270942, w0=-1.9376283299335955e-06, w1=-0.02594734252313674\n",
      "Regularized Logistic Regression(26/99): loss=0.6411637784025879, w0=-1.9509200100973793e-06, w1=-0.02604389769347473\n",
      "Regularized Logistic Regression(27/99): loss=0.6411597259137678, w0=-1.9628765227103824e-06, w1=-0.02612888418468885\n",
      "Regularized Logistic Regression(28/99): loss=0.6411566930837257, w0=-1.973631803841227e-06, w1=-0.026203738019284877\n",
      "Regularized Logistic Regression(29/99): loss=0.6411544180560348, w0=-1.983306389242811e-06, w1=-0.02626971025427836\n",
      "Regularized Logistic Regression(30/99): loss=0.6411527074581728, w0=-1.9920087490238567e-06, w1=-0.026327891695867845\n",
      "Regularized Logistic Regression(31/99): loss=0.6411514182059245, w0=-1.9998364903013704e-06, w1=-0.026379234192517278\n",
      "Regularized Logistic Regression(32/99): loss=0.6411504442071777, w0=-2.006877440781563e-06, w1=-0.02642456899707047\n",
      "Regularized Logistic Regression(33/99): loss=0.6411497066311042, w0=-2.013210624951284e-06, w1=-0.02646462261543731\n",
      "Regularized Logistic Regression(34/99): loss=0.6411491467753253, w0=-2.018907143424135e-06, w1=-0.02650003049772772\n",
      "Regularized Logistic Regression(35/99): loss=0.6411487208287433, w0=-2.0240309649601437e-06, w1=-0.02653134887555229\n",
      "Regularized Logistic Regression(36/99): loss=0.6411483960195445, w0=-2.0286396397527877e-06, w1=-0.026559065005005864\n",
      "Regularized Logistic Regression(37/99): loss=0.6411481477768811, w0=-2.0327849417416364e-06, w1=-0.026583606037328553\n",
      "Regularized Logistic Regression(38/99): loss=0.6411479576355191, w0=-2.0365134469538957e-06, w1=-0.02660534670732829\n",
      "Regularized Logistic Regression(39/99): loss=0.6411478116859518, w0=-2.0398670541956755e-06, w1=-0.026624616002484117\n",
      "Regularized Logistic Regression(40/99): loss=0.641147699425644, w0=-2.0428834537968877e-06, w1=-0.026641702952489674\n",
      "Regularized Logistic Regression(41/99): loss=0.6411476129058139, w0=-2.045596549555968e-06, w1=-0.026656861659230344\n",
      "Regularized Logistic Regression(42/99): loss=0.6411475460963495, w0=-2.048036838526551e-06, w1=-0.02667031567030275\n",
      "Regularized Logistic Regression(43/99): loss=0.6411474944120502, w0=-2.0502317528327154e-06, w1=-0.026682261784740837\n",
      "Regularized Logistic Regression(44/99): loss=0.6411474543584159, w0=-2.0522059672878275e-06, w1=-0.026692873367263566\n",
      "Regularized Logistic Regression(45/99): loss=0.6411474232662058, w0=-2.0539816762202846e-06, w1=-0.026702303236763772\n",
      "Regularized Logistic Regression(46/99): loss=0.6411473990920636, w0=-2.0555788425737615e-06, w1=-0.026710686185688823\n",
      "Regularized Logistic Regression(47/99): loss=0.641147380268412, w0=-2.057015422046476e-06, w1=-0.026718141179177143\n",
      "Regularized Logistic Regression(48/99): loss=0.6411473655901744, w0=-2.058307564760487e-06, w1=-0.026724773276128544\n",
      "Regularized Logistic Regression(49/99): loss=0.6411473541290886, w0=-2.0594697967051935e-06, w1=-0.026730675308649244\n",
      "Regularized Logistic Regression(50/99): loss=0.6411473451687408, w0=-2.060515182976576e-06, w1=-0.026735929351367482\n",
      "Regularized Logistic Regression(51/99): loss=0.6411473381551931, w0=-2.061455474632876e-06, w1=-0.026740608007874644\n",
      "Regularized Logistic Regression(52/99): loss=0.6411473326593771, w0=-2.0623012408063753e-06, w1=-0.026744775537884537\n",
      "Regularized Logistic Regression(53/99): loss=0.641147328348385, w0=-2.0630619875476526e-06, w1=-0.026748488845556792\n",
      "Regularized Logistic Regression(54/99): loss=0.6411473249635031, w0=-2.0637462647316035e-06, w1=-0.026751798346707704\n",
      "Regularized Logistic Regression(55/99): loss=0.6411473223033728, w0=-2.0643617622218816e-06, w1=-0.026754748730294736\n",
      "Regularized Logistic Regression(56/99): loss=0.6411473202110483, w0=-2.064915396370983e-06, w1=-0.026757379627529367\n",
      "Regularized Logistic Regression(57/99): loss=0.6411473185640351, w0=-2.065413387825517e-06, w1=-0.026759726200229855\n",
      "Regularized Logistic Regression(58/99): loss=0.6411473172666072, w0=-2.0658613315093383e-06, w1=-0.02676181965851218\n",
      "Regularized Logistic Regression(59/99): loss=0.6411473162438656, w0=-2.0662642595698144e-06, w1=-0.026763687716605467\n",
      "Regularized Logistic Regression(60/99): loss=0.6411473154371431, w0=-2.066626697993963e-06, w1=-0.026765354994449916\n",
      "Regularized Logistic Regression(61/99): loss=0.6411473148004357, w0=-2.0669527175303483e-06, w1=-0.026766843371750157\n",
      "Regularized Logistic Regression(62/99): loss=0.6411473142976374, w0=-2.0672459794889073e-06, w1=-0.02676817230030737\n",
      "Regularized Logistic Regression(63/99): loss=0.6411473139003807, w0=-2.067509776933508e-06, w1=-0.02676935907971327\n",
      "Regularized Logistic Regression(64/99): loss=0.6411473135863626, w0=-2.0677470717303833e-06, w1=-0.026770419100847882\n",
      "Regularized Logistic Regression(65/99): loss=0.6411473133380302, w0=-2.067960527869135e-06, w1=-0.026771366061063083\n",
      "Regularized Logistic Regression(66/99): loss=0.6411473131415633, w0=-2.0681525414311174e-06, w1=-0.02677221215445456\n",
      "Regularized Logistic Regression(67/99): loss=0.6411473129860682, w0=-2.0683252675424196e-06, w1=-0.02677296824019371\n",
      "Regularized Logistic Regression(68/99): loss=0.6411473128629565, w0=-2.068480644614762e-06, w1=-0.02677364399153063\n",
      "Regularized Logistic Regression(69/99): loss=0.6411473127654509, w0=-2.068620416147138e-06, w1=-0.02677424802775552\n",
      "Regularized Logistic Regression(70/99): loss=0.6411473126882009, w0=-2.06874615033363e-06, w1=-0.026774788031121474\n",
      "Regularized Logistic Regression(71/99): loss=0.6411473126269808, w0=-2.068859257698131e-06, w1=-0.026775270850499022\n",
      "Regularized Logistic Regression(72/99): loss=0.6411473125784508, w0=-2.0689610069545335e-06, w1=-0.02677570259330419\n",
      "Regularized Logistic Regression(73/99): loss=0.6411473125399697, w0=-2.069052539270948e-06, w1=-0.0267760887070677\n",
      "Regularized Logistic Regression(74/99): loss=0.6411473125094496, w0=-2.0691348810985815e-06, w1=-0.026776434051842376\n",
      "Regularized Logistic Regression(75/99): loss=0.6411473124852378, w0=-2.0692089557097145e-06, w1=-0.02677674296450405\n",
      "Regularized Logistic Regression(76/99): loss=0.641147312466026, w0=-2.0692755935747158e-06, w1=-0.026777019315878076\n",
      "Regularized Logistic Regression(77/99): loss=0.6411473124507787, w0=-2.069335541694931e-06, w1=-0.026777266561510426\n",
      "Regularized Logistic Regression(78/99): loss=0.6411473124386753, w0=-2.069389471996541e-06, w1=-0.026777487786807642\n",
      "Regularized Logistic Regression(79/99): loss=0.6411473124290655, w0=-2.0694379888798955e-06, w1=-0.026777685747184646\n",
      "Regularized Logistic Regression(80/99): loss=0.6411473124214347, w0=-2.069481636009325e-06, w1=-0.026777862903786094\n",
      "Regularized Logistic Regression(81/99): loss=0.6411473124153738, w0=-2.069520902419877e-06, w1=-0.026778021455276125\n",
      "Regularized Logistic Regression(82/99): loss=0.6411473124105593, w0=-2.0695562280097185e-06, w1=-0.026778163366143803\n",
      "Regularized Logistic Regression(83/99): loss=0.6411473124067341, w0=-2.0695880084800413e-06, w1=-0.026778290391907936\n",
      "Regularized Logistic Regression(84/99): loss=0.6411473124036945, w0=-2.0696165997780644e-06, w1=-0.026778404101572277\n",
      "Regularized Logistic Regression(85/99): loss=0.6411473124012791, w0=-2.069642322093147e-06, w1=-0.02677850589763429\n",
      "Regularized Logistic Regression(86/99): loss=0.641147312399359, w0=-2.06966546345098e-06, w1=-0.026778597033919216\n",
      "Regularized Logistic Regression(87/99): loss=0.6411473123978327, w0=-2.06968628294631e-06, w1=-0.026778678631479588\n",
      "Regularized Logistic Regression(88/99): loss=0.641147312396619, w0=-2.069705013650551e-06, w1=-0.026778751692774667\n",
      "Regularized Logistic Regression(89/99): loss=0.6411473123956541, w0=-2.069721865227017e-06, w1=-0.02677881711431774\n",
      "Regularized Logistic Regression(90/99): loss=0.6411473123948869, w0=-2.0697370262831717e-06, w1=-0.02677887569795981\n",
      "Regularized Logistic Regression(91/99): loss=0.6411473123942766, w0=-2.0697506664863837e-06, w1=-0.02677892816095842\n",
      "Regularized Logistic Regression(92/99): loss=0.641147312393791, w0=-2.06976293846695e-06, w1=-0.02677897514496588\n",
      "Regularized Logistic Regression(93/99): loss=0.6411473123934046, w0=-2.0697739795297956e-06, w1=-0.026779017224052373\n",
      "Regularized Logistic Regression(94/99): loss=0.6411473123930973, w0=-2.0697839131941125e-06, w1=-0.026779054911868928\n",
      "Regularized Logistic Regression(95/99): loss=0.6411473123928529, w0=-2.0697928505782264e-06, w1=-0.02677908866804696\n",
      "Regularized Logistic Regression(96/99): loss=0.6411473123926584, w0=-2.0698008916452634e-06, w1=-0.026779118903911388\n",
      "Regularized Logistic Regression(97/99): loss=0.6411473123925036, w0=-2.069808126323619e-06, w1=-0.02677914598758761\n",
      "Regularized Logistic Regression(98/99): loss=0.6411473123923803, w0=-2.0698146355148142e-06, w1=-0.026779170248561322\n",
      "Regularized Logistic Regression(99/99): loss=0.6411473123922822, w0=-2.069820492000079e-06, w1=-0.02677919198175602\n",
      "Regularized Logistic Regression(0/199): loss=0.693147180559945, w0=-2.0882930292778685e-07, w1=-0.003658898216597753\n",
      "Regularized Logistic Regression(1/199): loss=0.6784891515630495, w0=-3.961342228300053e-07, w1=-0.00678465775360275\n",
      "Regularized Logistic Regression(2/199): loss=0.6680075107837707, w0=-5.643031846318581e-07, w1=-0.009461358858280929\n",
      "Regularized Logistic Regression(3/199): loss=0.6604993716762532, w0=-7.154033413537734e-07, w1=-0.01175860559620912\n",
      "Regularized Logistic Regression(4/199): loss=0.6551120805505041, w0=-8.512399773915193e-07, w1=-0.013734185240072208\n",
      "Regularized Logistic Regression(5/199): loss=0.6512402094345634, w0=-9.734014242524225e-07, w1=-0.015436260305958866\n",
      "Regularized Logistic Regression(6/199): loss=0.6484530624535548, w0=-1.0832934134486223e-06, w1=-0.016905138715240988\n",
      "Regularized Logistic Regression(7/199): loss=0.6464436633095114, w0=-1.182165791675792e-06, w1=-0.018174692270827065\n",
      "Regularized Logistic Regression(8/199): loss=0.6449928115023803, w0=-1.2711337033217505e-06, w1=-0.019273492569536678\n",
      "Regularized Logistic Regression(9/199): loss=0.6439437073746082, w0=-1.3511947283307386e-06, w1=-0.020225723575334052\n",
      "Regularized Logistic Regression(10/199): loss=0.6431840000183142, w0=-1.42324301978774e-06, w1=-0.021051918612953524\n",
      "Regularized Logistic Regression(11/199): loss=0.6426330591202727, w0=-1.4880811743823323e-06, w1=-0.02176955921612948\n",
      "Regularized Logistic Regression(12/199): loss=0.642232931598934, w0=-1.546430353448919e-06, w1=-0.022393564813049646\n",
      "Regularized Logistic Regression(13/199): loss=0.6419419034077747, w0=-1.5989390239529494e-06, w1=-0.022936695613834587\n",
      "Regularized Logistic Regression(14/199): loss=0.6417299073780774, w0=-1.6461905867502848e-06, w1=-0.023409885991633423\n",
      "Regularized Logistic Regression(15/199): loss=0.6415752420036795, w0=-1.6887100890253016e-06, w1=-0.023822521794305027\n",
      "Regularized Logistic Regression(16/199): loss=0.6414622230652695, w0=-1.7269701689049833e-06, w1=-0.024182672100073552\n",
      "Regularized Logistic Regression(17/199): loss=0.6413795003014919, w0=-1.7613963459619677e-06, w1=-0.024497283707295245\n",
      "Regularized Logistic Regression(18/199): loss=0.6413188490443191, w0=-1.7923717469921945e-06, w1=-0.02477234494867808\n",
      "Regularized Logistic Regression(19/199): loss=0.641274301619076, w0=-1.8202413389288517e-06, w1=-0.025013024111463875\n",
      "Regularized Logistic Regression(20/199): loss=0.6412415221632937, w0=-1.8453157279043325e-06, w1=-0.02522378672911242\n",
      "Regularized Logistic Regression(21/199): loss=0.6412173560860205, w0=-1.867874573856507e-06, w1=-0.02540849521461615\n",
      "Regularized Logistic Regression(22/199): loss=0.6411995049893102, w0=-1.8881696627198177e-06, w1=-0.02557049367738958\n",
      "Regularized Logistic Regression(23/199): loss=0.6411862918355424, w0=-1.906427672483322e-06, w1=-0.025712680265342312\n",
      "Regularized Logistic Regression(24/199): loss=0.6411764911072533, w0=-1.92285266478488e-06, w1=-0.025837568971963435\n",
      "Regularized Logistic Regression(25/199): loss=0.6411692058270942, w0=-1.9376283299335955e-06, w1=-0.02594734252313674\n",
      "Regularized Logistic Regression(26/199): loss=0.6411637784025879, w0=-1.9509200100973793e-06, w1=-0.02604389769347473\n",
      "Regularized Logistic Regression(27/199): loss=0.6411597259137678, w0=-1.9628765227103824e-06, w1=-0.02612888418468885\n",
      "Regularized Logistic Regression(28/199): loss=0.6411566930837257, w0=-1.973631803841227e-06, w1=-0.026203738019284877\n",
      "Regularized Logistic Regression(29/199): loss=0.6411544180560348, w0=-1.983306389242811e-06, w1=-0.02626971025427836\n",
      "Regularized Logistic Regression(30/199): loss=0.6411527074581728, w0=-1.9920087490238567e-06, w1=-0.026327891695867845\n",
      "Regularized Logistic Regression(31/199): loss=0.6411514182059245, w0=-1.9998364903013704e-06, w1=-0.026379234192517278\n",
      "Regularized Logistic Regression(32/199): loss=0.6411504442071777, w0=-2.006877440781563e-06, w1=-0.02642456899707047\n",
      "Regularized Logistic Regression(33/199): loss=0.6411497066311042, w0=-2.013210624951284e-06, w1=-0.02646462261543731\n",
      "Regularized Logistic Regression(34/199): loss=0.6411491467753253, w0=-2.018907143424135e-06, w1=-0.02650003049772772\n",
      "Regularized Logistic Regression(35/199): loss=0.6411487208287433, w0=-2.0240309649601437e-06, w1=-0.02653134887555229\n",
      "Regularized Logistic Regression(36/199): loss=0.6411483960195445, w0=-2.0286396397527877e-06, w1=-0.026559065005005864\n",
      "Regularized Logistic Regression(37/199): loss=0.6411481477768811, w0=-2.0327849417416364e-06, w1=-0.026583606037328553\n",
      "Regularized Logistic Regression(38/199): loss=0.6411479576355191, w0=-2.0365134469538957e-06, w1=-0.02660534670732829\n",
      "Regularized Logistic Regression(39/199): loss=0.6411478116859518, w0=-2.0398670541956755e-06, w1=-0.026624616002484117\n",
      "Regularized Logistic Regression(40/199): loss=0.641147699425644, w0=-2.0428834537968877e-06, w1=-0.026641702952489674\n",
      "Regularized Logistic Regression(41/199): loss=0.6411476129058139, w0=-2.045596549555968e-06, w1=-0.026656861659230344\n",
      "Regularized Logistic Regression(42/199): loss=0.6411475460963495, w0=-2.048036838526551e-06, w1=-0.02667031567030275\n",
      "Regularized Logistic Regression(43/199): loss=0.6411474944120502, w0=-2.0502317528327154e-06, w1=-0.026682261784740837\n",
      "Regularized Logistic Regression(44/199): loss=0.6411474543584159, w0=-2.0522059672878275e-06, w1=-0.026692873367263566\n",
      "Regularized Logistic Regression(45/199): loss=0.6411474232662058, w0=-2.0539816762202846e-06, w1=-0.026702303236763772\n",
      "Regularized Logistic Regression(46/199): loss=0.6411473990920636, w0=-2.0555788425737615e-06, w1=-0.026710686185688823\n",
      "Regularized Logistic Regression(47/199): loss=0.641147380268412, w0=-2.057015422046476e-06, w1=-0.026718141179177143\n",
      "Regularized Logistic Regression(48/199): loss=0.6411473655901744, w0=-2.058307564760487e-06, w1=-0.026724773276128544\n",
      "Regularized Logistic Regression(49/199): loss=0.6411473541290886, w0=-2.0594697967051935e-06, w1=-0.026730675308649244\n",
      "Regularized Logistic Regression(50/199): loss=0.6411473451687408, w0=-2.060515182976576e-06, w1=-0.026735929351367482\n",
      "Regularized Logistic Regression(51/199): loss=0.6411473381551931, w0=-2.061455474632876e-06, w1=-0.026740608007874644\n",
      "Regularized Logistic Regression(52/199): loss=0.6411473326593771, w0=-2.0623012408063753e-06, w1=-0.026744775537884537\n",
      "Regularized Logistic Regression(53/199): loss=0.641147328348385, w0=-2.0630619875476526e-06, w1=-0.026748488845556792\n",
      "Regularized Logistic Regression(54/199): loss=0.6411473249635031, w0=-2.0637462647316035e-06, w1=-0.026751798346707704\n",
      "Regularized Logistic Regression(55/199): loss=0.6411473223033728, w0=-2.0643617622218816e-06, w1=-0.026754748730294736\n",
      "Regularized Logistic Regression(56/199): loss=0.6411473202110483, w0=-2.064915396370983e-06, w1=-0.026757379627529367\n",
      "Regularized Logistic Regression(57/199): loss=0.6411473185640351, w0=-2.065413387825517e-06, w1=-0.026759726200229855\n",
      "Regularized Logistic Regression(58/199): loss=0.6411473172666072, w0=-2.0658613315093383e-06, w1=-0.02676181965851218\n",
      "Regularized Logistic Regression(59/199): loss=0.6411473162438656, w0=-2.0662642595698144e-06, w1=-0.026763687716605467\n",
      "Regularized Logistic Regression(60/199): loss=0.6411473154371431, w0=-2.066626697993963e-06, w1=-0.026765354994449916\n",
      "Regularized Logistic Regression(61/199): loss=0.6411473148004357, w0=-2.0669527175303483e-06, w1=-0.026766843371750157\n",
      "Regularized Logistic Regression(62/199): loss=0.6411473142976374, w0=-2.0672459794889073e-06, w1=-0.02676817230030737\n",
      "Regularized Logistic Regression(63/199): loss=0.6411473139003807, w0=-2.067509776933508e-06, w1=-0.02676935907971327\n",
      "Regularized Logistic Regression(64/199): loss=0.6411473135863626, w0=-2.0677470717303833e-06, w1=-0.026770419100847882\n",
      "Regularized Logistic Regression(65/199): loss=0.6411473133380302, w0=-2.067960527869135e-06, w1=-0.026771366061063083\n",
      "Regularized Logistic Regression(66/199): loss=0.6411473131415633, w0=-2.0681525414311174e-06, w1=-0.02677221215445456\n",
      "Regularized Logistic Regression(67/199): loss=0.6411473129860682, w0=-2.0683252675424196e-06, w1=-0.02677296824019371\n",
      "Regularized Logistic Regression(68/199): loss=0.6411473128629565, w0=-2.068480644614762e-06, w1=-0.02677364399153063\n",
      "Regularized Logistic Regression(69/199): loss=0.6411473127654509, w0=-2.068620416147138e-06, w1=-0.02677424802775552\n",
      "Regularized Logistic Regression(70/199): loss=0.6411473126882009, w0=-2.06874615033363e-06, w1=-0.026774788031121474\n",
      "Regularized Logistic Regression(71/199): loss=0.6411473126269808, w0=-2.068859257698131e-06, w1=-0.026775270850499022\n",
      "Regularized Logistic Regression(72/199): loss=0.6411473125784508, w0=-2.0689610069545335e-06, w1=-0.02677570259330419\n",
      "Regularized Logistic Regression(73/199): loss=0.6411473125399697, w0=-2.069052539270948e-06, w1=-0.0267760887070677\n",
      "Regularized Logistic Regression(74/199): loss=0.6411473125094496, w0=-2.0691348810985815e-06, w1=-0.026776434051842376\n",
      "Regularized Logistic Regression(75/199): loss=0.6411473124852378, w0=-2.0692089557097145e-06, w1=-0.02677674296450405\n",
      "Regularized Logistic Regression(76/199): loss=0.641147312466026, w0=-2.0692755935747158e-06, w1=-0.026777019315878076\n",
      "Regularized Logistic Regression(77/199): loss=0.6411473124507787, w0=-2.069335541694931e-06, w1=-0.026777266561510426\n",
      "Regularized Logistic Regression(78/199): loss=0.6411473124386753, w0=-2.069389471996541e-06, w1=-0.026777487786807642\n",
      "Regularized Logistic Regression(79/199): loss=0.6411473124290655, w0=-2.0694379888798955e-06, w1=-0.026777685747184646\n",
      "Regularized Logistic Regression(80/199): loss=0.6411473124214347, w0=-2.069481636009325e-06, w1=-0.026777862903786094\n",
      "Regularized Logistic Regression(81/199): loss=0.6411473124153738, w0=-2.069520902419877e-06, w1=-0.026778021455276125\n",
      "Regularized Logistic Regression(82/199): loss=0.6411473124105593, w0=-2.0695562280097185e-06, w1=-0.026778163366143803\n",
      "Regularized Logistic Regression(83/199): loss=0.6411473124067341, w0=-2.0695880084800413e-06, w1=-0.026778290391907936\n",
      "Regularized Logistic Regression(84/199): loss=0.6411473124036945, w0=-2.0696165997780644e-06, w1=-0.026778404101572277\n",
      "Regularized Logistic Regression(85/199): loss=0.6411473124012791, w0=-2.069642322093147e-06, w1=-0.02677850589763429\n",
      "Regularized Logistic Regression(86/199): loss=0.641147312399359, w0=-2.06966546345098e-06, w1=-0.026778597033919216\n",
      "Regularized Logistic Regression(87/199): loss=0.6411473123978327, w0=-2.06968628294631e-06, w1=-0.026778678631479588\n",
      "Regularized Logistic Regression(88/199): loss=0.641147312396619, w0=-2.069705013650551e-06, w1=-0.026778751692774667\n",
      "Regularized Logistic Regression(89/199): loss=0.6411473123956541, w0=-2.069721865227017e-06, w1=-0.02677881711431774\n",
      "Regularized Logistic Regression(90/199): loss=0.6411473123948869, w0=-2.0697370262831717e-06, w1=-0.02677887569795981\n",
      "Regularized Logistic Regression(91/199): loss=0.6411473123942766, w0=-2.0697506664863837e-06, w1=-0.02677892816095842\n",
      "Regularized Logistic Regression(92/199): loss=0.641147312393791, w0=-2.06976293846695e-06, w1=-0.02677897514496588\n",
      "Regularized Logistic Regression(93/199): loss=0.6411473123934046, w0=-2.0697739795297956e-06, w1=-0.026779017224052373\n",
      "Regularized Logistic Regression(94/199): loss=0.6411473123930973, w0=-2.0697839131941125e-06, w1=-0.026779054911868928\n",
      "Regularized Logistic Regression(95/199): loss=0.6411473123928529, w0=-2.0697928505782264e-06, w1=-0.02677908866804696\n",
      "Regularized Logistic Regression(96/199): loss=0.6411473123926584, w0=-2.0698008916452634e-06, w1=-0.026779118903911388\n",
      "Regularized Logistic Regression(97/199): loss=0.6411473123925036, w0=-2.069808126323619e-06, w1=-0.02677914598758761\n",
      "Regularized Logistic Regression(98/199): loss=0.6411473123923803, w0=-2.0698146355148142e-06, w1=-0.026779170248561322\n",
      "Regularized Logistic Regression(99/199): loss=0.6411473123922822, w0=-2.069820492000079e-06, w1=-0.02677919198175602\n",
      "Regularized Logistic Regression(100/199): loss=0.6411473123922041, w0=-2.0698257612558216e-06, w1=-0.026779211451175292\n",
      "Regularized Logistic Regression(101/199): loss=0.6411473123921417, w0=-2.0698305021871697e-06, w1=-0.026779228893160888\n",
      "Regularized Logistic Regression(102/199): loss=0.6411473123920924, w0=-2.0698347677878043e-06, w1=-0.026779244519302758\n",
      "Regularized Logistic Regression(103/199): loss=0.6411473123920529, w0=-2.0698386057334994e-06, w1=-0.026779258519043347\n",
      "Regularized Logistic Regression(104/199): loss=0.6411473123920215, w0=-2.0698420589160377e-06, w1=-0.026779271062004253\n",
      "Regularized Logistic Regression(105/199): loss=0.6411473123919963, w0=-2.0698451659234945e-06, w1=-0.026779282300068992\n",
      "Regularized Logistic Regression(106/199): loss=0.6411473123919765, w0=-2.069847961472275e-06, w1=-0.026779292369244163\n",
      "Regularized Logistic Regression(107/199): loss=0.6411473123919607, w0=-2.069850476795756e-06, w1=-0.026779301391325366\n",
      "Regularized Logistic Regression(108/199): loss=0.641147312391948, w0=-2.069852739993898e-06, w1=-0.026779309475386936\n",
      "Regularized Logistic Regression(109/199): loss=0.6411473123919377, w0=-2.069854776347743e-06, w1=-0.026779316719115168\n",
      "Regularized Logistic Regression(110/199): loss=0.6411473123919298, w0=-2.0698566086023357e-06, w1=-0.02677932321000072\n",
      "Regularized Logistic Regression(111/199): loss=0.6411473123919234, w0=-2.0698582572212225e-06, w1=-0.026779329026406166\n",
      "Regularized Logistic Regression(112/199): loss=0.6411473123919182, w0=-2.069859740615415e-06, w1=-0.02677933423852086\n",
      "Regularized Logistic Regression(113/199): loss=0.6411473123919144, w0=-2.069861075349348e-06, w1=-0.026779338909215814\n",
      "Regularized Logistic Regression(114/199): loss=0.6411473123919111, w0=-2.0698622763261706e-06, w1=-0.02677934309480874\n",
      "Regularized Logistic Regression(115/199): loss=0.6411473123919086, w0=-2.0698633569544324e-06, w1=-0.02677934684574887\n",
      "Regularized Logistic Regression(116/199): loss=0.6411473123919065, w0=-2.0698643292980353e-06, w1=-0.02677935020722964\n",
      "Regularized Logistic Regression(117/199): loss=0.6411473123919048, w0=-2.069865204211137e-06, w1=-0.02677935321973785\n",
      "Regularized Logistic Regression(118/199): loss=0.6411473123919035, w0=-2.0698659914595093e-06, w1=-0.026779355919544705\n",
      "Regularized Logistic Regression(119/199): loss=0.6411473123919024, w0=-2.069866699829723e-06, w1=-0.026779358339145666\n",
      "Regularized Logistic Regression(120/199): loss=0.6411473123919016, w0=-2.069867337227367e-06, w1=-0.02677936050765465\n",
      "Regularized Logistic Regression(121/199): loss=0.6411473123919009, w0=-2.069867910765416e-06, w1=-0.026779362451156127\n",
      "Regularized Logistic Regression(122/199): loss=0.6411473123919003, w0=-2.0698684268437294e-06, w1=-0.026779364193020804\n",
      "Regularized Logistic Regression(123/199): loss=0.6411473123919001, w0=-2.0698688912205735e-06, w1=-0.026779365754189644\n",
      "Regularized Logistic Regression(124/199): loss=0.6411473123918996, w0=-2.0698693090769633e-06, w1=-0.0267793671534251\n",
      "Regularized Logistic Regression(125/199): loss=0.6411473123918993, w0=-2.0698696850745553e-06, w1=-0.026779368407539535\n",
      "Regularized Logistic Regression(126/199): loss=0.641147312391899, w0=-2.0698700234077287e-06, w1=-0.026779369531597676\n",
      "Regularized Logistic Regression(127/199): loss=0.641147312391899, w0=-2.069870327850437e-06, w1=-0.026779370539098894\n",
      "Regularized Logistic Regression(128/199): loss=0.6411473123918987, w0=-2.0698706017983703e-06, w1=-0.026779371442139085\n",
      "Regularized Logistic Regression(129/199): loss=0.6411473123918987, w0=-2.069870848306878e-06, w1=-0.026779372251558353\n",
      "Regularized Logistic Regression(130/199): loss=0.6411473123918986, w0=-2.0698710701250947e-06, w1=-0.026779372977070313\n",
      "Regularized Logistic Regression(131/199): loss=0.6411473123918985, w0=-2.06987126972664e-06, w1=-0.0267793736273797\n",
      "Regularized Logistic Regression(132/199): loss=0.6411473123918987, w0=-2.069871449337238e-06, w1=-0.0267793742102877\n",
      "Regularized Logistic Regression(133/199): loss=0.6411473123918983, w0=-2.0698716109595706e-06, w1=-0.02677937473278542\n",
      "Regularized Logistic Regression(134/199): loss=0.6411473123918986, w0=-2.069871756395634e-06, w1=-0.0267793752011376\n",
      "Regularized Logistic Regression(135/199): loss=0.6411473123918984, w0=-2.0698718872668557e-06, w1=-0.02677937562095943\n",
      "Regularized Logistic Regression(136/199): loss=0.6411473123918981, w0=-2.0698720050321973e-06, w1=-0.02677937599728301\n",
      "Regularized Logistic Regression(137/199): loss=0.6411473123918984, w0=-2.0698721110044347e-06, w1=-0.026779376334617994\n",
      "Regularized Logistic Regression(138/199): loss=0.6411473123918984, w0=-2.0698722063648138e-06, w1=-0.026779376637006583\n",
      "Regularized Logistic Regression(139/199): loss=0.6411473123918985, w0=-2.0698722921762287e-06, w1=-0.02677937690807114\n",
      "Regularized Logistic Regression(140/199): loss=0.6411473123918983, w0=-2.069872369395087e-06, w1=-0.02677937715105882\n",
      "Regularized Logistic Regression(141/199): loss=0.6411473123918983, w0=-2.069872438881979e-06, w1=-0.02677937736887937\n",
      "Regularized Logistic Regression(142/199): loss=0.6411473123918985, w0=-2.0698725014112798e-06, w1=-0.02677937756414093\n",
      "Regularized Logistic Regression(143/199): loss=0.6411473123918984, w0=-2.069872557679791e-06, w1=-0.026779377739181497\n",
      "Regularized Logistic Regression(144/199): loss=0.6411473123918983, w0=-2.069872608314511e-06, w1=-0.02677937789609583\n",
      "Regularized Logistic Regression(145/199): loss=0.6411473123918983, w0=-2.069872653879632e-06, w1=-0.026779378036762027\n",
      "Regularized Logistic Regression(146/199): loss=0.6411473123918984, w0=-2.069872694882833e-06, w1=-0.02677937816286368\n",
      "Regularized Logistic Regression(147/199): loss=0.6411473123918981, w0=-2.0698727317809395e-06, w1=-0.02677937827590975\n",
      "Regularized Logistic Regression(148/199): loss=0.6411473123918983, w0=-2.0698727649850207e-06, w1=-0.026779378377252437\n",
      "Regularized Logistic Regression(149/199): loss=0.6411473123918981, w0=-2.069872794864975e-06, w1=-0.02677937846810372\n",
      "Regularized Logistic Regression(150/199): loss=0.6411473123918981, w0=-2.0698728217536512e-06, w1=-0.026779378549550613\n",
      "Regularized Logistic Regression(151/199): loss=0.6411473123918983, w0=-2.0698728459505644e-06, w1=-0.02677937862256707\n",
      "Regularized Logistic Regression(152/199): loss=0.6411473123918983, w0=-2.0698728677252336e-06, w1=-0.026779378688025997\n",
      "Regularized Logistic Regression(153/199): loss=0.6411473123918984, w0=-2.0698728873201813e-06, w1=-0.02677937874670986\n",
      "Regularized Logistic Regression(154/199): loss=0.6411473123918984, w0=-2.069872904953646e-06, w1=-0.026779378799320342\n",
      "Regularized Logistic Regression(155/199): loss=0.6411473123918981, w0=-2.0698729208220106e-06, w1=-0.02677937884648605\n",
      "Regularized Logistic Regression(156/199): loss=0.6411473123918981, w0=-2.0698729351019917e-06, w1=-0.026779378888771087\n",
      "Regularized Logistic Regression(157/199): loss=0.6411473123918984, w0=-2.0698729479526098e-06, w1=-0.026779378926680707\n",
      "Regularized Logistic Regression(158/199): loss=0.6411473123918985, w0=-2.0698729595169643e-06, w1=-0.026779378960667788\n",
      "Regularized Logistic Regression(159/199): loss=0.6411473123918983, w0=-2.069872969923823e-06, w1=-0.026779378991138164\n",
      "Regularized Logistic Regression(160/199): loss=0.6411473123918983, w0=-2.0698729792890604e-06, w1=-0.026779379018455896\n",
      "Regularized Logistic Regression(161/199): loss=0.6411473123918981, w0=-2.069872987716951e-06, w1=-0.02677937904294768\n",
      "Regularized Logistic Regression(162/199): loss=0.6411473123918984, w0=-2.0698729953013263e-06, w1=-0.026779379064905472\n",
      "Regularized Logistic Regression(163/199): loss=0.6411473123918981, w0=-2.0698730021266255e-06, w1=-0.026779379084591826\n",
      "Regularized Logistic Regression(164/199): loss=0.6411473123918984, w0=-2.069873008268831e-06, w1=-0.026779379102242006\n",
      "Regularized Logistic Regression(165/199): loss=0.641147312391898, w0=-2.0698730137963208e-06, w1=-0.026779379118066386\n",
      "Regularized Logistic Regression(166/199): loss=0.6411473123918984, w0=-2.069873018770624e-06, w1=-0.026779379132253936\n",
      "Regularized Logistic Regression(167/199): loss=0.6411473123918983, w0=-2.0698730232471116e-06, w1=-0.026779379144974157\n",
      "Regularized Logistic Regression(168/199): loss=0.6411473123918981, w0=-2.0698730272756114e-06, w1=-0.026779379156378777\n",
      "Regularized Logistic Regression(169/199): loss=0.6411473123918985, w0=-2.0698730309009623e-06, w1=-0.026779379166603817\n",
      "Regularized Logistic Regression(170/199): loss=0.6411473123918983, w0=-2.069873034163515e-06, w1=-0.026779379175771757\n",
      "Regularized Logistic Regression(171/199): loss=0.6411473123918984, w0=-2.069873037099582e-06, w1=-0.026779379183991325\n",
      "Regularized Logistic Regression(172/199): loss=0.6411473123918981, w0=-2.069873039741839e-06, w1=-0.02677937919136103\n",
      "Regularized Logistic Regression(173/199): loss=0.6411473123918983, w0=-2.069873042119691e-06, w1=-0.026779379197968658\n",
      "Regularized Logistic Regression(174/199): loss=0.6411473123918985, w0=-2.0698730442596e-06, w1=-0.02677937920389288\n",
      "Regularized Logistic Regression(175/199): loss=0.6411473123918984, w0=-2.069873046185379e-06, w1=-0.026779379209204573\n",
      "Regularized Logistic Regression(176/199): loss=0.6411473123918985, w0=-2.069873047918459e-06, w1=-0.02677937921396733\n",
      "Regularized Logistic Regression(177/199): loss=0.6411473123918984, w0=-2.069873049478124e-06, w1=-0.026779379218237715\n",
      "Regularized Logistic Regression(178/199): loss=0.6411473123918984, w0=-2.069873050881729e-06, w1=-0.02677937922206655\n",
      "Regularized Logistic Regression(179/199): loss=0.6411473123918984, w0=-2.0698730521448896e-06, w1=-0.02677937922549939\n",
      "Regularized Logistic Regression(180/199): loss=0.6411473123918981, w0=-2.069873053281662e-06, w1=-0.02677937922857737\n",
      "Regularized Logistic Regression(181/199): loss=0.6411473123918985, w0=-2.069873054304693e-06, w1=-0.0267793792313373\n",
      "Regularized Logistic Regression(182/199): loss=0.6411473123918985, w0=-2.0698730552253655e-06, w1=-0.026779379233811844\n",
      "Regularized Logistic Regression(183/199): loss=0.6411473123918981, w0=-2.069873056053921e-06, w1=-0.02677937923603069\n",
      "Regularized Logistic Regression(184/199): loss=0.6411473123918983, w0=-2.069873056799577e-06, w1=-0.026779379238020283\n",
      "Regularized Logistic Regression(185/199): loss=0.6411473123918983, w0=-2.069873057470631e-06, w1=-0.02677937923980425\n",
      "Regularized Logistic Regression(186/199): loss=0.6411473123918981, w0=-2.0698730580745475e-06, w1=-0.026779379241403757\n",
      "Regularized Logistic Regression(187/199): loss=0.6411473123918983, w0=-2.0698730586180437e-06, w1=-0.026779379242838047\n",
      "Regularized Logistic Regression(188/199): loss=0.6411473123918984, w0=-2.0698730591071633e-06, w1=-0.026779379244124234\n",
      "Regularized Logistic Regression(189/199): loss=0.6411473123918981, w0=-2.0698730595473477e-06, w1=-0.026779379245277235\n",
      "Regularized Logistic Regression(190/199): loss=0.6411473123918983, w0=-2.0698730599434944e-06, w1=-0.026779379246311054\n",
      "Regularized Logistic Regression(191/199): loss=0.6411473123918981, w0=-2.069873060300009e-06, w1=-0.026779379247237958\n",
      "Regularized Logistic Regression(192/199): loss=0.6411473123918983, w0=-2.0698730606208573e-06, w1=-0.02677937924806932\n",
      "Regularized Logistic Regression(193/199): loss=0.6411473123918981, w0=-2.069873060909608e-06, w1=-0.02677937924881462\n",
      "Regularized Logistic Regression(194/199): loss=0.6411473123918983, w0=-2.0698730611694717e-06, w1=-0.0267793792494829\n",
      "Regularized Logistic Regression(195/199): loss=0.6411473123918984, w0=-2.069873061403339e-06, w1=-0.02677937925008214\n",
      "Regularized Logistic Regression(196/199): loss=0.6411473123918983, w0=-2.069873061613812e-06, w1=-0.02677937925061977\n",
      "Regularized Logistic Regression(197/199): loss=0.6411473123918983, w0=-2.0698730618032306e-06, w1=-0.026779379251101538\n",
      "Regularized Logistic Regression(198/199): loss=0.6411473123918983, w0=-2.069873061973701e-06, w1=-0.026779379251533605\n",
      "Regularized Logistic Regression(199/199): loss=0.6411473123918983, w0=-2.069873062127119e-06, w1=-0.02677937925192116\n",
      "Regularized Logistic Regression(0/299): loss=0.693147180559945, w0=-2.0882930292778685e-07, w1=-0.003658898216597753\n",
      "Regularized Logistic Regression(1/299): loss=0.6784891515630495, w0=-3.961342228300053e-07, w1=-0.00678465775360275\n",
      "Regularized Logistic Regression(2/299): loss=0.6680075107837707, w0=-5.643031846318581e-07, w1=-0.009461358858280929\n",
      "Regularized Logistic Regression(3/299): loss=0.6604993716762532, w0=-7.154033413537734e-07, w1=-0.01175860559620912\n",
      "Regularized Logistic Regression(4/299): loss=0.6551120805505041, w0=-8.512399773915193e-07, w1=-0.013734185240072208\n",
      "Regularized Logistic Regression(5/299): loss=0.6512402094345634, w0=-9.734014242524225e-07, w1=-0.015436260305958866\n",
      "Regularized Logistic Regression(6/299): loss=0.6484530624535548, w0=-1.0832934134486223e-06, w1=-0.016905138715240988\n",
      "Regularized Logistic Regression(7/299): loss=0.6464436633095114, w0=-1.182165791675792e-06, w1=-0.018174692270827065\n",
      "Regularized Logistic Regression(8/299): loss=0.6449928115023803, w0=-1.2711337033217505e-06, w1=-0.019273492569536678\n",
      "Regularized Logistic Regression(9/299): loss=0.6439437073746082, w0=-1.3511947283307386e-06, w1=-0.020225723575334052\n",
      "Regularized Logistic Regression(10/299): loss=0.6431840000183142, w0=-1.42324301978774e-06, w1=-0.021051918612953524\n",
      "Regularized Logistic Regression(11/299): loss=0.6426330591202727, w0=-1.4880811743823323e-06, w1=-0.02176955921612948\n",
      "Regularized Logistic Regression(12/299): loss=0.642232931598934, w0=-1.546430353448919e-06, w1=-0.022393564813049646\n",
      "Regularized Logistic Regression(13/299): loss=0.6419419034077747, w0=-1.5989390239529494e-06, w1=-0.022936695613834587\n",
      "Regularized Logistic Regression(14/299): loss=0.6417299073780774, w0=-1.6461905867502848e-06, w1=-0.023409885991633423\n",
      "Regularized Logistic Regression(15/299): loss=0.6415752420036795, w0=-1.6887100890253016e-06, w1=-0.023822521794305027\n",
      "Regularized Logistic Regression(16/299): loss=0.6414622230652695, w0=-1.7269701689049833e-06, w1=-0.024182672100073552\n",
      "Regularized Logistic Regression(17/299): loss=0.6413795003014919, w0=-1.7613963459619677e-06, w1=-0.024497283707295245\n",
      "Regularized Logistic Regression(18/299): loss=0.6413188490443191, w0=-1.7923717469921945e-06, w1=-0.02477234494867808\n",
      "Regularized Logistic Regression(19/299): loss=0.641274301619076, w0=-1.8202413389288517e-06, w1=-0.025013024111463875\n",
      "Regularized Logistic Regression(20/299): loss=0.6412415221632937, w0=-1.8453157279043325e-06, w1=-0.02522378672911242\n",
      "Regularized Logistic Regression(21/299): loss=0.6412173560860205, w0=-1.867874573856507e-06, w1=-0.02540849521461615\n",
      "Regularized Logistic Regression(22/299): loss=0.6411995049893102, w0=-1.8881696627198177e-06, w1=-0.02557049367738958\n",
      "Regularized Logistic Regression(23/299): loss=0.6411862918355424, w0=-1.906427672483322e-06, w1=-0.025712680265342312\n",
      "Regularized Logistic Regression(24/299): loss=0.6411764911072533, w0=-1.92285266478488e-06, w1=-0.025837568971963435\n",
      "Regularized Logistic Regression(25/299): loss=0.6411692058270942, w0=-1.9376283299335955e-06, w1=-0.02594734252313674\n",
      "Regularized Logistic Regression(26/299): loss=0.6411637784025879, w0=-1.9509200100973793e-06, w1=-0.02604389769347473\n",
      "Regularized Logistic Regression(27/299): loss=0.6411597259137678, w0=-1.9628765227103824e-06, w1=-0.02612888418468885\n",
      "Regularized Logistic Regression(28/299): loss=0.6411566930837257, w0=-1.973631803841227e-06, w1=-0.026203738019284877\n",
      "Regularized Logistic Regression(29/299): loss=0.6411544180560348, w0=-1.983306389242811e-06, w1=-0.02626971025427836\n",
      "Regularized Logistic Regression(30/299): loss=0.6411527074581728, w0=-1.9920087490238567e-06, w1=-0.026327891695867845\n",
      "Regularized Logistic Regression(31/299): loss=0.6411514182059245, w0=-1.9998364903013704e-06, w1=-0.026379234192517278\n",
      "Regularized Logistic Regression(32/299): loss=0.6411504442071777, w0=-2.006877440781563e-06, w1=-0.02642456899707047\n",
      "Regularized Logistic Regression(33/299): loss=0.6411497066311042, w0=-2.013210624951284e-06, w1=-0.02646462261543731\n",
      "Regularized Logistic Regression(34/299): loss=0.6411491467753253, w0=-2.018907143424135e-06, w1=-0.02650003049772772\n",
      "Regularized Logistic Regression(35/299): loss=0.6411487208287433, w0=-2.0240309649601437e-06, w1=-0.02653134887555229\n",
      "Regularized Logistic Regression(36/299): loss=0.6411483960195445, w0=-2.0286396397527877e-06, w1=-0.026559065005005864\n",
      "Regularized Logistic Regression(37/299): loss=0.6411481477768811, w0=-2.0327849417416364e-06, w1=-0.026583606037328553\n",
      "Regularized Logistic Regression(38/299): loss=0.6411479576355191, w0=-2.0365134469538957e-06, w1=-0.02660534670732829\n",
      "Regularized Logistic Regression(39/299): loss=0.6411478116859518, w0=-2.0398670541956755e-06, w1=-0.026624616002484117\n",
      "Regularized Logistic Regression(40/299): loss=0.641147699425644, w0=-2.0428834537968877e-06, w1=-0.026641702952489674\n",
      "Regularized Logistic Regression(41/299): loss=0.6411476129058139, w0=-2.045596549555968e-06, w1=-0.026656861659230344\n",
      "Regularized Logistic Regression(42/299): loss=0.6411475460963495, w0=-2.048036838526551e-06, w1=-0.02667031567030275\n",
      "Regularized Logistic Regression(43/299): loss=0.6411474944120502, w0=-2.0502317528327154e-06, w1=-0.026682261784740837\n",
      "Regularized Logistic Regression(44/299): loss=0.6411474543584159, w0=-2.0522059672878275e-06, w1=-0.026692873367263566\n",
      "Regularized Logistic Regression(45/299): loss=0.6411474232662058, w0=-2.0539816762202846e-06, w1=-0.026702303236763772\n",
      "Regularized Logistic Regression(46/299): loss=0.6411473990920636, w0=-2.0555788425737615e-06, w1=-0.026710686185688823\n",
      "Regularized Logistic Regression(47/299): loss=0.641147380268412, w0=-2.057015422046476e-06, w1=-0.026718141179177143\n",
      "Regularized Logistic Regression(48/299): loss=0.6411473655901744, w0=-2.058307564760487e-06, w1=-0.026724773276128544\n",
      "Regularized Logistic Regression(49/299): loss=0.6411473541290886, w0=-2.0594697967051935e-06, w1=-0.026730675308649244\n",
      "Regularized Logistic Regression(50/299): loss=0.6411473451687408, w0=-2.060515182976576e-06, w1=-0.026735929351367482\n",
      "Regularized Logistic Regression(51/299): loss=0.6411473381551931, w0=-2.061455474632876e-06, w1=-0.026740608007874644\n",
      "Regularized Logistic Regression(52/299): loss=0.6411473326593771, w0=-2.0623012408063753e-06, w1=-0.026744775537884537\n",
      "Regularized Logistic Regression(53/299): loss=0.641147328348385, w0=-2.0630619875476526e-06, w1=-0.026748488845556792\n",
      "Regularized Logistic Regression(54/299): loss=0.6411473249635031, w0=-2.0637462647316035e-06, w1=-0.026751798346707704\n",
      "Regularized Logistic Regression(55/299): loss=0.6411473223033728, w0=-2.0643617622218816e-06, w1=-0.026754748730294736\n",
      "Regularized Logistic Regression(56/299): loss=0.6411473202110483, w0=-2.064915396370983e-06, w1=-0.026757379627529367\n",
      "Regularized Logistic Regression(57/299): loss=0.6411473185640351, w0=-2.065413387825517e-06, w1=-0.026759726200229855\n",
      "Regularized Logistic Regression(58/299): loss=0.6411473172666072, w0=-2.0658613315093383e-06, w1=-0.02676181965851218\n",
      "Regularized Logistic Regression(59/299): loss=0.6411473162438656, w0=-2.0662642595698144e-06, w1=-0.026763687716605467\n",
      "Regularized Logistic Regression(60/299): loss=0.6411473154371431, w0=-2.066626697993963e-06, w1=-0.026765354994449916\n",
      "Regularized Logistic Regression(61/299): loss=0.6411473148004357, w0=-2.0669527175303483e-06, w1=-0.026766843371750157\n",
      "Regularized Logistic Regression(62/299): loss=0.6411473142976374, w0=-2.0672459794889073e-06, w1=-0.02676817230030737\n",
      "Regularized Logistic Regression(63/299): loss=0.6411473139003807, w0=-2.067509776933508e-06, w1=-0.02676935907971327\n",
      "Regularized Logistic Regression(64/299): loss=0.6411473135863626, w0=-2.0677470717303833e-06, w1=-0.026770419100847882\n",
      "Regularized Logistic Regression(65/299): loss=0.6411473133380302, w0=-2.067960527869135e-06, w1=-0.026771366061063083\n",
      "Regularized Logistic Regression(66/299): loss=0.6411473131415633, w0=-2.0681525414311174e-06, w1=-0.02677221215445456\n",
      "Regularized Logistic Regression(67/299): loss=0.6411473129860682, w0=-2.0683252675424196e-06, w1=-0.02677296824019371\n",
      "Regularized Logistic Regression(68/299): loss=0.6411473128629565, w0=-2.068480644614762e-06, w1=-0.02677364399153063\n",
      "Regularized Logistic Regression(69/299): loss=0.6411473127654509, w0=-2.068620416147138e-06, w1=-0.02677424802775552\n",
      "Regularized Logistic Regression(70/299): loss=0.6411473126882009, w0=-2.06874615033363e-06, w1=-0.026774788031121474\n",
      "Regularized Logistic Regression(71/299): loss=0.6411473126269808, w0=-2.068859257698131e-06, w1=-0.026775270850499022\n",
      "Regularized Logistic Regression(72/299): loss=0.6411473125784508, w0=-2.0689610069545335e-06, w1=-0.02677570259330419\n",
      "Regularized Logistic Regression(73/299): loss=0.6411473125399697, w0=-2.069052539270948e-06, w1=-0.0267760887070677\n",
      "Regularized Logistic Regression(74/299): loss=0.6411473125094496, w0=-2.0691348810985815e-06, w1=-0.026776434051842376\n",
      "Regularized Logistic Regression(75/299): loss=0.6411473124852378, w0=-2.0692089557097145e-06, w1=-0.02677674296450405\n",
      "Regularized Logistic Regression(76/299): loss=0.641147312466026, w0=-2.0692755935747158e-06, w1=-0.026777019315878076\n",
      "Regularized Logistic Regression(77/299): loss=0.6411473124507787, w0=-2.069335541694931e-06, w1=-0.026777266561510426\n",
      "Regularized Logistic Regression(78/299): loss=0.6411473124386753, w0=-2.069389471996541e-06, w1=-0.026777487786807642\n",
      "Regularized Logistic Regression(79/299): loss=0.6411473124290655, w0=-2.0694379888798955e-06, w1=-0.026777685747184646\n",
      "Regularized Logistic Regression(80/299): loss=0.6411473124214347, w0=-2.069481636009325e-06, w1=-0.026777862903786094\n",
      "Regularized Logistic Regression(81/299): loss=0.6411473124153738, w0=-2.069520902419877e-06, w1=-0.026778021455276125\n",
      "Regularized Logistic Regression(82/299): loss=0.6411473124105593, w0=-2.0695562280097185e-06, w1=-0.026778163366143803\n",
      "Regularized Logistic Regression(83/299): loss=0.6411473124067341, w0=-2.0695880084800413e-06, w1=-0.026778290391907936\n",
      "Regularized Logistic Regression(84/299): loss=0.6411473124036945, w0=-2.0696165997780644e-06, w1=-0.026778404101572277\n",
      "Regularized Logistic Regression(85/299): loss=0.6411473124012791, w0=-2.069642322093147e-06, w1=-0.02677850589763429\n",
      "Regularized Logistic Regression(86/299): loss=0.641147312399359, w0=-2.06966546345098e-06, w1=-0.026778597033919216\n",
      "Regularized Logistic Regression(87/299): loss=0.6411473123978327, w0=-2.06968628294631e-06, w1=-0.026778678631479588\n",
      "Regularized Logistic Regression(88/299): loss=0.641147312396619, w0=-2.069705013650551e-06, w1=-0.026778751692774667\n",
      "Regularized Logistic Regression(89/299): loss=0.6411473123956541, w0=-2.069721865227017e-06, w1=-0.02677881711431774\n",
      "Regularized Logistic Regression(90/299): loss=0.6411473123948869, w0=-2.0697370262831717e-06, w1=-0.02677887569795981\n",
      "Regularized Logistic Regression(91/299): loss=0.6411473123942766, w0=-2.0697506664863837e-06, w1=-0.02677892816095842\n",
      "Regularized Logistic Regression(92/299): loss=0.641147312393791, w0=-2.06976293846695e-06, w1=-0.02677897514496588\n",
      "Regularized Logistic Regression(93/299): loss=0.6411473123934046, w0=-2.0697739795297956e-06, w1=-0.026779017224052373\n",
      "Regularized Logistic Regression(94/299): loss=0.6411473123930973, w0=-2.0697839131941125e-06, w1=-0.026779054911868928\n",
      "Regularized Logistic Regression(95/299): loss=0.6411473123928529, w0=-2.0697928505782264e-06, w1=-0.02677908866804696\n",
      "Regularized Logistic Regression(96/299): loss=0.6411473123926584, w0=-2.0698008916452634e-06, w1=-0.026779118903911388\n",
      "Regularized Logistic Regression(97/299): loss=0.6411473123925036, w0=-2.069808126323619e-06, w1=-0.02677914598758761\n",
      "Regularized Logistic Regression(98/299): loss=0.6411473123923803, w0=-2.0698146355148142e-06, w1=-0.026779170248561322\n",
      "Regularized Logistic Regression(99/299): loss=0.6411473123922822, w0=-2.069820492000079e-06, w1=-0.02677919198175602\n",
      "Regularized Logistic Regression(100/299): loss=0.6411473123922041, w0=-2.0698257612558216e-06, w1=-0.026779211451175292\n",
      "Regularized Logistic Regression(101/299): loss=0.6411473123921417, w0=-2.0698305021871697e-06, w1=-0.026779228893160888\n",
      "Regularized Logistic Regression(102/299): loss=0.6411473123920924, w0=-2.0698347677878043e-06, w1=-0.026779244519302758\n",
      "Regularized Logistic Regression(103/299): loss=0.6411473123920529, w0=-2.0698386057334994e-06, w1=-0.026779258519043347\n",
      "Regularized Logistic Regression(104/299): loss=0.6411473123920215, w0=-2.0698420589160377e-06, w1=-0.026779271062004253\n",
      "Regularized Logistic Regression(105/299): loss=0.6411473123919963, w0=-2.0698451659234945e-06, w1=-0.026779282300068992\n",
      "Regularized Logistic Regression(106/299): loss=0.6411473123919765, w0=-2.069847961472275e-06, w1=-0.026779292369244163\n",
      "Regularized Logistic Regression(107/299): loss=0.6411473123919607, w0=-2.069850476795756e-06, w1=-0.026779301391325366\n",
      "Regularized Logistic Regression(108/299): loss=0.641147312391948, w0=-2.069852739993898e-06, w1=-0.026779309475386936\n",
      "Regularized Logistic Regression(109/299): loss=0.6411473123919377, w0=-2.069854776347743e-06, w1=-0.026779316719115168\n",
      "Regularized Logistic Regression(110/299): loss=0.6411473123919298, w0=-2.0698566086023357e-06, w1=-0.02677932321000072\n",
      "Regularized Logistic Regression(111/299): loss=0.6411473123919234, w0=-2.0698582572212225e-06, w1=-0.026779329026406166\n",
      "Regularized Logistic Regression(112/299): loss=0.6411473123919182, w0=-2.069859740615415e-06, w1=-0.02677933423852086\n",
      "Regularized Logistic Regression(113/299): loss=0.6411473123919144, w0=-2.069861075349348e-06, w1=-0.026779338909215814\n",
      "Regularized Logistic Regression(114/299): loss=0.6411473123919111, w0=-2.0698622763261706e-06, w1=-0.02677934309480874\n",
      "Regularized Logistic Regression(115/299): loss=0.6411473123919086, w0=-2.0698633569544324e-06, w1=-0.02677934684574887\n",
      "Regularized Logistic Regression(116/299): loss=0.6411473123919065, w0=-2.0698643292980353e-06, w1=-0.02677935020722964\n",
      "Regularized Logistic Regression(117/299): loss=0.6411473123919048, w0=-2.069865204211137e-06, w1=-0.02677935321973785\n",
      "Regularized Logistic Regression(118/299): loss=0.6411473123919035, w0=-2.0698659914595093e-06, w1=-0.026779355919544705\n",
      "Regularized Logistic Regression(119/299): loss=0.6411473123919024, w0=-2.069866699829723e-06, w1=-0.026779358339145666\n",
      "Regularized Logistic Regression(120/299): loss=0.6411473123919016, w0=-2.069867337227367e-06, w1=-0.02677936050765465\n",
      "Regularized Logistic Regression(121/299): loss=0.6411473123919009, w0=-2.069867910765416e-06, w1=-0.026779362451156127\n",
      "Regularized Logistic Regression(122/299): loss=0.6411473123919003, w0=-2.0698684268437294e-06, w1=-0.026779364193020804\n",
      "Regularized Logistic Regression(123/299): loss=0.6411473123919001, w0=-2.0698688912205735e-06, w1=-0.026779365754189644\n",
      "Regularized Logistic Regression(124/299): loss=0.6411473123918996, w0=-2.0698693090769633e-06, w1=-0.0267793671534251\n",
      "Regularized Logistic Regression(125/299): loss=0.6411473123918993, w0=-2.0698696850745553e-06, w1=-0.026779368407539535\n",
      "Regularized Logistic Regression(126/299): loss=0.641147312391899, w0=-2.0698700234077287e-06, w1=-0.026779369531597676\n",
      "Regularized Logistic Regression(127/299): loss=0.641147312391899, w0=-2.069870327850437e-06, w1=-0.026779370539098894\n",
      "Regularized Logistic Regression(128/299): loss=0.6411473123918987, w0=-2.0698706017983703e-06, w1=-0.026779371442139085\n",
      "Regularized Logistic Regression(129/299): loss=0.6411473123918987, w0=-2.069870848306878e-06, w1=-0.026779372251558353\n",
      "Regularized Logistic Regression(130/299): loss=0.6411473123918986, w0=-2.0698710701250947e-06, w1=-0.026779372977070313\n",
      "Regularized Logistic Regression(131/299): loss=0.6411473123918985, w0=-2.06987126972664e-06, w1=-0.0267793736273797\n",
      "Regularized Logistic Regression(132/299): loss=0.6411473123918987, w0=-2.069871449337238e-06, w1=-0.0267793742102877\n",
      "Regularized Logistic Regression(133/299): loss=0.6411473123918983, w0=-2.0698716109595706e-06, w1=-0.02677937473278542\n",
      "Regularized Logistic Regression(134/299): loss=0.6411473123918986, w0=-2.069871756395634e-06, w1=-0.0267793752011376\n",
      "Regularized Logistic Regression(135/299): loss=0.6411473123918984, w0=-2.0698718872668557e-06, w1=-0.02677937562095943\n",
      "Regularized Logistic Regression(136/299): loss=0.6411473123918981, w0=-2.0698720050321973e-06, w1=-0.02677937599728301\n",
      "Regularized Logistic Regression(137/299): loss=0.6411473123918984, w0=-2.0698721110044347e-06, w1=-0.026779376334617994\n",
      "Regularized Logistic Regression(138/299): loss=0.6411473123918984, w0=-2.0698722063648138e-06, w1=-0.026779376637006583\n",
      "Regularized Logistic Regression(139/299): loss=0.6411473123918985, w0=-2.0698722921762287e-06, w1=-0.02677937690807114\n",
      "Regularized Logistic Regression(140/299): loss=0.6411473123918983, w0=-2.069872369395087e-06, w1=-0.02677937715105882\n",
      "Regularized Logistic Regression(141/299): loss=0.6411473123918983, w0=-2.069872438881979e-06, w1=-0.02677937736887937\n",
      "Regularized Logistic Regression(142/299): loss=0.6411473123918985, w0=-2.0698725014112798e-06, w1=-0.02677937756414093\n",
      "Regularized Logistic Regression(143/299): loss=0.6411473123918984, w0=-2.069872557679791e-06, w1=-0.026779377739181497\n",
      "Regularized Logistic Regression(144/299): loss=0.6411473123918983, w0=-2.069872608314511e-06, w1=-0.02677937789609583\n",
      "Regularized Logistic Regression(145/299): loss=0.6411473123918983, w0=-2.069872653879632e-06, w1=-0.026779378036762027\n",
      "Regularized Logistic Regression(146/299): loss=0.6411473123918984, w0=-2.069872694882833e-06, w1=-0.02677937816286368\n",
      "Regularized Logistic Regression(147/299): loss=0.6411473123918981, w0=-2.0698727317809395e-06, w1=-0.02677937827590975\n",
      "Regularized Logistic Regression(148/299): loss=0.6411473123918983, w0=-2.0698727649850207e-06, w1=-0.026779378377252437\n",
      "Regularized Logistic Regression(149/299): loss=0.6411473123918981, w0=-2.069872794864975e-06, w1=-0.02677937846810372\n",
      "Regularized Logistic Regression(150/299): loss=0.6411473123918981, w0=-2.0698728217536512e-06, w1=-0.026779378549550613\n",
      "Regularized Logistic Regression(151/299): loss=0.6411473123918983, w0=-2.0698728459505644e-06, w1=-0.02677937862256707\n",
      "Regularized Logistic Regression(152/299): loss=0.6411473123918983, w0=-2.0698728677252336e-06, w1=-0.026779378688025997\n",
      "Regularized Logistic Regression(153/299): loss=0.6411473123918984, w0=-2.0698728873201813e-06, w1=-0.02677937874670986\n",
      "Regularized Logistic Regression(154/299): loss=0.6411473123918984, w0=-2.069872904953646e-06, w1=-0.026779378799320342\n",
      "Regularized Logistic Regression(155/299): loss=0.6411473123918981, w0=-2.0698729208220106e-06, w1=-0.02677937884648605\n",
      "Regularized Logistic Regression(156/299): loss=0.6411473123918981, w0=-2.0698729351019917e-06, w1=-0.026779378888771087\n",
      "Regularized Logistic Regression(157/299): loss=0.6411473123918984, w0=-2.0698729479526098e-06, w1=-0.026779378926680707\n",
      "Regularized Logistic Regression(158/299): loss=0.6411473123918985, w0=-2.0698729595169643e-06, w1=-0.026779378960667788\n",
      "Regularized Logistic Regression(159/299): loss=0.6411473123918983, w0=-2.069872969923823e-06, w1=-0.026779378991138164\n",
      "Regularized Logistic Regression(160/299): loss=0.6411473123918983, w0=-2.0698729792890604e-06, w1=-0.026779379018455896\n",
      "Regularized Logistic Regression(161/299): loss=0.6411473123918981, w0=-2.069872987716951e-06, w1=-0.02677937904294768\n",
      "Regularized Logistic Regression(162/299): loss=0.6411473123918984, w0=-2.0698729953013263e-06, w1=-0.026779379064905472\n",
      "Regularized Logistic Regression(163/299): loss=0.6411473123918981, w0=-2.0698730021266255e-06, w1=-0.026779379084591826\n",
      "Regularized Logistic Regression(164/299): loss=0.6411473123918984, w0=-2.069873008268831e-06, w1=-0.026779379102242006\n",
      "Regularized Logistic Regression(165/299): loss=0.641147312391898, w0=-2.0698730137963208e-06, w1=-0.026779379118066386\n",
      "Regularized Logistic Regression(166/299): loss=0.6411473123918984, w0=-2.069873018770624e-06, w1=-0.026779379132253936\n",
      "Regularized Logistic Regression(167/299): loss=0.6411473123918983, w0=-2.0698730232471116e-06, w1=-0.026779379144974157\n",
      "Regularized Logistic Regression(168/299): loss=0.6411473123918981, w0=-2.0698730272756114e-06, w1=-0.026779379156378777\n",
      "Regularized Logistic Regression(169/299): loss=0.6411473123918985, w0=-2.0698730309009623e-06, w1=-0.026779379166603817\n",
      "Regularized Logistic Regression(170/299): loss=0.6411473123918983, w0=-2.069873034163515e-06, w1=-0.026779379175771757\n",
      "Regularized Logistic Regression(171/299): loss=0.6411473123918984, w0=-2.069873037099582e-06, w1=-0.026779379183991325\n",
      "Regularized Logistic Regression(172/299): loss=0.6411473123918981, w0=-2.069873039741839e-06, w1=-0.02677937919136103\n",
      "Regularized Logistic Regression(173/299): loss=0.6411473123918983, w0=-2.069873042119691e-06, w1=-0.026779379197968658\n",
      "Regularized Logistic Regression(174/299): loss=0.6411473123918985, w0=-2.0698730442596e-06, w1=-0.02677937920389288\n",
      "Regularized Logistic Regression(175/299): loss=0.6411473123918984, w0=-2.069873046185379e-06, w1=-0.026779379209204573\n",
      "Regularized Logistic Regression(176/299): loss=0.6411473123918985, w0=-2.069873047918459e-06, w1=-0.02677937921396733\n",
      "Regularized Logistic Regression(177/299): loss=0.6411473123918984, w0=-2.069873049478124e-06, w1=-0.026779379218237715\n",
      "Regularized Logistic Regression(178/299): loss=0.6411473123918984, w0=-2.069873050881729e-06, w1=-0.02677937922206655\n",
      "Regularized Logistic Regression(179/299): loss=0.6411473123918984, w0=-2.0698730521448896e-06, w1=-0.02677937922549939\n",
      "Regularized Logistic Regression(180/299): loss=0.6411473123918981, w0=-2.069873053281662e-06, w1=-0.02677937922857737\n",
      "Regularized Logistic Regression(181/299): loss=0.6411473123918985, w0=-2.069873054304693e-06, w1=-0.0267793792313373\n",
      "Regularized Logistic Regression(182/299): loss=0.6411473123918985, w0=-2.0698730552253655e-06, w1=-0.026779379233811844\n",
      "Regularized Logistic Regression(183/299): loss=0.6411473123918981, w0=-2.069873056053921e-06, w1=-0.02677937923603069\n",
      "Regularized Logistic Regression(184/299): loss=0.6411473123918983, w0=-2.069873056799577e-06, w1=-0.026779379238020283\n",
      "Regularized Logistic Regression(185/299): loss=0.6411473123918983, w0=-2.069873057470631e-06, w1=-0.02677937923980425\n",
      "Regularized Logistic Regression(186/299): loss=0.6411473123918981, w0=-2.0698730580745475e-06, w1=-0.026779379241403757\n",
      "Regularized Logistic Regression(187/299): loss=0.6411473123918983, w0=-2.0698730586180437e-06, w1=-0.026779379242838047\n",
      "Regularized Logistic Regression(188/299): loss=0.6411473123918984, w0=-2.0698730591071633e-06, w1=-0.026779379244124234\n",
      "Regularized Logistic Regression(189/299): loss=0.6411473123918981, w0=-2.0698730595473477e-06, w1=-0.026779379245277235\n",
      "Regularized Logistic Regression(190/299): loss=0.6411473123918983, w0=-2.0698730599434944e-06, w1=-0.026779379246311054\n",
      "Regularized Logistic Regression(191/299): loss=0.6411473123918981, w0=-2.069873060300009e-06, w1=-0.026779379247237958\n",
      "Regularized Logistic Regression(192/299): loss=0.6411473123918983, w0=-2.0698730606208573e-06, w1=-0.02677937924806932\n",
      "Regularized Logistic Regression(193/299): loss=0.6411473123918981, w0=-2.069873060909608e-06, w1=-0.02677937924881462\n",
      "Regularized Logistic Regression(194/299): loss=0.6411473123918983, w0=-2.0698730611694717e-06, w1=-0.0267793792494829\n",
      "Regularized Logistic Regression(195/299): loss=0.6411473123918984, w0=-2.069873061403339e-06, w1=-0.02677937925008214\n",
      "Regularized Logistic Regression(196/299): loss=0.6411473123918983, w0=-2.069873061613812e-06, w1=-0.02677937925061977\n",
      "Regularized Logistic Regression(197/299): loss=0.6411473123918983, w0=-2.0698730618032306e-06, w1=-0.026779379251101538\n",
      "Regularized Logistic Regression(198/299): loss=0.6411473123918983, w0=-2.069873061973701e-06, w1=-0.026779379251533605\n",
      "Regularized Logistic Regression(199/299): loss=0.6411473123918983, w0=-2.069873062127119e-06, w1=-0.02677937925192116\n",
      "Regularized Logistic Regression(200/299): loss=0.6411473123918984, w0=-2.069873062265191e-06, w1=-0.026779379252268667\n",
      "Regularized Logistic Regression(201/299): loss=0.6411473123918983, w0=-2.0698730623894515e-06, w1=-0.026779379252580088\n",
      "Regularized Logistic Regression(202/299): loss=0.6411473123918981, w0=-2.069873062501282e-06, w1=-0.02677937925285936\n",
      "Regularized Logistic Regression(203/299): loss=0.6411473123918981, w0=-2.0698730626019255e-06, w1=-0.0267793792531098\n",
      "Regularized Logistic Regression(204/299): loss=0.6411473123918984, w0=-2.0698730626925013e-06, w1=-0.026779379253334252\n",
      "Regularized Logistic Regression(205/299): loss=0.6411473123918981, w0=-2.0698730627740164e-06, w1=-0.02677937925353561\n",
      "Regularized Logistic Regression(206/299): loss=0.6411473123918983, w0=-2.0698730628473775e-06, w1=-0.026779379253716304\n",
      "Regularized Logistic Regression(207/299): loss=0.6411473123918983, w0=-2.0698730629134007e-06, w1=-0.02677937925387814\n",
      "Regularized Logistic Regression(208/299): loss=0.6411473123918983, w0=-2.06987306297282e-06, w1=-0.02677937925402334\n",
      "Regularized Logistic Regression(209/299): loss=0.6411473123918983, w0=-2.069873063026296e-06, w1=-0.026779379254153583\n",
      "Regularized Logistic Regression(210/299): loss=0.6411473123918985, w0=-2.0698730630744225e-06, w1=-0.026779379254270312\n",
      "Regularized Logistic Regression(211/299): loss=0.6411473123918983, w0=-2.069873063117735e-06, w1=-0.026779379254375\n",
      "Regularized Logistic Regression(212/299): loss=0.641147312391898, w0=-2.069873063156715e-06, w1=-0.02677937925446892\n",
      "Regularized Logistic Regression(213/299): loss=0.6411473123918984, w0=-2.0698730631917963e-06, w1=-0.026779379254553145\n",
      "Regularized Logistic Regression(214/299): loss=0.6411473123918981, w0=-2.0698730632233686e-06, w1=-0.026779379254628675\n",
      "Regularized Logistic Regression(215/299): loss=0.6411473123918984, w0=-2.069873063251783e-06, w1=-0.02677937925469637\n",
      "Regularized Logistic Regression(216/299): loss=0.6411473123918981, w0=-2.0698730632773555e-06, w1=-0.026779379254757082\n",
      "Regularized Logistic Regression(217/299): loss=0.6411473123918981, w0=-2.0698730633003707e-06, w1=-0.026779379254811546\n",
      "Regularized Logistic Regression(218/299): loss=0.6411473123918984, w0=-2.0698730633210836e-06, w1=-0.0267793792548604\n",
      "Regularized Logistic Regression(219/299): loss=0.6411473123918985, w0=-2.069873063339725e-06, w1=-0.02677937925490422\n",
      "Regularized Logistic Regression(220/299): loss=0.6411473123918983, w0=-2.0698730633565023e-06, w1=-0.026779379254943524\n",
      "Regularized Logistic Regression(221/299): loss=0.6411473123918984, w0=-2.0698730633716015e-06, w1=-0.0267793792549788\n",
      "Regularized Logistic Regression(222/299): loss=0.6411473123918983, w0=-2.0698730633851904e-06, w1=-0.02677937925501041\n",
      "Regularized Logistic Regression(223/299): loss=0.6411473123918983, w0=-2.0698730633974208e-06, w1=-0.026779379255038732\n",
      "Regularized Logistic Regression(224/299): loss=0.6411473123918983, w0=-2.069873063408428e-06, w1=-0.026779379255064167\n",
      "Regularized Logistic Regression(225/299): loss=0.6411473123918983, w0=-2.069873063418334e-06, w1=-0.02677937925508698\n",
      "Regularized Logistic Regression(226/299): loss=0.6411473123918984, w0=-2.0698730634272494e-06, w1=-0.02677937925510745\n",
      "Regularized Logistic Regression(227/299): loss=0.6411473123918985, w0=-2.0698730634352734e-06, w1=-0.026779379255125833\n",
      "Regularized Logistic Regression(228/299): loss=0.6411473123918983, w0=-2.0698730634424948e-06, w1=-0.02677937925514232\n",
      "Regularized Logistic Regression(229/299): loss=0.6411473123918985, w0=-2.0698730634489936e-06, w1=-0.026779379255157075\n",
      "Regularized Logistic Regression(230/299): loss=0.6411473123918983, w0=-2.069873063454843e-06, w1=-0.026779379255170287\n",
      "Regularized Logistic Regression(231/299): loss=0.6411473123918981, w0=-2.0698730634601067e-06, w1=-0.02677937925518216\n",
      "Regularized Logistic Regression(232/299): loss=0.6411473123918983, w0=-2.0698730634648446e-06, w1=-0.026779379255192828\n",
      "Regularized Logistic Regression(233/299): loss=0.6411473123918985, w0=-2.0698730634691085e-06, w1=-0.026779379255202407\n",
      "Regularized Logistic Regression(234/299): loss=0.6411473123918981, w0=-2.069873063472946e-06, w1=-0.026779379255211004\n",
      "Regularized Logistic Regression(235/299): loss=0.6411473123918983, w0=-2.0698730634764e-06, w1=-0.026779379255218727\n",
      "Regularized Logistic Regression(236/299): loss=0.6411473123918984, w0=-2.069873063479508e-06, w1=-0.026779379255225666\n",
      "Regularized Logistic Regression(237/299): loss=0.6411473123918985, w0=-2.0698730634823057e-06, w1=-0.026779379255231894\n",
      "Regularized Logistic Regression(238/299): loss=0.6411473123918983, w0=-2.0698730634848235e-06, w1=-0.026779379255237493\n",
      "Regularized Logistic Regression(239/299): loss=0.6411473123918984, w0=-2.0698730634870898e-06, w1=-0.026779379255242527\n",
      "Regularized Logistic Regression(240/299): loss=0.6411473123918983, w0=-2.0698730634891294e-06, w1=-0.026779379255247048\n",
      "Regularized Logistic Regression(241/299): loss=0.6411473123918983, w0=-2.0698730634909654e-06, w1=-0.02677937925525111\n",
      "Regularized Logistic Regression(242/299): loss=0.6411473123918983, w0=-2.0698730634926175e-06, w1=-0.02677937925525476\n",
      "Regularized Logistic Regression(243/299): loss=0.6411473123918981, w0=-2.0698730634941045e-06, w1=-0.02677937925525804\n",
      "Regularized Logistic Regression(244/299): loss=0.6411473123918984, w0=-2.069873063495443e-06, w1=-0.02677937925526098\n",
      "Regularized Logistic Regression(245/299): loss=0.6411473123918984, w0=-2.0698730634966473e-06, w1=-0.02677937925526362\n",
      "Regularized Logistic Regression(246/299): loss=0.6411473123918983, w0=-2.0698730634977315e-06, w1=-0.026779379255265988\n",
      "Regularized Logistic Regression(247/299): loss=0.6411473123918983, w0=-2.0698730634987073e-06, w1=-0.026779379255268108\n",
      "Regularized Logistic Regression(248/299): loss=0.6411473123918983, w0=-2.069873063499585e-06, w1=-0.026779379255269884\n",
      "Regularized Logistic Regression(249/299): loss=0.6411473123918983, w0=-2.0698730635003755e-06, w1=-0.02677937925527148\n",
      "Regularized Logistic Regression(250/299): loss=0.641147312391898, w0=-2.069873063501087e-06, w1=-0.026779379255272892\n",
      "Regularized Logistic Regression(251/299): loss=0.6411473123918981, w0=-2.0698730635017274e-06, w1=-0.026779379255274162\n",
      "Regularized Logistic Regression(252/299): loss=0.6411473123918983, w0=-2.0698730635023038e-06, w1=-0.026779379255275296\n",
      "Regularized Logistic Regression(253/299): loss=0.6411473123918983, w0=-2.069873063502822e-06, w1=-0.026779379255276313\n",
      "Regularized Logistic Regression(254/299): loss=0.6411473123918984, w0=-2.069873063503289e-06, w1=-0.026779379255277225\n",
      "Regularized Logistic Regression(255/299): loss=0.6411473123918984, w0=-2.069873063503709e-06, w1=-0.026779379255278044\n",
      "Regularized Logistic Regression(256/299): loss=0.6411473123918984, w0=-2.069873063504087e-06, w1=-0.02677937925527878\n",
      "Regularized Logistic Regression(257/299): loss=0.6411473123918984, w0=-2.0698730635044273e-06, w1=-0.026779379255279435\n",
      "Regularized Logistic Regression(258/299): loss=0.6411473123918984, w0=-2.0698730635047335e-06, w1=-0.026779379255280022\n",
      "Regularized Logistic Regression(259/299): loss=0.6411473123918984, w0=-2.069873063505009e-06, w1=-0.026779379255280546\n",
      "Regularized Logistic Regression(260/299): loss=0.6411473123918984, w0=-2.0698730635052574e-06, w1=-0.026779379255281018\n",
      "Regularized Logistic Regression(261/299): loss=0.6411473123918983, w0=-2.0698730635054806e-06, w1=-0.02677937925528144\n",
      "Regularized Logistic Regression(262/299): loss=0.6411473123918984, w0=-2.0698730635056813e-06, w1=-0.026779379255281822\n",
      "Regularized Logistic Regression(263/299): loss=0.6411473123918983, w0=-2.069873063505862e-06, w1=-0.026779379255282162\n",
      "Regularized Logistic Regression(264/299): loss=0.6411473123918983, w0=-2.0698730635060248e-06, w1=-0.026779379255282468\n",
      "Regularized Logistic Regression(265/299): loss=0.6411473123918984, w0=-2.0698730635061713e-06, w1=-0.026779379255282742\n",
      "Regularized Logistic Regression(266/299): loss=0.6411473123918984, w0=-2.069873063506303e-06, w1=-0.026779379255282988\n",
      "Regularized Logistic Regression(267/299): loss=0.6411473123918983, w0=-2.0698730635064216e-06, w1=-0.02677937925528321\n",
      "Regularized Logistic Regression(268/299): loss=0.6411473123918984, w0=-2.0698730635065283e-06, w1=-0.0267793792552834\n",
      "Regularized Logistic Regression(269/299): loss=0.6411473123918984, w0=-2.0698730635066245e-06, w1=-0.02677937925528358\n",
      "Regularized Logistic Regression(270/299): loss=0.6411473123918984, w0=-2.069873063506711e-06, w1=-0.026779379255283734\n",
      "Regularized Logistic Regression(271/299): loss=0.6411473123918984, w0=-2.069873063506789e-06, w1=-0.026779379255283873\n",
      "Regularized Logistic Regression(272/299): loss=0.6411473123918984, w0=-2.0698730635068587e-06, w1=-0.026779379255284005\n",
      "Regularized Logistic Regression(273/299): loss=0.6411473123918985, w0=-2.0698730635069218e-06, w1=-0.026779379255284116\n",
      "Regularized Logistic Regression(274/299): loss=0.6411473123918985, w0=-2.0698730635069785e-06, w1=-0.026779379255284223\n",
      "Regularized Logistic Regression(275/299): loss=0.6411473123918985, w0=-2.0698730635070294e-06, w1=-0.026779379255284314\n",
      "Regularized Logistic Regression(276/299): loss=0.6411473123918985, w0=-2.069873063507075e-06, w1=-0.02677937925528439\n",
      "Regularized Logistic Regression(277/299): loss=0.6411473123918985, w0=-2.0698730635071166e-06, w1=-0.02677937925528446\n",
      "Regularized Logistic Regression(278/299): loss=0.6411473123918985, w0=-2.069873063507154e-06, w1=-0.02677937925528452\n",
      "Regularized Logistic Regression(279/299): loss=0.6411473123918985, w0=-2.0698730635071873e-06, w1=-0.026779379255284577\n",
      "Regularized Logistic Regression(280/299): loss=0.6411473123918985, w0=-2.0698730635072174e-06, w1=-0.026779379255284626\n",
      "Regularized Logistic Regression(281/299): loss=0.6411473123918985, w0=-2.0698730635072445e-06, w1=-0.02677937925528467\n",
      "Regularized Logistic Regression(282/299): loss=0.6411473123918985, w0=-2.069873063507269e-06, w1=-0.02677937925528471\n",
      "Regularized Logistic Regression(283/299): loss=0.6411473123918985, w0=-2.069873063507291e-06, w1=-0.026779379255284744\n",
      "Regularized Logistic Regression(284/299): loss=0.6411473123918985, w0=-2.069873063507311e-06, w1=-0.026779379255284775\n",
      "Regularized Logistic Regression(285/299): loss=0.6411473123918985, w0=-2.0698730635073288e-06, w1=-0.026779379255284803\n",
      "Regularized Logistic Regression(286/299): loss=0.6411473123918985, w0=-2.069873063507345e-06, w1=-0.026779379255284827\n",
      "Regularized Logistic Regression(287/299): loss=0.6411473123918985, w0=-2.0698730635073593e-06, w1=-0.02677937925528485\n",
      "Regularized Logistic Regression(288/299): loss=0.6411473123918985, w0=-2.0698730635073724e-06, w1=-0.026779379255284872\n",
      "Regularized Logistic Regression(289/299): loss=0.6411473123918985, w0=-2.0698730635073843e-06, w1=-0.02677937925528489\n",
      "Regularized Logistic Regression(290/299): loss=0.6411473123918985, w0=-2.069873063507395e-06, w1=-0.026779379255284907\n",
      "Regularized Logistic Regression(291/299): loss=0.6411473123918985, w0=-2.069873063507404e-06, w1=-0.02677937925528492\n",
      "Regularized Logistic Regression(292/299): loss=0.6411473123918985, w0=-2.0698730635074126e-06, w1=-0.026779379255284935\n",
      "Regularized Logistic Regression(293/299): loss=0.6411473123918985, w0=-2.0698730635074203e-06, w1=-0.026779379255284945\n",
      "Regularized Logistic Regression(294/299): loss=0.6411473123918985, w0=-2.069873063507427e-06, w1=-0.026779379255284955\n",
      "Regularized Logistic Regression(295/299): loss=0.6411473123918985, w0=-2.0698730635074334e-06, w1=-0.026779379255284966\n",
      "Regularized Logistic Regression(296/299): loss=0.6411473123918985, w0=-2.069873063507439e-06, w1=-0.026779379255284973\n",
      "Regularized Logistic Regression(297/299): loss=0.6411473123918985, w0=-2.069873063507444e-06, w1=-0.02677937925528498\n",
      "Regularized Logistic Regression(298/299): loss=0.6411473123918985, w0=-2.0698730635074486e-06, w1=-0.026779379255284987\n",
      "Regularized Logistic Regression(299/299): loss=0.6411473123918985, w0=-2.069873063507453e-06, w1=-0.026779379255284994\n",
      "Regularized Logistic Regression(0/99): loss=0.693147180559945, w0=-4.176586058555737e-07, w1=-0.007317796433195506\n",
      "Regularized Logistic Regression(1/99): loss=0.6631135260714237, w0=-8.326678932672681e-07, w1=-0.013966463329453083\n",
      "Regularized Logistic Regression(2/99): loss=0.6392199453236909, w0=-1.2463996377792245e-06, w1=-0.02006230340284925\n",
      "Regularized Logistic Regression(3/99): loss=0.6200710370267274, w0=-1.659551661864677e-06, w1=-0.02569522134138946\n",
      "Regularized Logistic Regression(4/99): loss=0.6046022792758989, w0=-2.072444402594911e-06, w1=-0.0309355604619541\n",
      "Regularized Logistic Regression(5/99): loss=0.591999855667077, w0=-2.4851968595250796e-06, w1=-0.03583929882076505\n",
      "Regularized Logistic Regression(6/99): loss=0.5816415444090455, w0=-2.897826050292845e-06, w1=-0.0404517250778323\n",
      "Regularized Logistic Regression(7/99): loss=0.573051100350308, w0=-3.3103013001483137e-06, w1=-0.044810007313822016\n",
      "Regularized Logistic Regression(8/99): loss=0.565862971711694, w0=-3.722572971608584e-06, w1=-0.04894501406638419\n",
      "Regularized Logistic Regression(9/99): loss=0.5597953222148011, w0=-4.134587116277433e-06, w1=-0.05288263696516249\n",
      "Regularized Logistic Regression(10/99): loss=0.5546296367980985, w0=-4.5462925267862145e-06, w1=-0.05664477610412627\n",
      "Regularized Logistic Regression(11/99): loss=0.5501954168532402, w0=-4.9576437652457895e-06, w1=-0.06025009092644788\n",
      "Regularized Logistic Regression(12/99): loss=0.5463587305604463, w0=-5.36860211598123e-06, w1=-0.06371458319492389\n",
      "Regularized Logistic Regression(13/99): loss=0.5430136451115412, w0=-5.779135512043805e-06, w1=-0.06705205642927656\n",
      "Regularized Logistic Regression(14/99): loss=0.5400757982867492, w0=-6.189217994170834e-06, w1=-0.07027448238043728\n",
      "Regularized Logistic Regression(15/99): loss=0.5374775543414433, w0=-6.598828994180225e-06, w1=-0.07339229626199492\n",
      "Regularized Logistic Regression(16/99): loss=0.5351643340936991, w0=-7.007952590621107e-06, w1=-0.07641463658866081\n",
      "Regularized Logistic Regression(17/99): loss=0.5330918178514118, w0=-7.416576807146122e-06, w1=-0.07934954144234359\n",
      "Regularized Logistic Regression(18/99): loss=0.5312238000285677, w0=-7.824692983108683e-06, w1=-0.08220411013529709\n",
      "Regularized Logistic Regression(19/99): loss=0.5295305329304837, w0=-8.232295224717367e-06, w1=-0.084984637170432\n",
      "Regularized Logistic Regression(20/99): loss=0.5279874398822021, w0=-8.63937993462322e-06, w1=-0.08769672386585056\n",
      "Regularized Logistic Regression(21/99): loss=0.526574108954785, w0=-9.045945413104483e-06, w1=-0.09034537185645178\n",
      "Regularized Logistic Regression(22/99): loss=0.5252735012156958, w0=-9.451991522325304e-06, w1=-0.09293506180514209\n",
      "Regularized Logistic Regression(23/99): loss=0.5240713240256595, w0=-9.857519404993317e-06, w1=-0.09546981997776516\n",
      "Regularized Logistic Regression(24/99): loss=0.5229555321087854, w0=-1.026253124931895e-05, w1=-0.09795327480847277\n",
      "Regularized Logistic Regression(25/99): loss=0.5219159281443849, w0=-1.0667030093055475e-05, w1=-0.1003887051692537\n",
      "Regularized Logistic Regression(26/99): loss=0.5209438413353147, w0=-1.107101966034925e-05, w1=-0.10277908173177465\n",
      "Regularized Logistic Regression(27/99): loss=0.5200318674221431, w0=-1.1474504226043188e-05, w1=-0.10512710255157136\n",
      "Regularized Logistic Regression(28/99): loss=0.5191736573841238, w0=-1.1877488502904083e-05, w1=-0.10743522379886165\n",
      "Regularized Logistic Regression(29/99): loss=0.5183637449217599, w0=-1.2279977547969364e-05, w1=-0.10970568639539036\n",
      "Regularized Logistic Regression(30/99): loss=0.5175974049877883, w0=-1.2681976684830828e-05, w1=-0.11194053918399616\n",
      "Regularized Logistic Regression(31/99): loss=0.5168705372961805, w0=-1.3083491439200034e-05, w1=-0.11414165915025225\n",
      "Regularized Logistic Regression(32/99): loss=0.5161795700188689, w0=-1.3484527485542887e-05, w1=-0.11631076912834214\n",
      "Regularized Logistic Regression(33/99): loss=0.5155213798708965, w0=-1.3885090602941092e-05, w1=-0.11844945335220046\n",
      "Regularized Logistic Regression(34/99): loss=0.514893225555969, w0=-1.428518663864664e-05, w1=-0.12055917115468621\n",
      "Regularized Logistic Regression(35/99): loss=0.5142926921477957, w0=-1.4684821478052e-05, w1=-0.12264126906964179\n",
      "Regularized Logistic Regression(36/99): loss=0.5137176444570565, w0=-1.5084001020011864e-05, w1=-0.12469699155212664\n",
      "Regularized Logistic Regression(37/99): loss=0.513166187808689, w0=-1.5482731156629265e-05, w1=-0.12672749049933665\n",
      "Regularized Logistic Regression(38/99): loss=0.5126366349517666, w0=-1.5881017756766122e-05, w1=-0.1287338337274515\n",
      "Regularized Logistic Regression(39/99): loss=0.5121274780614988, w0=-1.6278866652660395e-05, w1=-0.130717012536896\n",
      "Regularized Logistic Regression(40/99): loss=0.5116373649829121, w0=-1.6676283629133893e-05, w1=-0.13267794847943723\n",
      "Regularized Logistic Regression(41/99): loss=0.5111650790185701, w0=-1.707327441495935e-05, w1=-0.1346174994245271\n",
      "Regularized Logistic Regression(42/99): loss=0.5107095216860728, w0=-1.7469844676026086e-05, w1=-0.13653646500878697\n",
      "Regularized Logistic Regression(43/99): loss=0.510269697971077, w0=-1.786600001000227e-05, w1=-0.13843559154111518\n",
      "Regularized Logistic Regression(44/99): loss=0.5098447036829256, w0=-1.826174594224115e-05, w1=-0.14031557642620707\n",
      "Regularized Logistic Regression(45/99): loss=0.5094337145863918, w0=-1.865708792271954e-05, w1=-0.1421770721610296\n",
      "Regularized Logistic Regression(46/99): loss=0.509035977037449, w0=-1.9052031323831346e-05, w1=-0.14402068995175682\n",
      "Regularized Logistic Regression(47/99): loss=0.5086507998956974, w0=-1.9446581438887525e-05, w1=-0.14584700299265368\n",
      "Regularized Logistic Regression(48/99): loss=0.508277547522933, w0=-1.9840743481198193e-05, w1=-0.14765654944322187\n",
      "Regularized Logistic Regression(49/99): loss=0.5079156337078352, w0=-2.0234522583632617e-05, w1=-0.149449835135481\n",
      "Regularized Logistic Regression(50/99): loss=0.5075645163820302, w0=-2.0627923798569916e-05, w1=-0.15122733603941613\n",
      "Regularized Logistic Regression(51/99): loss=0.5072236930138041, w0=-2.1020952098167466e-05, w1=-0.15298950051130916\n",
      "Regularized Logistic Regression(52/99): loss=0.5068926965832765, w0=-2.141361237488602e-05, w1=-0.1547367513467925\n",
      "Regularized Logistic Regression(53/99): loss=0.5065710920574863, w0=-2.180590944222054e-05, w1=-0.15646948765796123\n",
      "Regularized Logistic Regression(54/99): loss=0.5062584732961344, w0=-2.2197848035594256e-05, w1=-0.15818808659170486\n",
      "Regularized Logistic Regression(55/99): loss=0.5059544603290268, w0=-2.25894328133805e-05, w1=-0.15989290490451427\n",
      "Regularized Logistic Regression(56/99): loss=0.5056586969549464, w0=-2.2980668358022874e-05, w1=-0.16158428040735573\n",
      "Regularized Logistic Regression(57/99): loss=0.5053708486190084, w0=-2.3371559177229304e-05, w1=-0.1632625332927432\n",
      "Regularized Logistic Regression(58/99): loss=0.5050906005317397, w0=-2.3762109705219752e-05, w1=-0.16492796735485793\n",
      "Regularized Logistic Regression(59/99): loss=0.5048176559983674, w0=-2.415232430401092e-05, w1=-0.16658087111243042\n",
      "Regularized Logistic Regression(60/99): loss=0.5045517349312475, w0=-2.454220726472421e-05, w1=-0.16822151884310738\n",
      "Regularized Logistic Regression(61/99): loss=0.5042925725221558, w0=-2.4931762808905677e-05, w1=-0.16985017153713675\n",
      "Regularized Logistic Regression(62/99): loss=0.5040399180543714, w0=-2.532099508984895e-05, w1=-0.1714670777774306\n",
      "Regularized Logistic Regression(63/99): loss=0.503793533837246, w0=-2.5709908193913612e-05, w1=-0.1730724745523638\n",
      "Regularized Logistic Regression(64/99): loss=0.503553194248285, w0=-2.6098506141833215e-05, w1=-0.1746665880070547\n",
      "Regularized Logistic Regression(65/99): loss=0.5033186848697925, w0=-2.6486792890008153e-05, w1=-0.17624963413832057\n",
      "Regularized Logistic Regression(66/99): loss=0.5030898017088369, w0=-2.6874772331779745e-05, w1=-0.17782181943801462\n",
      "Regularized Logistic Regression(67/99): loss=0.5028663504907914, w0=-2.7262448298682696e-05, w1=-0.17938334148900625\n",
      "Regularized Logistic Regression(68/99): loss=0.5026481460179569, w0=-2.764982456167376e-05, w1=-0.18093438951767946\n",
      "Regularized Logistic Regression(69/99): loss=0.5024350115858857, w0=-2.803690483233509e-05, w1=-0.18247514490646838\n",
      "Regularized Logistic Regression(70/99): loss=0.5022267784509505, w0=-2.8423692764051123e-05, w1=-0.18400578166963547\n",
      "Regularized Logistic Regression(71/99): loss=0.5020232853435381, w0=-2.8810191953158416e-05, w1=-0.1855264668952107\n",
      "Regularized Logistic Regression(72/99): loss=0.5018243780219276, w0=-2.9196405940067947e-05, w1=-0.1870373611557544\n",
      "Regularized Logistic Regression(73/99): loss=0.5016299088625449, w0=-2.9582338210359833e-05, w1=-0.18853861889037685\n",
      "Regularized Logistic Regression(74/99): loss=0.5014397364827939, w0=-2.9967992195850534e-05, w1=-0.19003038876023712\n",
      "Regularized Logistic Regression(75/99): loss=0.5012537253931388, w0=-3.035337127563283e-05, w1=-0.19151281397955497\n",
      "Regularized Logistic Regression(76/99): loss=0.5010717456754951, w0=-3.0738478777088894e-05, w1=-0.19298603262399797\n",
      "Regularized Logistic Regression(77/99): loss=0.5008936726853491, w0=-3.112331797687706e-05, w1=-0.19445017791815383\n",
      "Regularized Logistic Regression(78/99): loss=0.5007193867753084, w0=-3.150789210189273e-05, w1=-0.19590537850365153\n",
      "Regularized Logistic Regression(79/99): loss=0.5005487730380689, w0=-3.189220433020415e-05, w1=-0.19735175868937224\n",
      "Regularized Logistic Regression(80/99): loss=0.5003817210669955, w0=-3.227625779196364e-05, w1=-0.19878943868507226\n",
      "Regularized Logistic Regression(81/99): loss=0.5002181247327286, w0=-3.266005557029504e-05, w1=-0.20021853481963062\n",
      "Regularized Logistic Regression(82/99): loss=0.5000578819743919, w0=-3.304360070215803e-05, w1=-0.20163915974504584\n",
      "Regularized Logistic Regression(83/99): loss=0.4999008946041427, w0=-3.342689617919003e-05, w1=-0.20305142262720757\n",
      "Regularized Logistic Regression(84/99): loss=0.49974706812393116, w0=-3.380994494852646e-05, w1=-0.2044554293243955\n",
      "Regularized Logistic Regression(85/99): loss=0.49959631155346196, w0=-3.4192749913599975e-05, w1=-0.2058512825543818\n",
      "Regularized Logistic Regression(86/99): loss=0.4994485372684466, w0=-3.457531393491947e-05, w1=-0.20723908205094502\n",
      "Regularized Logistic Regression(87/99): loss=0.4993036608483414, w0=-3.495763983082945e-05, w1=-0.20861892471054128\n",
      "Regularized Logistic Regression(88/99): loss=0.49916160093282874, w0=-3.533973037825049e-05, w1=-0.20999090472982407\n",
      "Regularized Logistic Regression(89/99): loss=0.49902227908638985, w0=-3.572158831340139e-05, w1=-0.21135511373464885\n",
      "Regularized Logistic Regression(90/99): loss=0.49888561967036704, w0=-3.610321633250372e-05, w1=-0.21271164090115388\n",
      "Regularized Logistic Regression(91/99): loss=0.49875154972197644, w0=-3.648461709246926e-05, w1=-0.2140605730694603\n",
      "Regularized Logistic Regression(92/99): loss=0.49861999883978153, w0=-3.686579321157101e-05, w1=-0.21540199485050063\n",
      "Regularized Logistic Regression(93/99): loss=0.4984908990751816, w0=-3.7246747270098314e-05, w1=-0.21673598872644054\n",
      "Regularized Logistic Regression(94/99): loss=0.49836418482950906, w0=-3.762748181099656e-05, w1=-0.21806263514512825\n",
      "Regularized Logistic Regression(95/99): loss=0.49823979275636404, w0=-3.800799934049212e-05, w1=-0.21938201260897636\n",
      "Regularized Logistic Regression(96/99): loss=0.49811766166884913, w0=-3.838830232870289e-05, w1=-0.2206941977586454\n",
      "Regularized Logistic Regression(97/99): loss=0.49799773245139295, w0=-3.8768393210235035e-05, w1=-0.22199926545187776\n",
      "Regularized Logistic Regression(98/99): loss=0.4978799479758774, w0=-3.914827438476623e-05, w1=-0.22329728883780264\n",
      "Regularized Logistic Regression(99/99): loss=0.4977642530218078, w0=-3.9527948217616055e-05, w1=-0.2245883394270113\n",
      "Regularized Logistic Regression(0/199): loss=0.693147180559945, w0=-4.176586058555737e-07, w1=-0.007317796433195506\n",
      "Regularized Logistic Regression(1/199): loss=0.6631135260714237, w0=-8.326678932672681e-07, w1=-0.013966463329453083\n",
      "Regularized Logistic Regression(2/199): loss=0.6392199453236909, w0=-1.2463996377792245e-06, w1=-0.02006230340284925\n",
      "Regularized Logistic Regression(3/199): loss=0.6200710370267274, w0=-1.659551661864677e-06, w1=-0.02569522134138946\n",
      "Regularized Logistic Regression(4/199): loss=0.6046022792758989, w0=-2.072444402594911e-06, w1=-0.0309355604619541\n",
      "Regularized Logistic Regression(5/199): loss=0.591999855667077, w0=-2.4851968595250796e-06, w1=-0.03583929882076505\n",
      "Regularized Logistic Regression(6/199): loss=0.5816415444090455, w0=-2.897826050292845e-06, w1=-0.0404517250778323\n",
      "Regularized Logistic Regression(7/199): loss=0.573051100350308, w0=-3.3103013001483137e-06, w1=-0.044810007313822016\n",
      "Regularized Logistic Regression(8/199): loss=0.565862971711694, w0=-3.722572971608584e-06, w1=-0.04894501406638419\n",
      "Regularized Logistic Regression(9/199): loss=0.5597953222148011, w0=-4.134587116277433e-06, w1=-0.05288263696516249\n",
      "Regularized Logistic Regression(10/199): loss=0.5546296367980985, w0=-4.5462925267862145e-06, w1=-0.05664477610412627\n",
      "Regularized Logistic Regression(11/199): loss=0.5501954168532402, w0=-4.9576437652457895e-06, w1=-0.06025009092644788\n",
      "Regularized Logistic Regression(12/199): loss=0.5463587305604463, w0=-5.36860211598123e-06, w1=-0.06371458319492389\n",
      "Regularized Logistic Regression(13/199): loss=0.5430136451115412, w0=-5.779135512043805e-06, w1=-0.06705205642927656\n",
      "Regularized Logistic Regression(14/199): loss=0.5400757982867492, w0=-6.189217994170834e-06, w1=-0.07027448238043728\n",
      "Regularized Logistic Regression(15/199): loss=0.5374775543414433, w0=-6.598828994180225e-06, w1=-0.07339229626199492\n",
      "Regularized Logistic Regression(16/199): loss=0.5351643340936991, w0=-7.007952590621107e-06, w1=-0.07641463658866081\n",
      "Regularized Logistic Regression(17/199): loss=0.5330918178514118, w0=-7.416576807146122e-06, w1=-0.07934954144234359\n",
      "Regularized Logistic Regression(18/199): loss=0.5312238000285677, w0=-7.824692983108683e-06, w1=-0.08220411013529709\n",
      "Regularized Logistic Regression(19/199): loss=0.5295305329304837, w0=-8.232295224717367e-06, w1=-0.084984637170432\n",
      "Regularized Logistic Regression(20/199): loss=0.5279874398822021, w0=-8.63937993462322e-06, w1=-0.08769672386585056\n",
      "Regularized Logistic Regression(21/199): loss=0.526574108954785, w0=-9.045945413104483e-06, w1=-0.09034537185645178\n",
      "Regularized Logistic Regression(22/199): loss=0.5252735012156958, w0=-9.451991522325304e-06, w1=-0.09293506180514209\n",
      "Regularized Logistic Regression(23/199): loss=0.5240713240256595, w0=-9.857519404993317e-06, w1=-0.09546981997776516\n",
      "Regularized Logistic Regression(24/199): loss=0.5229555321087854, w0=-1.026253124931895e-05, w1=-0.09795327480847277\n",
      "Regularized Logistic Regression(25/199): loss=0.5219159281443849, w0=-1.0667030093055475e-05, w1=-0.1003887051692537\n",
      "Regularized Logistic Regression(26/199): loss=0.5209438413353147, w0=-1.107101966034925e-05, w1=-0.10277908173177465\n",
      "Regularized Logistic Regression(27/199): loss=0.5200318674221431, w0=-1.1474504226043188e-05, w1=-0.10512710255157136\n",
      "Regularized Logistic Regression(28/199): loss=0.5191736573841238, w0=-1.1877488502904083e-05, w1=-0.10743522379886165\n",
      "Regularized Logistic Regression(29/199): loss=0.5183637449217599, w0=-1.2279977547969364e-05, w1=-0.10970568639539036\n",
      "Regularized Logistic Regression(30/199): loss=0.5175974049877883, w0=-1.2681976684830828e-05, w1=-0.11194053918399616\n",
      "Regularized Logistic Regression(31/199): loss=0.5168705372961805, w0=-1.3083491439200034e-05, w1=-0.11414165915025225\n",
      "Regularized Logistic Regression(32/199): loss=0.5161795700188689, w0=-1.3484527485542887e-05, w1=-0.11631076912834214\n",
      "Regularized Logistic Regression(33/199): loss=0.5155213798708965, w0=-1.3885090602941092e-05, w1=-0.11844945335220046\n",
      "Regularized Logistic Regression(34/199): loss=0.514893225555969, w0=-1.428518663864664e-05, w1=-0.12055917115468621\n",
      "Regularized Logistic Regression(35/199): loss=0.5142926921477957, w0=-1.4684821478052e-05, w1=-0.12264126906964179\n",
      "Regularized Logistic Regression(36/199): loss=0.5137176444570565, w0=-1.5084001020011864e-05, w1=-0.12469699155212664\n",
      "Regularized Logistic Regression(37/199): loss=0.513166187808689, w0=-1.5482731156629265e-05, w1=-0.12672749049933665\n",
      "Regularized Logistic Regression(38/199): loss=0.5126366349517666, w0=-1.5881017756766122e-05, w1=-0.1287338337274515\n",
      "Regularized Logistic Regression(39/199): loss=0.5121274780614988, w0=-1.6278866652660395e-05, w1=-0.130717012536896\n",
      "Regularized Logistic Regression(40/199): loss=0.5116373649829121, w0=-1.6676283629133893e-05, w1=-0.13267794847943723\n",
      "Regularized Logistic Regression(41/199): loss=0.5111650790185701, w0=-1.707327441495935e-05, w1=-0.1346174994245271\n",
      "Regularized Logistic Regression(42/199): loss=0.5107095216860728, w0=-1.7469844676026086e-05, w1=-0.13653646500878697\n",
      "Regularized Logistic Regression(43/199): loss=0.510269697971077, w0=-1.786600001000227e-05, w1=-0.13843559154111518\n",
      "Regularized Logistic Regression(44/199): loss=0.5098447036829256, w0=-1.826174594224115e-05, w1=-0.14031557642620707\n",
      "Regularized Logistic Regression(45/199): loss=0.5094337145863918, w0=-1.865708792271954e-05, w1=-0.1421770721610296\n",
      "Regularized Logistic Regression(46/199): loss=0.509035977037449, w0=-1.9052031323831346e-05, w1=-0.14402068995175682\n",
      "Regularized Logistic Regression(47/199): loss=0.5086507998956974, w0=-1.9446581438887525e-05, w1=-0.14584700299265368\n",
      "Regularized Logistic Regression(48/199): loss=0.508277547522933, w0=-1.9840743481198193e-05, w1=-0.14765654944322187\n",
      "Regularized Logistic Regression(49/199): loss=0.5079156337078352, w0=-2.0234522583632617e-05, w1=-0.149449835135481\n",
      "Regularized Logistic Regression(50/199): loss=0.5075645163820302, w0=-2.0627923798569916e-05, w1=-0.15122733603941613\n",
      "Regularized Logistic Regression(51/199): loss=0.5072236930138041, w0=-2.1020952098167466e-05, w1=-0.15298950051130916\n",
      "Regularized Logistic Regression(52/199): loss=0.5068926965832765, w0=-2.141361237488602e-05, w1=-0.1547367513467925\n",
      "Regularized Logistic Regression(53/199): loss=0.5065710920574863, w0=-2.180590944222054e-05, w1=-0.15646948765796123\n",
      "Regularized Logistic Regression(54/199): loss=0.5062584732961344, w0=-2.2197848035594256e-05, w1=-0.15818808659170486\n",
      "Regularized Logistic Regression(55/199): loss=0.5059544603290268, w0=-2.25894328133805e-05, w1=-0.15989290490451427\n",
      "Regularized Logistic Regression(56/199): loss=0.5056586969549464, w0=-2.2980668358022874e-05, w1=-0.16158428040735573\n",
      "Regularized Logistic Regression(57/199): loss=0.5053708486190084, w0=-2.3371559177229304e-05, w1=-0.1632625332927432\n",
      "Regularized Logistic Regression(58/199): loss=0.5050906005317397, w0=-2.3762109705219752e-05, w1=-0.16492796735485793\n",
      "Regularized Logistic Regression(59/199): loss=0.5048176559983674, w0=-2.415232430401092e-05, w1=-0.16658087111243042\n",
      "Regularized Logistic Regression(60/199): loss=0.5045517349312475, w0=-2.454220726472421e-05, w1=-0.16822151884310738\n",
      "Regularized Logistic Regression(61/199): loss=0.5042925725221558, w0=-2.4931762808905677e-05, w1=-0.16985017153713675\n",
      "Regularized Logistic Regression(62/199): loss=0.5040399180543714, w0=-2.532099508984895e-05, w1=-0.1714670777774306\n",
      "Regularized Logistic Regression(63/199): loss=0.503793533837246, w0=-2.5709908193913612e-05, w1=-0.1730724745523638\n",
      "Regularized Logistic Regression(64/199): loss=0.503553194248285, w0=-2.6098506141833215e-05, w1=-0.1746665880070547\n",
      "Regularized Logistic Regression(65/199): loss=0.5033186848697925, w0=-2.6486792890008153e-05, w1=-0.17624963413832057\n",
      "Regularized Logistic Regression(66/199): loss=0.5030898017088369, w0=-2.6874772331779745e-05, w1=-0.17782181943801462\n",
      "Regularized Logistic Regression(67/199): loss=0.5028663504907914, w0=-2.7262448298682696e-05, w1=-0.17938334148900625\n",
      "Regularized Logistic Regression(68/199): loss=0.5026481460179569, w0=-2.764982456167376e-05, w1=-0.18093438951767946\n",
      "Regularized Logistic Regression(69/199): loss=0.5024350115858857, w0=-2.803690483233509e-05, w1=-0.18247514490646838\n",
      "Regularized Logistic Regression(70/199): loss=0.5022267784509505, w0=-2.8423692764051123e-05, w1=-0.18400578166963547\n",
      "Regularized Logistic Regression(71/199): loss=0.5020232853435381, w0=-2.8810191953158416e-05, w1=-0.1855264668952107\n",
      "Regularized Logistic Regression(72/199): loss=0.5018243780219276, w0=-2.9196405940067947e-05, w1=-0.1870373611557544\n",
      "Regularized Logistic Regression(73/199): loss=0.5016299088625449, w0=-2.9582338210359833e-05, w1=-0.18853861889037685\n",
      "Regularized Logistic Regression(74/199): loss=0.5014397364827939, w0=-2.9967992195850534e-05, w1=-0.19003038876023712\n",
      "Regularized Logistic Regression(75/199): loss=0.5012537253931388, w0=-3.035337127563283e-05, w1=-0.19151281397955497\n",
      "Regularized Logistic Regression(76/199): loss=0.5010717456754951, w0=-3.0738478777088894e-05, w1=-0.19298603262399797\n",
      "Regularized Logistic Regression(77/199): loss=0.5008936726853491, w0=-3.112331797687706e-05, w1=-0.19445017791815383\n",
      "Regularized Logistic Regression(78/199): loss=0.5007193867753084, w0=-3.150789210189273e-05, w1=-0.19590537850365153\n",
      "Regularized Logistic Regression(79/199): loss=0.5005487730380689, w0=-3.189220433020415e-05, w1=-0.19735175868937224\n",
      "Regularized Logistic Regression(80/199): loss=0.5003817210669955, w0=-3.227625779196364e-05, w1=-0.19878943868507226\n",
      "Regularized Logistic Regression(81/199): loss=0.5002181247327286, w0=-3.266005557029504e-05, w1=-0.20021853481963062\n",
      "Regularized Logistic Regression(82/199): loss=0.5000578819743919, w0=-3.304360070215803e-05, w1=-0.20163915974504584\n",
      "Regularized Logistic Regression(83/199): loss=0.4999008946041427, w0=-3.342689617919003e-05, w1=-0.20305142262720757\n",
      "Regularized Logistic Regression(84/199): loss=0.49974706812393116, w0=-3.380994494852646e-05, w1=-0.2044554293243955\n",
      "Regularized Logistic Regression(85/199): loss=0.49959631155346196, w0=-3.4192749913599975e-05, w1=-0.2058512825543818\n",
      "Regularized Logistic Regression(86/199): loss=0.4994485372684466, w0=-3.457531393491947e-05, w1=-0.20723908205094502\n",
      "Regularized Logistic Regression(87/199): loss=0.4993036608483414, w0=-3.495763983082945e-05, w1=-0.20861892471054128\n",
      "Regularized Logistic Regression(88/199): loss=0.49916160093282874, w0=-3.533973037825049e-05, w1=-0.20999090472982407\n",
      "Regularized Logistic Regression(89/199): loss=0.49902227908638985, w0=-3.572158831340139e-05, w1=-0.21135511373464885\n",
      "Regularized Logistic Regression(90/199): loss=0.49888561967036704, w0=-3.610321633250372e-05, w1=-0.21271164090115388\n",
      "Regularized Logistic Regression(91/199): loss=0.49875154972197644, w0=-3.648461709246926e-05, w1=-0.2140605730694603\n",
      "Regularized Logistic Regression(92/199): loss=0.49861999883978153, w0=-3.686579321157101e-05, w1=-0.21540199485050063\n",
      "Regularized Logistic Regression(93/199): loss=0.4984908990751816, w0=-3.7246747270098314e-05, w1=-0.21673598872644054\n",
      "Regularized Logistic Regression(94/199): loss=0.49836418482950906, w0=-3.762748181099656e-05, w1=-0.21806263514512825\n",
      "Regularized Logistic Regression(95/199): loss=0.49823979275636404, w0=-3.800799934049212e-05, w1=-0.21938201260897636\n",
      "Regularized Logistic Regression(96/199): loss=0.49811766166884913, w0=-3.838830232870289e-05, w1=-0.2206941977586454\n",
      "Regularized Logistic Regression(97/199): loss=0.49799773245139295, w0=-3.8768393210235035e-05, w1=-0.22199926545187776\n",
      "Regularized Logistic Regression(98/199): loss=0.4978799479758774, w0=-3.914827438476623e-05, w1=-0.22329728883780264\n",
      "Regularized Logistic Regression(99/199): loss=0.4977642530218078, w0=-3.9527948217616055e-05, w1=-0.2245883394270113\n",
      "Regularized Logistic Regression(100/199): loss=0.49765059420028024, w0=-3.990741704030376e-05, w1=-0.22587248715767808\n",
      "Regularized Logistic Regression(101/199): loss=0.4975389198815298, w0=-4.028668315109396e-05, w1=-0.22714980045798638\n",
      "Regularized Logistic Regression(102/199): loss=0.49742918012584636, w0=-4.06657488155305e-05, w1=-0.22842034630509955\n",
      "Regularized Logistic Regression(103/199): loss=0.4973213266176729, w0=-4.1044616266959015e-05, w1=-0.2296841902808976\n",
      "Regularized Logistic Regression(104/199): loss=0.4972153126027053, w0=-4.14232877070384e-05, w1=-0.23094139662468893\n",
      "Regularized Logistic Regression(105/199): loss=0.497111092827829, w0=-4.1801765306241646e-05, w1=-0.23219202828308996\n",
      "Regularized Logistic Regression(106/199): loss=0.49700862348374336, w0=-4.218005120434624e-05, w1=-0.2334361469572508\n",
      "Regularized Logistic Regression(107/199): loss=0.49690786215012417, w0=-4.25581475109146e-05, w1=-0.23467381314759508\n",
      "Regularized Logistic Regression(108/199): loss=0.49680876774319727, w0=-4.293605630576463e-05, w1=-0.2359050861962304\n",
      "Regularized Logistic Regression(109/199): loss=0.4967113004655943, w0=-4.331377963943093e-05, w1=-0.23713002432717492\n",
      "Regularized Logistic Regression(110/199): loss=0.4966154217583747, w0=-4.369131953361676e-05, w1=-0.23834868468453296\n",
      "Regularized Logistic Regression(111/199): loss=0.49652109425510826, w0=-4.406867798163701e-05, w1=-0.2395611233687481\n",
      "Regularized Logistic Regression(112/199): loss=0.4964282817379086, w0=-4.4445856948852576e-05, w1=-0.24076739547105225\n",
      "Regularized Logistic Regression(113/199): loss=0.4963369490953271, w0=-4.482285837309624e-05, w1=-0.2419675551062175\n",
      "Regularized Logistic Regression(114/199): loss=0.49624706228201565, w0=-4.519968416509038e-05, w1=-0.24316165544371607\n",
      "Regularized Logistic Regression(115/199): loss=0.4961585882800682, w0=-4.557633620885664e-05, w1=-0.24434974873738272\n",
      "Regularized Logistic Regression(116/199): loss=0.4960714950619683, w0=-4.595281636211791e-05, w1=-0.24553188635367\n",
      "Regularized Logistic Regression(117/199): loss=0.4959857515550601, w0=-4.632912645669266e-05, w1=-0.2467081187985772\n",
      "Regularized Logistic Regression(118/199): loss=0.49590132760747474, w0=-4.6705268298881966e-05, w1=-0.24787849574333562\n",
      "Regularized Logistic Regression(119/199): loss=0.49581819395544297, w0=-4.7081243669849366e-05, w1=-0.24904306604891807\n",
      "Regularized Logistic Regression(120/199): loss=0.495736322191931, w0=-4.745705432599368e-05, w1=-0.2502018777894446\n",
      "Regularized Logistic Regression(121/199): loss=0.495655684736539, w0=-4.783270199931507e-05, w1=-0.25135497827454517\n",
      "Regularized Logistic Regression(122/199): loss=0.4955762548066032, w0=-4.820818839777443e-05, w1=-0.2525024140707422\n",
      "Regularized Logistic Regression(123/199): loss=0.4954980063894504, w0=-4.858351520564633e-05, w1=-0.2536442310219042\n",
      "Regularized Logistic Regression(124/199): loss=0.49542091421575, w0=-4.895868408386566e-05, w1=-0.25478047426882866\n",
      "Regularized Logistic Regression(125/199): loss=0.4953449537339169, w0=-4.9333696670368045e-05, w1=-0.2559111882679972\n",
      "Regularized Logistic Regression(126/199): loss=0.49527010108551833, w0=-4.9708554580424334e-05, w1=-0.25703641680955197\n",
      "Regularized Logistic Regression(127/199): loss=0.4951963330816407, w0=-5.008325940696919e-05, w1=-0.25815620303453646\n",
      "Regularized Logistic Regression(128/199): loss=0.4951236271801739, w0=-5.045781272092392e-05, w1=-0.2592705894514394\n",
      "Regularized Logistic Regression(129/199): loss=0.49505196146397346, w0=-5.083221607151381e-05, w1=-0.26037961795207837\n",
      "Regularized Logistic Regression(130/199): loss=0.4949813146198642, w0=-5.1206470986579867e-05, w1=-0.26148332982686373\n",
      "Regularized Logistic Regression(131/199): loss=0.4949116659184466, w0=-5.158057897288536e-05, w1=-0.2625817657794686\n",
      "Regularized Logistic Regression(132/199): loss=0.4948429951946741, w0=-5.195454151641705e-05, w1=-0.2636749659409414\n",
      "Regularized Logistic Regression(133/199): loss=0.49477528282916783, w0=-5.232836008268139e-05, w1=-0.2647629698832886\n",
      "Regularized Logistic Regression(134/199): loss=0.4947085097302366, w0=-5.2702036116995684e-05, w1=-0.265845816632554\n",
      "Regularized Logistic Regression(135/199): loss=0.49464265731657403, w0=-5.3075571044774444e-05, w1=-0.2669235446814206\n",
      "Regularized Logistic Regression(136/199): loss=0.49457770750060404, w0=-5.3448966271810945e-05, w1=-0.2679961920013616\n",
      "Regularized Logistic Regression(137/199): loss=0.49451364267244713, w0=-5.3822223184554124e-05, w1=-0.26906379605435937\n",
      "Regularized Logistic Regression(138/199): loss=0.49445044568448254, w0=-5.4195343150380946e-05, w1=-0.27012639380421666\n",
      "Regularized Logistic Regression(139/199): loss=0.49438809983648124, w0=-5.456832751786429e-05, w1=-0.27118402172747813\n",
      "Regularized Logistic Regression(140/199): loss=0.49432658886128694, w0=-5.4941177617036504e-05, w1=-0.27223671582398334\n",
      "Regularized Logistic Regression(141/199): loss=0.49426589691102124, w0=-5.531389475964863e-05, w1=-0.2732845116270664\n",
      "Regularized Logistic Regression(142/199): loss=0.49420600854379426, w0=-5.568648023942555e-05, w1=-0.27432744421342087\n",
      "Regularized Logistic Regression(143/199): loss=0.49414690871089745, w0=-5.605893533231694e-05, w1=-0.27536554821264525\n",
      "Regularized Logistic Regression(144/199): loss=0.4940885827444604, w0=-5.643126129674428e-05, w1=-0.27639885781648277\n",
      "Regularized Logistic Regression(145/199): loss=0.49403101634555324, w0=-5.680345937384392e-05, w1=-0.2774274067877709\n",
      "Regularized Logistic Regression(146/199): loss=0.4939741955727172, w0=-5.717553078770631e-05, w1=-0.2784512284691137\n",
      "Regularized Logistic Regression(147/199): loss=0.4939181068309043, w0=-5.7547476745611436e-05, w1=-0.2794703557912877\n",
      "Regularized Logistic Regression(148/199): loss=0.4938627368608122, w0=-5.7919298438260635e-05, w1=-0.28048482128139574\n",
      "Regularized Logistic Regression(149/199): loss=0.4938080727285959, w0=-5.82909970400047e-05, w1=-0.2814946570707769\n",
      "Regularized Logistic Regression(150/199): loss=0.4937541018159476, w0=-5.866257370906854e-05, w1=-0.2824998949026858\n",
      "Regularized Logistic Regression(151/199): loss=0.49370081181051917, w0=-5.90340295877723e-05, w1=-0.2835005661397492\n",
      "Regularized Logistic Regression(152/199): loss=0.49364819069668636, w0=-5.940536580274908e-05, w1=-0.284496701771209\n",
      "Regularized Logistic Regression(153/199): loss=0.49359622674663195, w0=-5.977658346515936e-05, w1=-0.2854883324199637\n",
      "Regularized Logistic Regression(154/199): loss=0.49354490851174165, w0=-6.01476836709021e-05, w1=-0.28647548834941117\n",
      "Regularized Logistic Regression(155/199): loss=0.4934942248142955, w0=-6.0518667500822694e-05, w1=-0.2874581994701072\n",
      "Regularized Logistic Regression(156/199): loss=0.4934441647394488, w0=-6.088953602091768e-05, w1=-0.28843649534624066\n",
      "Regularized Logistic Regression(157/199): loss=0.49339471762748593, w0=-6.126029028253644e-05, w1=-0.2894104052019377\n",
      "Regularized Logistic Regression(158/199): loss=0.4933458730663412, w0=-6.163093132257986e-05, w1=-0.2903799579273987\n",
      "Regularized Logistic Regression(159/199): loss=0.49329762088437423, w0=-6.200146016369596e-05, w1=-0.2913451820848761\n",
      "Regularized Logistic Regression(160/199): loss=0.4932499511433902, w0=-6.237187781447267e-05, w1=-0.2923061059144967\n",
      "Regularized Logistic Regression(161/199): loss=0.49320285413189685, w0=-6.274218526962771e-05, w1=-0.29326275733993773\n",
      "Regularized Logistic Regression(162/199): loss=0.4931563203585883, w0=-6.311238351019569e-05, w1=-0.2942151639739588\n",
      "Regularized Logistic Regression(163/199): loss=0.4931103405460478, w0=-6.348247350371235e-05, w1=-0.2951633531237965\n",
      "Regularized Logistic Regression(164/199): loss=0.4930649056246602, w0=-6.385245620439624e-05, w1=-0.29610735179642805\n",
      "Regularized Logistic Regression(165/199): loss=0.49302000672672797, w0=-6.422233255332762e-05, w1=-0.2970471867037036\n",
      "Regularized Logistic Regression(166/199): loss=0.4929756351807821, w0=-6.45921034786248e-05, w1=-0.29798288426735864\n",
      "Regularized Logistic Regression(167/199): loss=0.4929317825060796, w0=-6.496176989561792e-05, w1=-0.2989144706239056\n",
      "Regularized Logistic Regression(168/199): loss=0.49288844040728497, w0=-6.533133270702015e-05, w1=-0.29984197162940907\n",
      "Regularized Logistic Regression(169/199): loss=0.4928456007693213, w0=-6.570079280309647e-05, w1=-0.30076541286415226\n",
      "Regularized Logistic Regression(170/199): loss=0.49280325565239474, w0=-6.607015106183002e-05, w1=-0.301684819637194\n",
      "Regularized Logistic Regression(171/199): loss=0.49276139728717583, w0=-6.643940834908604e-05, w1=-0.30260021699082235\n",
      "Regularized Logistic Regression(172/199): loss=0.49272001807013976, w0=-6.680856551877343e-05, w1=-0.3035116297049064\n",
      "Regularized Logistic Regression(173/199): loss=0.4926791105590557, w0=-6.717762341300404e-05, w1=-0.3044190823011522\n",
      "Regularized Logistic Regression(174/199): loss=0.49263866746862167, w0=-6.754658286224974e-05, w1=-0.3053225990472628\n",
      "Regularized Logistic Regression(175/199): loss=0.49259868166623744, w0=-6.791544468549713e-05, w1=-0.3062222039610071\n",
      "Regularized Logistic Regression(176/199): loss=0.49255914616791374, w0=-6.828420969040019e-05, w1=-0.3071179208141997\n",
      "Regularized Logistic Regression(177/199): loss=0.4925200541343105, w0=-6.865287867343068e-05, w1=-0.3080097731365947\n",
      "Regularized Logistic Regression(178/199): loss=0.4924813988669001, w0=-6.902145242002649e-05, w1=-0.30889778421969666\n",
      "Regularized Logistic Regression(179/199): loss=0.49244317380425146, w0=-6.938993170473785e-05, w1=-0.3097819771204884\n",
      "Regularized Logistic Regression(180/199): loss=0.4924053725184306, w0=-6.975831729137154e-05, w1=-0.3106623746650829\n",
      "Regularized Logistic Regression(181/199): loss=0.49236798871151377, w0=-7.012660993313304e-05, w1=-0.3115389994522967\n",
      "Regularized Logistic Regression(182/199): loss=0.49233101621220815, w0=-7.049481037276674e-05, w1=-0.31241187385714814\n",
      "Regularized Logistic Regression(183/199): loss=0.492294448972578, w0=-7.086291934269419e-05, w1=-0.31328102003428726\n",
      "Regularized Logistic Regression(184/199): loss=0.49225828106487135, w0=-7.12309375651504e-05, w1=-0.3141464599213499\n",
      "Regularized Logistic Regression(185/199): loss=0.49222250667844425, w0=-7.159886575231832e-05, w1=-0.31500821524224887\n",
      "Regularized Logistic Regression(186/199): loss=0.49218712011677795, w0=-7.196670460646139e-05, w1=-0.31586630751039435\n",
      "Regularized Logistic Regression(187/199): loss=0.4921521157945902, w0=-7.233445482005432e-05, w1=-0.3167207580318527\n",
      "Regularized Logistic Regression(188/199): loss=0.4921174882350288, w0=-7.270211707591202e-05, w1=-0.31757158790844003\n",
      "Regularized Logistic Regression(189/199): loss=0.4920832320669548, w0=-7.306969204731685e-05, w1=-0.31841881804075556\n",
      "Regularized Logistic Regression(190/199): loss=0.49204934202230594, w0=-7.343718039814402e-05, w1=-0.3192624691311558\n",
      "Regularized Logistic Regression(191/199): loss=0.49201581293353686, w0=-7.38045827829854e-05, w1=-0.3201025616866689\n",
      "Regularized Logistic Regression(192/199): loss=0.49198263973113904, w0=-7.417189984727152e-05, w1=-0.3209391160218531\n",
      "Regularized Logistic Regression(193/199): loss=0.49194981744123173, w0=-7.453913222739196e-05, w1=-0.3217721522616001\n",
      "Regularized Logistic Regression(194/199): loss=0.49191734118322583, w0=-7.490628055081419e-05, w1=-0.32260169034388453\n",
      "Regularized Logistic Regression(195/199): loss=0.4918852061675559, w0=-7.527334543620068e-05, w1=-0.3234277500224603\n",
      "Regularized Logistic Regression(196/199): loss=0.49185340769347824, w0=-7.564032749352443e-05, w1=-0.3242503508695057\n",
      "Regularized Logistic Regression(197/199): loss=0.491821941146934, w0=-7.60072273241831e-05, w1=-0.3250695122782189\n",
      "Regularized Logistic Regression(198/199): loss=0.4917908019984749, w0=-7.637404552111139e-05, w1=-0.3258852534653645\n",
      "Regularized Logistic Regression(199/199): loss=0.4917599858012474, w0=-7.674078266889206e-05, w1=-0.3266975934737715\n",
      "Regularized Logistic Regression(0/299): loss=0.693147180559945, w0=-4.176586058555737e-07, w1=-0.007317796433195506\n",
      "Regularized Logistic Regression(1/299): loss=0.6631135260714237, w0=-8.326678932672681e-07, w1=-0.013966463329453083\n",
      "Regularized Logistic Regression(2/299): loss=0.6392199453236909, w0=-1.2463996377792245e-06, w1=-0.02006230340284925\n",
      "Regularized Logistic Regression(3/299): loss=0.6200710370267274, w0=-1.659551661864677e-06, w1=-0.02569522134138946\n",
      "Regularized Logistic Regression(4/299): loss=0.6046022792758989, w0=-2.072444402594911e-06, w1=-0.0309355604619541\n",
      "Regularized Logistic Regression(5/299): loss=0.591999855667077, w0=-2.4851968595250796e-06, w1=-0.03583929882076505\n",
      "Regularized Logistic Regression(6/299): loss=0.5816415444090455, w0=-2.897826050292845e-06, w1=-0.0404517250778323\n",
      "Regularized Logistic Regression(7/299): loss=0.573051100350308, w0=-3.3103013001483137e-06, w1=-0.044810007313822016\n",
      "Regularized Logistic Regression(8/299): loss=0.565862971711694, w0=-3.722572971608584e-06, w1=-0.04894501406638419\n",
      "Regularized Logistic Regression(9/299): loss=0.5597953222148011, w0=-4.134587116277433e-06, w1=-0.05288263696516249\n",
      "Regularized Logistic Regression(10/299): loss=0.5546296367980985, w0=-4.5462925267862145e-06, w1=-0.05664477610412627\n",
      "Regularized Logistic Regression(11/299): loss=0.5501954168532402, w0=-4.9576437652457895e-06, w1=-0.06025009092644788\n",
      "Regularized Logistic Regression(12/299): loss=0.5463587305604463, w0=-5.36860211598123e-06, w1=-0.06371458319492389\n",
      "Regularized Logistic Regression(13/299): loss=0.5430136451115412, w0=-5.779135512043805e-06, w1=-0.06705205642927656\n",
      "Regularized Logistic Regression(14/299): loss=0.5400757982867492, w0=-6.189217994170834e-06, w1=-0.07027448238043728\n",
      "Regularized Logistic Regression(15/299): loss=0.5374775543414433, w0=-6.598828994180225e-06, w1=-0.07339229626199492\n",
      "Regularized Logistic Regression(16/299): loss=0.5351643340936991, w0=-7.007952590621107e-06, w1=-0.07641463658866081\n",
      "Regularized Logistic Regression(17/299): loss=0.5330918178514118, w0=-7.416576807146122e-06, w1=-0.07934954144234359\n",
      "Regularized Logistic Regression(18/299): loss=0.5312238000285677, w0=-7.824692983108683e-06, w1=-0.08220411013529709\n",
      "Regularized Logistic Regression(19/299): loss=0.5295305329304837, w0=-8.232295224717367e-06, w1=-0.084984637170432\n",
      "Regularized Logistic Regression(20/299): loss=0.5279874398822021, w0=-8.63937993462322e-06, w1=-0.08769672386585056\n",
      "Regularized Logistic Regression(21/299): loss=0.526574108954785, w0=-9.045945413104483e-06, w1=-0.09034537185645178\n",
      "Regularized Logistic Regression(22/299): loss=0.5252735012156958, w0=-9.451991522325304e-06, w1=-0.09293506180514209\n",
      "Regularized Logistic Regression(23/299): loss=0.5240713240256595, w0=-9.857519404993317e-06, w1=-0.09546981997776516\n",
      "Regularized Logistic Regression(24/299): loss=0.5229555321087854, w0=-1.026253124931895e-05, w1=-0.09795327480847277\n",
      "Regularized Logistic Regression(25/299): loss=0.5219159281443849, w0=-1.0667030093055475e-05, w1=-0.1003887051692537\n",
      "Regularized Logistic Regression(26/299): loss=0.5209438413353147, w0=-1.107101966034925e-05, w1=-0.10277908173177465\n",
      "Regularized Logistic Regression(27/299): loss=0.5200318674221431, w0=-1.1474504226043188e-05, w1=-0.10512710255157136\n",
      "Regularized Logistic Regression(28/299): loss=0.5191736573841238, w0=-1.1877488502904083e-05, w1=-0.10743522379886165\n",
      "Regularized Logistic Regression(29/299): loss=0.5183637449217599, w0=-1.2279977547969364e-05, w1=-0.10970568639539036\n",
      "Regularized Logistic Regression(30/299): loss=0.5175974049877883, w0=-1.2681976684830828e-05, w1=-0.11194053918399616\n",
      "Regularized Logistic Regression(31/299): loss=0.5168705372961805, w0=-1.3083491439200034e-05, w1=-0.11414165915025225\n",
      "Regularized Logistic Regression(32/299): loss=0.5161795700188689, w0=-1.3484527485542887e-05, w1=-0.11631076912834214\n",
      "Regularized Logistic Regression(33/299): loss=0.5155213798708965, w0=-1.3885090602941092e-05, w1=-0.11844945335220046\n",
      "Regularized Logistic Regression(34/299): loss=0.514893225555969, w0=-1.428518663864664e-05, w1=-0.12055917115468621\n",
      "Regularized Logistic Regression(35/299): loss=0.5142926921477957, w0=-1.4684821478052e-05, w1=-0.12264126906964179\n",
      "Regularized Logistic Regression(36/299): loss=0.5137176444570565, w0=-1.5084001020011864e-05, w1=-0.12469699155212664\n",
      "Regularized Logistic Regression(37/299): loss=0.513166187808689, w0=-1.5482731156629265e-05, w1=-0.12672749049933665\n",
      "Regularized Logistic Regression(38/299): loss=0.5126366349517666, w0=-1.5881017756766122e-05, w1=-0.1287338337274515\n",
      "Regularized Logistic Regression(39/299): loss=0.5121274780614988, w0=-1.6278866652660395e-05, w1=-0.130717012536896\n",
      "Regularized Logistic Regression(40/299): loss=0.5116373649829121, w0=-1.6676283629133893e-05, w1=-0.13267794847943723\n",
      "Regularized Logistic Regression(41/299): loss=0.5111650790185701, w0=-1.707327441495935e-05, w1=-0.1346174994245271\n",
      "Regularized Logistic Regression(42/299): loss=0.5107095216860728, w0=-1.7469844676026086e-05, w1=-0.13653646500878697\n",
      "Regularized Logistic Regression(43/299): loss=0.510269697971077, w0=-1.786600001000227e-05, w1=-0.13843559154111518\n",
      "Regularized Logistic Regression(44/299): loss=0.5098447036829256, w0=-1.826174594224115e-05, w1=-0.14031557642620707\n",
      "Regularized Logistic Regression(45/299): loss=0.5094337145863918, w0=-1.865708792271954e-05, w1=-0.1421770721610296\n",
      "Regularized Logistic Regression(46/299): loss=0.509035977037449, w0=-1.9052031323831346e-05, w1=-0.14402068995175682\n",
      "Regularized Logistic Regression(47/299): loss=0.5086507998956974, w0=-1.9446581438887525e-05, w1=-0.14584700299265368\n",
      "Regularized Logistic Regression(48/299): loss=0.508277547522933, w0=-1.9840743481198193e-05, w1=-0.14765654944322187\n",
      "Regularized Logistic Regression(49/299): loss=0.5079156337078352, w0=-2.0234522583632617e-05, w1=-0.149449835135481\n",
      "Regularized Logistic Regression(50/299): loss=0.5075645163820302, w0=-2.0627923798569916e-05, w1=-0.15122733603941613\n",
      "Regularized Logistic Regression(51/299): loss=0.5072236930138041, w0=-2.1020952098167466e-05, w1=-0.15298950051130916\n",
      "Regularized Logistic Regression(52/299): loss=0.5068926965832765, w0=-2.141361237488602e-05, w1=-0.1547367513467925\n",
      "Regularized Logistic Regression(53/299): loss=0.5065710920574863, w0=-2.180590944222054e-05, w1=-0.15646948765796123\n",
      "Regularized Logistic Regression(54/299): loss=0.5062584732961344, w0=-2.2197848035594256e-05, w1=-0.15818808659170486\n",
      "Regularized Logistic Regression(55/299): loss=0.5059544603290268, w0=-2.25894328133805e-05, w1=-0.15989290490451427\n",
      "Regularized Logistic Regression(56/299): loss=0.5056586969549464, w0=-2.2980668358022874e-05, w1=-0.16158428040735573\n",
      "Regularized Logistic Regression(57/299): loss=0.5053708486190084, w0=-2.3371559177229304e-05, w1=-0.1632625332927432\n",
      "Regularized Logistic Regression(58/299): loss=0.5050906005317397, w0=-2.3762109705219752e-05, w1=-0.16492796735485793\n",
      "Regularized Logistic Regression(59/299): loss=0.5048176559983674, w0=-2.415232430401092e-05, w1=-0.16658087111243042\n",
      "Regularized Logistic Regression(60/299): loss=0.5045517349312475, w0=-2.454220726472421e-05, w1=-0.16822151884310738\n",
      "Regularized Logistic Regression(61/299): loss=0.5042925725221558, w0=-2.4931762808905677e-05, w1=-0.16985017153713675\n",
      "Regularized Logistic Regression(62/299): loss=0.5040399180543714, w0=-2.532099508984895e-05, w1=-0.1714670777774306\n",
      "Regularized Logistic Regression(63/299): loss=0.503793533837246, w0=-2.5709908193913612e-05, w1=-0.1730724745523638\n",
      "Regularized Logistic Regression(64/299): loss=0.503553194248285, w0=-2.6098506141833215e-05, w1=-0.1746665880070547\n",
      "Regularized Logistic Regression(65/299): loss=0.5033186848697925, w0=-2.6486792890008153e-05, w1=-0.17624963413832057\n",
      "Regularized Logistic Regression(66/299): loss=0.5030898017088369, w0=-2.6874772331779745e-05, w1=-0.17782181943801462\n",
      "Regularized Logistic Regression(67/299): loss=0.5028663504907914, w0=-2.7262448298682696e-05, w1=-0.17938334148900625\n",
      "Regularized Logistic Regression(68/299): loss=0.5026481460179569, w0=-2.764982456167376e-05, w1=-0.18093438951767946\n",
      "Regularized Logistic Regression(69/299): loss=0.5024350115858857, w0=-2.803690483233509e-05, w1=-0.18247514490646838\n",
      "Regularized Logistic Regression(70/299): loss=0.5022267784509505, w0=-2.8423692764051123e-05, w1=-0.18400578166963547\n",
      "Regularized Logistic Regression(71/299): loss=0.5020232853435381, w0=-2.8810191953158416e-05, w1=-0.1855264668952107\n",
      "Regularized Logistic Regression(72/299): loss=0.5018243780219276, w0=-2.9196405940067947e-05, w1=-0.1870373611557544\n",
      "Regularized Logistic Regression(73/299): loss=0.5016299088625449, w0=-2.9582338210359833e-05, w1=-0.18853861889037685\n",
      "Regularized Logistic Regression(74/299): loss=0.5014397364827939, w0=-2.9967992195850534e-05, w1=-0.19003038876023712\n",
      "Regularized Logistic Regression(75/299): loss=0.5012537253931388, w0=-3.035337127563283e-05, w1=-0.19151281397955497\n",
      "Regularized Logistic Regression(76/299): loss=0.5010717456754951, w0=-3.0738478777088894e-05, w1=-0.19298603262399797\n",
      "Regularized Logistic Regression(77/299): loss=0.5008936726853491, w0=-3.112331797687706e-05, w1=-0.19445017791815383\n",
      "Regularized Logistic Regression(78/299): loss=0.5007193867753084, w0=-3.150789210189273e-05, w1=-0.19590537850365153\n",
      "Regularized Logistic Regression(79/299): loss=0.5005487730380689, w0=-3.189220433020415e-05, w1=-0.19735175868937224\n",
      "Regularized Logistic Regression(80/299): loss=0.5003817210669955, w0=-3.227625779196364e-05, w1=-0.19878943868507226\n",
      "Regularized Logistic Regression(81/299): loss=0.5002181247327286, w0=-3.266005557029504e-05, w1=-0.20021853481963062\n",
      "Regularized Logistic Regression(82/299): loss=0.5000578819743919, w0=-3.304360070215803e-05, w1=-0.20163915974504584\n",
      "Regularized Logistic Regression(83/299): loss=0.4999008946041427, w0=-3.342689617919003e-05, w1=-0.20305142262720757\n",
      "Regularized Logistic Regression(84/299): loss=0.49974706812393116, w0=-3.380994494852646e-05, w1=-0.2044554293243955\n",
      "Regularized Logistic Regression(85/299): loss=0.49959631155346196, w0=-3.4192749913599975e-05, w1=-0.2058512825543818\n",
      "Regularized Logistic Regression(86/299): loss=0.4994485372684466, w0=-3.457531393491947e-05, w1=-0.20723908205094502\n",
      "Regularized Logistic Regression(87/299): loss=0.4993036608483414, w0=-3.495763983082945e-05, w1=-0.20861892471054128\n",
      "Regularized Logistic Regression(88/299): loss=0.49916160093282874, w0=-3.533973037825049e-05, w1=-0.20999090472982407\n",
      "Regularized Logistic Regression(89/299): loss=0.49902227908638985, w0=-3.572158831340139e-05, w1=-0.21135511373464885\n",
      "Regularized Logistic Regression(90/299): loss=0.49888561967036704, w0=-3.610321633250372e-05, w1=-0.21271164090115388\n",
      "Regularized Logistic Regression(91/299): loss=0.49875154972197644, w0=-3.648461709246926e-05, w1=-0.2140605730694603\n",
      "Regularized Logistic Regression(92/299): loss=0.49861999883978153, w0=-3.686579321157101e-05, w1=-0.21540199485050063\n",
      "Regularized Logistic Regression(93/299): loss=0.4984908990751816, w0=-3.7246747270098314e-05, w1=-0.21673598872644054\n",
      "Regularized Logistic Regression(94/299): loss=0.49836418482950906, w0=-3.762748181099656e-05, w1=-0.21806263514512825\n",
      "Regularized Logistic Regression(95/299): loss=0.49823979275636404, w0=-3.800799934049212e-05, w1=-0.21938201260897636\n",
      "Regularized Logistic Regression(96/299): loss=0.49811766166884913, w0=-3.838830232870289e-05, w1=-0.2206941977586454\n",
      "Regularized Logistic Regression(97/299): loss=0.49799773245139295, w0=-3.8768393210235035e-05, w1=-0.22199926545187776\n",
      "Regularized Logistic Regression(98/299): loss=0.4978799479758774, w0=-3.914827438476623e-05, w1=-0.22329728883780264\n",
      "Regularized Logistic Regression(99/299): loss=0.4977642530218078, w0=-3.9527948217616055e-05, w1=-0.2245883394270113\n",
      "Regularized Logistic Regression(100/299): loss=0.49765059420028024, w0=-3.990741704030376e-05, w1=-0.22587248715767808\n",
      "Regularized Logistic Regression(101/299): loss=0.4975389198815298, w0=-4.028668315109396e-05, w1=-0.22714980045798638\n",
      "Regularized Logistic Regression(102/299): loss=0.49742918012584636, w0=-4.06657488155305e-05, w1=-0.22842034630509955\n",
      "Regularized Logistic Regression(103/299): loss=0.4973213266176729, w0=-4.1044616266959015e-05, w1=-0.2296841902808976\n",
      "Regularized Logistic Regression(104/299): loss=0.4972153126027053, w0=-4.14232877070384e-05, w1=-0.23094139662468893\n",
      "Regularized Logistic Regression(105/299): loss=0.497111092827829, w0=-4.1801765306241646e-05, w1=-0.23219202828308996\n",
      "Regularized Logistic Regression(106/299): loss=0.49700862348374336, w0=-4.218005120434624e-05, w1=-0.2334361469572508\n",
      "Regularized Logistic Regression(107/299): loss=0.49690786215012417, w0=-4.25581475109146e-05, w1=-0.23467381314759508\n",
      "Regularized Logistic Regression(108/299): loss=0.49680876774319727, w0=-4.293605630576463e-05, w1=-0.2359050861962304\n",
      "Regularized Logistic Regression(109/299): loss=0.4967113004655943, w0=-4.331377963943093e-05, w1=-0.23713002432717492\n",
      "Regularized Logistic Regression(110/299): loss=0.4966154217583747, w0=-4.369131953361676e-05, w1=-0.23834868468453296\n",
      "Regularized Logistic Regression(111/299): loss=0.49652109425510826, w0=-4.406867798163701e-05, w1=-0.2395611233687481\n",
      "Regularized Logistic Regression(112/299): loss=0.4964282817379086, w0=-4.4445856948852576e-05, w1=-0.24076739547105225\n",
      "Regularized Logistic Regression(113/299): loss=0.4963369490953271, w0=-4.482285837309624e-05, w1=-0.2419675551062175\n",
      "Regularized Logistic Regression(114/299): loss=0.49624706228201565, w0=-4.519968416509038e-05, w1=-0.24316165544371607\n",
      "Regularized Logistic Regression(115/299): loss=0.4961585882800682, w0=-4.557633620885664e-05, w1=-0.24434974873738272\n",
      "Regularized Logistic Regression(116/299): loss=0.4960714950619683, w0=-4.595281636211791e-05, w1=-0.24553188635367\n",
      "Regularized Logistic Regression(117/299): loss=0.4959857515550601, w0=-4.632912645669266e-05, w1=-0.2467081187985772\n",
      "Regularized Logistic Regression(118/299): loss=0.49590132760747474, w0=-4.6705268298881966e-05, w1=-0.24787849574333562\n",
      "Regularized Logistic Regression(119/299): loss=0.49581819395544297, w0=-4.7081243669849366e-05, w1=-0.24904306604891807\n",
      "Regularized Logistic Regression(120/299): loss=0.495736322191931, w0=-4.745705432599368e-05, w1=-0.2502018777894446\n",
      "Regularized Logistic Regression(121/299): loss=0.495655684736539, w0=-4.783270199931507e-05, w1=-0.25135497827454517\n",
      "Regularized Logistic Regression(122/299): loss=0.4955762548066032, w0=-4.820818839777443e-05, w1=-0.2525024140707422\n",
      "Regularized Logistic Regression(123/299): loss=0.4954980063894504, w0=-4.858351520564633e-05, w1=-0.2536442310219042\n",
      "Regularized Logistic Regression(124/299): loss=0.49542091421575, w0=-4.895868408386566e-05, w1=-0.25478047426882866\n",
      "Regularized Logistic Regression(125/299): loss=0.4953449537339169, w0=-4.9333696670368045e-05, w1=-0.2559111882679972\n",
      "Regularized Logistic Regression(126/299): loss=0.49527010108551833, w0=-4.9708554580424334e-05, w1=-0.25703641680955197\n",
      "Regularized Logistic Regression(127/299): loss=0.4951963330816407, w0=-5.008325940696919e-05, w1=-0.25815620303453646\n",
      "Regularized Logistic Regression(128/299): loss=0.4951236271801739, w0=-5.045781272092392e-05, w1=-0.2592705894514394\n",
      "Regularized Logistic Regression(129/299): loss=0.49505196146397346, w0=-5.083221607151381e-05, w1=-0.26037961795207837\n",
      "Regularized Logistic Regression(130/299): loss=0.4949813146198642, w0=-5.1206470986579867e-05, w1=-0.26148332982686373\n",
      "Regularized Logistic Regression(131/299): loss=0.4949116659184466, w0=-5.158057897288536e-05, w1=-0.2625817657794686\n",
      "Regularized Logistic Regression(132/299): loss=0.4948429951946741, w0=-5.195454151641705e-05, w1=-0.2636749659409414\n",
      "Regularized Logistic Regression(133/299): loss=0.49477528282916783, w0=-5.232836008268139e-05, w1=-0.2647629698832886\n",
      "Regularized Logistic Regression(134/299): loss=0.4947085097302366, w0=-5.2702036116995684e-05, w1=-0.265845816632554\n",
      "Regularized Logistic Regression(135/299): loss=0.49464265731657403, w0=-5.3075571044774444e-05, w1=-0.2669235446814206\n",
      "Regularized Logistic Regression(136/299): loss=0.49457770750060404, w0=-5.3448966271810945e-05, w1=-0.2679961920013616\n",
      "Regularized Logistic Regression(137/299): loss=0.49451364267244713, w0=-5.3822223184554124e-05, w1=-0.26906379605435937\n",
      "Regularized Logistic Regression(138/299): loss=0.49445044568448254, w0=-5.4195343150380946e-05, w1=-0.27012639380421666\n",
      "Regularized Logistic Regression(139/299): loss=0.49438809983648124, w0=-5.456832751786429e-05, w1=-0.27118402172747813\n",
      "Regularized Logistic Regression(140/299): loss=0.49432658886128694, w0=-5.4941177617036504e-05, w1=-0.27223671582398334\n",
      "Regularized Logistic Regression(141/299): loss=0.49426589691102124, w0=-5.531389475964863e-05, w1=-0.2732845116270664\n",
      "Regularized Logistic Regression(142/299): loss=0.49420600854379426, w0=-5.568648023942555e-05, w1=-0.27432744421342087\n",
      "Regularized Logistic Regression(143/299): loss=0.49414690871089745, w0=-5.605893533231694e-05, w1=-0.27536554821264525\n",
      "Regularized Logistic Regression(144/299): loss=0.4940885827444604, w0=-5.643126129674428e-05, w1=-0.27639885781648277\n",
      "Regularized Logistic Regression(145/299): loss=0.49403101634555324, w0=-5.680345937384392e-05, w1=-0.2774274067877709\n",
      "Regularized Logistic Regression(146/299): loss=0.4939741955727172, w0=-5.717553078770631e-05, w1=-0.2784512284691137\n",
      "Regularized Logistic Regression(147/299): loss=0.4939181068309043, w0=-5.7547476745611436e-05, w1=-0.2794703557912877\n",
      "Regularized Logistic Regression(148/299): loss=0.4938627368608122, w0=-5.7919298438260635e-05, w1=-0.28048482128139574\n",
      "Regularized Logistic Regression(149/299): loss=0.4938080727285959, w0=-5.82909970400047e-05, w1=-0.2814946570707769\n",
      "Regularized Logistic Regression(150/299): loss=0.4937541018159476, w0=-5.866257370906854e-05, w1=-0.2824998949026858\n",
      "Regularized Logistic Regression(151/299): loss=0.49370081181051917, w0=-5.90340295877723e-05, w1=-0.2835005661397492\n",
      "Regularized Logistic Regression(152/299): loss=0.49364819069668636, w0=-5.940536580274908e-05, w1=-0.284496701771209\n",
      "Regularized Logistic Regression(153/299): loss=0.49359622674663195, w0=-5.977658346515936e-05, w1=-0.2854883324199637\n",
      "Regularized Logistic Regression(154/299): loss=0.49354490851174165, w0=-6.01476836709021e-05, w1=-0.28647548834941117\n",
      "Regularized Logistic Regression(155/299): loss=0.4934942248142955, w0=-6.0518667500822694e-05, w1=-0.2874581994701072\n",
      "Regularized Logistic Regression(156/299): loss=0.4934441647394488, w0=-6.088953602091768e-05, w1=-0.28843649534624066\n",
      "Regularized Logistic Regression(157/299): loss=0.49339471762748593, w0=-6.126029028253644e-05, w1=-0.2894104052019377\n",
      "Regularized Logistic Regression(158/299): loss=0.4933458730663412, w0=-6.163093132257986e-05, w1=-0.2903799579273987\n",
      "Regularized Logistic Regression(159/299): loss=0.49329762088437423, w0=-6.200146016369596e-05, w1=-0.2913451820848761\n",
      "Regularized Logistic Regression(160/299): loss=0.4932499511433902, w0=-6.237187781447267e-05, w1=-0.2923061059144967\n",
      "Regularized Logistic Regression(161/299): loss=0.49320285413189685, w0=-6.274218526962771e-05, w1=-0.29326275733993773\n",
      "Regularized Logistic Regression(162/299): loss=0.4931563203585883, w0=-6.311238351019569e-05, w1=-0.2942151639739588\n",
      "Regularized Logistic Regression(163/299): loss=0.4931103405460478, w0=-6.348247350371235e-05, w1=-0.2951633531237965\n",
      "Regularized Logistic Regression(164/299): loss=0.4930649056246602, w0=-6.385245620439624e-05, w1=-0.29610735179642805\n",
      "Regularized Logistic Regression(165/299): loss=0.49302000672672797, w0=-6.422233255332762e-05, w1=-0.2970471867037036\n",
      "Regularized Logistic Regression(166/299): loss=0.4929756351807821, w0=-6.45921034786248e-05, w1=-0.29798288426735864\n",
      "Regularized Logistic Regression(167/299): loss=0.4929317825060796, w0=-6.496176989561792e-05, w1=-0.2989144706239056\n",
      "Regularized Logistic Regression(168/299): loss=0.49288844040728497, w0=-6.533133270702015e-05, w1=-0.29984197162940907\n",
      "Regularized Logistic Regression(169/299): loss=0.4928456007693213, w0=-6.570079280309647e-05, w1=-0.30076541286415226\n",
      "Regularized Logistic Regression(170/299): loss=0.49280325565239474, w0=-6.607015106183002e-05, w1=-0.301684819637194\n",
      "Regularized Logistic Regression(171/299): loss=0.49276139728717583, w0=-6.643940834908604e-05, w1=-0.30260021699082235\n",
      "Regularized Logistic Regression(172/299): loss=0.49272001807013976, w0=-6.680856551877343e-05, w1=-0.3035116297049064\n",
      "Regularized Logistic Regression(173/299): loss=0.4926791105590557, w0=-6.717762341300404e-05, w1=-0.3044190823011522\n",
      "Regularized Logistic Regression(174/299): loss=0.49263866746862167, w0=-6.754658286224974e-05, w1=-0.3053225990472628\n",
      "Regularized Logistic Regression(175/299): loss=0.49259868166623744, w0=-6.791544468549713e-05, w1=-0.3062222039610071\n",
      "Regularized Logistic Regression(176/299): loss=0.49255914616791374, w0=-6.828420969040019e-05, w1=-0.3071179208141997\n",
      "Regularized Logistic Regression(177/299): loss=0.4925200541343105, w0=-6.865287867343068e-05, w1=-0.3080097731365947\n",
      "Regularized Logistic Regression(178/299): loss=0.4924813988669001, w0=-6.902145242002649e-05, w1=-0.30889778421969666\n",
      "Regularized Logistic Regression(179/299): loss=0.49244317380425146, w0=-6.938993170473785e-05, w1=-0.3097819771204884\n",
      "Regularized Logistic Regression(180/299): loss=0.4924053725184306, w0=-6.975831729137154e-05, w1=-0.3106623746650829\n",
      "Regularized Logistic Regression(181/299): loss=0.49236798871151377, w0=-7.012660993313304e-05, w1=-0.3115389994522967\n",
      "Regularized Logistic Regression(182/299): loss=0.49233101621220815, w0=-7.049481037276674e-05, w1=-0.31241187385714814\n",
      "Regularized Logistic Regression(183/299): loss=0.492294448972578, w0=-7.086291934269419e-05, w1=-0.31328102003428726\n",
      "Regularized Logistic Regression(184/299): loss=0.49225828106487135, w0=-7.12309375651504e-05, w1=-0.3141464599213499\n",
      "Regularized Logistic Regression(185/299): loss=0.49222250667844425, w0=-7.159886575231832e-05, w1=-0.31500821524224887\n",
      "Regularized Logistic Regression(186/299): loss=0.49218712011677795, w0=-7.196670460646139e-05, w1=-0.31586630751039435\n",
      "Regularized Logistic Regression(187/299): loss=0.4921521157945902, w0=-7.233445482005432e-05, w1=-0.3167207580318527\n",
      "Regularized Logistic Regression(188/299): loss=0.4921174882350288, w0=-7.270211707591202e-05, w1=-0.31757158790844003\n",
      "Regularized Logistic Regression(189/299): loss=0.4920832320669548, w0=-7.306969204731685e-05, w1=-0.31841881804075556\n",
      "Regularized Logistic Regression(190/299): loss=0.49204934202230594, w0=-7.343718039814402e-05, w1=-0.3192624691311558\n",
      "Regularized Logistic Regression(191/299): loss=0.49201581293353686, w0=-7.38045827829854e-05, w1=-0.3201025616866689\n",
      "Regularized Logistic Regression(192/299): loss=0.49198263973113904, w0=-7.417189984727152e-05, w1=-0.3209391160218531\n",
      "Regularized Logistic Regression(193/299): loss=0.49194981744123173, w0=-7.453913222739196e-05, w1=-0.3217721522616001\n",
      "Regularized Logistic Regression(194/299): loss=0.49191734118322583, w0=-7.490628055081419e-05, w1=-0.32260169034388453\n",
      "Regularized Logistic Regression(195/299): loss=0.4918852061675559, w0=-7.527334543620068e-05, w1=-0.3234277500224603\n",
      "Regularized Logistic Regression(196/299): loss=0.49185340769347824, w0=-7.564032749352443e-05, w1=-0.3242503508695057\n",
      "Regularized Logistic Regression(197/299): loss=0.491821941146934, w0=-7.60072273241831e-05, w1=-0.3250695122782189\n",
      "Regularized Logistic Regression(198/299): loss=0.4917908019984749, w0=-7.637404552111139e-05, w1=-0.3258852534653645\n",
      "Regularized Logistic Regression(199/299): loss=0.4917599858012474, w0=-7.674078266889206e-05, w1=-0.3266975934737715\n",
      "Regularized Logistic Regression(200/299): loss=0.4917294881890367, w0=-7.710743934386543e-05, w1=-0.3275065511747855\n",
      "Regularized Logistic Regression(201/299): loss=0.49169930487436597, w0=-7.747401611423733e-05, w1=-0.3283121452706759\n",
      "Regularized Logistic Regression(202/299): loss=0.49166943164664967, w0=-7.784051354018576e-05, w1=-0.3291143942969976\n",
      "Regularized Logistic Regression(203/299): loss=0.49163986437040114, w0=-7.820693217396598e-05, w1=-0.32991331662490964\n",
      "Regularized Logistic Regression(204/299): loss=0.49161059898348947, w0=-7.857327256001427e-05, w1=-0.3307089304634514\n",
      "Regularized Logistic Regression(205/299): loss=0.49158163149544487, w0=-7.893953523505032e-05, w1=-0.33150125386177776\n",
      "Regularized Logistic Regression(206/299): loss=0.49155295798581466, w0=-7.930572072817822e-05, w1=-0.33229030471135357\n",
      "Regularized Logistic Regression(207/299): loss=0.4915245746025625, w0=-7.967182956098616e-05, w1=-0.3330761007481081\n",
      "Regularized Logistic Regression(208/299): loss=0.4914964775605135, w0=-8.003786224764478e-05, w1=-0.3338586595545507\n",
      "Regularized Logistic Regression(209/299): loss=0.49146866313984194, w0=-8.040381929500418e-05, w1=-0.3346379985618489\n",
      "Regularized Logistic Regression(210/299): loss=0.49144112768460163, w0=-8.076970120268973e-05, w1=-0.3354141350518711\n",
      "Regularized Logistic Regression(211/299): loss=0.49141386760129596, w0=-8.11355084631966e-05, w1=-0.3361870861591891\n",
      "Regularized Logistic Regression(212/299): loss=0.4913868793574877, w0=-8.150124156198296e-05, w1=-0.3369568688730485\n",
      "Regularized Logistic Regression(213/299): loss=0.49136015948044576, w0=-8.186690097756208e-05, w1=-0.33772350003930357\n",
      "Regularized Logistic Regression(214/299): loss=0.49133370455583053, w0=-8.223248718159314e-05, w1=-0.3384869963623178\n",
      "Regularized Logistic Regression(215/299): loss=0.49130751122641275, w0=-8.259800063897086e-05, w1=-0.33924737440683117\n",
      "Regularized Logistic Regression(216/299): loss=0.4912815761908289, w0=-8.296344180791397e-05, w1=-0.34000465059979607\n",
      "Regularized Logistic Regression(217/299): loss=0.4912558962023676, w0=-8.332881114005253e-05, w1=-0.34075884123217826\n",
      "Regularized Logistic Regression(218/299): loss=0.49123046806779114, w0=-8.369410908051409e-05, w1=-0.3415099624607314\n",
      "Regularized Logistic Regression(219/299): loss=0.49120528864618607, w0=-8.405933606800872e-05, w1=-0.34225803030973706\n",
      "Regularized Logistic Regression(220/299): loss=0.491180354847846, w0=-8.442449253491298e-05, w1=-0.34300306067271613\n",
      "Regularized Logistic Regression(221/299): loss=0.4911556636331821, w0=-8.478957890735272e-05, w1=-0.34374506931411153\n",
      "Regularized Logistic Regression(222/299): loss=0.4911312120116642, w0=-8.51545956052849e-05, w1=-0.3444840718709425\n",
      "Regularized Logistic Regression(223/299): loss=0.49110699704078786, w0=-8.551954304257824e-05, w1=-0.34522008385442865\n",
      "Regularized Logistic Regression(224/299): loss=0.4910830158250688, w0=-8.588442162709288e-05, w1=-0.34595312065158884\n",
      "Regularized Logistic Regression(225/299): loss=0.49105926551506474, w0=-8.624923176075906e-05, w1=-0.34668319752681204\n",
      "Regularized Logistic Regression(226/299): loss=0.4910357433064196, w0=-8.661397383965462e-05, w1=-0.34741032962340124\n",
      "Regularized Logistic Regression(227/299): loss=0.4910124464389356, w0=-8.697864825408171e-05, w1=-0.3481345319650936\n",
      "Regularized Logistic Regression(228/299): loss=0.49098937219566674, w0=-8.734325538864234e-05, w1=-0.3488558194575528\n",
      "Regularized Logistic Regression(229/299): loss=0.4909665179020361, w0=-8.7707795622313e-05, w1=-0.349574206889837\n",
      "Regularized Logistic Regression(230/299): loss=0.490943880924976, w0=-8.807226932851838e-05, w1=-0.3502897089358442\n",
      "Regularized Logistic Regression(231/299): loss=0.4909214586720883, w0=-8.843667687520403e-05, w1=-0.35100234015573156\n",
      "Regularized Logistic Regression(232/299): loss=0.49089924859082834, w0=-8.88010186249082e-05, w1=-0.35171211499731225\n",
      "Regularized Logistic Regression(233/299): loss=0.49087724816770756, w0=-8.916529493483264e-05, w1=-0.35241904779742944\n",
      "Regularized Logistic Regression(234/299): loss=0.49085545492751503, w0=-8.952950615691261e-05, w1=-0.35312315278330747\n",
      "Regularized Logistic Regression(235/299): loss=0.49083386643256116, w0=-8.989365263788586e-05, w1=-0.3538244440738809\n",
      "Regularized Logistic Regression(236/299): loss=0.49081248028193786, w0=-9.025773471936084e-05, w1=-0.3545229356811025\n",
      "Regularized Logistic Regression(237/299): loss=0.49079129411079736, w0=-9.062175273788399e-05, w1=-0.3552186415112293\n",
      "Regularized Logistic Regression(238/299): loss=0.49077030558964896, w0=-9.098570702500608e-05, w1=-0.35591157536608753\n",
      "Regularized Logistic Regression(239/299): loss=0.4907495124236732, w0=-9.13495979073479e-05, w1=-0.35660175094431845\n",
      "Regularized Logistic Regression(240/299): loss=0.4907289123520516, w0=-9.171342570666487e-05, w1=-0.3572891818426027\n",
      "Regularized Logistic Regression(241/299): loss=0.4907085031473139, w0=-9.207719073991107e-05, w1=-0.3579738815568666\n",
      "Regularized Logistic Regression(242/299): loss=0.4906882826147004, w0=-9.24408933193022e-05, w1=-0.35865586348346723\n",
      "Regularized Logistic Regression(243/299): loss=0.4906682485915383, w0=-9.280453375237794e-05, w1=-0.3593351409203603\n",
      "Regularized Logistic Regression(244/299): loss=0.4906483989466364, w0=-9.316811234206339e-05, w1=-0.36001172706824797\n",
      "Regularized Logistic Regression(245/299): loss=0.4906287315796895, w0=-9.353162938672978e-05, w1=-0.3606856350317098\n",
      "Regularized Logistic Regression(246/299): loss=0.4906092444207013, w0=-9.38950851802544e-05, w1=-0.36135687782031556\n",
      "Regularized Logistic Regression(247/299): loss=0.49058993542941753, w0=-9.425848001207976e-05, w1=-0.3620254683497196\n",
      "Regularized Logistic Regression(248/299): loss=0.4905708025947739, w0=-9.462181416727193e-05, w1=-0.3626914194427391\n",
      "Regularized Logistic Regression(249/299): loss=0.4905518439343581, w0=-9.498508792657829e-05, w1=-0.3633547438304146\n",
      "Regularized Logistic Regression(250/299): loss=0.4905330574938803, w0=-9.534830156648439e-05, w1=-0.3640154541530546\n",
      "Regularized Logistic Regression(251/299): loss=0.4905144413466613, w0=-9.571145535927017e-05, w1=-0.36467356296126363\n",
      "Regularized Logistic Regression(252/299): loss=0.4904959935931271, w0=-9.607454957306543e-05, w1=-0.3653290827169539\n",
      "Regularized Logistic Regression(253/299): loss=0.4904777123603197, w0=-9.643758447190462e-05, w1=-0.36598202579434264\n",
      "Regularized Logistic Regression(254/299): loss=0.49045959580141574, w0=-9.68005603157809e-05, w1=-0.3666324044809326\n",
      "Regularized Logistic Regression(255/299): loss=0.49044164209525737, w0=-9.716347736069956e-05, w1=-0.3672802309784775\n",
      "Regularized Logistic Regression(256/299): loss=0.49042384944589396, w0=-9.752633585873071e-05, w1=-0.36792551740393464\n",
      "Regularized Logistic Regression(257/299): loss=0.4904062160821335, w0=-9.788913605806131e-05, w1=-0.3685682757903999\n",
      "Regularized Logistic Regression(258/299): loss=0.49038874025710444, w0=-9.825187820304661e-05, w1=-0.3692085180880313\n",
      "Regularized Logistic Regression(259/299): loss=0.49037142024782765, w0=-9.86145625342608e-05, w1=-0.36984625616495603\n",
      "Regularized Logistic Regression(260/299): loss=0.4903542543547965, w0=-9.897718928854716e-05, w1=-0.3704815018081663\n",
      "Regularized Logistic Regression(261/299): loss=0.4903372409015676, w0=-9.933975869906744e-05, w1=-0.3711142667243992\n",
      "Regularized Logistic Regression(262/299): loss=0.49032037823436037, w0=-9.97022709953508e-05, w1=-0.3717445625410053\n",
      "Regularized Logistic Regression(263/299): loss=0.49030366472166437, w0=-0.00010006472640334185, w1=-0.37237240080680256\n",
      "Regularized Logistic Regression(264/299): loss=0.49028709875385684, w0=-0.0001004271251454484, w1=-0.3729977929929188\n",
      "Regularized Logistic Regression(265/299): loss=0.4902706787428271, w0=-0.00010078946744058831, w1=-0.37362075049362026\n",
      "Regularized Logistic Regression(266/299): loss=0.49025440312160934, w0=-0.00010115175350423601, w1=-0.3742412846271294\n",
      "Regularized Logistic Regression(267/299): loss=0.49023827034402434, w0=-0.00010151398354846819, w1=-0.37485940663642886\n",
      "Regularized Logistic Regression(268/299): loss=0.49022227888432707, w0=-0.0001018761577820091, w1=-0.37547512769005503\n",
      "Regularized Logistic Regression(269/299): loss=0.4902064272368637, w0=-0.00010223827641027515, w1=-0.3760884588828789\n",
      "Regularized Logistic Regression(270/299): loss=0.4901907139157347, w0=-0.00010260033963541905, w1=-0.37669941123687534\n",
      "Regularized Logistic Regression(271/299): loss=0.49017513745446545, w0=-0.00010296234765637326, w1=-0.37730799570188195\n",
      "Regularized Logistic Regression(272/299): loss=0.4901596964056836, w0=-0.00010332430066889304, w1=-0.37791422315634654\n",
      "Regularized Logistic Regression(273/299): loss=0.49014438934080296, w0=-0.00010368619886559882, w1=-0.37851810440806277\n",
      "Regularized Logistic Regression(274/299): loss=0.4901292148497153, w0=-0.00010404804243601815, w1=-0.3791196501948966\n",
      "Regularized Logistic Regression(275/299): loss=0.4901141715404869, w0=-0.00010440983156662701, w1=-0.37971887118550085\n",
      "Regularized Logistic Regression(276/299): loss=0.4900992580390615, w0=-0.00010477156644089067, w1=-0.38031577798002014\n",
      "Regularized Logistic Regression(277/299): loss=0.4900844729889716, w0=-0.00010513324723930402, w1=-0.3809103811107861\n",
      "Regularized Logistic Regression(278/299): loss=0.49006981505105224, w0=-0.00010549487413943137, w1=-0.38150269104300133\n",
      "Regularized Logistic Regression(279/299): loss=0.49005528290316325, w0=-0.00010585644731594577, w1=-0.3820927181754142\n",
      "Regularized Logistic Regression(280/299): loss=0.4900408752399164, w0=-0.00010621796694066778, w1=-0.3826804728409844\n",
      "Regularized Logistic Regression(281/299): loss=0.4900265907724073, w0=-0.00010657943318260385, w1=-0.38326596530753904\n",
      "Regularized Logistic Regression(282/299): loss=0.49001242822795393, w0=-0.0001069408462079841, w1=-0.3838492057784183\n",
      "Regularized Logistic Regression(283/299): loss=0.4899983863498394, w0=-0.00010730220618029969, w1=-0.3844302043931135\n",
      "Regularized Logistic Regression(284/299): loss=0.48998446389706074, w0=-0.00010766351326033971, w1=-0.3850089712278946\n",
      "Regularized Logistic Regression(285/299): loss=0.4899706596440818, w0=-0.00010802476760622759, w1=-0.38558551629643056\n",
      "Regularized Logistic Regression(286/299): loss=0.4899569723805916, w0=-0.00010838596937345703, w1=-0.3861598495503993\n",
      "Regularized Logistic Regression(287/299): loss=0.48994340091126704, w0=-0.00010874711871492757, w1=-0.3867319808800905\n",
      "Regularized Logistic Regression(288/299): loss=0.4899299440555413, w0=-0.00010910821578097954, w1=-0.38730192011499903\n",
      "Regularized Logistic Regression(289/299): loss=0.48991660064737463, w0=-0.00010946926071942872, w1=-0.3878696770244111\n",
      "Regularized Logistic Regression(290/299): loss=0.48990336953503255, w0=-0.0001098302536756005, w1=-0.38843526131798145\n",
      "Regularized Logistic Regression(291/299): loss=0.48989024958086563, w0=-0.00011019119479236362, w1=-0.38899868264630283\n",
      "Regularized Logistic Regression(292/299): loss=0.4898772396610945, w0=-0.00011055208421016345, w1=-0.38955995060146753\n",
      "Regularized Logistic Regression(293/299): loss=0.48986433866560025, w0=-0.00011091292206705491, w1=-0.39011907471762186\n",
      "Regularized Logistic Regression(294/299): loss=0.48985154549771703, w0=-0.00011127370849873489, w1=-0.39067606447151226\n",
      "Regularized Logistic Regression(295/299): loss=0.4898388590740282, w0=-0.00011163444363857434, w1=-0.39123092928302416\n",
      "Regularized Logistic Regression(296/299): loss=0.4898262783241694, w0=-0.00011199512761764994, w1=-0.3917836785157137\n",
      "Regularized Logistic Regression(297/299): loss=0.4898138021906328, w0=-0.00011235576056477531, w1=-0.39233432147733305\n",
      "Regularized Logistic Regression(298/299): loss=0.4898014296285739, w0=-0.00011271634260653185, w1=-0.3928828674203457\n",
      "Regularized Logistic Regression(299/299): loss=0.48978915960562597, w0=-0.00011307687386729923, w1=-0.3934293255424391\n",
      "Regularized Logistic Regression(0/99): loss=0.693147180559945, w0=-4.176586058555737e-07, w1=-0.007317796433195506\n",
      "Regularized Logistic Regression(1/99): loss=0.6631421228013892, w0=-8.319161077767281e-07, w1=-0.013953291295873332\n",
      "Regularized Logistic Regression(2/99): loss=0.6393674860242854, w0=-1.2441551717520702e-06, w1=-0.020025190714897684\n",
      "Regularized Logistic Regression(3/99): loss=0.620390429000631, w0=-1.655077154883349e-06, w1=-0.025625129559704127\n",
      "Regularized Logistic Regression(4/99): loss=0.6051220962008732, w0=-2.065003578756265e-06, w1=-0.030824726138656857\n",
      "Regularized Logistic Regression(5/99): loss=0.5927331483762237, w0=-2.4740541926482924e-06, w1=-0.03568092305159978\n",
      "Regularized Logistic Regression(6/99): loss=0.582591676281513, w0=-2.8822471821388617e-06, w1=-0.04023975636915049\n",
      "Regularized Logistic Regression(7/99): loss=0.574215552590394, w0=-3.289553624124686e-06, w1=-0.04453898557686695\n",
      "Regularized Logistic Regression(8/99): loss=0.5672357940142978, w0=-3.695926162476843e-06, w1=-0.048609955842565285\n",
      "Regularized Logistic Regression(9/99): loss=0.5613686953920635, w0=-4.101313544509095e-06, w1=-0.0524789492651343\n",
      "Regularized Logistic Regression(10/99): loss=0.556394859541102, w0=-4.50566755709128e-06, w1=-0.05616819057803113\n",
      "Regularized Logistic Regression(11/99): loss=0.5521435211062075, w0=-4.908945958105961e-06, w1=-0.05969661275742897\n",
      "Regularized Logistic Regression(12/99): loss=0.5484808579119941, w0=-5.311113354010735e-06, w1=-0.06308045082642151\n",
      "Regularized Logistic Regression(13/99): loss=0.5453012695561017, w0=-5.712141070414004e-06, w1=-0.066333709378637\n",
      "Regularized Logistic Regression(14/99): loss=0.5425208503791416, w0=-6.1120065704434954e-06, w1=-0.0694685351696763\n",
      "Regularized Logistic Regression(15/99): loss=0.5400724823057832, w0=-6.510692709231487e-06, w1=-0.0724955170367587\n",
      "Regularized Logistic Regression(16/99): loss=0.5379021249531704, w0=-6.908186969335768e-06, w1=-0.07542392937655916\n",
      "Regularized Logistic Regression(17/99): loss=0.5359659935945635, w0=-7.304480745243549e-06, w1=-0.07826193127233702\n",
      "Regularized Logistic Regression(18/99): loss=0.5342283986125909, w0=-7.699568704723056e-06, w1=-0.08101673043425148\n",
      "Regularized Logistic Regression(19/99): loss=0.5326600805230255, w0=-8.09344823407005e-06, w1=-0.08369471899400652\n",
      "Regularized Logistic Regression(20/99): loss=0.5312369185145541, w0=-8.48611896418225e-06, w1=-0.08630158662418201\n",
      "Regularized Logistic Regression(21/99): loss=0.5299389222896768, w0=-8.87758236993692e-06, w1=-0.08884241527142665\n",
      "Regularized Logistic Regression(22/99): loss=0.5287494401593299, w0=-9.267841433844919e-06, w1=-0.09132175889299035\n",
      "Regularized Logistic Regression(23/99): loss=0.5276545332665469, w0=-9.656900364939242e-06, w1=-0.09374371089357761\n",
      "Regularized Logistic Regression(24/99): loss=0.5266424782352863, w0=-1.0044764364533633e-05, w1=-0.09611196142181605\n",
      "Regularized Logistic Regression(25/99): loss=0.5257033697065763, w0=-1.0431439431436198e-05, w1=-0.09842984626505455\n",
      "Regularized Logistic Regression(26/99): loss=0.5248288010273802, w0=-1.0816932200206966e-05, w1=-0.10070038874998882\n",
      "Regularized Logistic Regression(27/99): loss=0.5240116064373118, w0=-1.1201249807001465e-05, w1=-0.10292633579427361\n",
      "Regularized Logistic Regression(28/99): loss=0.52324565191413, w0=-1.1584399778398927e-05, w1=-0.10511018904531838\n",
      "Regularized Logistic Regression(29/99): loss=0.5225256647226154, w0=-1.1966389939360165e-05, w1=-0.10725423187516511\n",
      "Regularized Logistic Regression(30/99): loss=0.5218470939036931, w0=-1.234722833709785e-05, w1=-0.10936055286575916\n",
      "Regularized Logistic Regression(31/99): loss=0.5212059956170689, w0=-1.2726923178180726e-05, w1=-0.11143106631013444\n",
      "Regularized Logistic Regression(32/99): loss=0.520598938539894, w0=-1.3105482776644422e-05, w1=-0.11346753016671744\n",
      "Regularized Logistic Regression(33/99): loss=0.5200229255210278, w0=-1.3482915511258117e-05, w1=-0.11547156183193594\n",
      "Regularized Logistic Regression(34/99): loss=0.5194753284656618, w0=-1.3859229790408971e-05, w1=-0.11744465203735036\n",
      "Regularized Logistic Regression(35/99): loss=0.5189538340309627, w0=-1.423443402332612e-05, w1=-0.11938817712905056\n",
      "Regularized Logistic Regression(36/99): loss=0.5184563981893033, w0=-1.4608536596581201e-05, w1=-0.12130340994704615\n",
      "Regularized Logistic Regression(37/99): loss=0.5179812080913131, w0=-1.4981545854981103e-05, w1=-0.12319152948923771\n",
      "Regularized Logistic Regression(38/99): loss=0.5175266499588558, w0=-1.5353470086116528e-05, w1=-0.12505362951698398\n",
      "Regularized Logistic Regression(39/99): loss=0.5170912819753442, w0=-1.5724317507952927e-05, w1=-0.12689072623628758\n",
      "Regularized Logistic Regression(40/99): loss=0.5166738113306185, w0=-1.609409625895229e-05, w1=-0.128703765169346\n",
      "Regularized Logistic Regression(41/99): loss=0.5162730747301179, w0=-1.6462814390299195e-05, w1=-0.1304936273150424\n",
      "Regularized Logistic Regression(42/99): loss=0.5158880218010486, w0=-1.683047985987479e-05, w1=-0.13226113468329057\n",
      "Regularized Logistic Regression(43/99): loss=0.515517700927825, w0=-1.719710052768139e-05, w1=-0.13400705527662019\n",
      "Regularized Logistic Regression(44/99): loss=0.5151612471299506, w0=-1.7562684152469044e-05, w1=-0.1357321075825875\n",
      "Regularized Logistic Regression(45/99): loss=0.5148178716614813, w0=-1.7927238389356432e-05, w1=-0.13743696463226895\n",
      "Regularized Logistic Regression(46/99): loss=0.51448685306516, w0=-1.829077078827238e-05, w1=-0.13912225767298309\n",
      "Regularized Logistic Regression(47/99): loss=0.5141675294586209, w0=-1.8653288793072876e-05, w1=-0.14078857949730267\n",
      "Regularized Logistic Regression(48/99): loss=0.513859291866507, w0=-1.901479974121223e-05, w1=-0.14243648746518747\n",
      "Regularized Logistic Regression(49/99): loss=0.5135615784424634, w0=-1.937531086386693e-05, w1=-0.1440665062515798\n",
      "Regularized Logistic Regression(50/99): loss=0.5132738694498801, w0=-1.9734829286427473e-05, w1=-0.14567913034792065\n",
      "Regularized Logistic Regression(51/99): loss=0.5129956828909722, w0=-2.009336202928745e-05, w1=-0.14727482634268735\n",
      "Regularized Logistic Regression(52/99): loss=0.5127265706909896, w0=-2.045091600887076e-05, w1=-0.14885403500314445\n",
      "Regularized Logistic Regression(53/99): loss=0.5124661153587379, w0=-2.0807498038847846e-05, w1=-0.1504171731779649\n",
      "Regularized Logistic Regression(54/99): loss=0.5122139270566057, w0=-2.1163114831499924e-05, w1=-0.15196463553817705\n",
      "Regularized Logistic Regression(55/99): loss=0.511969641023367, w0=-2.151777299919714e-05, w1=-0.15349679617195686\n",
      "Regularized Logistic Regression(56/99): loss=0.5117329153015042, w0=-2.187147905596245e-05, w1=-0.15501401004710855\n",
      "Regularized Logistic Regression(57/99): loss=0.5115034287279097, w0=-2.2224239419097777e-05, w1=-0.15651661435358627\n",
      "Regularized Logistic Regression(58/99): loss=0.5112808791528447, w0=-2.2576060410853168e-05, w1=-0.15800492973711103\n",
      "Regularized Logistic Regression(59/99): loss=0.5110649818571127, w0=-2.292694826012304e-05, w1=-0.15947926143378907\n",
      "Regularized Logistic Regression(60/99): loss=0.5108554681417011, w0=-2.327690910415646e-05, w1=-0.16093990031462535\n",
      "Regularized Logistic Regression(61/99): loss=0.5106520840677975, w0=-2.362594899027087e-05, w1=-0.16238712384792503\n",
      "Regularized Logistic Regression(62/99): loss=0.5104545893281851, w0=-2.397407387756062e-05, w1=-0.16382119698678765\n",
      "Regularized Logistic Regression(63/99): loss=0.510262756233662, w0=-2.4321289638593328e-05, w1=-0.16524237298818267\n",
      "Regularized Logistic Regression(64/99): loss=0.5100763688003679, w0=-2.4667602061088548e-05, w1=-0.1666508941694772\n",
      "Regularized Logistic Regression(65/99): loss=0.5098952219258371, w0=-2.501301684957434e-05, w1=-0.168046992607721\n",
      "Regularized Logistic Regression(66/99): loss=0.5097191206432258, w0=-2.535753962701839e-05, w1=-0.16943089078649023\n",
      "Regularized Logistic Regression(67/99): loss=0.5095478794445654, w0=-2.5701175936430968e-05, w1=-0.1708028021946535\n",
      "Regularized Logistic Regression(68/99): loss=0.5093813216651121, w0=-2.6043931242437873e-05, w1=-0.17216293188101273\n",
      "Regularized Logistic Regression(69/99): loss=0.5092192789218848, w0=-2.6385810932821943e-05, w1=-0.17351147696841956\n",
      "Regularized Logistic Regression(70/99): loss=0.5090615906003749, w0=-2.672682032003218e-05, w1=-0.174848627130639\n",
      "Regularized Logistic Regression(71/99): loss=0.508908103384188, w0=-2.7066964642659945e-05, w1=-0.17617456503494527\n",
      "Regularized Logistic Regression(72/99): loss=0.5087586708230224, w0=-2.740624906688195e-05, w1=-0.17748946675316912\n",
      "Regularized Logistic Regression(73/99): loss=0.5086131529349748, w0=-2.7744678687870037e-05, w1=-0.1787935021436829\n",
      "Regularized Logistic Regression(74/99): loss=0.508471415839645, w0=-2.8082258531167896e-05, w1=-0.18008683520659066\n",
      "Regularized Logistic Regression(75/99): loss=0.5083333314189474, w0=-2.8418993554035104e-05, w1=-0.18136962441420248\n",
      "Regularized Logistic Regression(76/99): loss=0.5081987770029013, w0=-2.8754888646758897e-05, w1=-0.1826420230186935\n",
      "Regularized Logistic Regression(77/99): loss=0.5080676350779937, w0=-2.908994863393427e-05, w1=-0.18390417933868922\n",
      "Regularized Logistic Regression(78/99): loss=0.5079397930159952, w0=-2.9424178275712997e-05, w1=-0.1851562370263749\n",
      "Regularized Logistic Regression(79/99): loss=0.5078151428213391, w0=-2.9757582269022277e-05, w1=-0.18639833531659675\n",
      "Regularized Logistic Regression(80/99): loss=0.5076935808954012, w0=-3.0090165248753756e-05, w1=-0.187630609259298\n",
      "Regularized Logistic Regression(81/99): loss=0.5075750078161907, w0=-3.042193178892359e-05, w1=-0.18885318993653263\n",
      "Regularized Logistic Regression(82/99): loss=0.5074593281321296, w0=-3.0752886403804394e-05, w1=-0.19006620466519056\n",
      "Regularized Logistic Regression(83/99): loss=0.507346450168743, w0=-3.108303354902981e-05, w1=-0.19126977718648372\n",
      "Regularized Logistic Regression(84/99): loss=0.5072362858471986, w0=-3.1412377622672474e-05, w1=-0.19246402784315955\n",
      "Regularized Logistic Regression(85/99): loss=0.5071287505137516, w0=-3.174092296629613e-05, w1=-0.19364907374532753\n",
      "Regularized Logistic Regression(86/99): loss=0.5070237627792388, w0=-3.2068673865982666e-05, w1=-0.1948250289257221\n",
      "Regularized Logistic Regression(87/99): loss=0.5069212443678611, w0=-3.2395634553334835e-05, w1=-0.19599200448515475\n",
      "Regularized Logistic Regression(88/99): loss=0.5068211199745555, w0=-3.27218092064553e-05, w1=-0.19715010872885633\n",
      "Regularized Logistic Regression(89/99): loss=0.5067233171303365, w0=-3.304720195090283e-05, w1=-0.19829944729435156\n",
      "Regularized Logistic Regression(90/99): loss=0.5066277660750322, w0=-3.337181686062623e-05, w1=-0.19944012327146313\n",
      "Regularized Logistic Regression(91/99): loss=0.5065343996369088, w0=-3.369565795887679e-05, w1=-0.20057223731499552\n",
      "Regularized Logistic Regression(92/99): loss=0.5064431531187031, w0=-3.401872921909979e-05, w1=-0.20169588775060612\n",
      "Regularized Logistic Regression(93/99): loss=0.506353964189645, w0=-3.434103456580573e-05, w1=-0.20281117067433788\n",
      "Regularized Logistic Regression(94/99): loss=0.5062667727830703, w0=-3.466257787542196e-05, w1=-0.2039181800462456\n",
      "Regularized Logistic Regression(95/99): loss=0.5061815209992716, w0=-3.4983362977125184e-05, w1=-0.20501700777852375\n",
      "Regularized Logistic Regression(96/99): loss=0.506098153013252, w0=-3.5303393653655494e-05, w1=-0.20610774381850502\n",
      "Regularized Logistic Regression(97/99): loss=0.5060166149870832, w0=-3.5622673642112455e-05, w1=-0.2071904762268815\n",
      "Regularized Logistic Regression(98/99): loss=0.5059368549865881, w0=-3.594120663473375e-05, w1=-0.2082652912514664\n",
      "Regularized Logistic Regression(99/99): loss=0.5058588229020884, w0=-3.6258996279656935e-05, w1=-0.20933227339679325\n",
      "Regularized Logistic Regression(0/199): loss=0.693147180559945, w0=-4.176586058555737e-07, w1=-0.007317796433195506\n",
      "Regularized Logistic Regression(1/199): loss=0.6631421228013892, w0=-8.319161077767281e-07, w1=-0.013953291295873332\n",
      "Regularized Logistic Regression(2/199): loss=0.6393674860242854, w0=-1.2441551717520702e-06, w1=-0.020025190714897684\n",
      "Regularized Logistic Regression(3/199): loss=0.620390429000631, w0=-1.655077154883349e-06, w1=-0.025625129559704127\n",
      "Regularized Logistic Regression(4/199): loss=0.6051220962008732, w0=-2.065003578756265e-06, w1=-0.030824726138656857\n",
      "Regularized Logistic Regression(5/199): loss=0.5927331483762237, w0=-2.4740541926482924e-06, w1=-0.03568092305159978\n",
      "Regularized Logistic Regression(6/199): loss=0.582591676281513, w0=-2.8822471821388617e-06, w1=-0.04023975636915049\n",
      "Regularized Logistic Regression(7/199): loss=0.574215552590394, w0=-3.289553624124686e-06, w1=-0.04453898557686695\n",
      "Regularized Logistic Regression(8/199): loss=0.5672357940142978, w0=-3.695926162476843e-06, w1=-0.048609955842565285\n",
      "Regularized Logistic Regression(9/199): loss=0.5613686953920635, w0=-4.101313544509095e-06, w1=-0.0524789492651343\n",
      "Regularized Logistic Regression(10/199): loss=0.556394859541102, w0=-4.50566755709128e-06, w1=-0.05616819057803113\n",
      "Regularized Logistic Regression(11/199): loss=0.5521435211062075, w0=-4.908945958105961e-06, w1=-0.05969661275742897\n",
      "Regularized Logistic Regression(12/199): loss=0.5484808579119941, w0=-5.311113354010735e-06, w1=-0.06308045082642151\n",
      "Regularized Logistic Regression(13/199): loss=0.5453012695561017, w0=-5.712141070414004e-06, w1=-0.066333709378637\n",
      "Regularized Logistic Regression(14/199): loss=0.5425208503791416, w0=-6.1120065704434954e-06, w1=-0.0694685351696763\n",
      "Regularized Logistic Regression(15/199): loss=0.5400724823057832, w0=-6.510692709231487e-06, w1=-0.0724955170367587\n",
      "Regularized Logistic Regression(16/199): loss=0.5379021249531704, w0=-6.908186969335768e-06, w1=-0.07542392937655916\n",
      "Regularized Logistic Regression(17/199): loss=0.5359659935945635, w0=-7.304480745243549e-06, w1=-0.07826193127233702\n",
      "Regularized Logistic Regression(18/199): loss=0.5342283986125909, w0=-7.699568704723056e-06, w1=-0.08101673043425148\n",
      "Regularized Logistic Regression(19/199): loss=0.5326600805230255, w0=-8.09344823407005e-06, w1=-0.08369471899400652\n",
      "Regularized Logistic Regression(20/199): loss=0.5312369185145541, w0=-8.48611896418225e-06, w1=-0.08630158662418201\n",
      "Regularized Logistic Regression(21/199): loss=0.5299389222896768, w0=-8.87758236993692e-06, w1=-0.08884241527142665\n",
      "Regularized Logistic Regression(22/199): loss=0.5287494401593299, w0=-9.267841433844919e-06, w1=-0.09132175889299035\n",
      "Regularized Logistic Regression(23/199): loss=0.5276545332665469, w0=-9.656900364939242e-06, w1=-0.09374371089357761\n",
      "Regularized Logistic Regression(24/199): loss=0.5266424782352863, w0=-1.0044764364533633e-05, w1=-0.09611196142181605\n",
      "Regularized Logistic Regression(25/199): loss=0.5257033697065763, w0=-1.0431439431436198e-05, w1=-0.09842984626505455\n",
      "Regularized Logistic Regression(26/199): loss=0.5248288010273802, w0=-1.0816932200206966e-05, w1=-0.10070038874998882\n",
      "Regularized Logistic Regression(27/199): loss=0.5240116064373118, w0=-1.1201249807001465e-05, w1=-0.10292633579427361\n",
      "Regularized Logistic Regression(28/199): loss=0.52324565191413, w0=-1.1584399778398927e-05, w1=-0.10511018904531838\n",
      "Regularized Logistic Regression(29/199): loss=0.5225256647226154, w0=-1.1966389939360165e-05, w1=-0.10725423187516511\n",
      "Regularized Logistic Regression(30/199): loss=0.5218470939036931, w0=-1.234722833709785e-05, w1=-0.10936055286575916\n",
      "Regularized Logistic Regression(31/199): loss=0.5212059956170689, w0=-1.2726923178180726e-05, w1=-0.11143106631013444\n",
      "Regularized Logistic Regression(32/199): loss=0.520598938539894, w0=-1.3105482776644422e-05, w1=-0.11346753016671744\n",
      "Regularized Logistic Regression(33/199): loss=0.5200229255210278, w0=-1.3482915511258117e-05, w1=-0.11547156183193594\n",
      "Regularized Logistic Regression(34/199): loss=0.5194753284656618, w0=-1.3859229790408971e-05, w1=-0.11744465203735036\n",
      "Regularized Logistic Regression(35/199): loss=0.5189538340309627, w0=-1.423443402332612e-05, w1=-0.11938817712905056\n",
      "Regularized Logistic Regression(36/199): loss=0.5184563981893033, w0=-1.4608536596581201e-05, w1=-0.12130340994704615\n",
      "Regularized Logistic Regression(37/199): loss=0.5179812080913131, w0=-1.4981545854981103e-05, w1=-0.12319152948923771\n",
      "Regularized Logistic Regression(38/199): loss=0.5175266499588558, w0=-1.5353470086116528e-05, w1=-0.12505362951698398\n",
      "Regularized Logistic Regression(39/199): loss=0.5170912819753442, w0=-1.5724317507952927e-05, w1=-0.12689072623628758\n",
      "Regularized Logistic Regression(40/199): loss=0.5166738113306185, w0=-1.609409625895229e-05, w1=-0.128703765169346\n",
      "Regularized Logistic Regression(41/199): loss=0.5162730747301179, w0=-1.6462814390299195e-05, w1=-0.1304936273150424\n",
      "Regularized Logistic Regression(42/199): loss=0.5158880218010486, w0=-1.683047985987479e-05, w1=-0.13226113468329057\n",
      "Regularized Logistic Regression(43/199): loss=0.515517700927825, w0=-1.719710052768139e-05, w1=-0.13400705527662019\n",
      "Regularized Logistic Regression(44/199): loss=0.5151612471299506, w0=-1.7562684152469044e-05, w1=-0.1357321075825875\n",
      "Regularized Logistic Regression(45/199): loss=0.5148178716614813, w0=-1.7927238389356432e-05, w1=-0.13743696463226895\n",
      "Regularized Logistic Regression(46/199): loss=0.51448685306516, w0=-1.829077078827238e-05, w1=-0.13912225767298309\n",
      "Regularized Logistic Regression(47/199): loss=0.5141675294586209, w0=-1.8653288793072876e-05, w1=-0.14078857949730267\n",
      "Regularized Logistic Regression(48/199): loss=0.513859291866507, w0=-1.901479974121223e-05, w1=-0.14243648746518747\n",
      "Regularized Logistic Regression(49/199): loss=0.5135615784424634, w0=-1.937531086386693e-05, w1=-0.1440665062515798\n",
      "Regularized Logistic Regression(50/199): loss=0.5132738694498801, w0=-1.9734829286427473e-05, w1=-0.14567913034792065\n",
      "Regularized Logistic Regression(51/199): loss=0.5129956828909722, w0=-2.009336202928745e-05, w1=-0.14727482634268735\n",
      "Regularized Logistic Regression(52/199): loss=0.5127265706909896, w0=-2.045091600887076e-05, w1=-0.14885403500314445\n",
      "Regularized Logistic Regression(53/199): loss=0.5124661153587379, w0=-2.0807498038847846e-05, w1=-0.1504171731779649\n",
      "Regularized Logistic Regression(54/199): loss=0.5122139270566057, w0=-2.1163114831499924e-05, w1=-0.15196463553817705\n",
      "Regularized Logistic Regression(55/199): loss=0.511969641023367, w0=-2.151777299919714e-05, w1=-0.15349679617195686\n",
      "Regularized Logistic Regression(56/199): loss=0.5117329153015042, w0=-2.187147905596245e-05, w1=-0.15501401004710855\n",
      "Regularized Logistic Regression(57/199): loss=0.5115034287279097, w0=-2.2224239419097777e-05, w1=-0.15651661435358627\n",
      "Regularized Logistic Regression(58/199): loss=0.5112808791528447, w0=-2.2576060410853168e-05, w1=-0.15800492973711103\n",
      "Regularized Logistic Regression(59/199): loss=0.5110649818571127, w0=-2.292694826012304e-05, w1=-0.15947926143378907\n",
      "Regularized Logistic Regression(60/199): loss=0.5108554681417011, w0=-2.327690910415646e-05, w1=-0.16093990031462535\n",
      "Regularized Logistic Regression(61/199): loss=0.5106520840677975, w0=-2.362594899027087e-05, w1=-0.16238712384792503\n",
      "Regularized Logistic Regression(62/199): loss=0.5104545893281851, w0=-2.397407387756062e-05, w1=-0.16382119698678765\n",
      "Regularized Logistic Regression(63/199): loss=0.510262756233662, w0=-2.4321289638593328e-05, w1=-0.16524237298818267\n",
      "Regularized Logistic Regression(64/199): loss=0.5100763688003679, w0=-2.4667602061088548e-05, w1=-0.1666508941694772\n",
      "Regularized Logistic Regression(65/199): loss=0.5098952219258371, w0=-2.501301684957434e-05, w1=-0.168046992607721\n",
      "Regularized Logistic Regression(66/199): loss=0.5097191206432258, w0=-2.535753962701839e-05, w1=-0.16943089078649023\n",
      "Regularized Logistic Regression(67/199): loss=0.5095478794445654, w0=-2.5701175936430968e-05, w1=-0.1708028021946535\n",
      "Regularized Logistic Regression(68/199): loss=0.5093813216651121, w0=-2.6043931242437873e-05, w1=-0.17216293188101273\n",
      "Regularized Logistic Regression(69/199): loss=0.5092192789218848, w0=-2.6385810932821943e-05, w1=-0.17351147696841956\n",
      "Regularized Logistic Regression(70/199): loss=0.5090615906003749, w0=-2.672682032003218e-05, w1=-0.174848627130639\n",
      "Regularized Logistic Regression(71/199): loss=0.508908103384188, w0=-2.7066964642659945e-05, w1=-0.17617456503494527\n",
      "Regularized Logistic Regression(72/199): loss=0.5087586708230224, w0=-2.740624906688195e-05, w1=-0.17748946675316912\n",
      "Regularized Logistic Regression(73/199): loss=0.5086131529349748, w0=-2.7744678687870037e-05, w1=-0.1787935021436829\n",
      "Regularized Logistic Regression(74/199): loss=0.508471415839645, w0=-2.8082258531167896e-05, w1=-0.18008683520659066\n",
      "Regularized Logistic Regression(75/199): loss=0.5083333314189474, w0=-2.8418993554035104e-05, w1=-0.18136962441420248\n",
      "Regularized Logistic Regression(76/199): loss=0.5081987770029013, w0=-2.8754888646758897e-05, w1=-0.1826420230186935\n",
      "Regularized Logistic Regression(77/199): loss=0.5080676350779937, w0=-2.908994863393427e-05, w1=-0.18390417933868922\n",
      "Regularized Logistic Regression(78/199): loss=0.5079397930159952, w0=-2.9424178275712997e-05, w1=-0.1851562370263749\n",
      "Regularized Logistic Regression(79/199): loss=0.5078151428213391, w0=-2.9757582269022277e-05, w1=-0.18639833531659675\n",
      "Regularized Logistic Regression(80/199): loss=0.5076935808954012, w0=-3.0090165248753756e-05, w1=-0.187630609259298\n",
      "Regularized Logistic Regression(81/199): loss=0.5075750078161907, w0=-3.042193178892359e-05, w1=-0.18885318993653263\n",
      "Regularized Logistic Regression(82/199): loss=0.5074593281321296, w0=-3.0752886403804394e-05, w1=-0.19006620466519056\n",
      "Regularized Logistic Regression(83/199): loss=0.507346450168743, w0=-3.108303354902981e-05, w1=-0.19126977718648372\n",
      "Regularized Logistic Regression(84/199): loss=0.5072362858471986, w0=-3.1412377622672474e-05, w1=-0.19246402784315955\n",
      "Regularized Logistic Regression(85/199): loss=0.5071287505137516, w0=-3.174092296629613e-05, w1=-0.19364907374532753\n",
      "Regularized Logistic Regression(86/199): loss=0.5070237627792388, w0=-3.2068673865982666e-05, w1=-0.1948250289257221\n",
      "Regularized Logistic Regression(87/199): loss=0.5069212443678611, w0=-3.2395634553334835e-05, w1=-0.19599200448515475\n",
      "Regularized Logistic Regression(88/199): loss=0.5068211199745555, w0=-3.27218092064553e-05, w1=-0.19715010872885633\n",
      "Regularized Logistic Regression(89/199): loss=0.5067233171303365, w0=-3.304720195090283e-05, w1=-0.19829944729435156\n",
      "Regularized Logistic Regression(90/199): loss=0.5066277660750322, w0=-3.337181686062623e-05, w1=-0.19944012327146313\n",
      "Regularized Logistic Regression(91/199): loss=0.5065343996369088, w0=-3.369565795887679e-05, w1=-0.20057223731499552\n",
      "Regularized Logistic Regression(92/199): loss=0.5064431531187031, w0=-3.401872921909979e-05, w1=-0.20169588775060612\n",
      "Regularized Logistic Regression(93/199): loss=0.506353964189645, w0=-3.434103456580573e-05, w1=-0.20281117067433788\n",
      "Regularized Logistic Regression(94/199): loss=0.5062667727830703, w0=-3.466257787542196e-05, w1=-0.2039181800462456\n",
      "Regularized Logistic Regression(95/199): loss=0.5061815209992716, w0=-3.4983362977125184e-05, w1=-0.20501700777852375\n",
      "Regularized Logistic Regression(96/199): loss=0.506098153013252, w0=-3.5303393653655494e-05, w1=-0.20610774381850502\n",
      "Regularized Logistic Regression(97/199): loss=0.5060166149870832, w0=-3.5622673642112455e-05, w1=-0.2071904762268815\n",
      "Regularized Logistic Regression(98/199): loss=0.5059368549865881, w0=-3.594120663473375e-05, w1=-0.2082652912514664\n",
      "Regularized Logistic Regression(99/199): loss=0.5058588229020884, w0=-3.6258996279656935e-05, w1=-0.20933227339679325\n",
      "Regularized Logistic Regression(100/199): loss=0.5057824703729819, w0=-3.657604618166472e-05, w1=-0.2103915054898354\n",
      "Regularized Logistic Regression(101/199): loss=0.5057077507159242, w0=-3.689235990291436e-05, w1=-0.2114430687420934\n",
      "Regularized Logistic Regression(102/199): loss=0.5056346188564179, w0=-3.720794096365153e-05, w1=-0.21248704280829656\n",
      "Regularized Logistic Regression(103/199): loss=0.5055630312636085, w0=-3.752279284290913e-05, w1=-0.21352350584193477\n",
      "Regularized Logistic Regression(104/199): loss=0.5054929458881205, w0=-3.7836918979191466e-05, w1=-0.21455253454782908\n",
      "Regularized Logistic Regression(105/199): loss=0.5054243221027602, w0=-3.8150322771144216e-05, w1=-0.21557420423193124\n",
      "Regularized Logistic Regression(106/199): loss=0.505357120645936, w0=-3.846300757821056e-05, w1=-0.21658858884852886\n",
      "Regularized Logistic Regression(107/199): loss=0.5052913035676505, w0=-3.877497672127386e-05, w1=-0.21759576104502232\n",
      "Regularized Logistic Regression(108/199): loss=0.5052268341779291, w0=-3.908623348328723e-05, w1=-0.21859579220442774\n",
      "Regularized Logistic Regression(109/199): loss=0.5051636769975598, w0=-3.939678110989041e-05, w1=-0.21958875248574766\n",
      "Regularized Logistic Regression(110/199): loss=0.5051017977110248, w0=-3.970662281001423e-05, w1=-0.22057471086234298\n",
      "Regularized Logistic Regression(111/199): loss=0.5050411631215138, w0=-4.001576175647298e-05, w1=-0.22155373515843085\n",
      "Regularized Logistic Regression(112/199): loss=0.5049817411079125, w0=-4.0324201086545055e-05, w1=-0.22252589208382262\n",
      "Regularized Logistic Regression(113/199): loss=0.5049235005836704, w0=-4.0631943902542185e-05, w1=-0.2234912472670105\n",
      "Regularized Logistic Regression(114/199): loss=0.5048664114574526, w0=-4.093899327236744e-05, w1=-0.22444986528670266\n",
      "Regularized Logistic Regression(115/199): loss=0.5048104445954896, w0=-4.124535223006244e-05, w1=-0.22540180970190132\n",
      "Regularized Logistic Regression(116/199): loss=0.5047555717855441, w0=-4.155102377634394e-05, w1=-0.2263471430806076\n",
      "Regularized Logistic Regression(117/199): loss=0.5047017657024125, w0=-4.185601087913009e-05, w1=-0.22728592702723865\n",
      "Regularized Logistic Regression(118/199): loss=0.5046489998748941, w0=-4.216031647405665e-05, w1=-0.22821822220883112\n",
      "Regularized Logistic Regression(119/199): loss=0.5045972486541508, w0=-4.246394346498335e-05, w1=-0.2291440883801006\n",
      "Regularized Logistic Regression(120/199): loss=0.5045464871834, w0=-4.2766894724490715e-05, w1=-0.2300635844074251\n",
      "Regularized Logistic Regression(121/199): loss=0.50449669136887, w0=-4.306917309436754e-05, w1=-0.23097676829181332\n",
      "Regularized Logistic Regression(122/199): loss=0.5044478378519668, w0=-4.33707813860892e-05, w1=-0.2318836971909155\n",
      "Regularized Logistic Regression(123/199): loss=0.5043999039825895, w0=-4.3671722381287115e-05, w1=-0.23278442744012975\n",
      "Regularized Logistic Regression(124/199): loss=0.5043528677935476, w0=-4.3971998832209484e-05, w1=-0.2336790145728582\n",
      "Regularized Logistic Regression(125/199): loss=0.5043067079760234, w0=-4.427161346217359e-05, w1=-0.2345675133399537\n",
      "Regularized Logistic Regression(126/199): loss=0.50426140385604, w0=-4.4570568966009725e-05, w1=-0.23544997772840737\n",
      "Regularized Logistic Regression(127/199): loss=0.5042169353718796, w0=-4.4868868010497136e-05, w1=-0.23632646097931476\n",
      "Regularized Logistic Regression(128/199): loss=0.504173283052419, w0=-4.516651323479196e-05, w1=-0.23719701560515952\n",
      "Regularized Logistic Regression(129/199): loss=0.5041304279963355, w0=-4.5463507250847495e-05, w1=-0.238061693406453\n",
      "Regularized Logistic Regression(130/199): loss=0.5040883518521444, w0=-4.575985264382691e-05, w1=-0.23892054548776073\n",
      "Regularized Logistic Regression(131/199): loss=0.5040470367990347, w0=-4.6055551972508545e-05, w1=-0.2397736222731489\n",
      "Regularized Logistic Regression(132/199): loss=0.5040064655284648, w0=-4.6350607769684035e-05, w1=-0.24062097352108192\n",
      "Regularized Logistic Regression(133/199): loss=0.5039666212264863, w0=-4.6645022542549334e-05, w1=-0.2414626483387972\n",
      "Regularized Logistic Regression(134/199): loss=0.5039274875567626, w0=-4.693879877308888e-05, w1=-0.2422986951961832\n",
      "Regularized Logistic Regression(135/199): loss=0.5038890486442541, w0=-4.7231938918452985e-05, w1=-0.24312916193918735\n",
      "Regularized Logistic Regression(136/199): loss=0.5038512890595401, w0=-4.752444541132862e-05, w1=-0.24395409580277438\n",
      "Regularized Logistic Regression(137/199): loss=0.5038141938037487, w0=-4.781632066030379e-05, w1=-0.24477354342346025\n",
      "Regularized Logistic Regression(138/199): loss=0.5037777482940712, w0=-4.810756705022548e-05, w1=-0.24558755085143763\n",
      "Regularized Logistic Regression(139/199): loss=0.5037419383498338, w0=-4.839818694255155e-05, w1=-0.24639616356231636\n",
      "Regularized Logistic Regression(140/199): loss=0.5037067501791039, w0=-4.8688182675696454e-05, w1=-0.24719942646849355\n",
      "Regularized Logistic Regression(141/199): loss=0.5036721703658075, w0=-4.897755656537113e-05, w1=-0.2479973839301729\n",
      "Regularized Logistic Regression(142/199): loss=0.5036381858573376, w0=-4.9266310904917004e-05, w1=-0.24879007976604842\n",
      "Regularized Logistic Regression(143/199): loss=0.5036047839526312, w0=-4.955444796563436e-05, w1=-0.24957755726366582\n",
      "Regularized Logistic Regression(144/199): loss=0.5035719522906965, w0=-4.984196999710512e-05, w1=-0.2503598591894795\n",
      "Regularized Logistic Regression(145/199): loss=0.5035396788395706, w0=-5.0128879227510186e-05, w1=-0.25113702779861385\n",
      "Regularized Logistic Regression(146/199): loss=0.5035079518856898, w0=-5.041517786394145e-05, w1=-0.251909104844347\n",
      "Regularized Logistic Regression(147/199): loss=0.5034767600236579, w0=-5.070086809270853e-05, w1=-0.2526761315873227\n",
      "Regularized Logistic Regression(148/199): loss=0.5034460921463909, w0=-5.098595207964043e-05, w1=-0.25343814880450816\n",
      "Regularized Logistic Regression(149/199): loss=0.5034159374356281, w0=-5.1270431970382135e-05, w1=-0.25419519679790187\n",
      "Regularized Logistic Regression(150/199): loss=0.5033862853527906, w0=-5.155430989068631e-05, w1=-0.2549473154030078\n",
      "Regularized Logistic Regression(151/199): loss=0.5033571256301758, w0=-5.183758794670014e-05, w1=-0.25569454399707975\n",
      "Regularized Logistic Regression(152/199): loss=0.5033284482624717, w0=-5.212026822524748e-05, w1=-0.25643692150714986\n",
      "Regularized Logistic Regression(153/199): loss=0.503300243498581, w0=-5.240235279410633e-05, w1=-0.25717448641784624\n",
      "Regularized Logistic Regression(154/199): loss=0.5032725018337387, w0=-5.2683843702281755e-05, w1=-0.2579072767790082\n",
      "Regularized Logistic Regression(155/199): loss=0.5032452140019155, w0=-5.296474298027438e-05, w1=-0.2586353302131101\n",
      "Regularized Logistic Regression(156/199): loss=0.5032183709684916, w0=-5.3245052640344485e-05, w1=-0.2593586839224954\n",
      "Regularized Logistic Regression(157/199): loss=0.5031919639231949, w0=-5.35247746767718e-05, w1=-0.2600773746964337\n",
      "Regularized Logistic Regression(158/199): loss=0.503165984273286, w0=-5.380391106611112e-05, w1=-0.2607914389180029\n",
      "Regularized Logistic Regression(159/199): loss=0.5031404236369892, w0=-5.408246376744377e-05, w1=-0.26150091257080404\n",
      "Regularized Logistic Regression(160/199): loss=0.5031152738371505, w0=-5.436043472262498e-05, w1=-0.26220583124551716\n",
      "Regularized Logistic Regression(161/199): loss=0.5030905268951222, w0=-5.4637825856527355e-05, w1=-0.2629062301462981\n",
      "Regularized Logistic Regression(162/199): loss=0.5030661750248581, w0=-5.4914639077280404e-05, w1=-0.2636021440970289\n",
      "Regularized Logistic Regression(163/199): loss=0.5030422106272151, w0=-5.5190876276506216e-05, w1=-0.2642936075474201\n",
      "Regularized Logistic Regression(164/199): loss=0.5030186262844536, w0=-5.546653932955143e-05, w1=-0.2649806545789742\n",
      "Regularized Logistic Regression(165/199): loss=0.5029954147549254, w0=-5.5741630095715466e-05, w1=-0.2656633189108129\n",
      "Regularized Logistic Regression(166/199): loss=0.5029725689679454, w0=-5.601615041847514e-05, w1=-0.2663416339053741\n",
      "Regularized Logistic Regression(167/199): loss=0.502950082018838, w0=-5.629010212570575e-05, w1=-0.2670156325739802\n",
      "Regularized Logistic Regression(168/199): loss=0.5029279471641528, w0=-5.656348702989864e-05, w1=-0.26768534758228435\n",
      "Regularized Logistic Regression(169/199): loss=0.5029061578170422, w0=-5.683630692837535e-05, w1=-0.26835081125559773\n",
      "Regularized Logistic Regression(170/199): loss=0.5028847075427946, w0=-5.7108563603498374e-05, w1=-0.2690120555841002\n",
      "Regularized Logistic Regression(171/199): loss=0.5028635900545179, w0=-5.738025882287863e-05, w1=-0.26966911222793966\n",
      "Regularized Logistic Regression(172/199): loss=0.5028427992089679, w0=-5.765139433957969e-05, w1=-0.2703220125222236\n",
      "Regularized Logistic Regression(173/199): loss=0.502822329002515, w0=-5.792197189231877e-05, w1=-0.2709707874819027\n",
      "Regularized Logistic Regression(174/199): loss=0.5028021735672453, w0=-5.8191993205664636e-05, w1=-0.27161546780655454\n",
      "Regularized Logistic Regression(175/199): loss=0.5027823271671892, w0=-5.84614599902324e-05, w1=-0.2722560838850665\n",
      "Regularized Logistic Regression(176/199): loss=0.5027627841946759, w0=-5.8730373942875297e-05, w1=-0.2728926658002222\n",
      "Regularized Logistic Regression(177/199): loss=0.5027435391668045, w0=-5.8998736746873476e-05, w1=-0.2735252433331954\n",
      "Regularized Logistic Regression(178/199): loss=0.5027245867220338, w0=-5.926655007211989e-05, w1=-0.2741538459679495\n",
      "Regularized Logistic Regression(179/199): loss=0.5027059216168781, w0=-5.9533815575303284e-05, w1=-0.2747785028955522\n",
      "Regularized Logistic Regression(180/199): loss=0.5026875387227147, w0=-5.980053490008838e-05, w1=-0.27539924301840085\n",
      "Regularized Logistic Regression(181/199): loss=0.5026694330226884, w0=-6.006670967729327e-05, w1=-0.2760160949543649\n",
      "Regularized Logistic Regression(182/199): loss=0.5026515996087209, w0=-6.0332341525064107e-05, w1=-0.27662908704084677\n",
      "Regularized Logistic Regression(183/199): loss=0.5026340336786115, w0=-6.0597432049047095e-05, w1=-0.277238247338763\n",
      "Regularized Logistic Regression(184/199): loss=0.5026167305332296, w0=-6.086198284255784e-05, w1=-0.2778436036364469\n",
      "Regularized Logistic Regression(185/199): loss=0.5025996855738, w0=-6.112599548674811e-05, w1=-0.2784451834534772\n",
      "Regularized Logistic Regression(186/199): loss=0.5025828942992676, w0=-6.138947155077005e-05, w1=-0.27904301404443166\n",
      "Regularized Logistic Regression(187/199): loss=0.5025663523037497, w0=-6.165241259193791e-05, w1=-0.2796371224025698\n",
      "Regularized Logistic Regression(188/199): loss=0.5025500552740644, w0=-6.191482015588729e-05, w1=-0.2802275352634454\n",
      "Regularized Logistic Regression(189/199): loss=0.5025339989873372, w0=-6.217669577673192e-05, w1=-0.28081427910845097\n",
      "Regularized Logistic Regression(190/199): loss=0.5025181793086804, w0=-6.243804097721817e-05, w1=-0.2813973801682949\n",
      "Regularized Logistic Regression(191/199): loss=0.5025025921889453, w0=-6.269885726887712e-05, w1=-0.2819768644264149\n",
      "Regularized Logistic Regression(192/199): loss=0.5024872336625411, w0=-6.29591461521743e-05, w1=-0.28255275762232707\n",
      "Regularized Logistic Regression(193/199): loss=0.5024720998453222, w0=-6.32189091166573e-05, w1=-0.2831250852549137\n",
      "Regularized Logistic Regression(194/199): loss=0.5024571869325376, w0=-6.347814764110101e-05, w1=-0.28369387258564954\n",
      "Regularized Logistic Regression(195/199): loss=0.5024424911968445, w0=-6.373686319365077e-05, w1=-0.28425914464177177\n",
      "Regularized Logistic Regression(196/199): loss=0.5024280089863784, w0=-6.399505723196325e-05, w1=-0.2848209262193893\n",
      "Regularized Logistic Regression(197/199): loss=0.5024137367228844, w0=-6.425273120334536e-05, w1=-0.2853792418865379\n",
      "Regularized Logistic Regression(198/199): loss=0.5023996708999005, w0=-6.450988654489089e-05, w1=-0.2859341159861792\n",
      "Regularized Logistic Regression(199/199): loss=0.5023858080809986, w0=-6.476652468361528e-05, w1=-0.2864855726391468\n",
      "Regularized Logistic Regression(0/299): loss=0.693147180559945, w0=-4.176586058555737e-07, w1=-0.007317796433195506\n",
      "Regularized Logistic Regression(1/299): loss=0.6631421228013892, w0=-8.319161077767281e-07, w1=-0.013953291295873332\n",
      "Regularized Logistic Regression(2/299): loss=0.6393674860242854, w0=-1.2441551717520702e-06, w1=-0.020025190714897684\n",
      "Regularized Logistic Regression(3/299): loss=0.620390429000631, w0=-1.655077154883349e-06, w1=-0.025625129559704127\n",
      "Regularized Logistic Regression(4/299): loss=0.6051220962008732, w0=-2.065003578756265e-06, w1=-0.030824726138656857\n",
      "Regularized Logistic Regression(5/299): loss=0.5927331483762237, w0=-2.4740541926482924e-06, w1=-0.03568092305159978\n",
      "Regularized Logistic Regression(6/299): loss=0.582591676281513, w0=-2.8822471821388617e-06, w1=-0.04023975636915049\n",
      "Regularized Logistic Regression(7/299): loss=0.574215552590394, w0=-3.289553624124686e-06, w1=-0.04453898557686695\n",
      "Regularized Logistic Regression(8/299): loss=0.5672357940142978, w0=-3.695926162476843e-06, w1=-0.048609955842565285\n",
      "Regularized Logistic Regression(9/299): loss=0.5613686953920635, w0=-4.101313544509095e-06, w1=-0.0524789492651343\n",
      "Regularized Logistic Regression(10/299): loss=0.556394859541102, w0=-4.50566755709128e-06, w1=-0.05616819057803113\n",
      "Regularized Logistic Regression(11/299): loss=0.5521435211062075, w0=-4.908945958105961e-06, w1=-0.05969661275742897\n",
      "Regularized Logistic Regression(12/299): loss=0.5484808579119941, w0=-5.311113354010735e-06, w1=-0.06308045082642151\n",
      "Regularized Logistic Regression(13/299): loss=0.5453012695561017, w0=-5.712141070414004e-06, w1=-0.066333709378637\n",
      "Regularized Logistic Regression(14/299): loss=0.5425208503791416, w0=-6.1120065704434954e-06, w1=-0.0694685351696763\n",
      "Regularized Logistic Regression(15/299): loss=0.5400724823057832, w0=-6.510692709231487e-06, w1=-0.0724955170367587\n",
      "Regularized Logistic Regression(16/299): loss=0.5379021249531704, w0=-6.908186969335768e-06, w1=-0.07542392937655916\n",
      "Regularized Logistic Regression(17/299): loss=0.5359659935945635, w0=-7.304480745243549e-06, w1=-0.07826193127233702\n",
      "Regularized Logistic Regression(18/299): loss=0.5342283986125909, w0=-7.699568704723056e-06, w1=-0.08101673043425148\n",
      "Regularized Logistic Regression(19/299): loss=0.5326600805230255, w0=-8.09344823407005e-06, w1=-0.08369471899400652\n",
      "Regularized Logistic Regression(20/299): loss=0.5312369185145541, w0=-8.48611896418225e-06, w1=-0.08630158662418201\n",
      "Regularized Logistic Regression(21/299): loss=0.5299389222896768, w0=-8.87758236993692e-06, w1=-0.08884241527142665\n",
      "Regularized Logistic Regression(22/299): loss=0.5287494401593299, w0=-9.267841433844919e-06, w1=-0.09132175889299035\n",
      "Regularized Logistic Regression(23/299): loss=0.5276545332665469, w0=-9.656900364939242e-06, w1=-0.09374371089357761\n",
      "Regularized Logistic Regression(24/299): loss=0.5266424782352863, w0=-1.0044764364533633e-05, w1=-0.09611196142181605\n",
      "Regularized Logistic Regression(25/299): loss=0.5257033697065763, w0=-1.0431439431436198e-05, w1=-0.09842984626505455\n",
      "Regularized Logistic Regression(26/299): loss=0.5248288010273802, w0=-1.0816932200206966e-05, w1=-0.10070038874998882\n",
      "Regularized Logistic Regression(27/299): loss=0.5240116064373118, w0=-1.1201249807001465e-05, w1=-0.10292633579427361\n",
      "Regularized Logistic Regression(28/299): loss=0.52324565191413, w0=-1.1584399778398927e-05, w1=-0.10511018904531838\n",
      "Regularized Logistic Regression(29/299): loss=0.5225256647226154, w0=-1.1966389939360165e-05, w1=-0.10725423187516511\n",
      "Regularized Logistic Regression(30/299): loss=0.5218470939036931, w0=-1.234722833709785e-05, w1=-0.10936055286575916\n",
      "Regularized Logistic Regression(31/299): loss=0.5212059956170689, w0=-1.2726923178180726e-05, w1=-0.11143106631013444\n",
      "Regularized Logistic Regression(32/299): loss=0.520598938539894, w0=-1.3105482776644422e-05, w1=-0.11346753016671744\n",
      "Regularized Logistic Regression(33/299): loss=0.5200229255210278, w0=-1.3482915511258117e-05, w1=-0.11547156183193594\n",
      "Regularized Logistic Regression(34/299): loss=0.5194753284656618, w0=-1.3859229790408971e-05, w1=-0.11744465203735036\n",
      "Regularized Logistic Regression(35/299): loss=0.5189538340309627, w0=-1.423443402332612e-05, w1=-0.11938817712905056\n",
      "Regularized Logistic Regression(36/299): loss=0.5184563981893033, w0=-1.4608536596581201e-05, w1=-0.12130340994704615\n",
      "Regularized Logistic Regression(37/299): loss=0.5179812080913131, w0=-1.4981545854981103e-05, w1=-0.12319152948923771\n",
      "Regularized Logistic Regression(38/299): loss=0.5175266499588558, w0=-1.5353470086116528e-05, w1=-0.12505362951698398\n",
      "Regularized Logistic Regression(39/299): loss=0.5170912819753442, w0=-1.5724317507952927e-05, w1=-0.12689072623628758\n",
      "Regularized Logistic Regression(40/299): loss=0.5166738113306185, w0=-1.609409625895229e-05, w1=-0.128703765169346\n",
      "Regularized Logistic Regression(41/299): loss=0.5162730747301179, w0=-1.6462814390299195e-05, w1=-0.1304936273150424\n",
      "Regularized Logistic Regression(42/299): loss=0.5158880218010486, w0=-1.683047985987479e-05, w1=-0.13226113468329057\n",
      "Regularized Logistic Regression(43/299): loss=0.515517700927825, w0=-1.719710052768139e-05, w1=-0.13400705527662019\n",
      "Regularized Logistic Regression(44/299): loss=0.5151612471299506, w0=-1.7562684152469044e-05, w1=-0.1357321075825875\n",
      "Regularized Logistic Regression(45/299): loss=0.5148178716614813, w0=-1.7927238389356432e-05, w1=-0.13743696463226895\n",
      "Regularized Logistic Regression(46/299): loss=0.51448685306516, w0=-1.829077078827238e-05, w1=-0.13912225767298309\n",
      "Regularized Logistic Regression(47/299): loss=0.5141675294586209, w0=-1.8653288793072876e-05, w1=-0.14078857949730267\n",
      "Regularized Logistic Regression(48/299): loss=0.513859291866507, w0=-1.901479974121223e-05, w1=-0.14243648746518747\n",
      "Regularized Logistic Regression(49/299): loss=0.5135615784424634, w0=-1.937531086386693e-05, w1=-0.1440665062515798\n",
      "Regularized Logistic Regression(50/299): loss=0.5132738694498801, w0=-1.9734829286427473e-05, w1=-0.14567913034792065\n",
      "Regularized Logistic Regression(51/299): loss=0.5129956828909722, w0=-2.009336202928745e-05, w1=-0.14727482634268735\n",
      "Regularized Logistic Regression(52/299): loss=0.5127265706909896, w0=-2.045091600887076e-05, w1=-0.14885403500314445\n",
      "Regularized Logistic Regression(53/299): loss=0.5124661153587379, w0=-2.0807498038847846e-05, w1=-0.1504171731779649\n",
      "Regularized Logistic Regression(54/299): loss=0.5122139270566057, w0=-2.1163114831499924e-05, w1=-0.15196463553817705\n",
      "Regularized Logistic Regression(55/299): loss=0.511969641023367, w0=-2.151777299919714e-05, w1=-0.15349679617195686\n",
      "Regularized Logistic Regression(56/299): loss=0.5117329153015042, w0=-2.187147905596245e-05, w1=-0.15501401004710855\n",
      "Regularized Logistic Regression(57/299): loss=0.5115034287279097, w0=-2.2224239419097777e-05, w1=-0.15651661435358627\n",
      "Regularized Logistic Regression(58/299): loss=0.5112808791528447, w0=-2.2576060410853168e-05, w1=-0.15800492973711103\n",
      "Regularized Logistic Regression(59/299): loss=0.5110649818571127, w0=-2.292694826012304e-05, w1=-0.15947926143378907\n",
      "Regularized Logistic Regression(60/299): loss=0.5108554681417011, w0=-2.327690910415646e-05, w1=-0.16093990031462535\n",
      "Regularized Logistic Regression(61/299): loss=0.5106520840677975, w0=-2.362594899027087e-05, w1=-0.16238712384792503\n",
      "Regularized Logistic Regression(62/299): loss=0.5104545893281851, w0=-2.397407387756062e-05, w1=-0.16382119698678765\n",
      "Regularized Logistic Regression(63/299): loss=0.510262756233662, w0=-2.4321289638593328e-05, w1=-0.16524237298818267\n",
      "Regularized Logistic Regression(64/299): loss=0.5100763688003679, w0=-2.4667602061088548e-05, w1=-0.1666508941694772\n",
      "Regularized Logistic Regression(65/299): loss=0.5098952219258371, w0=-2.501301684957434e-05, w1=-0.168046992607721\n",
      "Regularized Logistic Regression(66/299): loss=0.5097191206432258, w0=-2.535753962701839e-05, w1=-0.16943089078649023\n",
      "Regularized Logistic Regression(67/299): loss=0.5095478794445654, w0=-2.5701175936430968e-05, w1=-0.1708028021946535\n",
      "Regularized Logistic Regression(68/299): loss=0.5093813216651121, w0=-2.6043931242437873e-05, w1=-0.17216293188101273\n",
      "Regularized Logistic Regression(69/299): loss=0.5092192789218848, w0=-2.6385810932821943e-05, w1=-0.17351147696841956\n",
      "Regularized Logistic Regression(70/299): loss=0.5090615906003749, w0=-2.672682032003218e-05, w1=-0.174848627130639\n",
      "Regularized Logistic Regression(71/299): loss=0.508908103384188, w0=-2.7066964642659945e-05, w1=-0.17617456503494527\n",
      "Regularized Logistic Regression(72/299): loss=0.5087586708230224, w0=-2.740624906688195e-05, w1=-0.17748946675316912\n",
      "Regularized Logistic Regression(73/299): loss=0.5086131529349748, w0=-2.7744678687870037e-05, w1=-0.1787935021436829\n",
      "Regularized Logistic Regression(74/299): loss=0.508471415839645, w0=-2.8082258531167896e-05, w1=-0.18008683520659066\n",
      "Regularized Logistic Regression(75/299): loss=0.5083333314189474, w0=-2.8418993554035104e-05, w1=-0.18136962441420248\n",
      "Regularized Logistic Regression(76/299): loss=0.5081987770029013, w0=-2.8754888646758897e-05, w1=-0.1826420230186935\n",
      "Regularized Logistic Regression(77/299): loss=0.5080676350779937, w0=-2.908994863393427e-05, w1=-0.18390417933868922\n",
      "Regularized Logistic Regression(78/299): loss=0.5079397930159952, w0=-2.9424178275712997e-05, w1=-0.1851562370263749\n",
      "Regularized Logistic Regression(79/299): loss=0.5078151428213391, w0=-2.9757582269022277e-05, w1=-0.18639833531659675\n",
      "Regularized Logistic Regression(80/299): loss=0.5076935808954012, w0=-3.0090165248753756e-05, w1=-0.187630609259298\n",
      "Regularized Logistic Regression(81/299): loss=0.5075750078161907, w0=-3.042193178892359e-05, w1=-0.18885318993653263\n",
      "Regularized Logistic Regression(82/299): loss=0.5074593281321296, w0=-3.0752886403804394e-05, w1=-0.19006620466519056\n",
      "Regularized Logistic Regression(83/299): loss=0.507346450168743, w0=-3.108303354902981e-05, w1=-0.19126977718648372\n",
      "Regularized Logistic Regression(84/299): loss=0.5072362858471986, w0=-3.1412377622672474e-05, w1=-0.19246402784315955\n",
      "Regularized Logistic Regression(85/299): loss=0.5071287505137516, w0=-3.174092296629613e-05, w1=-0.19364907374532753\n",
      "Regularized Logistic Regression(86/299): loss=0.5070237627792388, w0=-3.2068673865982666e-05, w1=-0.1948250289257221\n",
      "Regularized Logistic Regression(87/299): loss=0.5069212443678611, w0=-3.2395634553334835e-05, w1=-0.19599200448515475\n",
      "Regularized Logistic Regression(88/299): loss=0.5068211199745555, w0=-3.27218092064553e-05, w1=-0.19715010872885633\n",
      "Regularized Logistic Regression(89/299): loss=0.5067233171303365, w0=-3.304720195090283e-05, w1=-0.19829944729435156\n",
      "Regularized Logistic Regression(90/299): loss=0.5066277660750322, w0=-3.337181686062623e-05, w1=-0.19944012327146313\n",
      "Regularized Logistic Regression(91/299): loss=0.5065343996369088, w0=-3.369565795887679e-05, w1=-0.20057223731499552\n",
      "Regularized Logistic Regression(92/299): loss=0.5064431531187031, w0=-3.401872921909979e-05, w1=-0.20169588775060612\n",
      "Regularized Logistic Regression(93/299): loss=0.506353964189645, w0=-3.434103456580573e-05, w1=-0.20281117067433788\n",
      "Regularized Logistic Regression(94/299): loss=0.5062667727830703, w0=-3.466257787542196e-05, w1=-0.2039181800462456\n",
      "Regularized Logistic Regression(95/299): loss=0.5061815209992716, w0=-3.4983362977125184e-05, w1=-0.20501700777852375\n",
      "Regularized Logistic Regression(96/299): loss=0.506098153013252, w0=-3.5303393653655494e-05, w1=-0.20610774381850502\n",
      "Regularized Logistic Regression(97/299): loss=0.5060166149870832, w0=-3.5622673642112455e-05, w1=-0.2071904762268815\n",
      "Regularized Logistic Regression(98/299): loss=0.5059368549865881, w0=-3.594120663473375e-05, w1=-0.2082652912514664\n",
      "Regularized Logistic Regression(99/299): loss=0.5058588229020884, w0=-3.6258996279656935e-05, w1=-0.20933227339679325\n",
      "Regularized Logistic Regression(100/299): loss=0.5057824703729819, w0=-3.657604618166472e-05, w1=-0.2103915054898354\n",
      "Regularized Logistic Regression(101/299): loss=0.5057077507159242, w0=-3.689235990291436e-05, w1=-0.2114430687420934\n",
      "Regularized Logistic Regression(102/299): loss=0.5056346188564179, w0=-3.720794096365153e-05, w1=-0.21248704280829656\n",
      "Regularized Logistic Regression(103/299): loss=0.5055630312636085, w0=-3.752279284290913e-05, w1=-0.21352350584193477\n",
      "Regularized Logistic Regression(104/299): loss=0.5054929458881205, w0=-3.7836918979191466e-05, w1=-0.21455253454782908\n",
      "Regularized Logistic Regression(105/299): loss=0.5054243221027602, w0=-3.8150322771144216e-05, w1=-0.21557420423193124\n",
      "Regularized Logistic Regression(106/299): loss=0.505357120645936, w0=-3.846300757821056e-05, w1=-0.21658858884852886\n",
      "Regularized Logistic Regression(107/299): loss=0.5052913035676505, w0=-3.877497672127386e-05, w1=-0.21759576104502232\n",
      "Regularized Logistic Regression(108/299): loss=0.5052268341779291, w0=-3.908623348328723e-05, w1=-0.21859579220442774\n",
      "Regularized Logistic Regression(109/299): loss=0.5051636769975598, w0=-3.939678110989041e-05, w1=-0.21958875248574766\n",
      "Regularized Logistic Regression(110/299): loss=0.5051017977110248, w0=-3.970662281001423e-05, w1=-0.22057471086234298\n",
      "Regularized Logistic Regression(111/299): loss=0.5050411631215138, w0=-4.001576175647298e-05, w1=-0.22155373515843085\n",
      "Regularized Logistic Regression(112/299): loss=0.5049817411079125, w0=-4.0324201086545055e-05, w1=-0.22252589208382262\n",
      "Regularized Logistic Regression(113/299): loss=0.5049235005836704, w0=-4.0631943902542185e-05, w1=-0.2234912472670105\n",
      "Regularized Logistic Regression(114/299): loss=0.5048664114574526, w0=-4.093899327236744e-05, w1=-0.22444986528670266\n",
      "Regularized Logistic Regression(115/299): loss=0.5048104445954896, w0=-4.124535223006244e-05, w1=-0.22540180970190132\n",
      "Regularized Logistic Regression(116/299): loss=0.5047555717855441, w0=-4.155102377634394e-05, w1=-0.2263471430806076\n",
      "Regularized Logistic Regression(117/299): loss=0.5047017657024125, w0=-4.185601087913009e-05, w1=-0.22728592702723865\n",
      "Regularized Logistic Regression(118/299): loss=0.5046489998748941, w0=-4.216031647405665e-05, w1=-0.22821822220883112\n",
      "Regularized Logistic Regression(119/299): loss=0.5045972486541508, w0=-4.246394346498335e-05, w1=-0.2291440883801006\n",
      "Regularized Logistic Regression(120/299): loss=0.5045464871834, w0=-4.2766894724490715e-05, w1=-0.2300635844074251\n",
      "Regularized Logistic Regression(121/299): loss=0.50449669136887, w0=-4.306917309436754e-05, w1=-0.23097676829181332\n",
      "Regularized Logistic Regression(122/299): loss=0.5044478378519668, w0=-4.33707813860892e-05, w1=-0.2318836971909155\n",
      "Regularized Logistic Regression(123/299): loss=0.5043999039825895, w0=-4.3671722381287115e-05, w1=-0.23278442744012975\n",
      "Regularized Logistic Regression(124/299): loss=0.5043528677935476, w0=-4.3971998832209484e-05, w1=-0.2336790145728582\n",
      "Regularized Logistic Regression(125/299): loss=0.5043067079760234, w0=-4.427161346217359e-05, w1=-0.2345675133399537\n",
      "Regularized Logistic Regression(126/299): loss=0.50426140385604, w0=-4.4570568966009725e-05, w1=-0.23544997772840737\n",
      "Regularized Logistic Regression(127/299): loss=0.5042169353718796, w0=-4.4868868010497136e-05, w1=-0.23632646097931476\n",
      "Regularized Logistic Regression(128/299): loss=0.504173283052419, w0=-4.516651323479196e-05, w1=-0.23719701560515952\n",
      "Regularized Logistic Regression(129/299): loss=0.5041304279963355, w0=-4.5463507250847495e-05, w1=-0.238061693406453\n",
      "Regularized Logistic Regression(130/299): loss=0.5040883518521444, w0=-4.575985264382691e-05, w1=-0.23892054548776073\n",
      "Regularized Logistic Regression(131/299): loss=0.5040470367990347, w0=-4.6055551972508545e-05, w1=-0.2397736222731489\n",
      "Regularized Logistic Regression(132/299): loss=0.5040064655284648, w0=-4.6350607769684035e-05, w1=-0.24062097352108192\n",
      "Regularized Logistic Regression(133/299): loss=0.5039666212264863, w0=-4.6645022542549334e-05, w1=-0.2414626483387972\n",
      "Regularized Logistic Regression(134/299): loss=0.5039274875567626, w0=-4.693879877308888e-05, w1=-0.2422986951961832\n",
      "Regularized Logistic Regression(135/299): loss=0.5038890486442541, w0=-4.7231938918452985e-05, w1=-0.24312916193918735\n",
      "Regularized Logistic Regression(136/299): loss=0.5038512890595401, w0=-4.752444541132862e-05, w1=-0.24395409580277438\n",
      "Regularized Logistic Regression(137/299): loss=0.5038141938037487, w0=-4.781632066030379e-05, w1=-0.24477354342346025\n",
      "Regularized Logistic Regression(138/299): loss=0.5037777482940712, w0=-4.810756705022548e-05, w1=-0.24558755085143763\n",
      "Regularized Logistic Regression(139/299): loss=0.5037419383498338, w0=-4.839818694255155e-05, w1=-0.24639616356231636\n",
      "Regularized Logistic Regression(140/299): loss=0.5037067501791039, w0=-4.8688182675696454e-05, w1=-0.24719942646849355\n",
      "Regularized Logistic Regression(141/299): loss=0.5036721703658075, w0=-4.897755656537113e-05, w1=-0.2479973839301729\n",
      "Regularized Logistic Regression(142/299): loss=0.5036381858573376, w0=-4.9266310904917004e-05, w1=-0.24879007976604842\n",
      "Regularized Logistic Regression(143/299): loss=0.5036047839526312, w0=-4.955444796563436e-05, w1=-0.24957755726366582\n",
      "Regularized Logistic Regression(144/299): loss=0.5035719522906965, w0=-4.984196999710512e-05, w1=-0.2503598591894795\n",
      "Regularized Logistic Regression(145/299): loss=0.5035396788395706, w0=-5.0128879227510186e-05, w1=-0.25113702779861385\n",
      "Regularized Logistic Regression(146/299): loss=0.5035079518856898, w0=-5.041517786394145e-05, w1=-0.251909104844347\n",
      "Regularized Logistic Regression(147/299): loss=0.5034767600236579, w0=-5.070086809270853e-05, w1=-0.2526761315873227\n",
      "Regularized Logistic Regression(148/299): loss=0.5034460921463909, w0=-5.098595207964043e-05, w1=-0.25343814880450816\n",
      "Regularized Logistic Regression(149/299): loss=0.5034159374356281, w0=-5.1270431970382135e-05, w1=-0.25419519679790187\n",
      "Regularized Logistic Regression(150/299): loss=0.5033862853527906, w0=-5.155430989068631e-05, w1=-0.2549473154030078\n",
      "Regularized Logistic Regression(151/299): loss=0.5033571256301758, w0=-5.183758794670014e-05, w1=-0.25569454399707975\n",
      "Regularized Logistic Regression(152/299): loss=0.5033284482624717, w0=-5.212026822524748e-05, w1=-0.25643692150714986\n",
      "Regularized Logistic Regression(153/299): loss=0.503300243498581, w0=-5.240235279410633e-05, w1=-0.25717448641784624\n",
      "Regularized Logistic Regression(154/299): loss=0.5032725018337387, w0=-5.2683843702281755e-05, w1=-0.2579072767790082\n",
      "Regularized Logistic Regression(155/299): loss=0.5032452140019155, w0=-5.296474298027438e-05, w1=-0.2586353302131101\n",
      "Regularized Logistic Regression(156/299): loss=0.5032183709684916, w0=-5.3245052640344485e-05, w1=-0.2593586839224954\n",
      "Regularized Logistic Regression(157/299): loss=0.5031919639231949, w0=-5.35247746767718e-05, w1=-0.2600773746964337\n",
      "Regularized Logistic Regression(158/299): loss=0.503165984273286, w0=-5.380391106611112e-05, w1=-0.2607914389180029\n",
      "Regularized Logistic Regression(159/299): loss=0.5031404236369892, w0=-5.408246376744377e-05, w1=-0.26150091257080404\n",
      "Regularized Logistic Regression(160/299): loss=0.5031152738371505, w0=-5.436043472262498e-05, w1=-0.26220583124551716\n",
      "Regularized Logistic Regression(161/299): loss=0.5030905268951222, w0=-5.4637825856527355e-05, w1=-0.2629062301462981\n",
      "Regularized Logistic Regression(162/299): loss=0.5030661750248581, w0=-5.4914639077280404e-05, w1=-0.2636021440970289\n",
      "Regularized Logistic Regression(163/299): loss=0.5030422106272151, w0=-5.5190876276506216e-05, w1=-0.2642936075474201\n",
      "Regularized Logistic Regression(164/299): loss=0.5030186262844536, w0=-5.546653932955143e-05, w1=-0.2649806545789742\n",
      "Regularized Logistic Regression(165/299): loss=0.5029954147549254, w0=-5.5741630095715466e-05, w1=-0.2656633189108129\n",
      "Regularized Logistic Regression(166/299): loss=0.5029725689679454, w0=-5.601615041847514e-05, w1=-0.2663416339053741\n",
      "Regularized Logistic Regression(167/299): loss=0.502950082018838, w0=-5.629010212570575e-05, w1=-0.2670156325739802\n",
      "Regularized Logistic Regression(168/299): loss=0.5029279471641528, w0=-5.656348702989864e-05, w1=-0.26768534758228435\n",
      "Regularized Logistic Regression(169/299): loss=0.5029061578170422, w0=-5.683630692837535e-05, w1=-0.26835081125559773\n",
      "Regularized Logistic Regression(170/299): loss=0.5028847075427946, w0=-5.7108563603498374e-05, w1=-0.2690120555841002\n",
      "Regularized Logistic Regression(171/299): loss=0.5028635900545179, w0=-5.738025882287863e-05, w1=-0.26966911222793966\n",
      "Regularized Logistic Regression(172/299): loss=0.5028427992089679, w0=-5.765139433957969e-05, w1=-0.2703220125222236\n",
      "Regularized Logistic Regression(173/299): loss=0.502822329002515, w0=-5.792197189231877e-05, w1=-0.2709707874819027\n",
      "Regularized Logistic Regression(174/299): loss=0.5028021735672453, w0=-5.8191993205664636e-05, w1=-0.27161546780655454\n",
      "Regularized Logistic Regression(175/299): loss=0.5027823271671892, w0=-5.84614599902324e-05, w1=-0.2722560838850665\n",
      "Regularized Logistic Regression(176/299): loss=0.5027627841946759, w0=-5.8730373942875297e-05, w1=-0.2728926658002222\n",
      "Regularized Logistic Regression(177/299): loss=0.5027435391668045, w0=-5.8998736746873476e-05, w1=-0.2735252433331954\n",
      "Regularized Logistic Regression(178/299): loss=0.5027245867220338, w0=-5.926655007211989e-05, w1=-0.2741538459679495\n",
      "Regularized Logistic Regression(179/299): loss=0.5027059216168781, w0=-5.9533815575303284e-05, w1=-0.2747785028955522\n",
      "Regularized Logistic Regression(180/299): loss=0.5026875387227147, w0=-5.980053490008838e-05, w1=-0.27539924301840085\n",
      "Regularized Logistic Regression(181/299): loss=0.5026694330226884, w0=-6.006670967729327e-05, w1=-0.2760160949543649\n",
      "Regularized Logistic Regression(182/299): loss=0.5026515996087209, w0=-6.0332341525064107e-05, w1=-0.27662908704084677\n",
      "Regularized Logistic Regression(183/299): loss=0.5026340336786115, w0=-6.0597432049047095e-05, w1=-0.277238247338763\n",
      "Regularized Logistic Regression(184/299): loss=0.5026167305332296, w0=-6.086198284255784e-05, w1=-0.2778436036364469\n",
      "Regularized Logistic Regression(185/299): loss=0.5025996855738, w0=-6.112599548674811e-05, w1=-0.2784451834534772\n",
      "Regularized Logistic Regression(186/299): loss=0.5025828942992676, w0=-6.138947155077005e-05, w1=-0.27904301404443166\n",
      "Regularized Logistic Regression(187/299): loss=0.5025663523037497, w0=-6.165241259193791e-05, w1=-0.2796371224025698\n",
      "Regularized Logistic Regression(188/299): loss=0.5025500552740644, w0=-6.191482015588729e-05, w1=-0.2802275352634454\n",
      "Regularized Logistic Regression(189/299): loss=0.5025339989873372, w0=-6.217669577673192e-05, w1=-0.28081427910845097\n",
      "Regularized Logistic Regression(190/299): loss=0.5025181793086804, w0=-6.243804097721817e-05, w1=-0.2813973801682949\n",
      "Regularized Logistic Regression(191/299): loss=0.5025025921889453, w0=-6.269885726887712e-05, w1=-0.2819768644264149\n",
      "Regularized Logistic Regression(192/299): loss=0.5024872336625411, w0=-6.29591461521743e-05, w1=-0.28255275762232707\n",
      "Regularized Logistic Regression(193/299): loss=0.5024720998453222, w0=-6.32189091166573e-05, w1=-0.2831250852549137\n",
      "Regularized Logistic Regression(194/299): loss=0.5024571869325376, w0=-6.347814764110101e-05, w1=-0.28369387258564954\n",
      "Regularized Logistic Regression(195/299): loss=0.5024424911968445, w0=-6.373686319365077e-05, w1=-0.28425914464177177\n",
      "Regularized Logistic Regression(196/299): loss=0.5024280089863784, w0=-6.399505723196325e-05, w1=-0.2848209262193893\n",
      "Regularized Logistic Regression(197/299): loss=0.5024137367228844, w0=-6.425273120334536e-05, w1=-0.2853792418865379\n",
      "Regularized Logistic Regression(198/299): loss=0.5023996708999005, w0=-6.450988654489089e-05, w1=-0.2859341159861792\n",
      "Regularized Logistic Regression(199/299): loss=0.5023858080809986, w0=-6.476652468361528e-05, w1=-0.2864855726391468\n",
      "Regularized Logistic Regression(200/299): loss=0.5023721448980727, w0=-6.502264703658821e-05, w1=-0.28703363574704016\n",
      "Regularized Logistic Regression(201/299): loss=0.5023586780496822, w0=-6.527825501106433e-05, w1=-0.2875783289950656\n",
      "Regularized Logistic Regression(202/299): loss=0.5023454042994394, w0=-6.55333500046119e-05, w1=-0.2881196758548288\n",
      "Regularized Logistic Regression(203/299): loss=0.5023323204744469, w0=-6.578793340523968e-05, w1=-0.28865769958707815\n",
      "Regularized Logistic Regression(204/299): loss=0.5023194234637783, w0=-6.604200659152172e-05, w1=-0.28919242324439925\n",
      "Regularized Logistic Regression(205/299): loss=0.5023067102170032, w0=-6.629557093272045e-05, w1=-0.28972386967386304\n",
      "Regularized Logistic Regression(206/299): loss=0.5022941777427548, w0=-6.654862778890788e-05, w1=-0.29025206151962785\n",
      "Regularized Logistic Regression(207/299): loss=0.5022818231073383, w0=-6.680117851108494e-05, w1=-0.2907770212254973\n",
      "Regularized Logistic Regression(208/299): loss=0.5022696434333782, w0=-6.705322444129913e-05, w1=-0.29129877103743407\n",
      "Regularized Logistic Regression(209/299): loss=0.5022576358985049, w0=-6.730476691276038e-05, w1=-0.29181733300602897\n",
      "Regularized Logistic Regression(210/299): loss=0.5022457977340764, w0=-6.755580724995515e-05, w1=-0.2923327289889308\n",
      "Regularized Logistic Regression(211/299): loss=0.5022341262239378, w0=-6.780634676875892e-05, w1=-0.2928449806532333\n",
      "Regularized Logistic Regression(212/299): loss=0.502222618703215, w0=-6.805638677654696e-05, w1=-0.29335410947782253\n",
      "Regularized Logistic Regression(213/299): loss=0.5022112725571402, w0=-6.83059285723034e-05, w1=-0.29386013675568384\n",
      "Regularized Logistic Regression(214/299): loss=0.5022000852199121, w0=-6.855497344672886e-05, w1=-0.2943630835961716\n",
      "Regularized Logistic Regression(215/299): loss=0.502189054173585, w0=-6.880352268234626e-05, w1=-0.29486297092723995\n",
      "Regularized Logistic Regression(216/299): loss=0.5021781769469912, w0=-6.905157755360526e-05, w1=-0.2953598194976376\n",
      "Regularized Logistic Regression(217/299): loss=0.5021674511146899, w0=-6.929913932698502e-05, w1=-0.29585364987906504\n",
      "Regularized Logistic Regression(218/299): loss=0.5021568742959459, w0=-6.954620926109557e-05, w1=-0.2963444824682976\n",
      "Regularized Logistic Regression(219/299): loss=0.5021464441537365, w0=-6.979278860677751e-05, w1=-0.2968323374892731\n",
      "Regularized Logistic Regression(220/299): loss=0.5021361583937834, w0=-7.003887860720042e-05, w1=-0.2973172349951451\n",
      "Regularized Logistic Regression(221/299): loss=0.5021260147636122, w0=-7.02844804979597e-05, w1=-0.2977991948703032\n",
      "Regularized Logistic Regression(222/299): loss=0.5021160110516347, w0=-7.052959550717209e-05, w1=-0.29827823683236054\n",
      "Regularized Logistic Regression(223/299): loss=0.5021061450862583, w0=-7.077422485556961e-05, w1=-0.29875438043410973\n",
      "Regularized Logistic Regression(224/299): loss=0.5020964147350162, w0=-7.101836975659233e-05, w1=-0.2992276450654456\n",
      "Regularized Logistic Regression(225/299): loss=0.5020868179037227, w0=-7.126203141647962e-05, w1=-0.2996980499552596\n",
      "Regularized Logistic Regression(226/299): loss=0.5020773525356481, w0=-7.15052110343601e-05, w1=-0.30016561417330273\n",
      "Regularized Logistic Regression(227/299): loss=0.5020680166107176, w0=-7.174790980234033e-05, w1=-0.3006303566320178\n",
      "Regularized Logistic Regression(228/299): loss=0.5020588081447286, w0=-7.199012890559213e-05, w1=-0.30109229608834553\n",
      "Regularized Logistic Regression(229/299): loss=0.5020497251885898, w0=-7.22318695224386e-05, w1=-0.3015514511454989\n",
      "Regularized Logistic Regression(230/299): loss=0.5020407658275791, w0=-7.247313282443898e-05, w1=-0.30200784025471256\n",
      "Regularized Logistic Regression(231/299): loss=0.5020319281806197, w0=-7.271391997647218e-05, w1=-0.30246148171696285\n",
      "Regularized Logistic Regression(232/299): loss=0.5020232103995772, w0=-7.29542321368191e-05, w1=-0.3029123936846619\n",
      "Regularized Logistic Regression(233/299): loss=0.5020146106685703, w0=-7.319407045724379e-05, w1=-0.3033605941633257\n",
      "Regularized Logistic Regression(234/299): loss=0.5020061272033028, w0=-7.343343608307335e-05, w1=-0.3038061010132159\n",
      "Regularized Logistic Regression(235/299): loss=0.5019977582504099, w0=-7.367233015327673e-05, w1=-0.30424893195095537\n",
      "Regularized Logistic Regression(236/299): loss=0.5019895020868216, w0=-7.391075380054235e-05, w1=-0.3046891045511215\n",
      "Regularized Logistic Regression(237/299): loss=0.5019813570191416, w0=-7.414870815135457e-05, w1=-0.3051266362478125\n",
      "Regularized Logistic Regression(238/299): loss=0.5019733213830425, w0=-7.43861943260691e-05, w1=-0.30556154433619204\n",
      "Regularized Logistic Regression(239/299): loss=0.5019653935426746, w0=-7.462321343898722e-05, w1=-0.3059938459740082\n",
      "Regularized Logistic Regression(240/299): loss=0.5019575718900886, w0=-7.485976659842903e-05, w1=-0.3064235581830917\n",
      "Regularized Logistic Regression(241/299): loss=0.5019498548446752, w0=-7.50958549068055e-05, w1=-0.3068506978508307\n",
      "Regularized Logistic Regression(242/299): loss=0.5019422408526156, w0=-7.53314794606896e-05, w1=-0.3072752817316225\n",
      "Regularized Logistic Regression(243/299): loss=0.5019347283863455, w0=-7.556664135088631e-05, w1=-0.30769732644830494\n",
      "Regularized Logistic Regression(244/299): loss=0.5019273159440333, w0=-7.580134166250165e-05, w1=-0.30811684849356585\n",
      "Regularized Logistic Regression(245/299): loss=0.5019200020490707, w0=-7.603558147501066e-05, w1=-0.3085338642313312\n",
      "Regularized Logistic Regression(246/299): loss=0.5019127852495728, w0=-7.626936186232447e-05, w1=-0.3089483898981348\n",
      "Regularized Logistic Regression(247/299): loss=0.5019056641178943, w0=-7.650268389285632e-05, w1=-0.30936044160446435\n",
      "Regularized Logistic Regression(248/299): loss=0.5018986372501525, w0=-7.673554862958663e-05, w1=-0.3097700353360912\n",
      "Regularized Logistic Regression(249/299): loss=0.5018917032657658, w0=-7.696795713012719e-05, w1=-0.31017718695537855\n",
      "Regularized Logistic Regression(250/299): loss=0.5018848608069998, w0=-7.719991044678433e-05, w1=-0.31058191220257114\n",
      "Regularized Logistic Regression(251/299): loss=0.5018781085385255, w0=-7.743140962662125e-05, w1=-0.31098422669706677\n",
      "Regularized Logistic Regression(252/299): loss=0.5018714451469874, w0=-7.766245571151936e-05, w1=-0.311384145938669\n",
      "Regularized Logistic Regression(253/299): loss=0.5018648693405816, w0=-7.789304973823883e-05, w1=-0.31178168530882144\n",
      "Regularized Logistic Regression(254/299): loss=0.5018583798486428, w0=-7.81231927384782e-05, w1=-0.31217686007182566\n",
      "Regularized Logistic Regression(255/299): loss=0.5018519754212429, w0=-7.835288573893308e-05, w1=-0.3125696853760403\n",
      "Regularized Logistic Regression(256/299): loss=0.5018456548287956, w0=-7.858212976135418e-05, w1=-0.3129601762550633\n",
      "Regularized Logistic Regression(257/299): loss=0.5018394168616732, w0=-7.881092582260424e-05, w1=-0.3133483476288982\n",
      "Regularized Logistic Regression(258/299): loss=0.5018332603298286, w0=-7.903927493471434e-05, w1=-0.3137342143051042\n",
      "Regularized Logistic Regression(259/299): loss=0.5018271840624304, w0=-7.926717810493938e-05, w1=-0.3141177909799287\n",
      "Regularized Logistic Regression(260/299): loss=0.5018211869075002, w0=-7.949463633581258e-05, w1=-0.3144990922394248\n",
      "Regularized Logistic Regression(261/299): loss=0.5018152677315638, w0=-7.972165062519943e-05, w1=-0.31487813256055236\n",
      "Regularized Logistic Regression(262/299): loss=0.5018094254193062, w0=-7.994822196635067e-05, w1=-0.31525492631226615\n",
      "Regularized Logistic Regression(263/299): loss=0.5018036588732357, w0=-8.017435134795462e-05, w1=-0.3156294877565857\n",
      "Regularized Logistic Regression(264/299): loss=0.5017979670133563, w0=-8.040003975418871e-05, w1=-0.31600183104965296\n",
      "Regularized Logistic Regression(265/299): loss=0.501792348776844, w0=-8.062528816477028e-05, w1=-0.316371970242774\n",
      "Regularized Logistic Regression(266/299): loss=0.5017868031177345, w0=-8.085009755500661e-05, w1=-0.31673991928344697\n",
      "Regularized Logistic Regression(267/299): loss=0.5017813290066143, w0=-8.107446889584426e-05, w1=-0.31710569201637584\n",
      "Regularized Logistic Regression(268/299): loss=0.5017759254303208, w0=-8.129840315391771e-05, w1=-0.31746930218447056\n",
      "Regularized Logistic Regression(269/299): loss=0.5017705913916457, w0=-8.15219012915973e-05, w1=-0.31783076342983424\n",
      "Regularized Logistic Regression(270/299): loss=0.5017653259090494, w0=-8.174496426703639e-05, w1=-0.31819008929473525\n",
      "Regularized Logistic Regression(271/299): loss=0.5017601280163773, w0=-8.196759303421797e-05, w1=-0.3185472932225685\n",
      "Regularized Logistic Regression(272/299): loss=0.5017549967625834, w0=-8.218978854300057e-05, w1=-0.318902388558802\n",
      "Regularized Logistic Regression(273/299): loss=0.501749931211462, w0=-8.241155173916337e-05, w1=-0.3192553885519118\n",
      "Regularized Logistic Regression(274/299): loss=0.5017449304413808, w0=-8.263288356445086e-05, w1=-0.31960630635430504\n",
      "Regularized Logistic Regression(275/299): loss=0.5017399935450237, w0=-8.285378495661672e-05, w1=-0.3199551550232288\n",
      "Regularized Logistic Regression(276/299): loss=0.5017351196291366, w0=-8.307425684946715e-05, w1=-0.32030194752166946\n",
      "Regularized Logistic Regression(277/299): loss=0.5017303078142786, w0=-8.329430017290353e-05, w1=-0.3206466967192387\n",
      "Regularized Logistic Regression(278/299): loss=0.5017255572345798, w0=-8.351391585296444e-05, w1=-0.3209894153930476\n",
      "Regularized Logistic Regression(279/299): loss=0.5017208670375025, w0=-8.373310481186717e-05, w1=-0.3213301162285711\n",
      "Regularized Logistic Regression(280/299): loss=0.5017162363836086, w0=-8.395186796804855e-05, w1=-0.32166881182049956\n",
      "Regularized Logistic Regression(281/299): loss=0.5017116644463309, w0=-8.417020623620519e-05, w1=-0.32200551467358024\n",
      "Regularized Logistic Regression(282/299): loss=0.5017071504117484, w0=-8.438812052733324e-05, w1=-0.3223402372034474\n",
      "Regularized Logistic Regression(283/299): loss=0.5017026934783692, w0=-8.460561174876745e-05, w1=-0.32267299173744257\n",
      "Regularized Logistic Regression(284/299): loss=0.5016982928569135, w0=-8.482268080421978e-05, w1=-0.32300379051542333\n",
      "Regularized Logistic Regression(285/299): loss=0.5016939477701051, w0=-8.503932859381738e-05, w1=-0.32333264569056197\n",
      "Regularized Logistic Regression(286/299): loss=0.5016896574524629, w0=-8.525555601414004e-05, w1=-0.3236595693301358\n",
      "Regularized Logistic Regression(287/299): loss=0.5016854211501016, w0=-8.547136395825715e-05, w1=-0.3239845734163042\n",
      "Regularized Logistic Regression(288/299): loss=0.5016812381205313, w0=-8.568675331576409e-05, w1=-0.32430766984687986\n",
      "Regularized Logistic Regression(289/299): loss=0.5016771076324649, w0=-8.59017249728181e-05, w1=-0.3246288704360862\n",
      "Regularized Logistic Regression(290/299): loss=0.501673028965627, w0=-8.611627981217361e-05, w1=-0.3249481869153084\n",
      "Regularized Logistic Regression(291/299): loss=0.5016690014105681, w0=-8.633041871321717e-05, w1=-0.3252656309338341\n",
      "Regularized Logistic Regression(292/299): loss=0.5016650242684806, w0=-8.654414255200174e-05, w1=-0.3255812140595854\n",
      "Regularized Logistic Regression(293/299): loss=0.5016610968510208, w0=-8.675745220128058e-05, w1=-0.3258949477798404\n",
      "Regularized Logistic Regression(294/299): loss=0.5016572184801324, w0=-8.697034853054064e-05, w1=-0.3262068435019471\n",
      "Regularized Logistic Regression(295/299): loss=0.5016533884878751, w0=-8.718283240603544e-05, w1=-0.3265169125540293\n",
      "Regularized Logistic Regression(296/299): loss=0.5016496062162539, w0=-8.739490469081754e-05, w1=-0.32682516618568197\n",
      "Regularized Logistic Regression(297/299): loss=0.5016458710170552, w0=-8.760656624477049e-05, w1=-0.32713161556865966\n",
      "Regularized Logistic Regression(298/299): loss=0.5016421822516827, w0=-8.781781792464035e-05, w1=-0.32743627179755547\n",
      "Regularized Logistic Regression(299/299): loss=0.5016385392909993, w0=-8.802866058406674e-05, w1=-0.3277391458904733\n",
      "Regularized Logistic Regression(0/99): loss=0.693147180559945, w0=-4.176586058555737e-07, w1=-0.007317796433195506\n",
      "Regularized Logistic Regression(1/99): loss=0.6634280901010439, w0=-8.243982528713279e-07, w1=-0.013821570960075812\n",
      "Regularized Logistic Regression(2/99): loss=0.640822212803381, w0=-1.2218593650579815e-06, w1=-0.019656675275621906\n",
      "Regularized Logistic Regression(3/99): loss=0.6234976083418551, w0=-1.6109214419849742e-06, w1=-0.02493392525522841\n",
      "Regularized Logistic Regression(4/99): loss=0.6101148060646779, w0=-1.9920565612691653e-06, w1=-0.029739134333005672\n",
      "Regularized Logistic Regression(5/99): loss=0.5996905637993731, w0=-2.365529087876996e-06, w1=-0.034140062672567285\n",
      "Regularized Logistic Regression(6/99): loss=0.5915011908334518, w0=-2.731503203849732e-06, w1=-0.03819120648935659\n",
      "Regularized Logistic Regression(7/99): loss=0.5850120877185038, w0=-3.0900987405446527e-06, w1=-0.04193708852420707\n",
      "Regularized Logistic Regression(8/99): loss=0.5798263425638969, w0=-3.4414191284968536e-06, w1=-0.0454145622702235\n",
      "Regularized Logistic Regression(9/99): loss=0.5756474453512466, w0=-3.785564694452396e-06, w1=-0.048654468447802954\n",
      "Regularized Logistic Regression(10/99): loss=0.5722523619659212, w0=-4.122638426749028e-06, w1=-0.05168285720831888\n",
      "Regularized Logistic Regression(11/99): loss=0.5694721112550061, w0=-4.45274797161723e-06, w1=-0.05452191074701123\n",
      "Regularized Logistic Regression(12/99): loss=0.5671777274641424, w0=-4.776005822016528e-06, w1=-0.05719065315315851\n",
      "Regularized Logistic Regression(13/99): loss=0.5652700725942258, w0=-5.09252870752142e-06, w1=-0.05970550518845955\n",
      "Regularized Logistic Regression(14/99): loss=0.5636724010809463, w0=-5.402436693224531e-06, w1=-0.062080723543531884\n",
      "Regularized Logistic Regression(15/99): loss=0.5623248978465659, w0=-5.705852234479014e-06, w1=-0.06432875247339687\n",
      "Regularized Logistic Regression(16/99): loss=0.5611806380508504, w0=-6.002899299326656e-06, w1=-0.06646050798322183\n",
      "Regularized Logistic Regression(17/99): loss=0.5602025772488578, w0=-6.293702601842209e-06, w1=-0.06848560944661036\n",
      "Regularized Logistic Regression(18/99): loss=0.559361293350666, w0=-6.578386955804767e-06, w1=-0.07041256982110708\n",
      "Regularized Logistic Regression(19/99): loss=0.5586332809507444, w0=-6.857076742409593e-06, w1=-0.07224895295286655\n",
      "Regularized Logistic Regression(20/99): loss=0.557999654373474, w0=-7.129895479343696e-06, w1=-0.07400150450476793\n",
      "Regularized Logistic Regression(21/99): loss=0.5574451552584001, w0=-7.396965476811059e-06, w1=-0.0756762615861697\n",
      "Regularized Logistic Regression(22/99): loss=0.5569573886083364, w0=-7.658407566563562e-06, w1=-0.07727864506558488\n",
      "Regularized Logistic Regression(23/99): loss=0.5565262313527967, w0=-7.914340891402662e-06, w1=-0.07881353771217714\n",
      "Regularized Logistic Regression(24/99): loss=0.5561433719969774, w0=-8.164882744321825e-06, w1=-0.08028535066985164\n",
      "Regularized Logistic Regression(25/99): loss=0.5558019504697509, w0=-8.410148448149637e-06, w1=-0.08169808026998623\n",
      "Regularized Logistic Regression(26/99): loss=0.5554962749941765, w0=-8.650251268090761e-06, w1=-0.08305535680018417\n",
      "Regularized Logistic Regression(27/99): loss=0.5552215984808738, w0=-8.88530235089864e-06, w1=-0.08436048654084306\n",
      "Regularized Logistic Regression(28/99): loss=0.554973941152328, w0=-9.115410685545858e-06, w1=-0.08561648813951137\n",
      "Regularized Logistic Regression(29/99): loss=0.5547499492451494, w0=-9.340683081200924e-06, w1=-0.08682612420053483\n",
      "Regularized Logistic Regression(30/99): loss=0.5545467819935362, w0=-9.561224159097904e-06, w1=-0.08799192881336229\n",
      "Regularized Logistic Regression(31/99): loss=0.5543620208763405, w0=-9.777136355522073e-06, w1=-0.08911623161885196\n",
      "Regularized Logistic Regression(32/99): loss=0.5541935964611966, w0=-9.988519933654388e-06, w1=-0.09020117891252516\n",
      "Regularized Logistic Regression(33/99): loss=0.5540397292106303, w0=-1.0195473002440257e-05, w1=-0.09124875220208667\n",
      "Regularized Logistic Regression(34/99): loss=0.5538988814065524, w0=-1.0398091540991685e-05, w1=-0.09226078456980363\n",
      "Regularized Logistic Regression(35/99): loss=0.5537697179598231, w0=-1.0596469427310867e-05, w1=-0.09323897513555356\n",
      "Regularized Logistic Regression(36/99): loss=0.5536510743442495, w0=-1.0790698470349956e-05, w1=-0.09418490187115149\n",
      "Regularized Logistic Regression(37/99): loss=0.5535419302620727, w0=-1.0980868444605925e-05, w1=-0.09510003297912452\n",
      "Regularized Logistic Regression(38/99): loss=0.5534413879351746, w0=-1.1167067126599172e-05, w1=-0.09598573701793019\n",
      "Regularized Logistic Regression(39/99): loss=0.5533486541413679, w0=-1.1349380332706395e-05, w1=-0.09684329192956719\n",
      "Regularized Logistic Regression(40/99): loss=0.5532630252922793, w0=-1.1527891957917567e-05, w1=-0.09767389310367877\n",
      "Regularized Logistic Regression(41/99): loss=0.5531838749891799, w0=-1.1702684015167875e-05, w1=-0.098478660593825\n",
      "Regularized Logistic Regression(42/99): loss=0.5531106436038761, w0=-1.1873836674961584e-05, w1=-0.09925864558603781\n",
      "Regularized Logistic Regression(43/99): loss=0.5530428295197592, w0=-1.204142830505884e-05, w1=-0.10001483620654605\n",
      "Regularized Logistic Regression(44/99): loss=0.5529799817382053, w0=-1.2205535510040707e-05, w1=-0.10074816274430033\n",
      "Regularized Logistic Regression(45/99): loss=0.5529216936115188, w0=-1.236623317060402e-05, w1=-0.10145950235430345\n",
      "Regularized Logistic Regression(46/99): loss=0.5528675975084518, w0=-1.2523594482467267e-05, w1=-0.10214968329949317\n",
      "Regularized Logistic Regression(47/99): loss=0.5528173602543563, w0=-1.2677690994793283e-05, w1=-0.10281948878181961\n",
      "Regularized Logistic Regression(48/99): loss=0.5527706792169944, w0=-1.2828592648054414e-05, w1=-0.1034696604070348\n",
      "Regularized Logistic Regression(49/99): loss=0.5527272789324407, w0=-1.297636781128251e-05, w1=-0.10410090132240032\n",
      "Regularized Logistic Regression(50/99): loss=0.552686908184393, w0=-1.3121083318659472e-05, w1=-0.10471387906191701\n",
      "Regularized Logistic Regression(51/99): loss=0.5526493374655611, w0=-1.3262804505415375e-05, w1=-0.10530922812967124\n",
      "Regularized Logistic Regression(52/99): loss=0.5526143567622311, w0=-1.3401595243010317e-05, w1=-0.10588755234839668\n",
      "Regularized Logistic Regression(53/99): loss=0.5525817736132509, w0=-1.3537517973583732e-05, w1=-0.10644942699729351\n",
      "Regularized Logistic Regression(54/99): loss=0.5525514114029494, w0=-1.3670633743661015e-05, w1=-0.10699540076046568\n",
      "Regularized Logistic Regression(55/99): loss=0.5525231078542585, w0=-1.3801002237112561e-05, w1=-0.10752599750498491\n",
      "Regularized Logistic Regression(56/99): loss=0.5524967136938614, w0=-1.3928681807364328e-05, w1=-0.10804171790551625\n",
      "Regularized Logistic Regression(57/99): loss=0.5524720914657325, w0=-1.405372950886255e-05, w1=-0.10854304093061866\n",
      "Regularized Logistic Regression(58/99): loss=0.5524491144732002, w0=-1.4176201127797933e-05, w1=-0.10903042520421129\n",
      "Regularized Logistic Regression(59/99): loss=0.5524276658327404, w0=-1.4296151212096973e-05, w1=-0.1095043102542801\n",
      "Regularized Logistic Regression(60/99): loss=0.552407637625302, w0=-1.4413633100689762e-05, w1=-0.10996511765962423\n",
      "Regularized Logistic Regression(61/99): loss=0.5523889301330583, w0=-1.4528698952065178e-05, w1=-0.11041325210432801\n",
      "Regularized Logistic Regression(62/99): loss=0.5523714511512843, w0=-1.4641399772125418e-05, w1=-0.11084910234863794\n",
      "Regularized Logistic Regression(63/99): loss=0.5523551153665107, w0=-1.4751785441352755e-05, w1=-0.11127304212404322\n",
      "Regularized Logistic Regression(64/99): loss=0.5523398437933629, w0=-1.4859904741302045e-05, w1=-0.11168543095956371\n",
      "Regularized Logistic Regression(65/99): loss=0.5523255632635221, w0=-1.4965805380433038e-05, w1=-0.11208661494554639\n",
      "Regularized Logistic Regression(66/99): loss=0.5523122059611281, w0=-1.5069534019296844e-05, w1=-0.11247692744063874\n",
      "Regularized Logistic Regression(67/99): loss=0.5522997089996786, w0=-1.5171136295091181e-05, w1=-0.11285668972704568\n",
      "Regularized Logistic Regression(68/99): loss=0.5522880140361129, w0=-1.527065684559909e-05, w1=-0.1132262116186752\n",
      "Regularized Logistic Regression(69/99): loss=0.5522770669183034, w0=-1.536813933252591e-05, w1=-0.11358579202632388\n",
      "Regularized Logistic Regression(70/99): loss=0.5522668173626287, w0=-1.5463626464249217e-05, w1=-0.11393571948365011\n",
      "Regularized Logistic Regression(71/99): loss=0.5522572186587009, w0=-1.5557160017996324e-05, w1=-0.11427627263732185\n",
      "Regularized Logistic Regression(72/99): loss=0.5522482273986538, w0=-1.564878086146387e-05, w1=-0.11460772070440049\n",
      "Regularized Logistic Regression(73/99): loss=0.5522398032286906, w0=-1.573852897389377e-05, w1=-0.11493032389972734\n",
      "Regularized Logistic Regression(74/99): loss=0.5522319086208406, w0=-1.5826443466619645e-05, w1=-0.11524433383582049\n",
      "Regularized Logistic Regression(75/99): loss=0.5522245086630956, w0=-1.5912562603097535e-05, w1=-0.11554999389755177\n",
      "Regularized Logistic Regression(76/99): loss=0.5522175708662862, w0=-1.599692381843457e-05, w1=-0.1158475395936616\n",
      "Regularized Logistic Regression(77/99): loss=0.5522110649862312, w0=-1.607956373842885e-05, w1=-0.11613719888697796\n",
      "Regularized Logistic Regression(78/99): loss=0.5522049628598323, w0=-1.6160518198133574e-05, w1=-0.11641919250503285\n",
      "Regularized Logistic Regression(79/99): loss=0.5521992382539245, w0=-1.6239822259958244e-05, w1=-0.11669373423261521\n",
      "Regularized Logistic Regression(80/99): loss=0.5521938667258028, w0=-1.631751023131924e-05, w1=-0.11696103118766159\n",
      "Regularized Logistic Regression(81/99): loss=0.5521888254944506, w0=-1.6393615681852096e-05, w1=-0.1172212840817515\n",
      "Regularized Logistic Regression(82/99): loss=0.5521840933215834, w0=-1.6468171460197117e-05, w1=-0.11747468746636962\n",
      "Regularized Logistic Regression(83/99): loss=0.5521796504017052, w0=-1.6541209710370003e-05, w1=-0.11772142996598968\n",
      "Regularized Logistic Regression(84/99): loss=0.5521754782604477, w0=-1.661276188772866e-05, w1=-0.11796169449893887\n",
      "Regularized Logistic Regression(85/99): loss=0.5521715596605211, w0=-1.668285877454711e-05, w1=-0.11819565848692337\n",
      "Regularized Logistic Regression(86/99): loss=0.5521678785146754, w0=-1.675153049520716e-05, w1=-0.11842349405401548\n",
      "Regularized Logistic Regression(87/99): loss=0.5521644198051127, w0=-1.681880653101813e-05, w1=-0.11864536821583369\n",
      "Regularized Logistic Regression(88/99): loss=0.5521611695088443, w0=-1.68847157346747e-05, w1=-0.11886144305958782\n",
      "Regularized Logistic Regression(89/99): loss=0.5521581145285293, w0=-1.6949286344362646e-05, w1=-0.11907187591559897\n",
      "Regularized Logistic Regression(90/99): loss=0.5521552426283671, w0=-1.701254599752193e-05, w1=-0.11927681952086042\n",
      "Regularized Logistic Regression(91/99): loss=0.552152542374656, w0=-1.7074521744276373e-05, w1=-0.11947642217515281\n",
      "Regularized Logistic Regression(92/99): loss=0.5521500030806582, w0=-1.7135240060538835e-05, w1=-0.11967082789018621\n",
      "Regularized Logistic Regression(93/99): loss=0.5521476147554427, w0=-1.7194726860800605e-05, w1=-0.11986017653220611\n",
      "Regularized Logistic Regression(94/99): loss=0.5521453680564076, w0=-1.7253007510613424e-05, w1=-0.12004460395846044\n",
      "Regularized Logistic Regression(95/99): loss=0.5521432542451951, w0=-1.7310106838772352e-05, w1=-0.12022424214790049\n",
      "Regularized Logistic Regression(96/99): loss=0.5521412651467567, w0=-1.736604914920739e-05, w1=-0.12039921932645063\n",
      "Regularized Logistic Regression(97/99): loss=0.5521393931113202, w0=-1.7420858232591598e-05, w1=-0.1205696600871634\n",
      "Regularized Logistic Regression(98/99): loss=0.552137630979052, w0=-1.7474557377673197e-05, w1=-0.12073568550554847\n",
      "Regularized Logistic Regression(99/99): loss=0.5521359720472077, w0=-1.7527169382338925e-05, w1=-0.12089741325034276\n",
      "Regularized Logistic Regression(0/199): loss=0.693147180559945, w0=-4.176586058555737e-07, w1=-0.007317796433195506\n",
      "Regularized Logistic Regression(1/199): loss=0.6634280901010439, w0=-8.243982528713279e-07, w1=-0.013821570960075812\n",
      "Regularized Logistic Regression(2/199): loss=0.640822212803381, w0=-1.2218593650579815e-06, w1=-0.019656675275621906\n",
      "Regularized Logistic Regression(3/199): loss=0.6234976083418551, w0=-1.6109214419849742e-06, w1=-0.02493392525522841\n",
      "Regularized Logistic Regression(4/199): loss=0.6101148060646779, w0=-1.9920565612691653e-06, w1=-0.029739134333005672\n",
      "Regularized Logistic Regression(5/199): loss=0.5996905637993731, w0=-2.365529087876996e-06, w1=-0.034140062672567285\n",
      "Regularized Logistic Regression(6/199): loss=0.5915011908334518, w0=-2.731503203849732e-06, w1=-0.03819120648935659\n",
      "Regularized Logistic Regression(7/199): loss=0.5850120877185038, w0=-3.0900987405446527e-06, w1=-0.04193708852420707\n",
      "Regularized Logistic Regression(8/199): loss=0.5798263425638969, w0=-3.4414191284968536e-06, w1=-0.0454145622702235\n",
      "Regularized Logistic Regression(9/199): loss=0.5756474453512466, w0=-3.785564694452396e-06, w1=-0.048654468447802954\n",
      "Regularized Logistic Regression(10/199): loss=0.5722523619659212, w0=-4.122638426749028e-06, w1=-0.05168285720831888\n",
      "Regularized Logistic Regression(11/199): loss=0.5694721112550061, w0=-4.45274797161723e-06, w1=-0.05452191074701123\n",
      "Regularized Logistic Regression(12/199): loss=0.5671777274641424, w0=-4.776005822016528e-06, w1=-0.05719065315315851\n",
      "Regularized Logistic Regression(13/199): loss=0.5652700725942258, w0=-5.09252870752142e-06, w1=-0.05970550518845955\n",
      "Regularized Logistic Regression(14/199): loss=0.5636724010809463, w0=-5.402436693224531e-06, w1=-0.062080723543531884\n",
      "Regularized Logistic Regression(15/199): loss=0.5623248978465659, w0=-5.705852234479014e-06, w1=-0.06432875247339687\n",
      "Regularized Logistic Regression(16/199): loss=0.5611806380508504, w0=-6.002899299326656e-06, w1=-0.06646050798322183\n",
      "Regularized Logistic Regression(17/199): loss=0.5602025772488578, w0=-6.293702601842209e-06, w1=-0.06848560944661036\n",
      "Regularized Logistic Regression(18/199): loss=0.559361293350666, w0=-6.578386955804767e-06, w1=-0.07041256982110708\n",
      "Regularized Logistic Regression(19/199): loss=0.5586332809507444, w0=-6.857076742409593e-06, w1=-0.07224895295286655\n",
      "Regularized Logistic Regression(20/199): loss=0.557999654373474, w0=-7.129895479343696e-06, w1=-0.07400150450476793\n",
      "Regularized Logistic Regression(21/199): loss=0.5574451552584001, w0=-7.396965476811059e-06, w1=-0.0756762615861697\n",
      "Regularized Logistic Regression(22/199): loss=0.5569573886083364, w0=-7.658407566563562e-06, w1=-0.07727864506558488\n",
      "Regularized Logistic Regression(23/199): loss=0.5565262313527967, w0=-7.914340891402662e-06, w1=-0.07881353771217714\n",
      "Regularized Logistic Regression(24/199): loss=0.5561433719969774, w0=-8.164882744321825e-06, w1=-0.08028535066985164\n",
      "Regularized Logistic Regression(25/199): loss=0.5558019504697509, w0=-8.410148448149637e-06, w1=-0.08169808026998623\n",
      "Regularized Logistic Regression(26/199): loss=0.5554962749941765, w0=-8.650251268090761e-06, w1=-0.08305535680018417\n",
      "Regularized Logistic Regression(27/199): loss=0.5552215984808738, w0=-8.88530235089864e-06, w1=-0.08436048654084306\n",
      "Regularized Logistic Regression(28/199): loss=0.554973941152328, w0=-9.115410685545858e-06, w1=-0.08561648813951137\n",
      "Regularized Logistic Regression(29/199): loss=0.5547499492451494, w0=-9.340683081200924e-06, w1=-0.08682612420053483\n",
      "Regularized Logistic Regression(30/199): loss=0.5545467819935362, w0=-9.561224159097904e-06, w1=-0.08799192881336229\n",
      "Regularized Logistic Regression(31/199): loss=0.5543620208763405, w0=-9.777136355522073e-06, w1=-0.08911623161885196\n",
      "Regularized Logistic Regression(32/199): loss=0.5541935964611966, w0=-9.988519933654388e-06, w1=-0.09020117891252516\n",
      "Regularized Logistic Regression(33/199): loss=0.5540397292106303, w0=-1.0195473002440257e-05, w1=-0.09124875220208667\n",
      "Regularized Logistic Regression(34/199): loss=0.5538988814065524, w0=-1.0398091540991685e-05, w1=-0.09226078456980363\n",
      "Regularized Logistic Regression(35/199): loss=0.5537697179598231, w0=-1.0596469427310867e-05, w1=-0.09323897513555356\n",
      "Regularized Logistic Regression(36/199): loss=0.5536510743442495, w0=-1.0790698470349956e-05, w1=-0.09418490187115149\n",
      "Regularized Logistic Regression(37/199): loss=0.5535419302620727, w0=-1.0980868444605925e-05, w1=-0.09510003297912452\n",
      "Regularized Logistic Regression(38/199): loss=0.5534413879351746, w0=-1.1167067126599172e-05, w1=-0.09598573701793019\n",
      "Regularized Logistic Regression(39/199): loss=0.5533486541413679, w0=-1.1349380332706395e-05, w1=-0.09684329192956719\n",
      "Regularized Logistic Regression(40/199): loss=0.5532630252922793, w0=-1.1527891957917567e-05, w1=-0.09767389310367877\n",
      "Regularized Logistic Regression(41/199): loss=0.5531838749891799, w0=-1.1702684015167875e-05, w1=-0.098478660593825\n",
      "Regularized Logistic Regression(42/199): loss=0.5531106436038761, w0=-1.1873836674961584e-05, w1=-0.09925864558603781\n",
      "Regularized Logistic Regression(43/199): loss=0.5530428295197592, w0=-1.204142830505884e-05, w1=-0.10001483620654605\n",
      "Regularized Logistic Regression(44/199): loss=0.5529799817382053, w0=-1.2205535510040707e-05, w1=-0.10074816274430033\n",
      "Regularized Logistic Regression(45/199): loss=0.5529216936115188, w0=-1.236623317060402e-05, w1=-0.10145950235430345\n",
      "Regularized Logistic Regression(46/199): loss=0.5528675975084518, w0=-1.2523594482467267e-05, w1=-0.10214968329949317\n",
      "Regularized Logistic Regression(47/199): loss=0.5528173602543563, w0=-1.2677690994793283e-05, w1=-0.10281948878181961\n",
      "Regularized Logistic Regression(48/199): loss=0.5527706792169944, w0=-1.2828592648054414e-05, w1=-0.1034696604070348\n",
      "Regularized Logistic Regression(49/199): loss=0.5527272789324407, w0=-1.297636781128251e-05, w1=-0.10410090132240032\n",
      "Regularized Logistic Regression(50/199): loss=0.552686908184393, w0=-1.3121083318659472e-05, w1=-0.10471387906191701\n",
      "Regularized Logistic Regression(51/199): loss=0.5526493374655611, w0=-1.3262804505415375e-05, w1=-0.10530922812967124\n",
      "Regularized Logistic Regression(52/199): loss=0.5526143567622311, w0=-1.3401595243010317e-05, w1=-0.10588755234839668\n",
      "Regularized Logistic Regression(53/199): loss=0.5525817736132509, w0=-1.3537517973583732e-05, w1=-0.10644942699729351\n",
      "Regularized Logistic Regression(54/199): loss=0.5525514114029494, w0=-1.3670633743661015e-05, w1=-0.10699540076046568\n",
      "Regularized Logistic Regression(55/199): loss=0.5525231078542585, w0=-1.3801002237112561e-05, w1=-0.10752599750498491\n",
      "Regularized Logistic Regression(56/199): loss=0.5524967136938614, w0=-1.3928681807364328e-05, w1=-0.10804171790551625\n",
      "Regularized Logistic Regression(57/199): loss=0.5524720914657325, w0=-1.405372950886255e-05, w1=-0.10854304093061866\n",
      "Regularized Logistic Regression(58/199): loss=0.5524491144732002, w0=-1.4176201127797933e-05, w1=-0.10903042520421129\n",
      "Regularized Logistic Regression(59/199): loss=0.5524276658327404, w0=-1.4296151212096973e-05, w1=-0.1095043102542801\n",
      "Regularized Logistic Regression(60/199): loss=0.552407637625302, w0=-1.4413633100689762e-05, w1=-0.10996511765962423\n",
      "Regularized Logistic Regression(61/199): loss=0.5523889301330583, w0=-1.4528698952065178e-05, w1=-0.11041325210432801\n",
      "Regularized Logistic Regression(62/199): loss=0.5523714511512843, w0=-1.4641399772125418e-05, w1=-0.11084910234863794\n",
      "Regularized Logistic Regression(63/199): loss=0.5523551153665107, w0=-1.4751785441352755e-05, w1=-0.11127304212404322\n",
      "Regularized Logistic Regression(64/199): loss=0.5523398437933629, w0=-1.4859904741302045e-05, w1=-0.11168543095956371\n",
      "Regularized Logistic Regression(65/199): loss=0.5523255632635221, w0=-1.4965805380433038e-05, w1=-0.11208661494554639\n",
      "Regularized Logistic Regression(66/199): loss=0.5523122059611281, w0=-1.5069534019296844e-05, w1=-0.11247692744063874\n",
      "Regularized Logistic Regression(67/199): loss=0.5522997089996786, w0=-1.5171136295091181e-05, w1=-0.11285668972704568\n",
      "Regularized Logistic Regression(68/199): loss=0.5522880140361129, w0=-1.527065684559909e-05, w1=-0.1132262116186752\n",
      "Regularized Logistic Regression(69/199): loss=0.5522770669183034, w0=-1.536813933252591e-05, w1=-0.11358579202632388\n",
      "Regularized Logistic Regression(70/199): loss=0.5522668173626287, w0=-1.5463626464249217e-05, w1=-0.11393571948365011\n",
      "Regularized Logistic Regression(71/199): loss=0.5522572186587009, w0=-1.5557160017996324e-05, w1=-0.11427627263732185\n",
      "Regularized Logistic Regression(72/199): loss=0.5522482273986538, w0=-1.564878086146387e-05, w1=-0.11460772070440049\n",
      "Regularized Logistic Regression(73/199): loss=0.5522398032286906, w0=-1.573852897389377e-05, w1=-0.11493032389972734\n",
      "Regularized Logistic Regression(74/199): loss=0.5522319086208406, w0=-1.5826443466619645e-05, w1=-0.11524433383582049\n",
      "Regularized Logistic Regression(75/199): loss=0.5522245086630956, w0=-1.5912562603097535e-05, w1=-0.11554999389755177\n",
      "Regularized Logistic Regression(76/199): loss=0.5522175708662862, w0=-1.599692381843457e-05, w1=-0.1158475395936616\n",
      "Regularized Logistic Regression(77/199): loss=0.5522110649862312, w0=-1.607956373842885e-05, w1=-0.11613719888697796\n",
      "Regularized Logistic Regression(78/199): loss=0.5522049628598323, w0=-1.6160518198133574e-05, w1=-0.11641919250503285\n",
      "Regularized Logistic Regression(79/199): loss=0.5521992382539245, w0=-1.6239822259958244e-05, w1=-0.11669373423261521\n",
      "Regularized Logistic Regression(80/199): loss=0.5521938667258028, w0=-1.631751023131924e-05, w1=-0.11696103118766159\n",
      "Regularized Logistic Regression(81/199): loss=0.5521888254944506, w0=-1.6393615681852096e-05, w1=-0.1172212840817515\n",
      "Regularized Logistic Regression(82/199): loss=0.5521840933215834, w0=-1.6468171460197117e-05, w1=-0.11747468746636962\n",
      "Regularized Logistic Regression(83/199): loss=0.5521796504017052, w0=-1.6541209710370003e-05, w1=-0.11772142996598968\n",
      "Regularized Logistic Regression(84/199): loss=0.5521754782604477, w0=-1.661276188772866e-05, w1=-0.11796169449893887\n",
      "Regularized Logistic Regression(85/199): loss=0.5521715596605211, w0=-1.668285877454711e-05, w1=-0.11819565848692337\n",
      "Regularized Logistic Regression(86/199): loss=0.5521678785146754, w0=-1.675153049520716e-05, w1=-0.11842349405401548\n",
      "Regularized Logistic Regression(87/199): loss=0.5521644198051127, w0=-1.681880653101813e-05, w1=-0.11864536821583369\n",
      "Regularized Logistic Regression(88/199): loss=0.5521611695088443, w0=-1.68847157346747e-05, w1=-0.11886144305958782\n",
      "Regularized Logistic Regression(89/199): loss=0.5521581145285293, w0=-1.6949286344362646e-05, w1=-0.11907187591559897\n",
      "Regularized Logistic Regression(90/199): loss=0.5521552426283671, w0=-1.701254599752193e-05, w1=-0.11927681952086042\n",
      "Regularized Logistic Regression(91/199): loss=0.552152542374656, w0=-1.7074521744276373e-05, w1=-0.11947642217515281\n",
      "Regularized Logistic Regression(92/199): loss=0.5521500030806582, w0=-1.7135240060538835e-05, w1=-0.11967082789018621\n",
      "Regularized Logistic Regression(93/199): loss=0.5521476147554427, w0=-1.7194726860800605e-05, w1=-0.11986017653220611\n",
      "Regularized Logistic Regression(94/199): loss=0.5521453680564076, w0=-1.7253007510613424e-05, w1=-0.12004460395846044\n",
      "Regularized Logistic Regression(95/199): loss=0.5521432542451951, w0=-1.7310106838772352e-05, w1=-0.12022424214790049\n",
      "Regularized Logistic Regression(96/199): loss=0.5521412651467567, w0=-1.736604914920739e-05, w1=-0.12039921932645063\n",
      "Regularized Logistic Regression(97/199): loss=0.5521393931113202, w0=-1.7420858232591598e-05, w1=-0.1205696600871634\n",
      "Regularized Logistic Regression(98/199): loss=0.552137630979052, w0=-1.7474557377673197e-05, w1=-0.12073568550554847\n",
      "Regularized Logistic Regression(99/199): loss=0.5521359720472077, w0=-1.7527169382338925e-05, w1=-0.12089741325034276\n",
      "Regularized Logistic Regression(100/199): loss=0.5521344100395914, w0=-1.7578716564415687e-05, w1=-0.12105495768997121\n",
      "Regularized Logistic Regression(101/199): loss=0.5521329390781547, w0=-1.7629220772217355e-05, w1=-0.12120842999492752\n",
      "Regularized Logistic Regression(102/199): loss=0.5521315536565712, w0=-1.7678703394843414e-05, w1=-0.12135793823628586\n",
      "Regularized Logistic Regression(103/199): loss=0.5521302486156526, w0=-1.7727185372235833e-05, w1=-0.12150358748054721\n",
      "Regularized Logistic Regression(104/199): loss=0.5521290191204636, w0=-1.7774687205000454e-05, w1=-0.12164547988099737\n",
      "Regularized Logistic Regression(105/199): loss=0.5521278606390193, w0=-1.7821228963999034e-05, w1=-0.121783714765756\n",
      "Regularized Logistic Regression(106/199): loss=0.552126768922444, w0=-1.7866830299717775e-05, w1=-0.12191838872267216\n",
      "Regularized Logistic Regression(107/199): loss=0.5521257399864921, w0=-1.791151045141814e-05, w1=-0.12204959568121608\n",
      "Regularized Logistic Regression(108/199): loss=0.5521247700943315, w0=-1.7955288256075442e-05, w1=-0.12217742699151028\n",
      "Regularized Logistic Regression(109/199): loss=0.5521238557404973, w0=-1.799818215711074e-05, w1=-0.12230197150062587\n",
      "Regularized Logistic Regression(110/199): loss=0.5521229936359356, w0=-1.8040210212921163e-05, w1=-0.12242331562627351\n",
      "Regularized Logistic Regression(111/199): loss=0.5521221806940576, w0=-1.8081390105213854e-05, w1=-0.12254154342799517\n",
      "Regularized Logistic Regression(112/199): loss=0.5521214140177342, w0=-1.8121739147148474e-05, w1=-0.1226567366759748\n",
      "Regularized Logistic Regression(113/199): loss=0.5521206908871643, w0=-1.8161274291293046e-05, w1=-0.12276897491755998\n",
      "Regularized Logistic Regression(114/199): loss=0.5521200087485539, w0=-1.820001213739788e-05, w1=-0.12287833554159866\n",
      "Regularized Logistic Regression(115/199): loss=0.5521193652035529, w0=-1.8237968939992065e-05, w1=-0.12298489384067564\n",
      "Regularized Logistic Regression(116/199): loss=0.5521187579993926, w0=-1.827516061580699e-05, w1=-0.12308872307133445\n",
      "Regularized Logistic Regression(117/199): loss=0.5521181850196776, w0=-1.831160275103117e-05, w1=-0.12318989451236953\n",
      "Regularized Logistic Regression(118/199): loss=0.5521176442757871, w0=-1.8347310608400554e-05, w1=-0.12328847752125918\n",
      "Regularized Logistic Regression(119/199): loss=0.5521171338988404, w0=-1.838229913412839e-05, w1=-0.12338453958881425\n",
      "Regularized Logistic Regression(120/199): loss=0.5521166521321932, w0=-1.8416582964678566e-05, w1=-0.12347814639211133\n",
      "Regularized Logistic Regression(121/199): loss=0.552116197324421, w0=-1.8450176433386297e-05, w1=-0.12356936184577412\n",
      "Regularized Logistic Regression(122/199): loss=0.5521157679227648, w0=-1.8483093576929868e-05, w1=-0.12365824815166337\n",
      "Regularized Logistic Regression(123/199): loss=0.5521153624669991, w0=-1.8515348141657045e-05, w1=-0.12374486584703877\n",
      "Regularized Logistic Regression(124/199): loss=0.5521149795836995, w0=-1.854695358976975e-05, w1=-0.12382927385124039\n",
      "Regularized Logistic Regression(125/199): loss=0.5521146179808805, w0=-1.8577923105370357e-05, w1=-0.12391152951095169\n",
      "Regularized Logistic Regression(126/199): loss=0.5521142764429792, w0=-1.8608269600373e-05, w1=-0.12399168864408774\n",
      "Regularized Logistic Regression(127/199): loss=0.5521139538261596, w0=-1.863800572028314e-05, w1=-0.12406980558236076\n",
      "Regularized Logistic Regression(128/199): loss=0.5521136490539198, w0=-1.8667143849848523e-05, w1=-0.12414593321256785\n",
      "Regularized Logistic Regression(129/199): loss=0.5521133611129785, w0=-1.869569611858464e-05, w1=-0.1242201230166443\n",
      "Regularized Logistic Regression(130/199): loss=0.552113089049422, w0=-1.8723674406177703e-05, w1=-0.12429242511052488\n",
      "Regularized Logistic Regression(131/199): loss=0.5521128319650995, w0=-1.875109034776799e-05, w1=-0.1243628882818553\n",
      "Regularized Logistic Regression(132/199): loss=0.5521125890142421, w0=-1.877795533911648e-05, w1=-0.12443156002659006\n",
      "Regularized Logistic Regression(133/199): loss=0.5521123594002973, w0=-1.8804280541657452e-05, w1=-0.12449848658451484\n",
      "Regularized Logistic Regression(134/199): loss=0.5521121423729638, w0=-1.883007688743985e-05, w1=-0.12456371297372748\n",
      "Regularized Logistic Regression(135/199): loss=0.5521119372254086, w0=-1.8855355083959948e-05, w1=-0.12462728302411469\n",
      "Regularized Logistic Regression(136/199): loss=0.5521117432916626, w0=-1.8880125618887903e-05, w1=-0.12468923940985327\n",
      "Regularized Logistic Regression(137/199): loss=0.5521115599441744, w0=-1.8904398764690702e-05, w1=-0.12474962368097095\n",
      "Regularized Logistic Regression(138/199): loss=0.5521113865915193, w0=-1.892818458315386e-05, w1=-0.12480847629399422\n",
      "Regularized Logistic Regression(139/199): loss=0.5521112226762482, w0=-1.8951492929804344e-05, w1=-0.1248658366417132\n",
      "Regularized Logistic Regression(140/199): loss=0.5521110676728714, w0=-1.8974333458236923e-05, w1=-0.12492174308209283\n",
      "Regularized Logistic Regression(141/199): loss=0.5521109210859649, w0=-1.8996715624346215e-05, w1=-0.12497623296635552\n",
      "Regularized Logistic Regression(142/199): loss=0.5521107824483944, w0=-1.9018648690466634e-05, w1=-0.12502934266625956\n",
      "Regularized Logistic Regression(143/199): loss=0.5521106513196474, w0=-1.9040141729422365e-05, w1=-0.1250811076006036\n",
      "Regularized Logistic Regression(144/199): loss=0.5521105272842676, w0=-1.9061203628489415e-05, w1=-0.1251315622609766\n",
      "Regularized Logistic Regression(145/199): loss=0.5521104099503851, w0=-1.9081843093271784e-05, w1=-0.12518074023677717\n",
      "Regularized Logistic Regression(146/199): loss=0.5521102989483353, w0=-1.9102068651493742e-05, w1=-0.12522867423952636\n",
      "Regularized Logistic Regression(147/199): loss=0.5521101939293624, w0=-1.9121888656710097e-05, w1=-0.12527539612649413\n",
      "Regularized Logistic Regression(148/199): loss=0.5521100945644001, w0=-1.9141311291936372e-05, w1=-0.12532093692366095\n",
      "Regularized Logistic Regression(149/199): loss=0.5521100005429274, w0=-1.916034457320068e-05, w1=-0.1253653268480337\n",
      "Regularized Logistic Regression(150/199): loss=0.5521099115718917, w0=-1.9178996353019097e-05, w1=-0.1254085953293371\n",
      "Regularized Logistic Regression(151/199): loss=0.5521098273746993, w0=-1.9197274323796283e-05, w1=-0.12545077103109725\n",
      "Regularized Logistic Regression(152/199): loss=0.5521097476902609, w0=-1.9215186021153063e-05, w1=-0.12549188187113752\n",
      "Regularized Logistic Regression(153/199): loss=0.5521096722721015, w0=-1.9232738827182527e-05, w1=-0.125531955041502\n",
      "Regularized Logistic Regression(154/199): loss=0.5521096008875175, w0=-1.9249939973636397e-05, w1=-0.12557101702782672\n",
      "Regularized Logistic Regression(155/199): loss=0.5521095333167851, w0=-1.926679654504314e-05, w1=-0.1256090936281711\n",
      "Regularized Logistic Regression(156/199): loss=0.552109469352419, w0=-1.928331548175939e-05, w1=-0.12564620997132914\n",
      "Regularized Logistic Regression(157/199): loss=0.5521094087984695, w0=-1.9299503582956208e-05, w1=-0.1256823905346328\n",
      "Regularized Logistic Regression(158/199): loss=0.5521093514698663, w0=-1.9315367509541578e-05, w1=-0.1257176591612643\n",
      "Regularized Logistic Regression(159/199): loss=0.5521092971917975, w0=-1.9330913787020633e-05, w1=-0.1257520390770913\n",
      "Regularized Logistic Regression(160/199): loss=0.5521092457991266, w0=-1.9346148808294955e-05, w1=-0.12578555290703763\n",
      "Regularized Logistic Regression(161/199): loss=0.5521091971358429, w0=-1.9361078836402345e-05, w1=-0.1258182226910054\n",
      "Regularized Logistic Regression(162/199): loss=0.5521091510545449, w0=-1.9375710007198356e-05, w1=-0.12585006989935993\n",
      "Regularized Logistic Regression(163/199): loss=0.5521091074159535, w0=-1.9390048331980896e-05, w1=-0.1258811154479885\n",
      "Regularized Logistic Regression(164/199): loss=0.5521090660884526, w0=-1.940409970005919e-05, w1=-0.12591137971295027\n",
      "Regularized Logistic Regression(165/199): loss=0.5521090269476581, w0=-1.941786988126828e-05, w1=-0.12594088254472366\n",
      "Regularized Logistic Regression(166/199): loss=0.552108989876011, w0=-1.943136452843034e-05, w1=-0.12596964328206584\n",
      "Regularized Logistic Regression(167/199): loss=0.5521089547623939, w0=-1.944458917976392e-05, w1=-0.12599768076549392\n",
      "Regularized Logistic Regression(168/199): loss=0.5521089215017696, w0=-1.9457549261242276e-05, w1=-0.12602501335040045\n",
      "Regularized Logistic Regression(169/199): loss=0.5521088899948406, w0=-1.9470250088901965e-05, w1=-0.12605165891981235\n",
      "Regularized Logistic Regression(170/199): loss=0.5521088601477296, w0=-1.9482696871102708e-05, w1=-0.12607763489680318\n",
      "Regularized Logistic Regression(171/199): loss=0.5521088318716757, w0=-1.9494894710739673e-05, w1=-0.12610295825656845\n",
      "Regularized Logistic Regression(172/199): loss=0.5521088050827498, w0=-1.9506848607409175e-05, w1=-0.1261276455381765\n",
      "Regularized Logistic Regression(173/199): loss=0.5521087797015866, w0=-1.951856345952883e-05, w1=-0.1261517128559984\n",
      "Regularized Logistic Regression(174/199): loss=0.5521087556531296, w0=-1.9530044066413154e-05, w1=-0.12617517591083327\n",
      "Regularized Logistic Regression(175/199): loss=0.552108732866394, w0=-1.9541295130305585e-05, w1=-0.1261980500007293\n",
      "Regularized Logistic Regression(176/199): loss=0.5521087112742398, w0=-1.955232125836786e-05, w1=-0.1262203500315167\n",
      "Regularized Logistic Regression(177/199): loss=0.5521086908131602, w0=-1.9563126964627707e-05, w1=-0.12624209052705523\n",
      "Regularized Logistic Regression(178/199): loss=0.5521086714230807, w0=-1.9573716671885744e-05, w1=-0.1262632856392085\n",
      "Regularized Logistic Regression(179/199): loss=0.5521086530471697, w0=-1.9584094713582447e-05, w1=-0.1262839491575481\n",
      "Regularized Logistic Regression(180/199): loss=0.5521086356316605, w0=-1.9594265335626113e-05, w1=-0.12630409451879915\n",
      "Regularized Logistic Regression(181/199): loss=0.5521086191256818, w0=-1.9604232698182603e-05, w1=-0.12632373481603243\n",
      "Regularized Logistic Regression(182/199): loss=0.5521086034810998, w0=-1.9614000877427693e-05, w1=-0.1263428828076107\n",
      "Regularized Logistic Regression(183/199): loss=0.5521085886523682, w0=-1.9623573867262915e-05, w1=-0.12636155092589604\n",
      "Regularized Logistic Regression(184/199): loss=0.5521085745963854, w0=-1.963295558099559e-05, w1=-0.12637975128572482\n",
      "Regularized Logistic Regression(185/199): loss=0.5521085612723621, w0=-1.964214985298386e-05, w1=-0.12639749569265668\n",
      "Regularized Logistic Regression(186/199): loss=0.5521085486416939, w0=-1.965116044024749e-05, w1=-0.12641479565100294\n",
      "Regularized Logistic Regression(187/199): loss=0.5521085366678431, w0=-1.9659991024045145e-05, w1=-0.12643166237164183\n",
      "Regularized Logistic Regression(188/199): loss=0.5521085253162256, w0=-1.966864521141887e-05, w1=-0.12644810677962798\n",
      "Regularized Logistic Regression(189/199): loss=0.5521085145541044, w0=-1.967712653670651e-05, w1=-0.12646413952159474\n",
      "Regularized Logistic Regression(190/199): loss=0.5521085043504902, w0=-1.9685438463022694e-05, w1=-0.12647977097296723\n",
      "Regularized Logistic Regression(191/199): loss=0.5521084946760458, w0=-1.9693584383709124e-05, w1=-0.12649501124497642\n",
      "Regularized Logistic Regression(192/199): loss=0.552108485502996, w0=-1.9701567623754772e-05, w1=-0.1265098701914937\n",
      "Regularized Logistic Regression(193/199): loss=0.5521084768050443, w0=-1.9709391441186672e-05, w1=-0.1265243574156821\n",
      "Regularized Logistic Regression(194/199): loss=0.5521084685572918, w0=-1.971705902843188e-05, w1=-0.1265384822764706\n",
      "Regularized Logistic Regression(195/199): loss=0.5521084607361627, w0=-1.9724573513651276e-05, w1=-0.12655225389485886\n",
      "Regularized Logistic Regression(196/199): loss=0.552108453319332, w0=-1.9731937962045786e-05, w1=-0.12656568116005562\n",
      "Regularized Logistic Regression(197/199): loss=0.5521084462856588, w0=-1.9739155377135568e-05, w1=-0.12657877273545512\n",
      "Regularized Logistic Regression(198/199): loss=0.5521084396151215, w0=-1.9746228702012817e-05, w1=-0.12659153706445608\n",
      "Regularized Logistic Regression(199/199): loss=0.5521084332887595, w0=-1.975316082056869e-05, w1=-0.12660398237612672\n",
      "Regularized Logistic Regression(0/299): loss=0.693147180559945, w0=-4.176586058555737e-07, w1=-0.007317796433195506\n",
      "Regularized Logistic Regression(1/299): loss=0.6634280901010439, w0=-8.243982528713279e-07, w1=-0.013821570960075812\n",
      "Regularized Logistic Regression(2/299): loss=0.640822212803381, w0=-1.2218593650579815e-06, w1=-0.019656675275621906\n",
      "Regularized Logistic Regression(3/299): loss=0.6234976083418551, w0=-1.6109214419849742e-06, w1=-0.02493392525522841\n",
      "Regularized Logistic Regression(4/299): loss=0.6101148060646779, w0=-1.9920565612691653e-06, w1=-0.029739134333005672\n",
      "Regularized Logistic Regression(5/299): loss=0.5996905637993731, w0=-2.365529087876996e-06, w1=-0.034140062672567285\n",
      "Regularized Logistic Regression(6/299): loss=0.5915011908334518, w0=-2.731503203849732e-06, w1=-0.03819120648935659\n",
      "Regularized Logistic Regression(7/299): loss=0.5850120877185038, w0=-3.0900987405446527e-06, w1=-0.04193708852420707\n",
      "Regularized Logistic Regression(8/299): loss=0.5798263425638969, w0=-3.4414191284968536e-06, w1=-0.0454145622702235\n",
      "Regularized Logistic Regression(9/299): loss=0.5756474453512466, w0=-3.785564694452396e-06, w1=-0.048654468447802954\n",
      "Regularized Logistic Regression(10/299): loss=0.5722523619659212, w0=-4.122638426749028e-06, w1=-0.05168285720831888\n",
      "Regularized Logistic Regression(11/299): loss=0.5694721112550061, w0=-4.45274797161723e-06, w1=-0.05452191074701123\n",
      "Regularized Logistic Regression(12/299): loss=0.5671777274641424, w0=-4.776005822016528e-06, w1=-0.05719065315315851\n",
      "Regularized Logistic Regression(13/299): loss=0.5652700725942258, w0=-5.09252870752142e-06, w1=-0.05970550518845955\n",
      "Regularized Logistic Regression(14/299): loss=0.5636724010809463, w0=-5.402436693224531e-06, w1=-0.062080723543531884\n",
      "Regularized Logistic Regression(15/299): loss=0.5623248978465659, w0=-5.705852234479014e-06, w1=-0.06432875247339687\n",
      "Regularized Logistic Regression(16/299): loss=0.5611806380508504, w0=-6.002899299326656e-06, w1=-0.06646050798322183\n",
      "Regularized Logistic Regression(17/299): loss=0.5602025772488578, w0=-6.293702601842209e-06, w1=-0.06848560944661036\n",
      "Regularized Logistic Regression(18/299): loss=0.559361293350666, w0=-6.578386955804767e-06, w1=-0.07041256982110708\n",
      "Regularized Logistic Regression(19/299): loss=0.5586332809507444, w0=-6.857076742409593e-06, w1=-0.07224895295286655\n",
      "Regularized Logistic Regression(20/299): loss=0.557999654373474, w0=-7.129895479343696e-06, w1=-0.07400150450476793\n",
      "Regularized Logistic Regression(21/299): loss=0.5574451552584001, w0=-7.396965476811059e-06, w1=-0.0756762615861697\n",
      "Regularized Logistic Regression(22/299): loss=0.5569573886083364, w0=-7.658407566563562e-06, w1=-0.07727864506558488\n",
      "Regularized Logistic Regression(23/299): loss=0.5565262313527967, w0=-7.914340891402662e-06, w1=-0.07881353771217714\n",
      "Regularized Logistic Regression(24/299): loss=0.5561433719969774, w0=-8.164882744321825e-06, w1=-0.08028535066985164\n",
      "Regularized Logistic Regression(25/299): loss=0.5558019504697509, w0=-8.410148448149637e-06, w1=-0.08169808026998623\n",
      "Regularized Logistic Regression(26/299): loss=0.5554962749941765, w0=-8.650251268090761e-06, w1=-0.08305535680018417\n",
      "Regularized Logistic Regression(27/299): loss=0.5552215984808738, w0=-8.88530235089864e-06, w1=-0.08436048654084306\n",
      "Regularized Logistic Regression(28/299): loss=0.554973941152328, w0=-9.115410685545858e-06, w1=-0.08561648813951137\n",
      "Regularized Logistic Regression(29/299): loss=0.5547499492451494, w0=-9.340683081200924e-06, w1=-0.08682612420053483\n",
      "Regularized Logistic Regression(30/299): loss=0.5545467819935362, w0=-9.561224159097904e-06, w1=-0.08799192881336229\n",
      "Regularized Logistic Regression(31/299): loss=0.5543620208763405, w0=-9.777136355522073e-06, w1=-0.08911623161885196\n",
      "Regularized Logistic Regression(32/299): loss=0.5541935964611966, w0=-9.988519933654388e-06, w1=-0.09020117891252516\n",
      "Regularized Logistic Regression(33/299): loss=0.5540397292106303, w0=-1.0195473002440257e-05, w1=-0.09124875220208667\n",
      "Regularized Logistic Regression(34/299): loss=0.5538988814065524, w0=-1.0398091540991685e-05, w1=-0.09226078456980363\n",
      "Regularized Logistic Regression(35/299): loss=0.5537697179598231, w0=-1.0596469427310867e-05, w1=-0.09323897513555356\n",
      "Regularized Logistic Regression(36/299): loss=0.5536510743442495, w0=-1.0790698470349956e-05, w1=-0.09418490187115149\n",
      "Regularized Logistic Regression(37/299): loss=0.5535419302620727, w0=-1.0980868444605925e-05, w1=-0.09510003297912452\n",
      "Regularized Logistic Regression(38/299): loss=0.5534413879351746, w0=-1.1167067126599172e-05, w1=-0.09598573701793019\n",
      "Regularized Logistic Regression(39/299): loss=0.5533486541413679, w0=-1.1349380332706395e-05, w1=-0.09684329192956719\n",
      "Regularized Logistic Regression(40/299): loss=0.5532630252922793, w0=-1.1527891957917567e-05, w1=-0.09767389310367877\n",
      "Regularized Logistic Regression(41/299): loss=0.5531838749891799, w0=-1.1702684015167875e-05, w1=-0.098478660593825\n",
      "Regularized Logistic Regression(42/299): loss=0.5531106436038761, w0=-1.1873836674961584e-05, w1=-0.09925864558603781\n",
      "Regularized Logistic Regression(43/299): loss=0.5530428295197592, w0=-1.204142830505884e-05, w1=-0.10001483620654605\n",
      "Regularized Logistic Regression(44/299): loss=0.5529799817382053, w0=-1.2205535510040707e-05, w1=-0.10074816274430033\n",
      "Regularized Logistic Regression(45/299): loss=0.5529216936115188, w0=-1.236623317060402e-05, w1=-0.10145950235430345\n",
      "Regularized Logistic Regression(46/299): loss=0.5528675975084518, w0=-1.2523594482467267e-05, w1=-0.10214968329949317\n",
      "Regularized Logistic Regression(47/299): loss=0.5528173602543563, w0=-1.2677690994793283e-05, w1=-0.10281948878181961\n",
      "Regularized Logistic Regression(48/299): loss=0.5527706792169944, w0=-1.2828592648054414e-05, w1=-0.1034696604070348\n",
      "Regularized Logistic Regression(49/299): loss=0.5527272789324407, w0=-1.297636781128251e-05, w1=-0.10410090132240032\n",
      "Regularized Logistic Regression(50/299): loss=0.552686908184393, w0=-1.3121083318659472e-05, w1=-0.10471387906191701\n",
      "Regularized Logistic Regression(51/299): loss=0.5526493374655611, w0=-1.3262804505415375e-05, w1=-0.10530922812967124\n",
      "Regularized Logistic Regression(52/299): loss=0.5526143567622311, w0=-1.3401595243010317e-05, w1=-0.10588755234839668\n",
      "Regularized Logistic Regression(53/299): loss=0.5525817736132509, w0=-1.3537517973583732e-05, w1=-0.10644942699729351\n",
      "Regularized Logistic Regression(54/299): loss=0.5525514114029494, w0=-1.3670633743661015e-05, w1=-0.10699540076046568\n",
      "Regularized Logistic Regression(55/299): loss=0.5525231078542585, w0=-1.3801002237112561e-05, w1=-0.10752599750498491\n",
      "Regularized Logistic Regression(56/299): loss=0.5524967136938614, w0=-1.3928681807364328e-05, w1=-0.10804171790551625\n",
      "Regularized Logistic Regression(57/299): loss=0.5524720914657325, w0=-1.405372950886255e-05, w1=-0.10854304093061866\n",
      "Regularized Logistic Regression(58/299): loss=0.5524491144732002, w0=-1.4176201127797933e-05, w1=-0.10903042520421129\n",
      "Regularized Logistic Regression(59/299): loss=0.5524276658327404, w0=-1.4296151212096973e-05, w1=-0.1095043102542801\n",
      "Regularized Logistic Regression(60/299): loss=0.552407637625302, w0=-1.4413633100689762e-05, w1=-0.10996511765962423\n",
      "Regularized Logistic Regression(61/299): loss=0.5523889301330583, w0=-1.4528698952065178e-05, w1=-0.11041325210432801\n",
      "Regularized Logistic Regression(62/299): loss=0.5523714511512843, w0=-1.4641399772125418e-05, w1=-0.11084910234863794\n",
      "Regularized Logistic Regression(63/299): loss=0.5523551153665107, w0=-1.4751785441352755e-05, w1=-0.11127304212404322\n",
      "Regularized Logistic Regression(64/299): loss=0.5523398437933629, w0=-1.4859904741302045e-05, w1=-0.11168543095956371\n",
      "Regularized Logistic Regression(65/299): loss=0.5523255632635221, w0=-1.4965805380433038e-05, w1=-0.11208661494554639\n",
      "Regularized Logistic Regression(66/299): loss=0.5523122059611281, w0=-1.5069534019296844e-05, w1=-0.11247692744063874\n",
      "Regularized Logistic Regression(67/299): loss=0.5522997089996786, w0=-1.5171136295091181e-05, w1=-0.11285668972704568\n",
      "Regularized Logistic Regression(68/299): loss=0.5522880140361129, w0=-1.527065684559909e-05, w1=-0.1132262116186752\n",
      "Regularized Logistic Regression(69/299): loss=0.5522770669183034, w0=-1.536813933252591e-05, w1=-0.11358579202632388\n",
      "Regularized Logistic Regression(70/299): loss=0.5522668173626287, w0=-1.5463626464249217e-05, w1=-0.11393571948365011\n",
      "Regularized Logistic Regression(71/299): loss=0.5522572186587009, w0=-1.5557160017996324e-05, w1=-0.11427627263732185\n",
      "Regularized Logistic Regression(72/299): loss=0.5522482273986538, w0=-1.564878086146387e-05, w1=-0.11460772070440049\n",
      "Regularized Logistic Regression(73/299): loss=0.5522398032286906, w0=-1.573852897389377e-05, w1=-0.11493032389972734\n",
      "Regularized Logistic Regression(74/299): loss=0.5522319086208406, w0=-1.5826443466619645e-05, w1=-0.11524433383582049\n",
      "Regularized Logistic Regression(75/299): loss=0.5522245086630956, w0=-1.5912562603097535e-05, w1=-0.11554999389755177\n",
      "Regularized Logistic Regression(76/299): loss=0.5522175708662862, w0=-1.599692381843457e-05, w1=-0.1158475395936616\n",
      "Regularized Logistic Regression(77/299): loss=0.5522110649862312, w0=-1.607956373842885e-05, w1=-0.11613719888697796\n",
      "Regularized Logistic Regression(78/299): loss=0.5522049628598323, w0=-1.6160518198133574e-05, w1=-0.11641919250503285\n",
      "Regularized Logistic Regression(79/299): loss=0.5521992382539245, w0=-1.6239822259958244e-05, w1=-0.11669373423261521\n",
      "Regularized Logistic Regression(80/299): loss=0.5521938667258028, w0=-1.631751023131924e-05, w1=-0.11696103118766159\n",
      "Regularized Logistic Regression(81/299): loss=0.5521888254944506, w0=-1.6393615681852096e-05, w1=-0.1172212840817515\n",
      "Regularized Logistic Regression(82/299): loss=0.5521840933215834, w0=-1.6468171460197117e-05, w1=-0.11747468746636962\n",
      "Regularized Logistic Regression(83/299): loss=0.5521796504017052, w0=-1.6541209710370003e-05, w1=-0.11772142996598968\n",
      "Regularized Logistic Regression(84/299): loss=0.5521754782604477, w0=-1.661276188772866e-05, w1=-0.11796169449893887\n",
      "Regularized Logistic Regression(85/299): loss=0.5521715596605211, w0=-1.668285877454711e-05, w1=-0.11819565848692337\n",
      "Regularized Logistic Regression(86/299): loss=0.5521678785146754, w0=-1.675153049520716e-05, w1=-0.11842349405401548\n",
      "Regularized Logistic Regression(87/299): loss=0.5521644198051127, w0=-1.681880653101813e-05, w1=-0.11864536821583369\n",
      "Regularized Logistic Regression(88/299): loss=0.5521611695088443, w0=-1.68847157346747e-05, w1=-0.11886144305958782\n",
      "Regularized Logistic Regression(89/299): loss=0.5521581145285293, w0=-1.6949286344362646e-05, w1=-0.11907187591559897\n",
      "Regularized Logistic Regression(90/299): loss=0.5521552426283671, w0=-1.701254599752193e-05, w1=-0.11927681952086042\n",
      "Regularized Logistic Regression(91/299): loss=0.552152542374656, w0=-1.7074521744276373e-05, w1=-0.11947642217515281\n",
      "Regularized Logistic Regression(92/299): loss=0.5521500030806582, w0=-1.7135240060538835e-05, w1=-0.11967082789018621\n",
      "Regularized Logistic Regression(93/299): loss=0.5521476147554427, w0=-1.7194726860800605e-05, w1=-0.11986017653220611\n",
      "Regularized Logistic Regression(94/299): loss=0.5521453680564076, w0=-1.7253007510613424e-05, w1=-0.12004460395846044\n",
      "Regularized Logistic Regression(95/299): loss=0.5521432542451951, w0=-1.7310106838772352e-05, w1=-0.12022424214790049\n",
      "Regularized Logistic Regression(96/299): loss=0.5521412651467567, w0=-1.736604914920739e-05, w1=-0.12039921932645063\n",
      "Regularized Logistic Regression(97/299): loss=0.5521393931113202, w0=-1.7420858232591598e-05, w1=-0.1205696600871634\n",
      "Regularized Logistic Regression(98/299): loss=0.552137630979052, w0=-1.7474557377673197e-05, w1=-0.12073568550554847\n",
      "Regularized Logistic Regression(99/299): loss=0.5521359720472077, w0=-1.7527169382338925e-05, w1=-0.12089741325034276\n",
      "Regularized Logistic Regression(100/299): loss=0.5521344100395914, w0=-1.7578716564415687e-05, w1=-0.12105495768997121\n",
      "Regularized Logistic Regression(101/299): loss=0.5521329390781547, w0=-1.7629220772217355e-05, w1=-0.12120842999492752\n",
      "Regularized Logistic Regression(102/299): loss=0.5521315536565712, w0=-1.7678703394843414e-05, w1=-0.12135793823628586\n",
      "Regularized Logistic Regression(103/299): loss=0.5521302486156526, w0=-1.7727185372235833e-05, w1=-0.12150358748054721\n",
      "Regularized Logistic Regression(104/299): loss=0.5521290191204636, w0=-1.7774687205000454e-05, w1=-0.12164547988099737\n",
      "Regularized Logistic Regression(105/299): loss=0.5521278606390193, w0=-1.7821228963999034e-05, w1=-0.121783714765756\n",
      "Regularized Logistic Regression(106/299): loss=0.552126768922444, w0=-1.7866830299717775e-05, w1=-0.12191838872267216\n",
      "Regularized Logistic Regression(107/299): loss=0.5521257399864921, w0=-1.791151045141814e-05, w1=-0.12204959568121608\n",
      "Regularized Logistic Regression(108/299): loss=0.5521247700943315, w0=-1.7955288256075442e-05, w1=-0.12217742699151028\n",
      "Regularized Logistic Regression(109/299): loss=0.5521238557404973, w0=-1.799818215711074e-05, w1=-0.12230197150062587\n",
      "Regularized Logistic Regression(110/299): loss=0.5521229936359356, w0=-1.8040210212921163e-05, w1=-0.12242331562627351\n",
      "Regularized Logistic Regression(111/299): loss=0.5521221806940576, w0=-1.8081390105213854e-05, w1=-0.12254154342799517\n",
      "Regularized Logistic Regression(112/299): loss=0.5521214140177342, w0=-1.8121739147148474e-05, w1=-0.1226567366759748\n",
      "Regularized Logistic Regression(113/299): loss=0.5521206908871643, w0=-1.8161274291293046e-05, w1=-0.12276897491755998\n",
      "Regularized Logistic Regression(114/299): loss=0.5521200087485539, w0=-1.820001213739788e-05, w1=-0.12287833554159866\n",
      "Regularized Logistic Regression(115/299): loss=0.5521193652035529, w0=-1.8237968939992065e-05, w1=-0.12298489384067564\n",
      "Regularized Logistic Regression(116/299): loss=0.5521187579993926, w0=-1.827516061580699e-05, w1=-0.12308872307133445\n",
      "Regularized Logistic Regression(117/299): loss=0.5521181850196776, w0=-1.831160275103117e-05, w1=-0.12318989451236953\n",
      "Regularized Logistic Regression(118/299): loss=0.5521176442757871, w0=-1.8347310608400554e-05, w1=-0.12328847752125918\n",
      "Regularized Logistic Regression(119/299): loss=0.5521171338988404, w0=-1.838229913412839e-05, w1=-0.12338453958881425\n",
      "Regularized Logistic Regression(120/299): loss=0.5521166521321932, w0=-1.8416582964678566e-05, w1=-0.12347814639211133\n",
      "Regularized Logistic Regression(121/299): loss=0.552116197324421, w0=-1.8450176433386297e-05, w1=-0.12356936184577412\n",
      "Regularized Logistic Regression(122/299): loss=0.5521157679227648, w0=-1.8483093576929868e-05, w1=-0.12365824815166337\n",
      "Regularized Logistic Regression(123/299): loss=0.5521153624669991, w0=-1.8515348141657045e-05, w1=-0.12374486584703877\n",
      "Regularized Logistic Regression(124/299): loss=0.5521149795836995, w0=-1.854695358976975e-05, w1=-0.12382927385124039\n",
      "Regularized Logistic Regression(125/299): loss=0.5521146179808805, w0=-1.8577923105370357e-05, w1=-0.12391152951095169\n",
      "Regularized Logistic Regression(126/299): loss=0.5521142764429792, w0=-1.8608269600373e-05, w1=-0.12399168864408774\n",
      "Regularized Logistic Regression(127/299): loss=0.5521139538261596, w0=-1.863800572028314e-05, w1=-0.12406980558236076\n",
      "Regularized Logistic Regression(128/299): loss=0.5521136490539198, w0=-1.8667143849848523e-05, w1=-0.12414593321256785\n",
      "Regularized Logistic Regression(129/299): loss=0.5521133611129785, w0=-1.869569611858464e-05, w1=-0.1242201230166443\n",
      "Regularized Logistic Regression(130/299): loss=0.552113089049422, w0=-1.8723674406177703e-05, w1=-0.12429242511052488\n",
      "Regularized Logistic Regression(131/299): loss=0.5521128319650995, w0=-1.875109034776799e-05, w1=-0.1243628882818553\n",
      "Regularized Logistic Regression(132/299): loss=0.5521125890142421, w0=-1.877795533911648e-05, w1=-0.12443156002659006\n",
      "Regularized Logistic Regression(133/299): loss=0.5521123594002973, w0=-1.8804280541657452e-05, w1=-0.12449848658451484\n",
      "Regularized Logistic Regression(134/299): loss=0.5521121423729638, w0=-1.883007688743985e-05, w1=-0.12456371297372748\n",
      "Regularized Logistic Regression(135/299): loss=0.5521119372254086, w0=-1.8855355083959948e-05, w1=-0.12462728302411469\n",
      "Regularized Logistic Regression(136/299): loss=0.5521117432916626, w0=-1.8880125618887903e-05, w1=-0.12468923940985327\n",
      "Regularized Logistic Regression(137/299): loss=0.5521115599441744, w0=-1.8904398764690702e-05, w1=-0.12474962368097095\n",
      "Regularized Logistic Regression(138/299): loss=0.5521113865915193, w0=-1.892818458315386e-05, w1=-0.12480847629399422\n",
      "Regularized Logistic Regression(139/299): loss=0.5521112226762482, w0=-1.8951492929804344e-05, w1=-0.1248658366417132\n",
      "Regularized Logistic Regression(140/299): loss=0.5521110676728714, w0=-1.8974333458236923e-05, w1=-0.12492174308209283\n",
      "Regularized Logistic Regression(141/299): loss=0.5521109210859649, w0=-1.8996715624346215e-05, w1=-0.12497623296635552\n",
      "Regularized Logistic Regression(142/299): loss=0.5521107824483944, w0=-1.9018648690466634e-05, w1=-0.12502934266625956\n",
      "Regularized Logistic Regression(143/299): loss=0.5521106513196474, w0=-1.9040141729422365e-05, w1=-0.1250811076006036\n",
      "Regularized Logistic Regression(144/299): loss=0.5521105272842676, w0=-1.9061203628489415e-05, w1=-0.1251315622609766\n",
      "Regularized Logistic Regression(145/299): loss=0.5521104099503851, w0=-1.9081843093271784e-05, w1=-0.12518074023677717\n",
      "Regularized Logistic Regression(146/299): loss=0.5521102989483353, w0=-1.9102068651493742e-05, w1=-0.12522867423952636\n",
      "Regularized Logistic Regression(147/299): loss=0.5521101939293624, w0=-1.9121888656710097e-05, w1=-0.12527539612649413\n",
      "Regularized Logistic Regression(148/299): loss=0.5521100945644001, w0=-1.9141311291936372e-05, w1=-0.12532093692366095\n",
      "Regularized Logistic Regression(149/299): loss=0.5521100005429274, w0=-1.916034457320068e-05, w1=-0.1253653268480337\n",
      "Regularized Logistic Regression(150/299): loss=0.5521099115718917, w0=-1.9178996353019097e-05, w1=-0.1254085953293371\n",
      "Regularized Logistic Regression(151/299): loss=0.5521098273746993, w0=-1.9197274323796283e-05, w1=-0.12545077103109725\n",
      "Regularized Logistic Regression(152/299): loss=0.5521097476902609, w0=-1.9215186021153063e-05, w1=-0.12549188187113752\n",
      "Regularized Logistic Regression(153/299): loss=0.5521096722721015, w0=-1.9232738827182527e-05, w1=-0.125531955041502\n",
      "Regularized Logistic Regression(154/299): loss=0.5521096008875175, w0=-1.9249939973636397e-05, w1=-0.12557101702782672\n",
      "Regularized Logistic Regression(155/299): loss=0.5521095333167851, w0=-1.926679654504314e-05, w1=-0.1256090936281711\n",
      "Regularized Logistic Regression(156/299): loss=0.552109469352419, w0=-1.928331548175939e-05, w1=-0.12564620997132914\n",
      "Regularized Logistic Regression(157/299): loss=0.5521094087984695, w0=-1.9299503582956208e-05, w1=-0.1256823905346328\n",
      "Regularized Logistic Regression(158/299): loss=0.5521093514698663, w0=-1.9315367509541578e-05, w1=-0.1257176591612643\n",
      "Regularized Logistic Regression(159/299): loss=0.5521092971917975, w0=-1.9330913787020633e-05, w1=-0.1257520390770913\n",
      "Regularized Logistic Regression(160/299): loss=0.5521092457991266, w0=-1.9346148808294955e-05, w1=-0.12578555290703763\n",
      "Regularized Logistic Regression(161/299): loss=0.5521091971358429, w0=-1.9361078836402345e-05, w1=-0.1258182226910054\n",
      "Regularized Logistic Regression(162/299): loss=0.5521091510545449, w0=-1.9375710007198356e-05, w1=-0.12585006989935993\n",
      "Regularized Logistic Regression(163/299): loss=0.5521091074159535, w0=-1.9390048331980896e-05, w1=-0.1258811154479885\n",
      "Regularized Logistic Regression(164/299): loss=0.5521090660884526, w0=-1.940409970005919e-05, w1=-0.12591137971295027\n",
      "Regularized Logistic Regression(165/299): loss=0.5521090269476581, w0=-1.941786988126828e-05, w1=-0.12594088254472366\n",
      "Regularized Logistic Regression(166/299): loss=0.552108989876011, w0=-1.943136452843034e-05, w1=-0.12596964328206584\n",
      "Regularized Logistic Regression(167/299): loss=0.5521089547623939, w0=-1.944458917976392e-05, w1=-0.12599768076549392\n",
      "Regularized Logistic Regression(168/299): loss=0.5521089215017696, w0=-1.9457549261242276e-05, w1=-0.12602501335040045\n",
      "Regularized Logistic Regression(169/299): loss=0.5521088899948406, w0=-1.9470250088901965e-05, w1=-0.12605165891981235\n",
      "Regularized Logistic Regression(170/299): loss=0.5521088601477296, w0=-1.9482696871102708e-05, w1=-0.12607763489680318\n",
      "Regularized Logistic Regression(171/299): loss=0.5521088318716757, w0=-1.9494894710739673e-05, w1=-0.12610295825656845\n",
      "Regularized Logistic Regression(172/299): loss=0.5521088050827498, w0=-1.9506848607409175e-05, w1=-0.1261276455381765\n",
      "Regularized Logistic Regression(173/299): loss=0.5521087797015866, w0=-1.951856345952883e-05, w1=-0.1261517128559984\n",
      "Regularized Logistic Regression(174/299): loss=0.5521087556531296, w0=-1.9530044066413154e-05, w1=-0.12617517591083327\n",
      "Regularized Logistic Regression(175/299): loss=0.552108732866394, w0=-1.9541295130305585e-05, w1=-0.1261980500007293\n",
      "Regularized Logistic Regression(176/299): loss=0.5521087112742398, w0=-1.955232125836786e-05, w1=-0.1262203500315167\n",
      "Regularized Logistic Regression(177/299): loss=0.5521086908131602, w0=-1.9563126964627707e-05, w1=-0.12624209052705523\n",
      "Regularized Logistic Regression(178/299): loss=0.5521086714230807, w0=-1.9573716671885744e-05, w1=-0.1262632856392085\n",
      "Regularized Logistic Regression(179/299): loss=0.5521086530471697, w0=-1.9584094713582447e-05, w1=-0.1262839491575481\n",
      "Regularized Logistic Regression(180/299): loss=0.5521086356316605, w0=-1.9594265335626113e-05, w1=-0.12630409451879915\n",
      "Regularized Logistic Regression(181/299): loss=0.5521086191256818, w0=-1.9604232698182603e-05, w1=-0.12632373481603243\n",
      "Regularized Logistic Regression(182/299): loss=0.5521086034810998, w0=-1.9614000877427693e-05, w1=-0.1263428828076107\n",
      "Regularized Logistic Regression(183/299): loss=0.5521085886523682, w0=-1.9623573867262915e-05, w1=-0.12636155092589604\n",
      "Regularized Logistic Regression(184/299): loss=0.5521085745963854, w0=-1.963295558099559e-05, w1=-0.12637975128572482\n",
      "Regularized Logistic Regression(185/299): loss=0.5521085612723621, w0=-1.964214985298386e-05, w1=-0.12639749569265668\n",
      "Regularized Logistic Regression(186/299): loss=0.5521085486416939, w0=-1.965116044024749e-05, w1=-0.12641479565100294\n",
      "Regularized Logistic Regression(187/299): loss=0.5521085366678431, w0=-1.9659991024045145e-05, w1=-0.12643166237164183\n",
      "Regularized Logistic Regression(188/299): loss=0.5521085253162256, w0=-1.966864521141887e-05, w1=-0.12644810677962798\n",
      "Regularized Logistic Regression(189/299): loss=0.5521085145541044, w0=-1.967712653670651e-05, w1=-0.12646413952159474\n",
      "Regularized Logistic Regression(190/299): loss=0.5521085043504902, w0=-1.9685438463022694e-05, w1=-0.12647977097296723\n",
      "Regularized Logistic Regression(191/299): loss=0.5521084946760458, w0=-1.9693584383709124e-05, w1=-0.12649501124497642\n",
      "Regularized Logistic Regression(192/299): loss=0.552108485502996, w0=-1.9701567623754772e-05, w1=-0.1265098701914937\n",
      "Regularized Logistic Regression(193/299): loss=0.5521084768050443, w0=-1.9709391441186672e-05, w1=-0.1265243574156821\n",
      "Regularized Logistic Regression(194/299): loss=0.5521084685572918, w0=-1.971705902843188e-05, w1=-0.1265384822764706\n",
      "Regularized Logistic Regression(195/299): loss=0.5521084607361627, w0=-1.9724573513651276e-05, w1=-0.12655225389485886\n",
      "Regularized Logistic Regression(196/299): loss=0.552108453319332, w0=-1.9731937962045786e-05, w1=-0.12656568116005562\n",
      "Regularized Logistic Regression(197/299): loss=0.5521084462856588, w0=-1.9739155377135568e-05, w1=-0.12657877273545512\n",
      "Regularized Logistic Regression(198/299): loss=0.5521084396151215, w0=-1.9746228702012817e-05, w1=-0.12659153706445608\n",
      "Regularized Logistic Regression(199/299): loss=0.5521084332887595, w0=-1.975316082056869e-05, w1=-0.12660398237612672\n",
      "Regularized Logistic Regression(200/299): loss=0.5521084272886134, w0=-1.9759954558694917e-05, w1=-0.12661611669072123\n",
      "Regularized Logistic Regression(201/299): loss=0.5521084215976744, w0=-1.9766612685460614e-05, w1=-0.12662794782505263\n",
      "Regularized Logistic Regression(202/299): loss=0.5521084161998306, w0=-1.977313791426489e-05, w1=-0.12663948339772213\n",
      "Regularized Logistic Regression(203/299): loss=0.5521084110798202, w0=-1.977953290396567e-05, w1=-0.1266507308342138\n",
      "Regularized Logistic Regression(204/299): loss=0.5521084062231871, w0=-1.9785800259985307e-05, w1=-0.12666169737185343\n",
      "Regularized Logistic Regression(205/299): loss=0.5521084016162352, w0=-1.979194253539343e-05, w1=-0.12667239006463973\n",
      "Regularized Logistic Regression(206/299): loss=0.5521083972459896, w0=-1.9797962231967533e-05, w1=-0.12668281578794718\n",
      "Regularized Logistic Regression(207/299): loss=0.552108393100158, w0=-1.9803861801231757e-05, w1=-0.12669298124310605\n",
      "Regularized Logistic Regression(208/299): loss=0.5521083891670943, w0=-1.9809643645474354e-05, w1=-0.12670289296186507\n",
      "Regularized Logistic Regression(209/299): loss=0.5521083854357631, w0=-1.981531011874425e-05, w1=-0.1267125573107339\n",
      "Regularized Logistic Regression(210/299): loss=0.5521083818957089, w0=-1.9820863527827128e-05, w1=-0.12672198049521566\n",
      "Regularized Logistic Regression(211/299): loss=0.5521083785370239, w0=-1.9826306133201545e-05, w1=-0.12673116856392688\n",
      "Regularized Logistic Regression(212/299): loss=0.5521083753503191, w0=-1.9831640149975407e-05, w1=-0.12674012741261195\n",
      "Regularized Logistic Regression(213/299): loss=0.5521083723266978, w0=-1.9836867748803274e-05, w1=-0.12674886278805034\n",
      "Regularized Logistic Regression(214/299): loss=0.5521083694577285, w0=-1.984199105678485e-05, w1=-0.12675738029186498\n",
      "Regularized Logistic Regression(215/299): loss=0.5521083667354203, w0=-1.9847012158345126e-05, w1=-0.12676568538423022\n",
      "Regularized Logistic Regression(216/299): loss=0.5521083641521997, w0=-1.985193309609646e-05, w1=-0.12677378338748332\n",
      "Regularized Logistic Regression(217/299): loss=0.5521083617008888, w0=-1.985675587168307e-05, w1=-0.1267816794896436\n",
      "Regularized Logistic Regression(218/299): loss=0.5521083593746844, w0=-1.986148244660824e-05, w1=-0.12678937874783722\n",
      "Regularized Logistic Regression(219/299): loss=0.5521083571671374, w0=-1.986611474304461e-05, w1=-0.1267968860916371\n",
      "Regularized Logistic Regression(220/299): loss=0.5521083550721354, w0=-1.9870654644627953e-05, w1=-0.12680420632631312\n",
      "Regularized Logistic Regression(221/299): loss=0.5521083530838835, w0=-1.9875103997234698e-05, w1=-0.12681134413599973\n",
      "Regularized Logistic Regression(222/299): loss=0.5521083511968891, w0=-1.9879464609743623e-05, w1=-0.1268183040867813\n",
      "Regularized Logistic Regression(223/299): loss=0.5521083494059449, w0=-1.988373825478199e-05, w1=-0.12682509062969727\n",
      "Regularized Logistic Regression(224/299): loss=0.5521083477061139, w0=-1.9887926669456465e-05, w1=-0.12683170810366992\n",
      "Regularized Logistic Regression(225/299): loss=0.5521083460927159, w0=-1.989203155606915e-05, w1=-0.12683816073835683\n",
      "Regularized Logistic Regression(226/299): loss=0.5521083445613131, w0=-1.9896054582819015e-05, w1=-0.12684445265692884\n",
      "Regularized Logistic Regression(227/299): loss=0.5521083431076987, w0=-1.9899997384489035e-05, w1=-0.1268505878787769\n",
      "Regularized Logistic Regression(228/299): loss=0.5521083417278831, w0=-1.990386156311937e-05, w1=-0.1268565703221499\n",
      "Regularized Logistic Regression(229/299): loss=0.5521083404180842, w0=-1.990764868866681e-05, w1=-0.12686240380672142\n",
      "Regularized Logistic Regression(230/299): loss=0.5521083391747148, w0=-1.9911360299650805e-05, w1=-0.1268680920560955\n",
      "Regularized Logistic Regression(231/299): loss=0.5521083379943739, w0=-1.9914997903786388e-05, w1=-0.12687363870024274\n",
      "Regularized Logistic Regression(232/299): loss=0.5521083368738363, w0=-1.9918562978604197e-05, w1=-0.1268790472778771\n",
      "Regularized Logistic Regression(233/299): loss=0.5521083358100435, w0=-1.992205697205792e-05, w1=-0.12688432123876975\n",
      "Regularized Logistic Regression(234/299): loss=0.5521083348000948, w0=-1.992548130311938e-05, w1=-0.12688946394600514\n",
      "Regularized Logistic Regression(235/299): loss=0.552108333841239, w0=-1.992883736236154e-05, w1=-0.12689447867817671\n",
      "Regularized Logistic Regression(236/299): loss=0.5521083329308675, w0=-1.9932126512529692e-05, w1=-0.12689936863152876\n",
      "Regularized Logistic Regression(237/299): loss=0.5521083320665059, w0=-1.9935350089101024e-05, w1=-0.12690413692204197\n",
      "Regularized Logistic Regression(238/299): loss=0.5521083312458067, w0=-1.9938509400832857e-05, w1=-0.12690878658746518\n",
      "Regularized Logistic Regression(239/299): loss=0.5521083304665446, w0=-1.9941605730299738e-05, w1=-0.12691332058929586\n",
      "Regularized Logistic Regression(240/299): loss=0.5521083297266077, w0=-1.994464033441967e-05, w1=-0.1269177418147092\n",
      "Regularized Logistic Regression(241/299): loss=0.552108329023993, w0=-1.994761444496968e-05, w1=-0.12692205307843923\n",
      "Regularized Logistic Regression(242/299): loss=0.5521083283568011, w0=-1.9950529269090903e-05, w1=-0.12692625712460945\n",
      "Regularized Logistic Regression(243/299): loss=0.5521083277232289, w0=-1.9953385989783468e-05, w1=-0.12693035662851804\n",
      "Regularized Logistic Regression(244/299): loss=0.5521083271215675, w0=-1.995618576639135e-05, w1=-0.12693435419837804\n",
      "Regularized Logistic Regression(245/299): loss=0.5521083265501944, w0=-1.9958929735077413e-05, w1=-0.1269382523770115\n",
      "Regularized Logistic Regression(246/299): loss=0.5521083260075713, w0=-1.996161900928883e-05, w1=-0.12694205364350122\n",
      "Regularized Logistic Regression(247/299): loss=0.5521083254922381, w0=-1.9964254680213083e-05, w1=-0.12694576041479969\n",
      "Regularized Logistic Regression(248/299): loss=0.5521083250028098, w0=-1.9966837817224775e-05, w1=-0.12694937504729867\n",
      "Regularized Logistic Regression(249/299): loss=0.5521083245379725, w0=-1.9969369468323355e-05, w1=-0.1269528998383557\n",
      "Regularized Logistic Regression(250/299): loss=0.5521083240964795, w0=-1.9971850660562026e-05, w1=-0.12695633702778508\n",
      "Regularized Logistic Regression(251/299): loss=0.5521083236771472, w0=-1.9974282400467982e-05, w1=-0.1269596887993084\n",
      "Regularized Logistic Regression(252/299): loss=0.5521083232788536, w0=-1.9976665674454147e-05, w1=-0.126962957281969\n",
      "Regularized Logistic Regression(253/299): loss=0.5521083229005337, w0=-1.99790014492226e-05, w1=-0.12696614455150948\n",
      "Regularized Logistic Regression(254/299): loss=0.5521083225411765, w0=-1.9981290672159865e-05, w1=-0.1269692526317166\n",
      "Regularized Logistic Regression(255/299): loss=0.5521083221998228, w0=-1.998353427172419e-05, w1=-0.12697228349572906\n",
      "Regularized Logistic Regression(256/299): loss=0.5521083218755627, w0=-1.9985733157825038e-05, w1=-0.12697523906731323\n",
      "Regularized Logistic Regression(257/299): loss=0.5521083215675323, w0=-1.998788822219492e-05, w1=-0.12697812122210733\n",
      "Regularized Logistic Regression(258/299): loss=0.5521083212749122, w0=-1.9990000338753694e-05, w1=-0.12698093178883177\n",
      "Regularized Logistic Regression(259/299): loss=0.5521083209969242, w0=-1.9992070363965564e-05, w1=-0.1269836725504711\n",
      "Regularized Logistic Regression(260/299): loss=0.5521083207328302, w0=-1.9994099137188837e-05, w1=-0.12698634524542432\n",
      "Regularized Logistic Regression(261/299): loss=0.5521083204819296, w0=-1.9996087481018668e-05, w1=-0.12698895156862708\n",
      "Regularized Logistic Regression(262/299): loss=0.552108320243557, w0=-1.9998036201622856e-05, w1=-0.12699149317264374\n",
      "Regularized Logistic Regression(263/299): loss=0.5521083200170813, w0=-1.9999946089070905e-05, w1=-0.12699397166873388\n",
      "Regularized Logistic Regression(264/299): loss=0.5521083198019036, w0=-2.0001817917656442e-05, w1=-0.12699638862789014\n",
      "Regularized Logistic Regression(265/299): loss=0.5521083195974549, w0=-2.0003652446213147e-05, w1=-0.1269987455818502\n",
      "Regularized Logistic Regression(266/299): loss=0.5521083194031955, w0=-2.0005450418424305e-05, w1=-0.1270010440240839\n",
      "Regularized Logistic Regression(267/299): loss=0.5521083192186134, w0=-2.000721256312616e-05, w1=-0.12700328541075362\n",
      "Regularized Logistic Regression(268/299): loss=0.5521083190432212, w0=-2.0008939594605122e-05, w1=-0.12700547116165253\n",
      "Regularized Logistic Regression(269/299): loss=0.5521083188765582, w0=-2.0010632212889018e-05, w1=-0.1270076026611161\n",
      "Regularized Logistic Regression(270/299): loss=0.5521083187181856, w0=-2.001229110403247e-05, w1=-0.12700968125891432\n",
      "Regularized Logistic Regression(271/299): loss=0.5521083185676872, w0=-2.001391694039655e-05, w1=-0.12701170827111727\n",
      "Regularized Logistic Regression(272/299): loss=0.5521083184246682, w0=-2.0015510380922777e-05, w1=-0.12701368498094348\n",
      "Regularized Logistic Regression(273/299): loss=0.5521083182887533, w0=-2.0017072071401654e-05, w1=-0.12701561263958172\n",
      "Regularized Logistic Regression(274/299): loss=0.5521083181595864, w0=-2.0018602644735747e-05, w1=-0.1270174924669956\n",
      "Regularized Logistic Regression(275/299): loss=0.5521083180368297, w0=-2.002010272119753e-05, w1=-0.12701932565270677\n",
      "Regularized Logistic Regression(276/299): loss=0.5521083179201619, w0=-2.0021572908682002e-05, w1=-0.1270211133565578\n",
      "Regularized Logistic Regression(277/299): loss=0.5521083178092784, w0=-2.002301380295427e-05, w1=-0.12702285670945704\n",
      "Regularized Logistic Regression(278/299): loss=0.5521083177038896, w0=-2.0024425987892113e-05, w1=-0.12702455681410318\n",
      "Regularized Logistic Regression(279/299): loss=0.5521083176037211, w0=-2.0025810035723707e-05, w1=-0.1270262147456923\n",
      "Regularized Logistic Regression(280/299): loss=0.5521083175085119, w0=-2.0027166507260582e-05, w1=-0.12702783155260816\n",
      "Regularized Logistic Regression(281/299): loss=0.5521083174180138, w0=-2.002849595212585e-05, w1=-0.12702940825709186\n",
      "Regularized Logistic Regression(282/299): loss=0.552108317331992, w0=-2.0029798908977914e-05, w1=-0.1270309458558982\n",
      "Regularized Logistic Regression(283/299): loss=0.5521083172502227, w0=-2.0031075905729637e-05, w1=-0.127032445320934\n",
      "Regularized Logistic Regression(284/299): loss=0.5521083171724942, w0=-2.0032327459763143e-05, w1=-0.1270339075998787\n",
      "Regularized Logistic Regression(285/299): loss=0.5521083170986046, w0=-2.0033554078140285e-05, w1=-0.12703533361679356\n",
      "Regularized Logistic Regression(286/299): loss=0.5521083170283627, w0=-2.0034756257808917e-05, w1=-0.12703672427270976\n",
      "Regularized Logistic Regression(287/299): loss=0.552108316961587, w0=-2.0035934485805e-05, w1=-0.12703808044620754\n",
      "Regularized Logistic Regression(288/299): loss=0.5521083168981047, w0=-2.003708923945066e-05, w1=-0.12703940299397626\n",
      "Regularized Logistic Regression(289/299): loss=0.5521083168377515, w0=-2.003822098654829e-05, w1=-0.12704069275136226\n",
      "Regularized Logistic Regression(290/299): loss=0.5521083167803721, w0=-2.003933018557074e-05, w1=-0.1270419505329037\n",
      "Regularized Logistic Regression(291/299): loss=0.5521083167258188, w0=-2.0040417285847713e-05, w1=-0.1270431771328497\n",
      "Regularized Logistic Regression(292/299): loss=0.5521083166739508, w0=-2.0041482727748393e-05, w1=-0.12704437332566942\n",
      "Regularized Logistic Regression(293/299): loss=0.5521083166246348, w0=-2.0042526942860452e-05, w1=-0.12704553986654413\n",
      "Regularized Logistic Regression(294/299): loss=0.5521083165777438, w0=-2.0043550354165447e-05, w1=-0.12704667749185186\n",
      "Regularized Logistic Regression(295/299): loss=0.5521083165331582, w0=-2.004455337621069e-05, w1=-0.12704778691963597\n",
      "Regularized Logistic Regression(296/299): loss=0.5521083164907629, w0=-2.0045536415277697e-05, w1=-0.12704886885006378\n",
      "Regularized Logistic Regression(297/299): loss=0.5521083164504494, w0=-2.0046499869547265e-05, w1=-0.12704992396587386\n",
      "Regularized Logistic Regression(298/299): loss=0.552108316412115, w0=-2.0047444129261213e-05, w1=-0.12705095293280966\n",
      "Regularized Logistic Regression(299/299): loss=0.5521083163756607, w0=-2.0048369576880904e-05, w1=-0.1270519564000461\n",
      "Regularized Logistic Regression(0/99): loss=0.693147180559945, w0=-4.176586058555737e-07, w1=-0.007317796433195506\n",
      "Regularized Logistic Regression(1/99): loss=0.6662877630975907, w0=-7.492197038173245e-07, w1=-0.012504367602100622\n",
      "Regularized Logistic Regression(2/99): loss=0.653414684464226, w0=-1.0137866556326789e-06, w1=-0.016232653622325788\n",
      "Regularized Logistic Regression(3/99): loss=0.647185388860454, w0=-1.2253259380017438e-06, w1=-0.01894041802495178\n",
      "Regularized Logistic Regression(4/99): loss=0.6441451844822947, w0=-1.394573940878888e-06, w1=-0.020922507354608547\n",
      "Regularized Logistic Regression(5/99): loss=0.6426491833051524, w0=-1.5299924707836787e-06, w1=-0.022382711720015633\n",
      "Regularized Logistic Regression(6/99): loss=0.6419069325010537, w0=-1.6383246548419712e-06, w1=-0.023464410476049614\n",
      "Regularized Logistic Regression(7/99): loss=0.6415354760147872, w0=-1.7249676340760395e-06, w1=-0.02426973271825032\n",
      "Regularized Logistic Regression(8/99): loss=0.6413478670418447, w0=-1.7942473359670463e-06, w1=-0.024872084370651405\n",
      "Regularized Logistic Regression(9/99): loss=0.6412521674768492, w0=-1.8496316217130458e-06, w1=-0.02532460055113685\n",
      "Regularized Logistic Regression(10/99): loss=0.6412028232043003, w0=-1.8938995636278358e-06, w1=-0.025665972025723398\n",
      "Regularized Logistic Regression(11/99): loss=0.6411770842003885, w0=-1.929277179143988e-06, w1=-0.025924520717787485\n",
      "Regularized Logistic Regression(12/99): loss=0.6411634919865822, w0=-1.9575466172339436e-06, w1=-0.026121080091927006\n",
      "Regularized Logistic Regression(13/99): loss=0.6411562214350786, w0=-1.9801340298694776e-06, w1=-0.026271046672126586\n",
      "Regularized Logistic Regression(14/99): loss=0.641152280961687, w0=-1.9981802462836776e-06, w1=-0.02638585026447541\n",
      "Regularized Logistic Regression(15/99): loss=0.6411501170959085, w0=-2.012597561698609e-06, w1=-0.026474013173277255\n",
      "Regularized Logistic Regression(16/99): loss=0.6411489135073188, w0=-2.0241153238308208e-06, w1=-0.02654191696791863\n",
      "Regularized Logistic Regression(17/99): loss=0.641148235811227, w0=-2.03331649406327e-06, w1=-0.026594360060856696\n",
      "Regularized Logistic Regression(18/99): loss=0.641147849848947, w0=-2.040666947263192e-06, w1=-0.026634964953176646\n",
      "Regularized Logistic Regression(19/99): loss=0.641147627732346, w0=-2.046538936894963e-06, w1=-0.02666647697345266\n",
      "Regularized Logistic Regression(20/99): loss=0.6411474987063065, w0=-2.0512298769601037e-06, w1=-0.026690984368459347\n",
      "Regularized Logistic Regression(21/99): loss=0.6411474231349608, w0=-2.0549773685488416e-06, w1=-0.026710081149961068\n",
      "Regularized Logistic Regression(22/99): loss=0.6411473785532767, w0=-2.0579712173433847e-06, w1=-0.026724988102430784\n",
      "Regularized Logistic Regression(23/99): loss=0.6411473520901461, w0=-2.0603630416583427e-06, w1=-0.026736643082295793\n",
      "Regularized Logistic Regression(24/99): loss=0.6411473362988505, w0=-2.0622739521722027e-06, w1=-0.026745768682735826\n",
      "Regularized Logistic Regression(25/99): loss=0.6411473268334708, w0=-2.063800689109085e-06, w1=-0.026752923144310638\n",
      "Regularized Logistic Regression(26/99): loss=0.6411473211383827, w0=-2.065020525911606e-06, w1=-0.026758538811424964\n",
      "Regularized Logistic Regression(27/99): loss=0.6411473177008413, w0=-2.0659951868275282e-06, w1=-0.02676295129206712\n",
      "Regularized Logistic Regression(28/99): loss=0.6411473156203581, w0=-2.0667739763961004e-06, w1=-0.026766421648998496\n",
      "Regularized Logistic Regression(29/99): loss=0.6411473143583284, w0=-2.067396279192451e-06, w1=-0.026769153346330444\n",
      "Regularized Logistic Regression(30/99): loss=0.6411473135912934, w0=-2.0678935564469306e-06, w1=-0.026771305233406477\n",
      "Regularized Logistic Regression(31/99): loss=0.6411473131243337, w0=-2.0682909407478428e-06, w1=-0.026773001523206486\n",
      "Regularized Logistic Regression(32/99): loss=0.6411473128396495, w0=-2.0686085097068895e-06, w1=-0.026774339482982388\n",
      "Regularized Logistic Regression(33/99): loss=0.6411473126658757, w0=-2.06886230320883e-06, w1=-0.026775395377439837\n",
      "Regularized Logistic Regression(34/99): loss=0.6411473125596862, w0=-2.069065135869128e-06, w1=-0.026776229072814454\n",
      "Regularized Logistic Regression(35/99): loss=0.6411473124947337, w0=-2.0692272459352265e-06, w1=-0.026776887611632157\n",
      "Regularized Logistic Regression(36/99): loss=0.6411473124549696, w0=-2.0693568135663038e-06, w1=-0.026777407994015966\n",
      "Regularized Logistic Regression(37/99): loss=0.6411473124306071, w0=-2.0694603747949102e-06, w1=-0.026777819345736413\n",
      "Regularized Logistic Regression(38/99): loss=0.6411473124156697, w0=-2.0695431521766895e-06, w1=-0.026778144611125338\n",
      "Regularized Logistic Regression(39/99): loss=0.6411473124065051, w0=-2.0696093189035425e-06, w1=-0.02677840187702893\n",
      "Regularized Logistic Regression(40/99): loss=0.6411473124008787, w0=-2.069662209776679e-06, w1=-0.026778605409672872\n",
      "Regularized Logistic Regression(41/99): loss=0.6411473123974224, w0=-2.0697044897376572e-06, w1=-0.026778766467709415\n",
      "Regularized Logistic Regression(42/99): loss=0.6411473123952984, w0=-2.069738288500777e-06, w1=-0.026778893940498174\n",
      "Regularized Logistic Regression(43/99): loss=0.6411473123939917, w0=-2.0697653081095006e-06, w1=-0.02677899484968753\n",
      "Regularized Logistic Regression(44/99): loss=0.6411473123931881, w0=-2.069786908865618e-06, w1=-0.026779074743741905\n",
      "Regularized Logistic Regression(45/99): loss=0.6411473123926931, w0=-2.0698041779827614e-06, w1=-0.026779138008517313\n",
      "Regularized Logistic Regression(46/99): loss=0.6411473123923883, w0=-2.069817984439618e-06, w1=-0.026779188111922786\n",
      "Regularized Logistic Regression(47/99): loss=0.6411473123922007, w0=-2.069829022808718e-06, w1=-0.026779227796797235\n",
      "Regularized Logistic Regression(48/99): loss=0.6411473123920848, w0=-2.069837848277904e-06, w1=-0.026779259233034175\n",
      "Regularized Logistic Regression(49/99): loss=0.6411473123920134, w0=-2.069844904635364e-06, w1=-0.02677928413764382\n",
      "Regularized Logistic Regression(50/99): loss=0.6411473123919695, w0=-2.069850546632876e-06, w1=-0.0267793038695412\n",
      "Regularized Logistic Regression(51/99): loss=0.6411473123919423, w0=-2.0698550578572886e-06, w1=-0.02677931950442483\n",
      "Regularized Logistic Regression(52/99): loss=0.6411473123919255, w0=-2.0698586650129496e-06, w1=-0.02677933189393667\n",
      "Regularized Logistic Regression(53/99): loss=0.6411473123919149, w0=-2.0698615493362827e-06, w1=-0.026779341712431782\n",
      "Regularized Logistic Regression(54/99): loss=0.6411473123919087, w0=-2.069863855718678e-06, w1=-0.02677934949395424\n",
      "Regularized Logistic Regression(55/99): loss=0.6411473123919048, w0=-2.0698656999980768e-06, w1=-0.026779355661481312\n",
      "Regularized Logistic Regression(56/99): loss=0.6411473123919021, w0=-2.069867174787005e-06, w1=-0.026779360550059354\n",
      "Regularized Logistic Regression(57/99): loss=0.6411473123919007, w0=-2.069868354131034e-06, w1=-0.026779364425111314\n",
      "Regularized Logistic Regression(58/99): loss=0.6411473123918996, w0=-2.069869297232424e-06, w1=-0.026779367496921287\n",
      "Regularized Logistic Regression(59/99): loss=0.6411473123918991, w0=-2.0698700514267206e-06, w1=-0.02677936993210557\n",
      "Regularized Logistic Regression(60/99): loss=0.6411473123918987, w0=-2.0698706545621634e-06, w1=-0.02677937186269025\n",
      "Regularized Logistic Regression(61/99): loss=0.6411473123918985, w0=-2.069871136901817e-06, w1=-0.026779373393298486\n",
      "Regularized Logistic Regression(62/99): loss=0.6411473123918987, w0=-2.069871522644229e-06, w1=-0.026779374606846065\n",
      "Regularized Logistic Regression(63/99): loss=0.6411473123918986, w0=-2.069871831139063e-06, w1=-0.0267793755690481\n",
      "Regularized Logistic Regression(64/99): loss=0.6411473123918984, w0=-2.069872077859016e-06, w1=-0.026779376331990328\n",
      "Regularized Logistic Regression(65/99): loss=0.6411473123918985, w0=-2.069872275176851e-06, w1=-0.026779376936958216\n",
      "Regularized Logistic Regression(66/99): loss=0.6411473123918983, w0=-2.0698724329866263e-06, w1=-0.026779377416677342\n",
      "Regularized Logistic Regression(67/99): loss=0.6411473123918983, w0=-2.069872559200411e-06, w1=-0.026779377797091043\n",
      "Regularized Logistic Regression(68/99): loss=0.6411473123918984, w0=-2.069872660145408e-06, w1=-0.026779378098765266\n",
      "Regularized Logistic Regression(69/99): loss=0.6411473123918984, w0=-2.0698727408815124e-06, w1=-0.02677937833800505\n",
      "Regularized Logistic Regression(70/99): loss=0.6411473123918983, w0=-2.0698728054551966e-06, w1=-0.02677937852773708\n",
      "Regularized Logistic Regression(71/99): loss=0.6411473123918981, w0=-2.0698728571025418e-06, w1=-0.02677937867821046\n",
      "Regularized Logistic Regression(72/99): loss=0.6411473123918983, w0=-2.069872898411562e-06, w1=-0.026779378797551573\n",
      "Regularized Logistic Regression(73/99): loss=0.6411473123918984, w0=-2.069872931452032e-06, w1=-0.026779378892204147\n",
      "Regularized Logistic Regression(74/99): loss=0.6411473123918981, w0=-2.0698729578792667e-06, w1=-0.02677937896727729\n",
      "Regularized Logistic Regression(75/99): loss=0.6411473123918983, w0=-2.0698729790171374e-06, w1=-0.026779379026822824\n",
      "Regularized Logistic Regression(76/99): loss=0.6411473123918983, w0=-2.069872995924462e-06, w1=-0.02677937907405388\n",
      "Regularized Logistic Regression(77/99): loss=0.6411473123918984, w0=-2.06987300944806e-06, w1=-0.026779379111517395\n",
      "Regularized Logistic Regression(78/99): loss=0.6411473123918984, w0=-2.069873020265215e-06, w1=-0.02677937914123445\n",
      "Regularized Logistic Regression(79/99): loss=0.6411473123918981, w0=-2.069873028917635e-06, w1=-0.02677937916480689\n",
      "Regularized Logistic Regression(80/99): loss=0.6411473123918983, w0=-2.0698730358385897e-06, w1=-0.02677937918350615\n",
      "Regularized Logistic Regression(81/99): loss=0.6411473123918984, w0=-2.0698730413746132e-06, w1=-0.026779379198339327\n",
      "Regularized Logistic Regression(82/99): loss=0.6411473123918983, w0=-2.0698730458028642e-06, w1=-0.026779379210106563\n",
      "Regularized Logistic Regression(83/99): loss=0.6411473123918981, w0=-2.0698730493450395e-06, w1=-0.026779379219441467\n",
      "Regularized Logistic Regression(84/99): loss=0.6411473123918984, w0=-2.069873052178459e-06, w1=-0.026779379226847012\n",
      "Regularized Logistic Regression(85/99): loss=0.6411473123918984, w0=-2.069873054444958e-06, w1=-0.02677937923272175\n",
      "Regularized Logistic Regression(86/99): loss=0.6411473123918984, w0=-2.069873056257978e-06, w1=-0.02677937923738329\n",
      "Regularized Logistic Regression(87/99): loss=0.6411473123918984, w0=-2.069873057708255e-06, w1=-0.026779379241081314\n",
      "Regularized Logistic Regression(88/99): loss=0.641147312391898, w0=-2.0698730588683813e-06, w1=-0.026779379244015446\n",
      "Regularized Logistic Regression(89/99): loss=0.6411473123918985, w0=-2.0698730597964025e-06, w1=-0.026779379246342886\n",
      "Regularized Logistic Regression(90/99): loss=0.6411473123918981, w0=-2.0698730605387654e-06, w1=-0.026779379248189662\n",
      "Regularized Logistic Regression(91/99): loss=0.6411473123918981, w0=-2.0698730611326126e-06, w1=-0.02677937924965467\n",
      "Regularized Logistic Regression(92/99): loss=0.6411473123918986, w0=-2.0698730616076613e-06, w1=-0.026779379250817494\n",
      "Regularized Logistic Regression(93/99): loss=0.6411473123918985, w0=-2.0698730619876793e-06, w1=-0.02677937925173983\n",
      "Regularized Logistic Regression(94/99): loss=0.6411473123918981, w0=-2.0698730622916815e-06, w1=-0.02677937925247191\n",
      "Regularized Logistic Regression(95/99): loss=0.6411473123918985, w0=-2.0698730625348723e-06, w1=-0.02677937925305255\n",
      "Regularized Logistic Regression(96/99): loss=0.6411473123918983, w0=-2.069873062729415e-06, w1=-0.026779379253513202\n",
      "Regularized Logistic Regression(97/99): loss=0.6411473123918985, w0=-2.0698730628850408e-06, w1=-0.02677937925387892\n",
      "Regularized Logistic Regression(98/99): loss=0.6411473123918983, w0=-2.0698730630095385e-06, w1=-0.026779379254168997\n",
      "Regularized Logistic Regression(99/99): loss=0.6411473123918984, w0=-2.0698730631091327e-06, w1=-0.026779379254399143\n",
      "Regularized Logistic Regression(0/199): loss=0.693147180559945, w0=-4.176586058555737e-07, w1=-0.007317796433195506\n",
      "Regularized Logistic Regression(1/199): loss=0.6662877630975907, w0=-7.492197038173245e-07, w1=-0.012504367602100622\n",
      "Regularized Logistic Regression(2/199): loss=0.653414684464226, w0=-1.0137866556326789e-06, w1=-0.016232653622325788\n",
      "Regularized Logistic Regression(3/199): loss=0.647185388860454, w0=-1.2253259380017438e-06, w1=-0.01894041802495178\n",
      "Regularized Logistic Regression(4/199): loss=0.6441451844822947, w0=-1.394573940878888e-06, w1=-0.020922507354608547\n",
      "Regularized Logistic Regression(5/199): loss=0.6426491833051524, w0=-1.5299924707836787e-06, w1=-0.022382711720015633\n",
      "Regularized Logistic Regression(6/199): loss=0.6419069325010537, w0=-1.6383246548419712e-06, w1=-0.023464410476049614\n",
      "Regularized Logistic Regression(7/199): loss=0.6415354760147872, w0=-1.7249676340760395e-06, w1=-0.02426973271825032\n",
      "Regularized Logistic Regression(8/199): loss=0.6413478670418447, w0=-1.7942473359670463e-06, w1=-0.024872084370651405\n",
      "Regularized Logistic Regression(9/199): loss=0.6412521674768492, w0=-1.8496316217130458e-06, w1=-0.02532460055113685\n",
      "Regularized Logistic Regression(10/199): loss=0.6412028232043003, w0=-1.8938995636278358e-06, w1=-0.025665972025723398\n",
      "Regularized Logistic Regression(11/199): loss=0.6411770842003885, w0=-1.929277179143988e-06, w1=-0.025924520717787485\n",
      "Regularized Logistic Regression(12/199): loss=0.6411634919865822, w0=-1.9575466172339436e-06, w1=-0.026121080091927006\n",
      "Regularized Logistic Regression(13/199): loss=0.6411562214350786, w0=-1.9801340298694776e-06, w1=-0.026271046672126586\n",
      "Regularized Logistic Regression(14/199): loss=0.641152280961687, w0=-1.9981802462836776e-06, w1=-0.02638585026447541\n",
      "Regularized Logistic Regression(15/199): loss=0.6411501170959085, w0=-2.012597561698609e-06, w1=-0.026474013173277255\n",
      "Regularized Logistic Regression(16/199): loss=0.6411489135073188, w0=-2.0241153238308208e-06, w1=-0.02654191696791863\n",
      "Regularized Logistic Regression(17/199): loss=0.641148235811227, w0=-2.03331649406327e-06, w1=-0.026594360060856696\n",
      "Regularized Logistic Regression(18/199): loss=0.641147849848947, w0=-2.040666947263192e-06, w1=-0.026634964953176646\n",
      "Regularized Logistic Regression(19/199): loss=0.641147627732346, w0=-2.046538936894963e-06, w1=-0.02666647697345266\n",
      "Regularized Logistic Regression(20/199): loss=0.6411474987063065, w0=-2.0512298769601037e-06, w1=-0.026690984368459347\n",
      "Regularized Logistic Regression(21/199): loss=0.6411474231349608, w0=-2.0549773685488416e-06, w1=-0.026710081149961068\n",
      "Regularized Logistic Regression(22/199): loss=0.6411473785532767, w0=-2.0579712173433847e-06, w1=-0.026724988102430784\n",
      "Regularized Logistic Regression(23/199): loss=0.6411473520901461, w0=-2.0603630416583427e-06, w1=-0.026736643082295793\n",
      "Regularized Logistic Regression(24/199): loss=0.6411473362988505, w0=-2.0622739521722027e-06, w1=-0.026745768682735826\n",
      "Regularized Logistic Regression(25/199): loss=0.6411473268334708, w0=-2.063800689109085e-06, w1=-0.026752923144310638\n",
      "Regularized Logistic Regression(26/199): loss=0.6411473211383827, w0=-2.065020525911606e-06, w1=-0.026758538811424964\n",
      "Regularized Logistic Regression(27/199): loss=0.6411473177008413, w0=-2.0659951868275282e-06, w1=-0.02676295129206712\n",
      "Regularized Logistic Regression(28/199): loss=0.6411473156203581, w0=-2.0667739763961004e-06, w1=-0.026766421648998496\n",
      "Regularized Logistic Regression(29/199): loss=0.6411473143583284, w0=-2.067396279192451e-06, w1=-0.026769153346330444\n",
      "Regularized Logistic Regression(30/199): loss=0.6411473135912934, w0=-2.0678935564469306e-06, w1=-0.026771305233406477\n",
      "Regularized Logistic Regression(31/199): loss=0.6411473131243337, w0=-2.0682909407478428e-06, w1=-0.026773001523206486\n",
      "Regularized Logistic Regression(32/199): loss=0.6411473128396495, w0=-2.0686085097068895e-06, w1=-0.026774339482982388\n",
      "Regularized Logistic Regression(33/199): loss=0.6411473126658757, w0=-2.06886230320883e-06, w1=-0.026775395377439837\n",
      "Regularized Logistic Regression(34/199): loss=0.6411473125596862, w0=-2.069065135869128e-06, w1=-0.026776229072814454\n",
      "Regularized Logistic Regression(35/199): loss=0.6411473124947337, w0=-2.0692272459352265e-06, w1=-0.026776887611632157\n",
      "Regularized Logistic Regression(36/199): loss=0.6411473124549696, w0=-2.0693568135663038e-06, w1=-0.026777407994015966\n",
      "Regularized Logistic Regression(37/199): loss=0.6411473124306071, w0=-2.0694603747949102e-06, w1=-0.026777819345736413\n",
      "Regularized Logistic Regression(38/199): loss=0.6411473124156697, w0=-2.0695431521766895e-06, w1=-0.026778144611125338\n",
      "Regularized Logistic Regression(39/199): loss=0.6411473124065051, w0=-2.0696093189035425e-06, w1=-0.02677840187702893\n",
      "Regularized Logistic Regression(40/199): loss=0.6411473124008787, w0=-2.069662209776679e-06, w1=-0.026778605409672872\n",
      "Regularized Logistic Regression(41/199): loss=0.6411473123974224, w0=-2.0697044897376572e-06, w1=-0.026778766467709415\n",
      "Regularized Logistic Regression(42/199): loss=0.6411473123952984, w0=-2.069738288500777e-06, w1=-0.026778893940498174\n",
      "Regularized Logistic Regression(43/199): loss=0.6411473123939917, w0=-2.0697653081095006e-06, w1=-0.02677899484968753\n",
      "Regularized Logistic Regression(44/199): loss=0.6411473123931881, w0=-2.069786908865618e-06, w1=-0.026779074743741905\n",
      "Regularized Logistic Regression(45/199): loss=0.6411473123926931, w0=-2.0698041779827614e-06, w1=-0.026779138008517313\n",
      "Regularized Logistic Regression(46/199): loss=0.6411473123923883, w0=-2.069817984439618e-06, w1=-0.026779188111922786\n",
      "Regularized Logistic Regression(47/199): loss=0.6411473123922007, w0=-2.069829022808718e-06, w1=-0.026779227796797235\n",
      "Regularized Logistic Regression(48/199): loss=0.6411473123920848, w0=-2.069837848277904e-06, w1=-0.026779259233034175\n",
      "Regularized Logistic Regression(49/199): loss=0.6411473123920134, w0=-2.069844904635364e-06, w1=-0.02677928413764382\n",
      "Regularized Logistic Regression(50/199): loss=0.6411473123919695, w0=-2.069850546632876e-06, w1=-0.0267793038695412\n",
      "Regularized Logistic Regression(51/199): loss=0.6411473123919423, w0=-2.0698550578572886e-06, w1=-0.02677931950442483\n",
      "Regularized Logistic Regression(52/199): loss=0.6411473123919255, w0=-2.0698586650129496e-06, w1=-0.02677933189393667\n",
      "Regularized Logistic Regression(53/199): loss=0.6411473123919149, w0=-2.0698615493362827e-06, w1=-0.026779341712431782\n",
      "Regularized Logistic Regression(54/199): loss=0.6411473123919087, w0=-2.069863855718678e-06, w1=-0.02677934949395424\n",
      "Regularized Logistic Regression(55/199): loss=0.6411473123919048, w0=-2.0698656999980768e-06, w1=-0.026779355661481312\n",
      "Regularized Logistic Regression(56/199): loss=0.6411473123919021, w0=-2.069867174787005e-06, w1=-0.026779360550059354\n",
      "Regularized Logistic Regression(57/199): loss=0.6411473123919007, w0=-2.069868354131034e-06, w1=-0.026779364425111314\n",
      "Regularized Logistic Regression(58/199): loss=0.6411473123918996, w0=-2.069869297232424e-06, w1=-0.026779367496921287\n",
      "Regularized Logistic Regression(59/199): loss=0.6411473123918991, w0=-2.0698700514267206e-06, w1=-0.02677936993210557\n",
      "Regularized Logistic Regression(60/199): loss=0.6411473123918987, w0=-2.0698706545621634e-06, w1=-0.02677937186269025\n",
      "Regularized Logistic Regression(61/199): loss=0.6411473123918985, w0=-2.069871136901817e-06, w1=-0.026779373393298486\n",
      "Regularized Logistic Regression(62/199): loss=0.6411473123918987, w0=-2.069871522644229e-06, w1=-0.026779374606846065\n",
      "Regularized Logistic Regression(63/199): loss=0.6411473123918986, w0=-2.069871831139063e-06, w1=-0.0267793755690481\n",
      "Regularized Logistic Regression(64/199): loss=0.6411473123918984, w0=-2.069872077859016e-06, w1=-0.026779376331990328\n",
      "Regularized Logistic Regression(65/199): loss=0.6411473123918985, w0=-2.069872275176851e-06, w1=-0.026779376936958216\n",
      "Regularized Logistic Regression(66/199): loss=0.6411473123918983, w0=-2.0698724329866263e-06, w1=-0.026779377416677342\n",
      "Regularized Logistic Regression(67/199): loss=0.6411473123918983, w0=-2.069872559200411e-06, w1=-0.026779377797091043\n",
      "Regularized Logistic Regression(68/199): loss=0.6411473123918984, w0=-2.069872660145408e-06, w1=-0.026779378098765266\n",
      "Regularized Logistic Regression(69/199): loss=0.6411473123918984, w0=-2.0698727408815124e-06, w1=-0.02677937833800505\n",
      "Regularized Logistic Regression(70/199): loss=0.6411473123918983, w0=-2.0698728054551966e-06, w1=-0.02677937852773708\n",
      "Regularized Logistic Regression(71/199): loss=0.6411473123918981, w0=-2.0698728571025418e-06, w1=-0.02677937867821046\n",
      "Regularized Logistic Regression(72/199): loss=0.6411473123918983, w0=-2.069872898411562e-06, w1=-0.026779378797551573\n",
      "Regularized Logistic Regression(73/199): loss=0.6411473123918984, w0=-2.069872931452032e-06, w1=-0.026779378892204147\n",
      "Regularized Logistic Regression(74/199): loss=0.6411473123918981, w0=-2.0698729578792667e-06, w1=-0.02677937896727729\n",
      "Regularized Logistic Regression(75/199): loss=0.6411473123918983, w0=-2.0698729790171374e-06, w1=-0.026779379026822824\n",
      "Regularized Logistic Regression(76/199): loss=0.6411473123918983, w0=-2.069872995924462e-06, w1=-0.02677937907405388\n",
      "Regularized Logistic Regression(77/199): loss=0.6411473123918984, w0=-2.06987300944806e-06, w1=-0.026779379111517395\n",
      "Regularized Logistic Regression(78/199): loss=0.6411473123918984, w0=-2.069873020265215e-06, w1=-0.02677937914123445\n",
      "Regularized Logistic Regression(79/199): loss=0.6411473123918981, w0=-2.069873028917635e-06, w1=-0.02677937916480689\n",
      "Regularized Logistic Regression(80/199): loss=0.6411473123918983, w0=-2.0698730358385897e-06, w1=-0.02677937918350615\n",
      "Regularized Logistic Regression(81/199): loss=0.6411473123918984, w0=-2.0698730413746132e-06, w1=-0.026779379198339327\n",
      "Regularized Logistic Regression(82/199): loss=0.6411473123918983, w0=-2.0698730458028642e-06, w1=-0.026779379210106563\n",
      "Regularized Logistic Regression(83/199): loss=0.6411473123918981, w0=-2.0698730493450395e-06, w1=-0.026779379219441467\n",
      "Regularized Logistic Regression(84/199): loss=0.6411473123918984, w0=-2.069873052178459e-06, w1=-0.026779379226847012\n",
      "Regularized Logistic Regression(85/199): loss=0.6411473123918984, w0=-2.069873054444958e-06, w1=-0.02677937923272175\n",
      "Regularized Logistic Regression(86/199): loss=0.6411473123918984, w0=-2.069873056257978e-06, w1=-0.02677937923738329\n",
      "Regularized Logistic Regression(87/199): loss=0.6411473123918984, w0=-2.069873057708255e-06, w1=-0.026779379241081314\n",
      "Regularized Logistic Regression(88/199): loss=0.641147312391898, w0=-2.0698730588683813e-06, w1=-0.026779379244015446\n",
      "Regularized Logistic Regression(89/199): loss=0.6411473123918985, w0=-2.0698730597964025e-06, w1=-0.026779379246342886\n",
      "Regularized Logistic Regression(90/199): loss=0.6411473123918981, w0=-2.0698730605387654e-06, w1=-0.026779379248189662\n",
      "Regularized Logistic Regression(91/199): loss=0.6411473123918981, w0=-2.0698730611326126e-06, w1=-0.02677937924965467\n",
      "Regularized Logistic Regression(92/199): loss=0.6411473123918986, w0=-2.0698730616076613e-06, w1=-0.026779379250817494\n",
      "Regularized Logistic Regression(93/199): loss=0.6411473123918985, w0=-2.0698730619876793e-06, w1=-0.02677937925173983\n",
      "Regularized Logistic Regression(94/199): loss=0.6411473123918981, w0=-2.0698730622916815e-06, w1=-0.02677937925247191\n",
      "Regularized Logistic Regression(95/199): loss=0.6411473123918985, w0=-2.0698730625348723e-06, w1=-0.02677937925305255\n",
      "Regularized Logistic Regression(96/199): loss=0.6411473123918983, w0=-2.069873062729415e-06, w1=-0.026779379253513202\n",
      "Regularized Logistic Regression(97/199): loss=0.6411473123918985, w0=-2.0698730628850408e-06, w1=-0.02677937925387892\n",
      "Regularized Logistic Regression(98/199): loss=0.6411473123918983, w0=-2.0698730630095385e-06, w1=-0.026779379254168997\n",
      "Regularized Logistic Regression(99/199): loss=0.6411473123918984, w0=-2.0698730631091327e-06, w1=-0.026779379254399143\n",
      "Regularized Logistic Regression(100/199): loss=0.6411473123918985, w0=-2.069873063188804e-06, w1=-0.026779379254581893\n",
      "Regularized Logistic Regression(101/199): loss=0.6411473123918983, w0=-2.0698730632525404e-06, w1=-0.02677937925472687\n",
      "Regularized Logistic Regression(102/199): loss=0.6411473123918984, w0=-2.0698730633035284e-06, w1=-0.02677937925484203\n",
      "Regularized Logistic Regression(103/199): loss=0.6411473123918981, w0=-2.0698730633443177e-06, w1=-0.026779379254933393\n",
      "Regularized Logistic Regression(104/199): loss=0.6411473123918981, w0=-2.0698730633769497e-06, w1=-0.026779379255005956\n",
      "Regularized Logistic Regression(105/199): loss=0.6411473123918983, w0=-2.0698730634030552e-06, w1=-0.026779379255063483\n",
      "Regularized Logistic Regression(106/199): loss=0.6411473123918984, w0=-2.06987306342394e-06, w1=-0.02677937925510922\n",
      "Regularized Logistic Regression(107/199): loss=0.641147312391898, w0=-2.069873063440648e-06, w1=-0.0267793792551456\n",
      "Regularized Logistic Regression(108/199): loss=0.6411473123918983, w0=-2.069873063454013e-06, w1=-0.0267793792551744\n",
      "Regularized Logistic Regression(109/199): loss=0.6411473123918984, w0=-2.0698730634647057e-06, w1=-0.026779379255197307\n",
      "Regularized Logistic Regression(110/199): loss=0.6411473123918983, w0=-2.0698730634732603e-06, w1=-0.02677937925521554\n",
      "Regularized Logistic Regression(111/199): loss=0.6411473123918984, w0=-2.0698730634801047e-06, w1=-0.02677937925523008\n",
      "Regularized Logistic Regression(112/199): loss=0.6411473123918983, w0=-2.0698730634855808e-06, w1=-0.026779379255241653\n",
      "Regularized Logistic Regression(113/199): loss=0.6411473123918983, w0=-2.0698730634899616e-06, w1=-0.02677937925525087\n",
      "Regularized Logistic Regression(114/199): loss=0.6411473123918984, w0=-2.0698730634934667e-06, w1=-0.02677937925525822\n",
      "Regularized Logistic Regression(115/199): loss=0.6411473123918984, w0=-2.0698730634962708e-06, w1=-0.02677937925526407\n",
      "Regularized Logistic Regression(116/199): loss=0.6411473123918983, w0=-2.069873063498514e-06, w1=-0.026779379255268704\n",
      "Regularized Logistic Regression(117/199): loss=0.6411473123918984, w0=-2.069873063500309e-06, w1=-0.026779379255272136\n",
      "Regularized Logistic Regression(118/199): loss=0.6411473123918981, w0=-2.0698730635017447e-06, w1=-0.02677937925527483\n",
      "Regularized Logistic Regression(119/199): loss=0.6411473123918983, w0=-2.0698730635028937e-06, w1=-0.026779379255276962\n",
      "Regularized Logistic Regression(120/199): loss=0.6411473123918985, w0=-2.0698730635038127e-06, w1=-0.026779379255278655\n",
      "Regularized Logistic Regression(121/199): loss=0.6411473123918985, w0=-2.069873063504548e-06, w1=-0.02677937925527999\n",
      "Regularized Logistic Regression(122/199): loss=0.6411473123918984, w0=-2.0698730635051362e-06, w1=-0.026779379255281045\n",
      "Regularized Logistic Regression(123/199): loss=0.6411473123918984, w0=-2.0698730635056068e-06, w1=-0.02677937925528189\n",
      "Regularized Logistic Regression(124/199): loss=0.6411473123918983, w0=-2.0698730635059833e-06, w1=-0.026779379255282558\n",
      "Regularized Logistic Regression(125/199): loss=0.6411473123918983, w0=-2.0698730635062844e-06, w1=-0.026779379255283092\n",
      "Regularized Logistic Regression(126/199): loss=0.6411473123918984, w0=-2.0698730635065254e-06, w1=-0.026779379255283495\n",
      "Regularized Logistic Regression(127/199): loss=0.6411473123918984, w0=-2.069873063506718e-06, w1=-0.026779379255283838\n",
      "Regularized Logistic Regression(128/199): loss=0.6411473123918984, w0=-2.0698730635068722e-06, w1=-0.02677937925528409\n",
      "Regularized Logistic Regression(129/199): loss=0.6411473123918983, w0=-2.0698730635069955e-06, w1=-0.026779379255284314\n",
      "Regularized Logistic Regression(130/199): loss=0.6411473123918984, w0=-2.069873063507094e-06, w1=-0.026779379255284442\n",
      "Regularized Logistic Regression(131/199): loss=0.6411473123918984, w0=-2.0698730635071734e-06, w1=-0.026779379255284595\n",
      "Regularized Logistic Regression(132/199): loss=0.6411473123918984, w0=-2.0698730635072365e-06, w1=-0.026779379255284664\n",
      "Regularized Logistic Regression(133/199): loss=0.6411473123918984, w0=-2.069873063507287e-06, w1=-0.026779379255284768\n",
      "Regularized Logistic Regression(134/199): loss=0.6411473123918984, w0=-2.069873063507327e-06, w1=-0.026779379255284803\n",
      "Regularized Logistic Regression(135/199): loss=0.6411473123918984, w0=-2.0698730635073597e-06, w1=-0.026779379255284876\n",
      "Regularized Logistic Regression(136/199): loss=0.6411473123918984, w0=-2.0698730635073855e-06, w1=-0.026779379255284886\n",
      "Regularized Logistic Regression(137/199): loss=0.6411473123918984, w0=-2.0698730635074063e-06, w1=-0.02677937925528494\n",
      "Regularized Logistic Regression(138/199): loss=0.6411473123918984, w0=-2.069873063507423e-06, w1=-0.02677937925528494\n",
      "Regularized Logistic Regression(139/199): loss=0.6411473123918984, w0=-2.069873063507436e-06, w1=-0.026779379255284987\n",
      "Regularized Logistic Regression(140/199): loss=0.6411473123918984, w0=-2.0698730635074465e-06, w1=-0.026779379255284976\n",
      "Regularized Logistic Regression(141/199): loss=0.6411473123918984, w0=-2.069873063507455e-06, w1=-0.026779379255285014\n",
      "Regularized Logistic Regression(142/199): loss=0.6411473123918984, w0=-2.0698730635074618e-06, w1=-0.026779379255284997\n",
      "Regularized Logistic Regression(143/199): loss=0.6411473123918984, w0=-2.0698730635074673e-06, w1=-0.02677937925528503\n",
      "Regularized Logistic Regression(144/199): loss=0.6411473123918984, w0=-2.0698730635074715e-06, w1=-0.02677937925528501\n",
      "Regularized Logistic Regression(145/199): loss=0.6411473123918984, w0=-2.069873063507475e-06, w1=-0.026779379255285042\n",
      "Regularized Logistic Regression(146/199): loss=0.6411473123918984, w0=-2.0698730635074774e-06, w1=-0.02677937925528502\n",
      "Regularized Logistic Regression(147/199): loss=0.6411473123918984, w0=-2.06987306350748e-06, w1=-0.02677937925528505\n",
      "Regularized Logistic Regression(148/199): loss=0.6411473123918984, w0=-2.0698730635074817e-06, w1=-0.026779379255285025\n",
      "Regularized Logistic Regression(149/199): loss=0.6411473123918984, w0=-2.0698730635074834e-06, w1=-0.026779379255285053\n",
      "Regularized Logistic Regression(150/199): loss=0.6411473123918984, w0=-2.069873063507484e-06, w1=-0.026779379255285028\n",
      "Regularized Logistic Regression(151/199): loss=0.6411473123918984, w0=-2.069873063507485e-06, w1=-0.026779379255285056\n",
      "Regularized Logistic Regression(152/199): loss=0.6411473123918984, w0=-2.069873063507486e-06, w1=-0.02677937925528503\n",
      "Regularized Logistic Regression(153/199): loss=0.6411473123918984, w0=-2.0698730635074868e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(154/199): loss=0.6411473123918984, w0=-2.069873063507487e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(155/199): loss=0.6411473123918984, w0=-2.0698730635074876e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(156/199): loss=0.6411473123918984, w0=-2.0698730635074876e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(157/199): loss=0.6411473123918984, w0=-2.069873063507488e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(158/199): loss=0.6411473123918984, w0=-2.069873063507488e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(159/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(160/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(161/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(162/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(163/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(164/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(165/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(166/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(167/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(168/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(169/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(170/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(171/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(172/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(173/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(174/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(175/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(176/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(177/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(178/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(179/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(180/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(181/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(182/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(183/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(184/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(185/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(186/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(187/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(188/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(189/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(190/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(191/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(192/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(193/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(194/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(195/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(196/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(197/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(198/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(199/199): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(0/299): loss=0.693147180559945, w0=-4.176586058555737e-07, w1=-0.007317796433195506\n",
      "Regularized Logistic Regression(1/299): loss=0.6662877630975907, w0=-7.492197038173245e-07, w1=-0.012504367602100622\n",
      "Regularized Logistic Regression(2/299): loss=0.653414684464226, w0=-1.0137866556326789e-06, w1=-0.016232653622325788\n",
      "Regularized Logistic Regression(3/299): loss=0.647185388860454, w0=-1.2253259380017438e-06, w1=-0.01894041802495178\n",
      "Regularized Logistic Regression(4/299): loss=0.6441451844822947, w0=-1.394573940878888e-06, w1=-0.020922507354608547\n",
      "Regularized Logistic Regression(5/299): loss=0.6426491833051524, w0=-1.5299924707836787e-06, w1=-0.022382711720015633\n",
      "Regularized Logistic Regression(6/299): loss=0.6419069325010537, w0=-1.6383246548419712e-06, w1=-0.023464410476049614\n",
      "Regularized Logistic Regression(7/299): loss=0.6415354760147872, w0=-1.7249676340760395e-06, w1=-0.02426973271825032\n",
      "Regularized Logistic Regression(8/299): loss=0.6413478670418447, w0=-1.7942473359670463e-06, w1=-0.024872084370651405\n",
      "Regularized Logistic Regression(9/299): loss=0.6412521674768492, w0=-1.8496316217130458e-06, w1=-0.02532460055113685\n",
      "Regularized Logistic Regression(10/299): loss=0.6412028232043003, w0=-1.8938995636278358e-06, w1=-0.025665972025723398\n",
      "Regularized Logistic Regression(11/299): loss=0.6411770842003885, w0=-1.929277179143988e-06, w1=-0.025924520717787485\n",
      "Regularized Logistic Regression(12/299): loss=0.6411634919865822, w0=-1.9575466172339436e-06, w1=-0.026121080091927006\n",
      "Regularized Logistic Regression(13/299): loss=0.6411562214350786, w0=-1.9801340298694776e-06, w1=-0.026271046672126586\n",
      "Regularized Logistic Regression(14/299): loss=0.641152280961687, w0=-1.9981802462836776e-06, w1=-0.02638585026447541\n",
      "Regularized Logistic Regression(15/299): loss=0.6411501170959085, w0=-2.012597561698609e-06, w1=-0.026474013173277255\n",
      "Regularized Logistic Regression(16/299): loss=0.6411489135073188, w0=-2.0241153238308208e-06, w1=-0.02654191696791863\n",
      "Regularized Logistic Regression(17/299): loss=0.641148235811227, w0=-2.03331649406327e-06, w1=-0.026594360060856696\n",
      "Regularized Logistic Regression(18/299): loss=0.641147849848947, w0=-2.040666947263192e-06, w1=-0.026634964953176646\n",
      "Regularized Logistic Regression(19/299): loss=0.641147627732346, w0=-2.046538936894963e-06, w1=-0.02666647697345266\n",
      "Regularized Logistic Regression(20/299): loss=0.6411474987063065, w0=-2.0512298769601037e-06, w1=-0.026690984368459347\n",
      "Regularized Logistic Regression(21/299): loss=0.6411474231349608, w0=-2.0549773685488416e-06, w1=-0.026710081149961068\n",
      "Regularized Logistic Regression(22/299): loss=0.6411473785532767, w0=-2.0579712173433847e-06, w1=-0.026724988102430784\n",
      "Regularized Logistic Regression(23/299): loss=0.6411473520901461, w0=-2.0603630416583427e-06, w1=-0.026736643082295793\n",
      "Regularized Logistic Regression(24/299): loss=0.6411473362988505, w0=-2.0622739521722027e-06, w1=-0.026745768682735826\n",
      "Regularized Logistic Regression(25/299): loss=0.6411473268334708, w0=-2.063800689109085e-06, w1=-0.026752923144310638\n",
      "Regularized Logistic Regression(26/299): loss=0.6411473211383827, w0=-2.065020525911606e-06, w1=-0.026758538811424964\n",
      "Regularized Logistic Regression(27/299): loss=0.6411473177008413, w0=-2.0659951868275282e-06, w1=-0.02676295129206712\n",
      "Regularized Logistic Regression(28/299): loss=0.6411473156203581, w0=-2.0667739763961004e-06, w1=-0.026766421648998496\n",
      "Regularized Logistic Regression(29/299): loss=0.6411473143583284, w0=-2.067396279192451e-06, w1=-0.026769153346330444\n",
      "Regularized Logistic Regression(30/299): loss=0.6411473135912934, w0=-2.0678935564469306e-06, w1=-0.026771305233406477\n",
      "Regularized Logistic Regression(31/299): loss=0.6411473131243337, w0=-2.0682909407478428e-06, w1=-0.026773001523206486\n",
      "Regularized Logistic Regression(32/299): loss=0.6411473128396495, w0=-2.0686085097068895e-06, w1=-0.026774339482982388\n",
      "Regularized Logistic Regression(33/299): loss=0.6411473126658757, w0=-2.06886230320883e-06, w1=-0.026775395377439837\n",
      "Regularized Logistic Regression(34/299): loss=0.6411473125596862, w0=-2.069065135869128e-06, w1=-0.026776229072814454\n",
      "Regularized Logistic Regression(35/299): loss=0.6411473124947337, w0=-2.0692272459352265e-06, w1=-0.026776887611632157\n",
      "Regularized Logistic Regression(36/299): loss=0.6411473124549696, w0=-2.0693568135663038e-06, w1=-0.026777407994015966\n",
      "Regularized Logistic Regression(37/299): loss=0.6411473124306071, w0=-2.0694603747949102e-06, w1=-0.026777819345736413\n",
      "Regularized Logistic Regression(38/299): loss=0.6411473124156697, w0=-2.0695431521766895e-06, w1=-0.026778144611125338\n",
      "Regularized Logistic Regression(39/299): loss=0.6411473124065051, w0=-2.0696093189035425e-06, w1=-0.02677840187702893\n",
      "Regularized Logistic Regression(40/299): loss=0.6411473124008787, w0=-2.069662209776679e-06, w1=-0.026778605409672872\n",
      "Regularized Logistic Regression(41/299): loss=0.6411473123974224, w0=-2.0697044897376572e-06, w1=-0.026778766467709415\n",
      "Regularized Logistic Regression(42/299): loss=0.6411473123952984, w0=-2.069738288500777e-06, w1=-0.026778893940498174\n",
      "Regularized Logistic Regression(43/299): loss=0.6411473123939917, w0=-2.0697653081095006e-06, w1=-0.02677899484968753\n",
      "Regularized Logistic Regression(44/299): loss=0.6411473123931881, w0=-2.069786908865618e-06, w1=-0.026779074743741905\n",
      "Regularized Logistic Regression(45/299): loss=0.6411473123926931, w0=-2.0698041779827614e-06, w1=-0.026779138008517313\n",
      "Regularized Logistic Regression(46/299): loss=0.6411473123923883, w0=-2.069817984439618e-06, w1=-0.026779188111922786\n",
      "Regularized Logistic Regression(47/299): loss=0.6411473123922007, w0=-2.069829022808718e-06, w1=-0.026779227796797235\n",
      "Regularized Logistic Regression(48/299): loss=0.6411473123920848, w0=-2.069837848277904e-06, w1=-0.026779259233034175\n",
      "Regularized Logistic Regression(49/299): loss=0.6411473123920134, w0=-2.069844904635364e-06, w1=-0.02677928413764382\n",
      "Regularized Logistic Regression(50/299): loss=0.6411473123919695, w0=-2.069850546632876e-06, w1=-0.0267793038695412\n",
      "Regularized Logistic Regression(51/299): loss=0.6411473123919423, w0=-2.0698550578572886e-06, w1=-0.02677931950442483\n",
      "Regularized Logistic Regression(52/299): loss=0.6411473123919255, w0=-2.0698586650129496e-06, w1=-0.02677933189393667\n",
      "Regularized Logistic Regression(53/299): loss=0.6411473123919149, w0=-2.0698615493362827e-06, w1=-0.026779341712431782\n",
      "Regularized Logistic Regression(54/299): loss=0.6411473123919087, w0=-2.069863855718678e-06, w1=-0.02677934949395424\n",
      "Regularized Logistic Regression(55/299): loss=0.6411473123919048, w0=-2.0698656999980768e-06, w1=-0.026779355661481312\n",
      "Regularized Logistic Regression(56/299): loss=0.6411473123919021, w0=-2.069867174787005e-06, w1=-0.026779360550059354\n",
      "Regularized Logistic Regression(57/299): loss=0.6411473123919007, w0=-2.069868354131034e-06, w1=-0.026779364425111314\n",
      "Regularized Logistic Regression(58/299): loss=0.6411473123918996, w0=-2.069869297232424e-06, w1=-0.026779367496921287\n",
      "Regularized Logistic Regression(59/299): loss=0.6411473123918991, w0=-2.0698700514267206e-06, w1=-0.02677936993210557\n",
      "Regularized Logistic Regression(60/299): loss=0.6411473123918987, w0=-2.0698706545621634e-06, w1=-0.02677937186269025\n",
      "Regularized Logistic Regression(61/299): loss=0.6411473123918985, w0=-2.069871136901817e-06, w1=-0.026779373393298486\n",
      "Regularized Logistic Regression(62/299): loss=0.6411473123918987, w0=-2.069871522644229e-06, w1=-0.026779374606846065\n",
      "Regularized Logistic Regression(63/299): loss=0.6411473123918986, w0=-2.069871831139063e-06, w1=-0.0267793755690481\n",
      "Regularized Logistic Regression(64/299): loss=0.6411473123918984, w0=-2.069872077859016e-06, w1=-0.026779376331990328\n",
      "Regularized Logistic Regression(65/299): loss=0.6411473123918985, w0=-2.069872275176851e-06, w1=-0.026779376936958216\n",
      "Regularized Logistic Regression(66/299): loss=0.6411473123918983, w0=-2.0698724329866263e-06, w1=-0.026779377416677342\n",
      "Regularized Logistic Regression(67/299): loss=0.6411473123918983, w0=-2.069872559200411e-06, w1=-0.026779377797091043\n",
      "Regularized Logistic Regression(68/299): loss=0.6411473123918984, w0=-2.069872660145408e-06, w1=-0.026779378098765266\n",
      "Regularized Logistic Regression(69/299): loss=0.6411473123918984, w0=-2.0698727408815124e-06, w1=-0.02677937833800505\n",
      "Regularized Logistic Regression(70/299): loss=0.6411473123918983, w0=-2.0698728054551966e-06, w1=-0.02677937852773708\n",
      "Regularized Logistic Regression(71/299): loss=0.6411473123918981, w0=-2.0698728571025418e-06, w1=-0.02677937867821046\n",
      "Regularized Logistic Regression(72/299): loss=0.6411473123918983, w0=-2.069872898411562e-06, w1=-0.026779378797551573\n",
      "Regularized Logistic Regression(73/299): loss=0.6411473123918984, w0=-2.069872931452032e-06, w1=-0.026779378892204147\n",
      "Regularized Logistic Regression(74/299): loss=0.6411473123918981, w0=-2.0698729578792667e-06, w1=-0.02677937896727729\n",
      "Regularized Logistic Regression(75/299): loss=0.6411473123918983, w0=-2.0698729790171374e-06, w1=-0.026779379026822824\n",
      "Regularized Logistic Regression(76/299): loss=0.6411473123918983, w0=-2.069872995924462e-06, w1=-0.02677937907405388\n",
      "Regularized Logistic Regression(77/299): loss=0.6411473123918984, w0=-2.06987300944806e-06, w1=-0.026779379111517395\n",
      "Regularized Logistic Regression(78/299): loss=0.6411473123918984, w0=-2.069873020265215e-06, w1=-0.02677937914123445\n",
      "Regularized Logistic Regression(79/299): loss=0.6411473123918981, w0=-2.069873028917635e-06, w1=-0.02677937916480689\n",
      "Regularized Logistic Regression(80/299): loss=0.6411473123918983, w0=-2.0698730358385897e-06, w1=-0.02677937918350615\n",
      "Regularized Logistic Regression(81/299): loss=0.6411473123918984, w0=-2.0698730413746132e-06, w1=-0.026779379198339327\n",
      "Regularized Logistic Regression(82/299): loss=0.6411473123918983, w0=-2.0698730458028642e-06, w1=-0.026779379210106563\n",
      "Regularized Logistic Regression(83/299): loss=0.6411473123918981, w0=-2.0698730493450395e-06, w1=-0.026779379219441467\n",
      "Regularized Logistic Regression(84/299): loss=0.6411473123918984, w0=-2.069873052178459e-06, w1=-0.026779379226847012\n",
      "Regularized Logistic Regression(85/299): loss=0.6411473123918984, w0=-2.069873054444958e-06, w1=-0.02677937923272175\n",
      "Regularized Logistic Regression(86/299): loss=0.6411473123918984, w0=-2.069873056257978e-06, w1=-0.02677937923738329\n",
      "Regularized Logistic Regression(87/299): loss=0.6411473123918984, w0=-2.069873057708255e-06, w1=-0.026779379241081314\n",
      "Regularized Logistic Regression(88/299): loss=0.641147312391898, w0=-2.0698730588683813e-06, w1=-0.026779379244015446\n",
      "Regularized Logistic Regression(89/299): loss=0.6411473123918985, w0=-2.0698730597964025e-06, w1=-0.026779379246342886\n",
      "Regularized Logistic Regression(90/299): loss=0.6411473123918981, w0=-2.0698730605387654e-06, w1=-0.026779379248189662\n",
      "Regularized Logistic Regression(91/299): loss=0.6411473123918981, w0=-2.0698730611326126e-06, w1=-0.02677937924965467\n",
      "Regularized Logistic Regression(92/299): loss=0.6411473123918986, w0=-2.0698730616076613e-06, w1=-0.026779379250817494\n",
      "Regularized Logistic Regression(93/299): loss=0.6411473123918985, w0=-2.0698730619876793e-06, w1=-0.02677937925173983\n",
      "Regularized Logistic Regression(94/299): loss=0.6411473123918981, w0=-2.0698730622916815e-06, w1=-0.02677937925247191\n",
      "Regularized Logistic Regression(95/299): loss=0.6411473123918985, w0=-2.0698730625348723e-06, w1=-0.02677937925305255\n",
      "Regularized Logistic Regression(96/299): loss=0.6411473123918983, w0=-2.069873062729415e-06, w1=-0.026779379253513202\n",
      "Regularized Logistic Regression(97/299): loss=0.6411473123918985, w0=-2.0698730628850408e-06, w1=-0.02677937925387892\n",
      "Regularized Logistic Regression(98/299): loss=0.6411473123918983, w0=-2.0698730630095385e-06, w1=-0.026779379254168997\n",
      "Regularized Logistic Regression(99/299): loss=0.6411473123918984, w0=-2.0698730631091327e-06, w1=-0.026779379254399143\n",
      "Regularized Logistic Regression(100/299): loss=0.6411473123918985, w0=-2.069873063188804e-06, w1=-0.026779379254581893\n",
      "Regularized Logistic Regression(101/299): loss=0.6411473123918983, w0=-2.0698730632525404e-06, w1=-0.02677937925472687\n",
      "Regularized Logistic Regression(102/299): loss=0.6411473123918984, w0=-2.0698730633035284e-06, w1=-0.02677937925484203\n",
      "Regularized Logistic Regression(103/299): loss=0.6411473123918981, w0=-2.0698730633443177e-06, w1=-0.026779379254933393\n",
      "Regularized Logistic Regression(104/299): loss=0.6411473123918981, w0=-2.0698730633769497e-06, w1=-0.026779379255005956\n",
      "Regularized Logistic Regression(105/299): loss=0.6411473123918983, w0=-2.0698730634030552e-06, w1=-0.026779379255063483\n",
      "Regularized Logistic Regression(106/299): loss=0.6411473123918984, w0=-2.06987306342394e-06, w1=-0.02677937925510922\n",
      "Regularized Logistic Regression(107/299): loss=0.641147312391898, w0=-2.069873063440648e-06, w1=-0.0267793792551456\n",
      "Regularized Logistic Regression(108/299): loss=0.6411473123918983, w0=-2.069873063454013e-06, w1=-0.0267793792551744\n",
      "Regularized Logistic Regression(109/299): loss=0.6411473123918984, w0=-2.0698730634647057e-06, w1=-0.026779379255197307\n",
      "Regularized Logistic Regression(110/299): loss=0.6411473123918983, w0=-2.0698730634732603e-06, w1=-0.02677937925521554\n",
      "Regularized Logistic Regression(111/299): loss=0.6411473123918984, w0=-2.0698730634801047e-06, w1=-0.02677937925523008\n",
      "Regularized Logistic Regression(112/299): loss=0.6411473123918983, w0=-2.0698730634855808e-06, w1=-0.026779379255241653\n",
      "Regularized Logistic Regression(113/299): loss=0.6411473123918983, w0=-2.0698730634899616e-06, w1=-0.02677937925525087\n",
      "Regularized Logistic Regression(114/299): loss=0.6411473123918984, w0=-2.0698730634934667e-06, w1=-0.02677937925525822\n",
      "Regularized Logistic Regression(115/299): loss=0.6411473123918984, w0=-2.0698730634962708e-06, w1=-0.02677937925526407\n",
      "Regularized Logistic Regression(116/299): loss=0.6411473123918983, w0=-2.069873063498514e-06, w1=-0.026779379255268704\n",
      "Regularized Logistic Regression(117/299): loss=0.6411473123918984, w0=-2.069873063500309e-06, w1=-0.026779379255272136\n",
      "Regularized Logistic Regression(118/299): loss=0.6411473123918981, w0=-2.0698730635017447e-06, w1=-0.02677937925527483\n",
      "Regularized Logistic Regression(119/299): loss=0.6411473123918983, w0=-2.0698730635028937e-06, w1=-0.026779379255276962\n",
      "Regularized Logistic Regression(120/299): loss=0.6411473123918985, w0=-2.0698730635038127e-06, w1=-0.026779379255278655\n",
      "Regularized Logistic Regression(121/299): loss=0.6411473123918985, w0=-2.069873063504548e-06, w1=-0.02677937925527999\n",
      "Regularized Logistic Regression(122/299): loss=0.6411473123918984, w0=-2.0698730635051362e-06, w1=-0.026779379255281045\n",
      "Regularized Logistic Regression(123/299): loss=0.6411473123918984, w0=-2.0698730635056068e-06, w1=-0.02677937925528189\n",
      "Regularized Logistic Regression(124/299): loss=0.6411473123918983, w0=-2.0698730635059833e-06, w1=-0.026779379255282558\n",
      "Regularized Logistic Regression(125/299): loss=0.6411473123918983, w0=-2.0698730635062844e-06, w1=-0.026779379255283092\n",
      "Regularized Logistic Regression(126/299): loss=0.6411473123918984, w0=-2.0698730635065254e-06, w1=-0.026779379255283495\n",
      "Regularized Logistic Regression(127/299): loss=0.6411473123918984, w0=-2.069873063506718e-06, w1=-0.026779379255283838\n",
      "Regularized Logistic Regression(128/299): loss=0.6411473123918984, w0=-2.0698730635068722e-06, w1=-0.02677937925528409\n",
      "Regularized Logistic Regression(129/299): loss=0.6411473123918983, w0=-2.0698730635069955e-06, w1=-0.026779379255284314\n",
      "Regularized Logistic Regression(130/299): loss=0.6411473123918984, w0=-2.069873063507094e-06, w1=-0.026779379255284442\n",
      "Regularized Logistic Regression(131/299): loss=0.6411473123918984, w0=-2.0698730635071734e-06, w1=-0.026779379255284595\n",
      "Regularized Logistic Regression(132/299): loss=0.6411473123918984, w0=-2.0698730635072365e-06, w1=-0.026779379255284664\n",
      "Regularized Logistic Regression(133/299): loss=0.6411473123918984, w0=-2.069873063507287e-06, w1=-0.026779379255284768\n",
      "Regularized Logistic Regression(134/299): loss=0.6411473123918984, w0=-2.069873063507327e-06, w1=-0.026779379255284803\n",
      "Regularized Logistic Regression(135/299): loss=0.6411473123918984, w0=-2.0698730635073597e-06, w1=-0.026779379255284876\n",
      "Regularized Logistic Regression(136/299): loss=0.6411473123918984, w0=-2.0698730635073855e-06, w1=-0.026779379255284886\n",
      "Regularized Logistic Regression(137/299): loss=0.6411473123918984, w0=-2.0698730635074063e-06, w1=-0.02677937925528494\n",
      "Regularized Logistic Regression(138/299): loss=0.6411473123918984, w0=-2.069873063507423e-06, w1=-0.02677937925528494\n",
      "Regularized Logistic Regression(139/299): loss=0.6411473123918984, w0=-2.069873063507436e-06, w1=-0.026779379255284987\n",
      "Regularized Logistic Regression(140/299): loss=0.6411473123918984, w0=-2.0698730635074465e-06, w1=-0.026779379255284976\n",
      "Regularized Logistic Regression(141/299): loss=0.6411473123918984, w0=-2.069873063507455e-06, w1=-0.026779379255285014\n",
      "Regularized Logistic Regression(142/299): loss=0.6411473123918984, w0=-2.0698730635074618e-06, w1=-0.026779379255284997\n",
      "Regularized Logistic Regression(143/299): loss=0.6411473123918984, w0=-2.0698730635074673e-06, w1=-0.02677937925528503\n",
      "Regularized Logistic Regression(144/299): loss=0.6411473123918984, w0=-2.0698730635074715e-06, w1=-0.02677937925528501\n",
      "Regularized Logistic Regression(145/299): loss=0.6411473123918984, w0=-2.069873063507475e-06, w1=-0.026779379255285042\n",
      "Regularized Logistic Regression(146/299): loss=0.6411473123918984, w0=-2.0698730635074774e-06, w1=-0.02677937925528502\n",
      "Regularized Logistic Regression(147/299): loss=0.6411473123918984, w0=-2.06987306350748e-06, w1=-0.02677937925528505\n",
      "Regularized Logistic Regression(148/299): loss=0.6411473123918984, w0=-2.0698730635074817e-06, w1=-0.026779379255285025\n",
      "Regularized Logistic Regression(149/299): loss=0.6411473123918984, w0=-2.0698730635074834e-06, w1=-0.026779379255285053\n",
      "Regularized Logistic Regression(150/299): loss=0.6411473123918984, w0=-2.069873063507484e-06, w1=-0.026779379255285028\n",
      "Regularized Logistic Regression(151/299): loss=0.6411473123918984, w0=-2.069873063507485e-06, w1=-0.026779379255285056\n",
      "Regularized Logistic Regression(152/299): loss=0.6411473123918984, w0=-2.069873063507486e-06, w1=-0.02677937925528503\n",
      "Regularized Logistic Regression(153/299): loss=0.6411473123918984, w0=-2.0698730635074868e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(154/299): loss=0.6411473123918984, w0=-2.069873063507487e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(155/299): loss=0.6411473123918984, w0=-2.0698730635074876e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(156/299): loss=0.6411473123918984, w0=-2.0698730635074876e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(157/299): loss=0.6411473123918984, w0=-2.069873063507488e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(158/299): loss=0.6411473123918984, w0=-2.069873063507488e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(159/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(160/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(161/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(162/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(163/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(164/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(165/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(166/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(167/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(168/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(169/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(170/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(171/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(172/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(173/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(174/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(175/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(176/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(177/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(178/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(179/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(180/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(181/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(182/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(183/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(184/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(185/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(186/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(187/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(188/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(189/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(190/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(191/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(192/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(193/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(194/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(195/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(196/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(197/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(198/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(199/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(200/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(201/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(202/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(203/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(204/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(205/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(206/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(207/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(208/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(209/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(210/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(211/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(212/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(213/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(214/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(215/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(216/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(217/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(218/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(219/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(220/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(221/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(222/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(223/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(224/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(225/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(226/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(227/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(228/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(229/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(230/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(231/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(232/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(233/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(234/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(235/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(236/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(237/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(238/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(239/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(240/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(241/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(242/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(243/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(244/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(245/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(246/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(247/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(248/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(249/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(250/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(251/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(252/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(253/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(254/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(255/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(256/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(257/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(258/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(259/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(260/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(261/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(262/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(263/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(264/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(265/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(266/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(267/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(268/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(269/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(270/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(271/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(272/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(273/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(274/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(275/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(276/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(277/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(278/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(279/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(280/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(281/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(282/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(283/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(284/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(285/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(286/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(287/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(288/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(289/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(290/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(291/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(292/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(293/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(294/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(295/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(296/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(297/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Regularized Logistic Regression(298/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.026779379255285035\n",
      "Regularized Logistic Regression(299/299): loss=0.6411473123918984, w0=-2.0698730635074884e-06, w1=-0.02677937925528506\n",
      "Best Hyperparameters:\n",
      "Gamma: 0.2, Lambda: 0.001, Max Iterations: 300\n",
      "Best F1 Score: 0.7704552612097489\n",
      "Test accuracy: 0.749523824035839, precision: 0.2341756191052605, recall: 0.7873494997456334, F1: 0.36098584979007936\n"
     ]
    }
   ],
   "source": [
    "# Define ranges for hyperparameters\n",
    "gamma_values = [0.01, 0.05, 0.1, 0.2]\n",
    "lambda_values = [0.001, 0.01, 0.1, 1.0]\n",
    "max_iters_values = [100, 200, 300]\n",
    "\n",
    "# Initialize the best scores\n",
    "best_f1 = 0\n",
    "best_params = {}\n",
    "\n",
    "# Grid search for hyperparameters\n",
    "for gamma in gamma_values:\n",
    "    for lambda_ in lambda_values:\n",
    "        for max_iters in max_iters_values:\n",
    "            # Initialize weights\n",
    "            initial_w = np.zeros(x_train_filtered_2_OHE_train_fixed.shape[1])\n",
    "            \n",
    "            # Train the model\n",
    "            w, loss = reg_logistic_regression(y_train_train_fixed, x_train_filtered_2_OHE_train_fixed, lambda_, initial_w, max_iters, gamma)\n",
    "            \n",
    "            # Predict on the training set\n",
    "            y_pred_train = predict_logistic_regression(x_train_filtered_2_OHE_train_fixed, w)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy_train, precision_train, recall_train, f1_train = accuracy_precision_recall_f1(y_train_train_fixed, y_pred_train)\n",
    "            \n",
    "            # Update best parameters if current F1 is better\n",
    "            if f1_train > best_f1:\n",
    "                best_f1 = f1_train\n",
    "                best_params = {\n",
    "                    'gamma': gamma,\n",
    "                    'lambda_': lambda_,\n",
    "                    'max_iters': max_iters,\n",
    "                    'weights': w\n",
    "                }\n",
    "\n",
    "# Output the best parameters and their performance\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(f\"Gamma: {best_params['gamma']}, Lambda: {best_params['lambda_']}, Max Iterations: {best_params['max_iters']}\")\n",
    "print(f\"Best F1 Score: {best_f1}\")\n",
    "\n",
    "# Use the best model to predict on the test set\n",
    "y_pred_test = predict_logistic_regression(x_train_filtered_2_OHE_test, best_params['weights'])\n",
    "accuracy_test, precision_test, recall_test, f1_test = accuracy_precision_recall_f1(y_train_test, y_pred_test)\n",
    "\n",
    "# Output test set metrics\n",
    "print(f\"Test accuracy: {accuracy_test}, precision: {precision_test}, recall: {recall_test}, F1: {f1_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests with different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train_filtered_2_OHE_train_fixed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Fit the model to the training data\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m rf_model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mx_train_filtered_2_OHE_train_fixed\u001b[49m, y_train_train_fixed)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Predict on the test set\u001b[39;00m\n\u001b[1;32m      8\u001b[0m y_pred_test_rf \u001b[38;5;241m=\u001b[39m rf_model\u001b[38;5;241m.\u001b[39mpredict(x_train_filtered_2_OHE_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train_filtered_2_OHE_train_fixed' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "rf_model.fit(x_train_filtered_2_OHE_train_fixed, y_train_train_fixed)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_test_rf = rf_model.predict(x_train_filtered_2_OHE_test)\n",
    "\n",
    "# Calculate and print the test metrics\n",
    "accuracy_test_rf = accuracy_score(y_train_test, y_pred_test_rf)\n",
    "precision_test_rf = precision_score(y_train_test, y_pred_test_rf)\n",
    "recall_test_rf = recall_score(y_train_test, y_pred_test_rf)\n",
    "f1_test_rf = f1_score(y_train_test, y_pred_test_rf)\n",
    "\n",
    "print(f\"Random Forest Test accuracy: {accuracy_test_rf}, precision: {precision_test_rf}, recall: {recall_test_rf}, F1: {f1_test_rf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Test accuracy: 0.8674630868392582, precision: 0.24405506883604505, recall: 0.23393316195372751, F1: 0.23888694434721736\n"
     ]
    }
   ],
   "source": [
    "# Initialize KNN model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Fit the model\n",
    "knn_model.fit(x_train_filtered_2_OHE_train_fixed, y_train_train_fixed)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_test_knn = knn_model.predict(x_train_filtered_2_OHE_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_test_knn = accuracy_score(y_train_test, y_pred_test_knn)\n",
    "precision_test_knn = precision_score(y_train_test, y_pred_test_knn)\n",
    "recall_test_knn = recall_score(y_train_test, y_pred_test_knn)\n",
    "f1_test_knn = f1_score(y_train_test, y_pred_test_knn)\n",
    "\n",
    "print(f\"KNN Test accuracy: {accuracy_test_knn}, precision: {precision_test_knn}, recall: {recall_test_knn}, F1: {f1_test_knn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
