{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.genfromtxt(\"data/x_train.csv\", delimiter=\",\", skip_header=1)\n",
    "features = np.genfromtxt(\"data/x_train.csv\", delimiter=\",\", dtype=str, max_rows=1)\n",
    "y_train = np.genfromtxt(\"data/y_train.csv\", delimiter=\",\", skip_header=1)\n",
    "y_features = np.genfromtxt(\"data/y_train.csv\", delimiter=\",\", dtype=str, max_rows=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection and Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all features become categorical for homogeneity\n",
    "# we only keep possibly relevant features\n",
    "# we will also do a dimensionality reduction because a lot of features are highly correlated\n",
    "\n",
    "mapping_dict = {\n",
    "    \"GENHLTH\": lambda value: value if value <= 5 else -1,\n",
    "    \"PHYSHLTH\": lambda value: 0 if value == 0 else 10 if 1 <= value <= 10 else 20 if 11 <= value <= 20 else 30 if 21 <= value <= 30 else -1,\n",
    "    \"MENTHLTH\": lambda value: 0 if value == 0 else 10 if 1 <= value <= 10 else 20 if 11 <= value <= 20 else 30 if 21 <= value <= 30 else -1,\n",
    "    \"POORHLTH\": lambda value: 0 if value == 0 else 10 if 1 <= value <= 10 else 20 if 11 <= value <= 20 else 30 if 21 <= value <= 30 else -1,\n",
    "    \"HLTHPLN1\": lambda value: 1 if value == 1 else 0,\n",
    "    \"MEDCOST\": lambda value: 1 if value == 1 else 0,\n",
    "    \"CHECKUP1\": lambda value: value if value <= 4 or value==8 else -1,\n",
    "    \"BPHIGH4\": lambda value: 1 if value == 1 else 0,\n",
    "    \"BPMEDS\": lambda value: 1 if value == 1 else 0,\n",
    "    \"BLOODCHO\": lambda value: 1 if value == 1 else 0,\n",
    "    \"CHOLCHK\": lambda value: value if value <= 4 else -1,\n",
    "    # \"CVDINFR4\": lambda value: 1 if value == 1 else 0,\n",
    "    # \"CVDCRHD4\": lambda value: 1 if value == 1 else 0,\n",
    "    \"TOLDHI2\": lambda value: 1 if value == 1 else 0,\n",
    "    \"CVDSTRK3\": lambda value: 1 if value == 1 else 0,\n",
    "    \"ASTHMA3\": lambda value: 1 if value == 1 else 0,\n",
    "    \"ASTHNOW\": lambda value: 1 if value == 1 else 0,\n",
    "    \"CHCSCNCR\": lambda value: 1 if value == 1 else 0,\n",
    "    \"CHCOCNCR\": lambda value: 1 if value == 1 else 0,\n",
    "    \"CHCCOPD1\": lambda value: 1 if value == 1 else 0,\n",
    "    \"HAVARTH3\": lambda value: 1 if value == 1 else 0,\n",
    "    \"ADDEPEV2\": lambda value: 1 if value == 1 else 0,\n",
    "    \"CHCKIDNY\": lambda value: 1 if value == 1 else 0,\n",
    "    \"DIABETE3\": lambda value: 1 if value == 1 else 0,\n",
    "    \"SEX\": lambda value: 1 if value == 1 else 0,\n",
    "    \"MARITAL\": lambda value: value,\n",
    "    \"EDUCA\": lambda value: value,\n",
    "    \"VETERAN3\": lambda value: 1 if value == 1 else 0,\n",
    "    \"INCOME2\": lambda value: value if value <= 8 else -1,\n",
    "    \"INTERNET\": lambda value: 1 if value == 1 else 0,\n",
    "    \"WTKG3\": lambda value: 77 if value <= 77 else 132 if value <= 132 else 187 if value <= 187 else 242 if value <= 242 else 295 if value <= 295 else -1,\n",
    "    \"QLACTLM2\": lambda value: 1 if value == 1 else 0,\n",
    "    \"USEEQUIP\": lambda value: 1 if value == 1 else 0,\n",
    "    \"BLIND\": lambda value: 1 if value == 1 else 0,\n",
    "    \"DECIDE\": lambda value: 1 if value == 1 else 0,\n",
    "    \"DIFFWALK\": lambda value: 1 if value == 1 else 0,\n",
    "    \"DIFFDRES\": lambda value: 1 if value == 1 else 0,\n",
    "    \"DIFFALON\": lambda value: 1 if value == 1 else 0,\n",
    "    \"SMOKE100\": lambda value: 1 if value == 1 else 0,\n",
    "    \"SMOKDAY2\": lambda value: value if value <= 3 else -1,\n",
    "    \"LASTSMK2\": lambda value: value if value <= 8 else -1,\n",
    "    \"USENOW3\": lambda value: value if value <= 3 else -1,\n",
    "    \"AVEDRNK2\": lambda value: str(value//20) if value <=76 else -1,\n",
    "    \"DRNK3GE5\": lambda value: str(value//20) if value <=76 else -1,\n",
    "    \"EXERANY2\": lambda value: 1 if value == 1 else 0,\n",
    "    # \"EXERHMM1\": lambda value: str(value//200) if value <= 959 and value not in [777,999] else -1,\n",
    "    \"LMTJOIN3\": lambda value: value if value <= 2 else -1,\n",
    "    \"FLUSHOT6\": lambda value: 1 if value == 1 else 0,\n",
    "    \"PDIABTST\": lambda value: 1 if value == 1 else 0,\n",
    "    \"PREDIAB1\": lambda value: 1 if value == 1 else 0,\n",
    "    \"INSULIN\": lambda value: 1 if value == 1 else 0,\n",
    "    \"CIMEMLOS\": lambda value: 1 if value == 1 else 0,\n",
    "    \"_RFHLTH\": lambda value: value if value <= 2 else -1,\n",
    "    \"_HCVU651\": lambda value: value if value <= 2 else -1,\n",
    "    \"_RFHYPE5\": lambda value: value if value <= 2 else -1,\n",
    "    \"_CHOLCHK\": lambda value: value if value <= 3 else -1,\n",
    "    \"_RFCHOL\": lambda value: value if value <= 2 else -1,\n",
    "    # \"_MICHD\": lambda value: value if value <= 2 else -1,\n",
    "    \"_LTASTH1\": lambda value: value if value <= 2 else -1,\n",
    "    \"_CASTHM1\": lambda value: value if value <= 2 else -1,\n",
    "    \"_DRDXAR1\": lambda value: 1 if value == 1 else 0,\n",
    "    \"_AGEG5YR\": lambda value: value,\n",
    "    \"_AGE_G\": lambda value: value,\n",
    "    \"HTM4\": lambda value: 1.50 if value <= 1.50 else 1.60 if value <= 1.60 else 1.75 if value <= 1.75 else 1.90 if value <= 1.90 else 2.00 if value > 1.90 else -1,\n",
    "    \"_RFBMI5\": lambda value: 1 if value == 2 else 0,\n",
    "    \"_EDUCAG\": lambda value: value if value <= 4 else -1,\n",
    "    \"_SMOKER3\": lambda value: value if value <= 3 else -1,\n",
    "    \"_RFBING5\": lambda value: 1 if value == 2 else 0,\n",
    "    \"_BMI5CAT\": lambda value: value if value <= 4 else -1,\n",
    "    \"_RFDRHV5\": lambda value: 1 if value == 2 else 0,\n",
    "    \"FTJUDA1_\": lambda value: 0 if value == 0 else 10 if value <= 10 else 30 if value <= 30 else 70 if value <= 70 else 99 if value <=99 else -1,\n",
    "    \"MAXVO2_\": lambda value: value//10 if value <= 50 else -1,\n",
    "    \"ACTIN11_\": lambda value: value if value <= 2 else -1,\n",
    "    \"ACTIN21_\": lambda value: value if value <= 2 else -1,\n",
    "    \"_PACAT1\": lambda value: value if value <= 9 else -1,\n",
    "    \"_PA150R2\": lambda value: value if value <= 9 else -1,\n",
    "    \"_PA300R2\": lambda value: value if value <= 9 else -1,\n",
    "    \"_PASTRNG\":  lambda value: value if value <= 9 else -1,\n",
    "    \"_PASTAE1\": lambda value: value if value <= 9 else -1,\n",
    "    \"_LMTACT1\": lambda value: value if value <= 9 else -1,\n",
    "    \"_LMTWRK1\": lambda value: value if value <= 3 else -1,\n",
    "    \"_LMTSCL1\": lambda value: value if value <= 4 else -1,\n",
    "    \"_INCOMG\": lambda value: value if value <= 5 else -1\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 140 features over 322\n",
      "{'Id': 0.0, '_STATE': 0.0, 'FMONTH': 0.0, 'IDATE': 0.0, 'IMONTH': 0.0, 'IDAY': 0.0, 'IYEAR': 0.0, 'DISPCODE': 0.0, 'SEQNO': 0.0, '_PSU': 0.0, 'CTELENUM': 0.4248708610785195, 'PVTRESD1': 0.4248708610785195, 'COLGHOUS': 0.9999024791625398, 'STATERES': 0.42487390860469015, 'CELLFON3': 0.4248708610785195, 'LADULT': 0.9999024791625398, 'NUMADULT': 0.4249257165495909, 'NUMMEN': 0.4252030414311183, 'NUMWOMEN': 0.4252060889572889, 'CTELNUM1': 0.5751291389214805, 'CELLFON2': 0.5751291389214805, 'CADULT': 0.5751321864476511, 'PVTRESD2': 0.5751291389214805, 'CCLGHOUS': 0.9975589315373246, 'CSTATE': 0.5751260913953099, 'LANDLINE': 0.5768570862602282, 'HHADULT': 0.5867828789979734, 'GENHLTH': 6.095052341261981e-06, 'PHYSHLTH': 3.0475261706309903e-06, 'MENTHLTH': 0.0, 'POORHLTH': 0.4871775336370701, 'HLTHPLN1': 0.0, 'PERSDOC2': 0.0, 'MEDCOST': 3.0475261706309903e-06, 'CHECKUP1': 3.0475261706309903e-06, 'BPHIGH4': 3.0475261706309903e-06, 'BPMEDS': 0.5983330031846649, 'BLOODCHO': 0.0, 'CHOLCHK': 0.133484693799808, 'TOLDHI2': 0.133484693799808, 'CVDSTRK3': 0.0, 'ASTHMA3': 0.0, 'ASTHNOW': 0.8659637039633078, 'CHCSCNCR': 3.0475261706309903e-06, 'CHCOCNCR': 0.0, 'CHCCOPD1': 0.0, 'HAVARTH3': 3.0475261706309903e-06, 'ADDEPEV2': 0.0, 'CHCKIDNY': 0.0, 'DIABETE3': 1.523763085315495e-05, 'DIABAGE2': 0.8713334450759596, 'SEX': 0.0, 'MARITAL': 0.0, 'EDUCA': 0.0, 'RENTHOM1': 0.0, 'NUMHHOL2': 0.4248708610785195, 'NUMPHON2': 0.9698599661724595, 'CPDEMO1': 0.4248708610785195, 'VETERAN3': 3.0475261706309903e-06, 'EMPLOY1': 0.0, 'CHILDREN': 9.14257851189297e-06, 'INCOME2': 0.007530437167629177, 'INTERNET': 0.009773416429213587, 'WEIGHT2': 0.012025538269309887, 'HEIGHT3': 0.013430447833970774, 'PREGNANT': 0.8522803114571746, 'QLACTLM2': 0.021128498940984657, 'USEEQUIP': 0.023596995139195758, 'BLIND': 0.025273134533042804, 'DECIDE': 0.026598808417267285, 'DIFFWALK': 0.02788181693510293, 'DIFFDRES': 0.0287442668413915, 'DIFFALON': 0.030054703094762825, 'SMOKE100': 0.03212397336462127, 'SMOKDAY2': 0.5832325110091883, 'STOPSMK2': 0.8609566184649611, 'LASTSMK2': 0.7233882395965076, 'USENOW3': 0.03354412056013531, 'ALCDAY5': 0.0358541453974736, 'AVEDRNK2': 0.5208405077178601, 'DRNK3GE5': 0.5217730507260732, 'MAXDRNKS': 0.5227177838389687, 'FRUITJU1': 0.06319959772654547, 'FRUIT1': 0.06572599692199857, 'FVBEANS': 0.06847181800173709, 'FVGREEN': 0.0705471833239368, 'FVORANG': 0.0724061742880217, 'VEGETAB1': 0.07467048623280052, 'EXERANY2': 0.0798604233013851, 'EXRACT11': 0.32861169945296903, 'EXEROFT1': 0.3326100537888369, 'EXERHMM1': 0.33342069575022476, 'EXRACT21': 0.33428314565651335, 'EXEROFT2': 0.5523976412147439, 'EXERHMM2': 0.5528212473524616, 'STRENGTH': 0.08730248221006598, 'LMTJOIN3': 0.6915050207993662, 'ARTHDIS2': 0.691919484358572, 'ARTHSOCL': 0.6923888033888491, 'JOINPAIN': 0.6981516753775123, 'SEATBELT': 0.09115455528974355, 'FLUSHOT6': 0.0932543008213083, 'FLSHTMY2': 0.5691651302055556, 'IMFVPLAC': 0.5668429152635348, 'PNEUVAC3': 0.09454035686531458, 'HIVTST6': 0.09776463955384217, 'HIVTSTD3': 0.7418227254026544, 'WHRTST10': 0.7422585216450547, 'PDIABTST': 0.8127417069194082, 'PREDIAB1': 0.8127417069194082, 'INSULIN': 0.9338382068356011, 'BLDSUGAR': 0.9338412543617718, 'FEETCHK2': 0.9338473494141131, 'DOCTDIAB': 0.9338503969402837, 'CHKHEMO3': 0.9338503969402837, 'FEETCHK': 0.9344599021744099, 'EYEEXAM': 0.9338503969402837, 'DIABEYE': 0.9338534444664544, 'DIABEDU': 0.9338534444664544, 'CAREGIV1': 0.7536501744708732, 'CRGVREL1': 0.9456534657991376, 'CRGVLNG1': 0.9457083212702089, 'CRGVHRS1': 0.9457875569506453, 'CRGVPRB1': 0.9458515550002285, 'CRGVPERS': 0.9459277431544943, 'CRGVHOUS': 0.9459612659423713, 'CRGVMST2': 0.946104499672391, 'CRGVEXPT': 0.8086519267984213, 'VIDFCLT2': 0.9927682203970927, 'VIREDIF3': 0.992789553080287, 'VIPRFVS2': 0.992789553080287, 'VINOCRE2': 0.9975741691681778, 'VIEYEXM2': 0.9942980785347494, 'VIINSUR2': 0.9928078382373109, 'VICTRCT4': 0.9928139332896522, 'VIGLUMA2': 0.9928139332896522, 'VIMACDG2': 0.992823075868164, 'CIMEMLOS': 0.7359562375241897, 'CDHOUSE': 0.9705669922440459, 'CDASSIST': 0.9706035625580934, 'CDHELP': 0.990601429289774, 'CDSOCIAL': 0.9706523229768236, 'CDDISCUS': 0.9706827982385299, 'WTCHSALT': 0.906693891233791, 'LONGWTCH': 0.9425998445761653, 'DRADVISE': 0.9067700793880568, 'ASTHMAGE': 0.9976198820607372, 'ASATTACK': 0.9985067121763909, 'ASERVIST': 0.9993051640330961, 'ASDRVIST': 0.9993051640330961, 'ASRCHKUP': 0.9985158547549027, 'ASACTLIM': 0.9985158547549027, 'ASYMPTOM': 0.9985189022810733, 'ASNOSLEP': 0.9989577460496442, 'ASTHMED3': 0.998521949807244, 'ASINHALR': 0.998521949807244, 'HAREHAB1': 0.9976442622701023, 'STREHAB1': 0.9984244289697838, 'CVDASPRN': 0.9571944474073171, 'ASPUNSAF': 0.9699788196931142, 'RLIVPAIN': 0.9872217227665443, 'RDUCHART': 0.9872186752403737, 'RDUCSTRK': 0.9872217227665443, 'ARTTODAY': 0.955551830801347, 'ARTHWGT': 0.9555823060630533, 'ARTHEXER': 0.9555944961677358, 'ARTHEDU': 0.9556188763771009, 'TETANUS': 0.9082968899995428, 'HPVADVC2': 0.9753485608057659, 'HPVADSHT': 0.9966507687384766, 'SHINGLE2': 0.9366145031770461, 'HADMAM': 0.9481951026254438, 'HOWLONG': 0.9578618556386853, 'HADPAP2': 0.9482133877824676, 'LASTPAP2': 0.9516723299861337, 'HPVTEST': 0.9482408155180032, 'HPLSTTST': 0.9841985768052783, 'HADHYST2': 0.9487588949670105, 'PROFEXAM': 0.9871363920337666, 'LENGEXAM': 0.9886814878022765, 'BLDSTOOL': 0.8773736419461502, 'LSTBLDS3': 0.9552013652917244, 'HADSIGM3': 0.8774467825742454, 'HADSGCO1': 0.9111280418120591, 'LASTSIG3': 0.911137184390571, 'PCPSAAD2': 0.9851463574443445, 'PCPSADI1': 0.9851646426013684, 'PCPSARE1': 0.9851798802322215, 'PSATEST1': 0.9851798802322215, 'PSATIME': 0.9919545309095342, 'PCPSARS1': 0.991963673488046, 'PCPSADE1': 0.9981836744023039, 'PCDMDECN': 0.9994544928154571, 'SCNTMNY1': 0.842680604019687, 'SCNTMEL1': 0.8339311563838054, 'SCNTPAID': 0.9208435552440306, 'SCNTWRK1': 0.9208770780319075, 'SCNTLPAD': 0.9422189038048364, 'SCNTLWK1': 0.9134776844896155, 'SXORIENT': 0.621463726819754, 'TRNSGNDR': 0.6216648635470157, 'RCSGENDR': 0.8642418516769013, 'RCSRLTN2': 0.864714218233349, 'CASTHDX2': 0.8808752495162052, 'CASTHNO2': 0.9853383515930944, 'EMTSUPRT': 0.9540463528730553, 'LSATISFY': 0.9540768281347616, 'ADPLEASR': 0.9538574062504762, 'ADDOWN': 0.9538970240906944, 'ADSLEEP': 0.9539183567738888, 'ADENERGY': 0.9539305468785714, 'ADEAT1': 0.953957974614107, 'ADFAIL': 0.9539732122449601, 'ADTHINK': 0.9540097825590077, 'ADMOVE': 0.9540433053468846, 'MISTMNT': 0.9540798756609322, 'ADANXEV': 0.9541012083441267, 'QSTVER': 0.0, 'QSTLANG': 4.266536638883386e-05, 'MSCODE': 0.434083532692337, '_STSTR': 0.0, '_STRWT': 6.704557575388179e-05, '_RAWRAKE': 0.0, '_WT2RAKE': 0.0, '_CHISPNC': 0.27560607676718424, '_CRACE1': 0.851881085528822, '_CPRACE': 0.851881085528822, '_CLLCPWT': 0.8642875645694608, '_DUALUSE': 0.0, '_DUALCOR': 0.3751017111859448, '_LLCPWT': 0.0, '_RFHLTH': 0.0, '_HCVU651': 0.0, '_RFHYPE5': 0.0, '_CHOLCHK': 0.0, '_RFCHOL': 0.133484693799808, '_LTASTH1': 0.0, '_CASTHM1': 0.0, '_ASTHMS1': 0.0, '_DRDXAR1': 0.005738491779298155, '_PRACE1': 0.0, '_MRACE1': 0.0, '_HISPANC': 0.0, '_RACE': 0.0, '_RACEG21': 0.0, '_RACEGR3': 0.0, '_RACE_G1': 0.016572447315891324, '_AGEG5YR': 0.0, '_AGE65YR': 0.0, '_AGE80': 0.0, '_AGE_G': 0.0, 'HTIN4': 0.03876758041659683, 'HTM4': 0.0346442775077331, 'WTKG3': 0.07011138708153657, '_BMI5': 0.0825056760174928, '_BMI5CAT': 0.0825056760174928, '_RFBMI5': 0.0, '_CHLDCNT': 0.0, '_EDUCAG': 0.0, '_INCOMG': 0.0, '_SMOKER3': 0.0, '_RFSMOK3': 0.0, 'DRNKANY5': 0.0, 'DROCDY3_': 0.0, '_RFBING5': 0.0, '_DRNKWEK': 0.0, '_RFDRHV5': 0.0, 'FTJUDA1_': 0.08644612735611867, 'FRUTDA1_': 0.08206073719658068, 'BEANDAY_': 0.08954241394547975, 'GRENDAY_': 0.08500464747741021, 'ORNGDAY_': 0.08825026284913222, 'VEGEDA1_': 0.09323296813811388, '_MISFRTN': 0.0, '_MISVEGN': 0.0, '_FRTRESP': 0.0, '_VEGRESP': 0.0, '_FRUTSUM': 0.09787130296981425, '_VEGESUM': 0.11460222164657839, '_FRTLT1': 0.0, '_VEGLT1': 0.0, '_FRT16': 0.0, '_VEG23': 0.0, '_FRUITEX': 0.0, '_VEGETEX': 0.0, '_TOTINDA': 0.0, 'METVL11_': 0.3313483779541957, 'METVL21_': 0.34261812973318906, 'MAXVO2_': 0.0, 'FC60_': 0.0, 'ACTIN11_': 0.33849482682432536, 'ACTIN21_': 0.3495451567190333, 'PADUR1_': 0.34678714553461226, 'PADUR2_': 0.562704374723818, 'PAFREQ1_': 0.3390586191658921, 'PAFREQ2_': 0.5574321544486264, '_MINAC11': 0.3509287336004998, '_MINAC21': 0.3560150547792829, 'STRFREQ_': 0.09903241044082466, 'PAMISS1_': 0.0, 'PAMIN11_': 0.35764243375439986, 'PAMIN21_': 0.36266780440977037, 'PA1MIN_': 0.3439407560912429, 'PAVIG11_': 0.34676276532524725, 'PAVIG21_': 0.35659713227787343, 'PA1VIGM_': 0.3406920931933503, '_PACAT1': 0.0, '_PAINDX1': 0.0, '_PA150R2': 0.0, '_PA300R2': 0.0, '_PA30021': 0.0, '_PASTRNG': 0.0, '_PAREC1': 0.0, '_PASTAE1': 0.0, '_LMTACT1': 0.005738491779298155, '_LMTWRK1': 0.005738491779298155, '_LMTSCL1': 0.005738491779298155, '_RFSEAT2': 0.0, '_RFSEAT3': 0.0, '_FLSHOT6': 0.6441799868956375, '_PNEUMO2': 0.6441799868956375, '_AIDTST3': 0.09776463955384217}\n"
     ]
    }
   ],
   "source": [
    "# for each feature in mapping_dict, check the ratio of nan values in x_train and keep only the ones with less than 10%\n",
    "# The features with a high nan value ratio won't be too useful and are in anyway correlated with other features\n",
    "\n",
    "def select_features_with_low_nan_ratio(x_train, features, threshold=0.1):\n",
    "    nan_ratios = {}\n",
    "    for feature in features:\n",
    "        nan_ratios[feature] = np.sum(np.isnan(x_train[:, features == feature])) / len(x_train)\n",
    "\n",
    "    selected_features = [feature for feature in nan_ratios if nan_ratios[feature] < threshold]\n",
    "\n",
    "    print(f\"Selected {len(selected_features)} features over {len(features)}\")\n",
    "    print(nan_ratios)\n",
    "    return selected_features\n",
    "\n",
    "selected_features = select_features_with_low_nan_ratio(x_train, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328135, 322)\n",
      "(328135, 62)\n"
     ]
    }
   ],
   "source": [
    "# apply the mapping to the selected features and keep only those in the training\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "def apply_mapping(x_train, selected_features, mapping_dict):\n",
    "    x_train_filtered = np.zeros((x_train.shape[0], len(selected_features)))\n",
    "    for feature in selected_features:\n",
    "        feature_values = x_train[:, features == feature].flatten()\n",
    "        if feature_values.size > 0:\n",
    "            x_train_filtered[:, selected_features.index(feature)] = np.array([mapping_dict[feature](value) for value in feature_values])\n",
    "    return x_train_filtered\n",
    "\n",
    "x_train_filtered = apply_mapping(x_train, selected_features, mapping_dict)\n",
    "print(x_train_filtered.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the links between the features and the target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot proportion of y=1 for each value of a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature = \"PHYSHLTH\"\n",
    "# print(x_train[:, features == feature].flatten())\n",
    "# print(\"Max value:\", np.nanmax(x_train[:, features == feature][x_train[:, features == feature] != 99.0]))\n",
    "# print(\"Min value:\", np.min(x_train[:, features == feature][~np.isnan(x_train[:, features == feature])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature = \"WTKG3\" # categorical feature\n",
    "feature_index = selected_features.index(feature)\n",
    "feature_values = x_train_filtered[:, feature_index]\n",
    "unique_values = np.unique(feature_values)\n",
    "print(len(unique_values))\n",
    "target_values = y_train[:, -1] # values in {1, -1}\n",
    "\n",
    "# plot a bar plot (one bar for each unique value of the feature) showing the proportion of target values for each unique value of the feature\n",
    "proportions = []\n",
    "for value in unique_values:\n",
    "    target_values_for_value = target_values[feature_values == value]\n",
    "    proportions.append(np.sum(target_values_for_value == 1) / len(target_values_for_value))\n",
    "\n",
    "plt.bar(unique_values, proportions)\n",
    "plt.xlabel(feature)\n",
    "plt.ylabel('Proportion of target values')\n",
    "plt.title('Proportion of target values for each unique value of the feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GENHLTH', 'PHYSHLTH', 'MENTHLTH', 'HLTHPLN1', 'MEDCOST', 'CHECKUP1', 'BPHIGH4', 'BLOODCHO', 'CVDSTRK3', 'ASTHMA3', 'CHCSCNCR', 'CHCOCNCR', 'CHCCOPD1', 'HAVARTH3', 'ADDEPEV2', 'CHCKIDNY', 'DIABETE3', 'SEX', 'MARITAL', 'EDUCA', 'VETERAN3', 'INCOME2', 'INTERNET', 'WTKG3', 'QLACTLM2', 'USEEQUIP', 'BLIND', 'DECIDE', 'DIFFWALK', 'DIFFDRES', 'DIFFALON', 'SMOKE100', 'USENOW3', 'EXERANY2', 'FLUSHOT6', '_RFHLTH', '_HCVU651', '_RFHYPE5', '_CHOLCHK', '_LTASTH1', '_CASTHM1', '_DRDXAR1', '_AGEG5YR', '_AGE_G', 'HTM4', '_RFBMI5', '_EDUCAG', '_SMOKER3', '_RFBING5', '_BMI5CAT', '_RFDRHV5', 'FTJUDA1_', 'MAXVO2_', '_PACAT1', '_PA150R2', '_PA300R2', '_PASTRNG', '_PASTAE1', '_LMTACT1', '_LMTWRK1', '_LMTSCL1', '_INCOMG']\n"
     ]
    }
   ],
   "source": [
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_feature_entropies(x_train_filtered, y_train, selected_features):\n",
    "    # Function to calculate entropy\n",
    "    def calculate_entropy(probabilities):\n",
    "        # Calculate entropy: H(X) = -sum(p(x) * log2(p(x)))\n",
    "        assert np.isclose(np.sum(probabilities), 1), f\"Probabilities do not sum to 1: {np.sum(probabilities)}\"\n",
    "        return -np.sum([p * np.log2(p) for p in probabilities if p > 0])\n",
    "\n",
    "    entropies = []\n",
    "    entropies_normalized = []\n",
    "    joint_entropies = []\n",
    "    joint_entropies_normalized = []\n",
    "    mutual_informations = []\n",
    "    NMI = []\n",
    "\n",
    "    # Filter the rows based on target values\n",
    "    Y1 = x_train_filtered[y_train[:, 1] == 1]\n",
    "    Y_minus1 = x_train_filtered[y_train[:, 1] == -1]\n",
    "\n",
    "    # Total counts for probabilities\n",
    "    count_Y1 = len(Y1)\n",
    "    count_Y_minus1 = len(Y_minus1)\n",
    "    total_count = count_Y1 + count_Y_minus1\n",
    "\n",
    "    # Calculate probabilities of Y\n",
    "    p_Y1 = count_Y1 / total_count\n",
    "    p_Y_minus1 = count_Y_minus1 / total_count\n",
    "\n",
    "    # Entropy of Y\n",
    "    entropy_Y = calculate_entropy([p_Y1, p_Y_minus1])\n",
    "\n",
    "    # Iterate over the indices of the features\n",
    "    for feature in selected_features:\n",
    "        feature_index = selected_features.index(feature)\n",
    "        # Get the feature values for the entire dataset (to find all unique values)\n",
    "        all_feature_values = x_train_filtered[:, feature_index]\n",
    "        unique_values_all, counts_all = np.unique(all_feature_values, return_counts=True)\n",
    "\n",
    "        # Get the feature values for the filtered rows where Y=1\n",
    "        feature_values_Y1 = Y1[:, feature_index]\n",
    "        non_nan_values_Y1 = feature_values_Y1[~np.isnan(feature_values_Y1)]\n",
    "\n",
    "        # Get the counts for each unique value in filtered rows where Y=1\n",
    "        unique_values_filtered_Y1, counts_filtered_Y1 = np.unique(non_nan_values_Y1, return_counts=True)\n",
    "\n",
    "        # Create a dictionary for filtered counts where Y=1\n",
    "        filtered_count_dict_Y1 = dict(zip(unique_values_filtered_Y1, counts_filtered_Y1))\n",
    "\n",
    "        # Initialize the probabilities list for Y=1\n",
    "        probabilities_Y1 = []\n",
    "\n",
    "        # Calculate probabilities for Y=1\n",
    "        for value in unique_values_all:\n",
    "            count = filtered_count_dict_Y1.get(value, 0)  # Default to 0 if not found\n",
    "            probabilities_Y1.append(count / np.sum(counts_filtered_Y1) if np.sum(counts_filtered_Y1) > 0 else 0)  # Probabilities\n",
    "\n",
    "        # Compute the entropy for Y=1\n",
    "        entropy_Y1 = calculate_entropy(probabilities_Y1)\n",
    "\n",
    "        # Get the feature values for the filtered rows where Y=-1\n",
    "        feature_values_Y_minus1 = Y_minus1[:, feature_index]\n",
    "        non_nan_values_Y_minus1 = feature_values_Y_minus1[~np.isnan(feature_values_Y_minus1)]\n",
    "\n",
    "        # Get the counts for each unique value in filtered rows where Y=-1\n",
    "        unique_values_filtered_Y_minus1, counts_filtered_Y_minus1 = np.unique(non_nan_values_Y_minus1, return_counts=True)\n",
    "\n",
    "        # Create a dictionary for filtered counts where Y=-1\n",
    "        filtered_count_dict_Y_minus1 = dict(zip(unique_values_filtered_Y_minus1, counts_filtered_Y_minus1))\n",
    "\n",
    "        # Initialize the probabilities list for Y=-1\n",
    "        probabilities_Y_minus1 = []\n",
    "\n",
    "        # Calculate probabilities for Y=-1\n",
    "        for value in unique_values_all:\n",
    "            count = filtered_count_dict_Y_minus1.get(value, 0)  # Default to 0 if not found\n",
    "            probabilities_Y_minus1.append(count / np.sum(counts_filtered_Y_minus1) if np.sum(counts_filtered_Y_minus1) > 0 else 0)  # Probabilities\n",
    "\n",
    "        # Compute the entropy for Y=-1\n",
    "        entropy_Y_minus1 = calculate_entropy(probabilities_Y_minus1)\n",
    "\n",
    "        # Calculate the joint entropy H(f|Y)\n",
    "        joint_entropy = (p_Y1 * entropy_Y1) + (p_Y_minus1 * entropy_Y_minus1)\n",
    "\n",
    "        # Normalize entropies\n",
    "        entropy_normalized_Y1 = entropy_Y1 / np.log2(len(unique_values_all)) if len(unique_values_all) > 1 else 0\n",
    "        entropy_normalized_Y_minus1 = entropy_Y_minus1 / np.log2(len(unique_values_all)) if len(unique_values_all) > 1 else 0\n",
    "\n",
    "        # Append results\n",
    "        entropies.append((entropy_Y1, entropy_Y_minus1))\n",
    "        entropies_normalized.append((entropy_normalized_Y1, entropy_normalized_Y_minus1))\n",
    "        joint_entropies.append(joint_entropy)\n",
    "        joint_entropies_normalized.append(joint_entropy / np.log2(len(unique_values_all)) if len(unique_values_all) > 1 else 0)\n",
    "\n",
    "        # Mutual information = H(F) - H(F|Y)\n",
    "        entropy_feature = calculate_entropy(counts_all / np.sum(counts_all))\n",
    "        mutual_information = (entropy_feature - joint_entropy) / np.log2(len(unique_values_all))\n",
    "        mutual_informations.append(mutual_information)\n",
    "\n",
    "        # Normalized mutual information\n",
    "        NMI.append(2 * mutual_information / (entropy_Y + entropy_feature))\n",
    "\n",
    "    return entropies, entropies_normalized, joint_entropies, joint_entropies_normalized, mutual_informations, NMI\n",
    "\n",
    "entropies, entropies_normalized, joint_entropies, joint_entropies_normalized, mutual_informations, NMI = calculate_feature_entropies(x_train_filtered, y_train, selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7973488736551978\n"
     ]
    }
   ],
   "source": [
    "# sum of the mutual information for each feature\n",
    "print(np.sum(NMI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('GENHLTH', 0.01321113640460762), ('PHYSHLTH', 0.009465773719164312), ('MENTHLTH', 0.001487180916286728), ('HLTHPLN1', 0.0032673803251792558), ('MEDCOST', 0.0005795253487274069), ('CHECKUP1', 0.0024800541453470137), ('BPHIGH4', 0.0478826544186464), ('BLOODCHO', 0.014636901441908828), ('CVDSTRK3', 0.0559481971302017), ('ASTHMA3', 0.0032167539973076615), ('CHCSCNCR', 0.009650875436771853), ('CHCOCNCR', 0.009686492535194857), ('CHCCOPD1', 0.042339928072805406), ('HAVARTH3', 0.02796113677382566), ('ADDEPEV2', 0.005429524363422098), ('CHCKIDNY', 0.026891298955736033), ('DIABETE3', 0.036962355781966265), ('SEX', 0.005068909697124422), ('MARITAL', 0.0036540298199253386), ('EDUCA', 0.0015638338373940759), ('VETERAN3', 0.01731348553254677), ('INCOME2', 0.0017217992677516978), ('INTERNET', 0.021965453781663914), ('WTKG3', 0.0009731529124204024), ('QLACTLM2', 0.03454218574194), ('USEEQUIP', 0.041964879616894844), ('BLIND', 0.016412816795208174), ('DECIDE', 0.012812447226052454), ('DIFFWALK', 0.04775788820169959), ('DIFFDRES', 0.020554989394446464), ('DIFFALON', 0.025270038876386666), ('SMOKE100', 0.011486275030265648), ('USENOW3', 0.00010626363921034166), ('EXERANY2', 0.004585858938372109), ('FLUSHOT6', 0.004763999917630558), ('_RFHLTH', 0.03521567391506271), ('_HCVU651', 0.020068173974302144), ('_RFHYPE5', 0.029866905590159806), ('_CHOLCHK', 0.006542831509135432), ('_LTASTH1', 0.002070215310565385), ('_CASTHM1', 0.002883255475691825), ('_DRDXAR1', 0.02796113677382566), ('_AGEG5YR', 0.005506602147614817), ('_AGE_G', 0.011063525075486915), ('HTM4', 0.00018797089331380672), ('_RFBMI5', 0.0040024830106487694), ('_EDUCAG', 0.0019454876391059943), ('_SMOKER3', 0.004558292576642996), ('_RFBING5', 0.006406018551231407), ('_BMI5CAT', 0.001239903358431248), ('_RFDRHV5', 0.0014494765545677824), ('FTJUDA1_', 2.54766512386378e-05), ('MAXVO2_', 0.012411767109424541), ('_PACAT1', 0.0014868274291499621), ('_PA150R2', 0.001901911990568184), ('_PA300R2', 0.0019614223114472816), ('_PASTRNG', 0.001058228178274276), ('_PASTAE1', 0.00104694288765896), ('_LMTACT1', 0.009796385021022145), ('_LMTWRK1', 0.01100219631925277), ('_LMTSCL1', 0.00970531825390301), ('_INCOMG', 0.002368967153438779)]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABNoAAAIDCAYAAADSanOXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd3gUVfv/8c+mU0KHANJBBEEEA0p5AOlNRYogCkhVxEeadESKFClCKAKCCUWkSFORKCIdAaWKFFFpoSRU6VKSnN8f/LJflrTZZCLJw/t1XXNd7MyZs/eG3Sn3nOIwxhgBAAAAAAAASBaPhx0AAAAAAAAA8L+ARBsAAAAAAABgAxJtAAAAAAAAgA1ItAEAAAAAAAA2INEGAAAAAAAA2IBEGwAAAAAAAGADEm0AAAAAAACADUi0AQAAAAAAADYg0QYAAAAAAADYgEQbAABI9ebMmSOHwyE/Pz+dOHEi1vbnn39epUuXdllXqFAhORwOPf/883HWOW/ePDkcDjkcDm3YsMG5fujQoXI4HLpw4YJt8a9du1bly5dXhgwZ5HA49NVXX8Vbds+ePapevboyZ84sh8OhoKAg2+K437Rp0zRnzpwUqTs1ifnuHD9+/GGHEsuxY8fk7++vZs2axbl9wYIFcjgc+vTTT//lyAAAQFKRaAMAAGnG7du39f7771su7+/vr02bNunIkSOxtoWEhChTpkx2hhcnY4xatGghb29vffPNN9q2bZuqV68eb/kOHTooPDxcixYt0rZt2/Tqq6+mSFyPSqKtUaNG2rZtm/LkyfOwQ4mlcOHCmjBhgpYvX64FCxa4bIuIiNC7776revXq6a233npIEQIAAHeRaAMAAGlG/fr1tWDBAv3666+Wyv/nP//RY489ppCQEJf1R44c0aZNm9SyZcuUCNPFmTNndOnSJTVp0kS1atVSxYoVlTVr1njL79+/X7Vr11aDBg1UsWJF5c6dO8VjtNPNmzcfdggucubMqYoVK8rX1/dhhxKnzp07q0GDBnr33XcVHh7uXP/WW2/JGKPg4OCHGB0AAHAXiTYAAJBm9O3bV9mzZ1e/fv0slffw8FDbtm01d+5cRUdHO9eHhIQof/78ql27drLi2bJli2rVqiV/f3+lT59elStX1qpVq5zbhw4dqnz58kmS+vXrJ4fDoUKFCsVZV0wXx8jISE2fPt3ZrTVGRESE3nrrLeXLl08+Pj4qXLiwhg0bpsjISJd6hg0bpueee07ZsmVTpkyZ9Mwzzyg4OFjGGGeZQoUK6cCBA9q4caPzfWLiiq+r5YYNG2J1s43psrtp0yZVrlxZ6dOnV4cOHSRJV69eVe/evVW4cGH5+PjoscceU48ePXTjxg2XepcsWaLnnntOmTNnVvr06VWkSBFnHQm5fPmyOnbsqGzZsiljxoxq1KiRjh49KofDoaFDh8b6u8Z8nh49eihDhgy6evVqrDpbtmypgIAA3b1717lu8eLFqlSpkjJkyKCMGTOqXr162rNnj8t+7dq1U8aMGfXXX3+pYcOGypgxo/Lnz6/33ntPt2/fTvSzxCTT3nzzTUnS559/rm+++UZTp07VY489luj+AAAg9SDRBgAA0gx/f3+9//77Wr16tdatW2dpnw4dOujMmTNavXq1JCkqKkpz585Vu3bt5OGR9EuhjRs3qmbNmrpy5YqCg4O1cOFC+fv768UXX9TixYslSZ06ddLy5cslSe+++662bdumFStWxFlfTBdHSWrevLm2bdvmfB0REaFnn31Wq1ev1gcffKDvvvtOHTt21OjRo9W5c2eXeo4fP6633npLX375pZYvX66mTZvq3Xff1Ycffugss2LFChUpUkTlypVzvk98cSUmPDxcrVu31muvvabQ0FB17dpVN2/eVPXq1TV37lx169ZN3333nfr166c5c+bopZdecib9tm3bppYtW6pIkSJatGiRVq1apQ8++CBW8vBB0dHRevHFF7VgwQL169dPK1as0HPPPaf69esnGm+HDh108+ZNffnlly7rL1++rK+//lqtW7eWt7e3JGnUqFFq1aqVnnzySX355Zf6/PPPde3aNVWtWlUHDx502f/u3bt66aWXVKtWLX399dfq0KGDJk6cqDFjxiQaU548efTJJ5/o22+/1ejRo9W9e3c1a9ZMr732WqL7AgCAVMYAAACkcrNnzzaSzI4dO8zt27dNkSJFTPny5U10dLQxxpjq1aubUqVKuexTsGBB06hRI+f25s2bG2OMWbVqlXE4HObYsWNmyZIlRpJZv369c78hQ4YYSeb8+fMJxlSxYkWTK1cuc+3aNee6yMhIU7p0aZMvXz5nbMeOHTOSzLhx4yx9VknmnXfecVn31ltvmYwZM5oTJ064rB8/fryRZA4cOBBnXVFRUebu3btm+PDhJnv27M6YjDGmVKlSpnr16rH2iflbHzt2zGX9+vXrY/2tqlevbiSZtWvXupQdPXq08fDwMDt27HBZv3TpUiPJhIaGusR/+fLlOOOPz6pVq4wkM3369FjvK8kMGTIkwc/zzDPPmMqVK7vsO23aNCPJ/Pbbb8YYY8LCwoyXl5d59913Xcpdu3bN5M6d27Ro0cK57o033jCSzJdffulStmHDhuaJJ56w/LlatGhhJJmAgIBEv38AACB1okUbAABIU3x8fDRixAjt3LkzVquk+HTo0EHffPONLl68qODgYNWoUSPeLpxW3LhxQz///LOaN2+ujBkzOtd7enqqTZs2OnXqlA4fPpzk+h/07bffqkaNGsqbN68iIyOdS4MGDSTda10XY926dapdu7YyZ84sT09PeXt764MPPtDFixd17tw522KKkTVrVtWsWTNWvKVLl1bZsmVd4q1Xr55L99MKFSpIklq0aKEvv/xSp0+ftvSeMZ+3RYsWLutbtWplaf/27dtr69atLv9Hs2fPVoUKFZyz165evVqRkZFq27aty2fw8/NT9erVXbrQSpLD4dCLL77osq5MmTJxzpIbn+HDh0uSunXrphw5cljeDwAApB4k2gAAQJrz6quv6plnntGgQYNcxtOKT/PmzeXn56eJEydq5cqV6tixY7Le/++//5YxJs6ZLPPmzStJunjxYrLe435nz57VypUr5e3t7bKUKlVKknThwgVJ0i+//KK6detKkmbNmqWffvpJO3bs0KBBgyRJ//zzj20xxYjrb3D27Fnt27cvVrz+/v4yxjjjrVatmr766itnQitfvnwqXbq0Fi5cmOB7Xrx4UV5eXsqWLZvL+oCAAEsxv/766/L19XXOunrw4EHt2LFD7du3d/kM0r1k4IOfY/Hixc7PECN9+vTy8/NzWefr66tbt25ZiimmvHQvmQwAANImr4cdAAAAgLscDofGjBmjOnXqaObMmYmWT58+vV599VWNHj1amTJlUtOmTZP1/lmzZpWHh4fLLJExzpw5I0m2tkjKkSOHypQpo5EjR8a5PSa5t2jRInl7e+vbb791Sfp89dVXlt8rZr8HB/F/MLEU4/4JG+6PN126dLFme71/e4zGjRurcePGun37trZv367Ro0frtddeU6FChVSpUqU498+ePbsiIyN16dIll2RbREREwh/u/8uaNasaN26sefPmacSIEZo9e7b8/PxcWsTFxLh06VIVLFjQUr0AAAAk2gAAQJpUu3Zt1alTR8OHD1f+/PkTLf/222/r7Nmzql69eqyWR+7KkCGDnnvuOS1fvlzjx49XunTpJN0bpH/+/PnKly+fihcvnqz3uN8LL7yg0NBQFS1aVFmzZo23nMPhkJeXlzw9PZ3r/vnnH33++eexyvr6+sbZwi2mS+2+ffv0xBNPONd/8803bsU7atQoZc+eXYULF7a0j6+vr6pXr64sWbJo9erV2rNnT7yJturVq2vs2LFavHix3n77bef6RYsWWY6xffv2+vLLLxUaGqr58+erSZMmypIli3N7vXr15OXlpSNHjqhZs2aW6wUAAI82Em0AACDNGjNmjAIDA3Xu3DlnN8r4lC1b1q2WXYkZPXq06tSpoxo1aqh3797y8fHRtGnTtH//fi1cuDDOll5JNXz4cK1Zs0aVK1dWt27d9MQTT+jWrVs6fvy4QkNDNWPGDOXLl0+NGjXShAkT9Nprr+nNN9/UxYsXNX78eGeXxPs99dRTWrRokRYvXqwiRYrIz89PTz31lCpUqKAnnnhCvXv3VmRkpLJmzaoVK1Zoy5YtluPt0aOHli1bpmrVqqlnz54qU6aMoqOjFRYWph9++EHvvfeennvuOX3wwQc6deqUatWqpXz58uny5cuaNGmSvL29Vb169Xjrr1+/vqpUqaL33ntPV69eVWBgoLZt26Z58+ZJkqXZZOvWrat8+fKpa9euioiIcOk2Kt1LOA4fPlyDBg3S0aNHVb9+fWXNmlVnz57VL7/8ogwZMmjYsGGW/yYAAODRQKINAACkWeXKlVOrVq20YMGCf/29q1evrnXr1mnIkCFq166doqOj9fTTT+ubb77RCy+8YOt75cmTRzt37tSHH36ocePG6dSpU/L391fhwoWdCSBJqlmzpkJCQjRmzBi9+OKLeuyxx9S5c2flypUr1rh0w4YNU3h4uDp37qxr166pYMGCOn78uDw9PbVy5Ur997//VZcuXeTr66tXX31VU6dOVaNGjSzFmyFDBm3evFkfffSRZs6cqWPHjildunQqUKCAateu7Ww199xzz2nnzp3q16+fzp8/ryxZsqh8+fJat25dgolTDw8PrVy5Uu+9954++ugj3blzR1WqVNH8+fNVsWJFl5ZpCdXRtm1bjRo1Svnz51etWrVilRkwYICefPJJTZo0SQsXLtTt27eVO3duVahQQV26dLH0twAAAI8WhzHGPOwgAAAAgORasGCBXn/9df3000+qXLnyww4HAAA8gki0AQAAIM1ZuHChTp8+raeeekoeHh7avn27xo0bp3Llymnjxo0POzwAAPCIousoAAAA0hx/f38tWrRII0aM0I0bN5QnTx61a9dOI0aMeNihAQCARxgt2gAAAAAAAAAbJD4lEwAAAAAAAIBEkWgDAAAAAAAAbECiDQAAAAAAALABkyHEITo6WmfOnJG/v78cDsfDDgcAAAAAAAAPkTFG165dU968eeXhEX+7NRJtcThz5ozy58//sMMAAAAAAABAKnLy5Enly5cv3u0k2uLg7+8v6d4fL1OmTA85GgAAAAAAADxMV69eVf78+Z05o/iQaItDTHfRTJkykWgDAAAAAACAJCU6xBiTIQAAAAAAAAA2INEGAAAAAAAA2IBEGwAAAAAAAGADEm0AAAAAAACADUi0AQAAAAAAADYg0QYAAAAAAADYgEQbAAAAAAAAYAMSbQAAAAAAAIANSLQBAAAAAAAANiDRBgAAAAAAANiARBsAAAAAAABgAxJtAAAAAAAAgA1ItAEAAAAAAAA2INEGAAAAAAAA2IBEGwAAAAAAAGADr4cdQGoX2GdekvfdNa6tjZEAAAAAAAAgNaNFGwAAAAAAAGADEm0AAAAAAACADUi0AQAAAAAAADYg0QYAAAAAAADYgEQbAAAAAAAAYAMSbQAAAAAAAIANSLQBAAAAAAAANiDRBgAAAAAAANiARBsAAAAAAABgAxJtAAAAAAAAgA1ItAEAAAAAAAA2INEGAAAAAAAA2IBEGwAAAAAAAGADEm0AAAAAAACADUi0AQAAAAAAADYg0QYAAAAAAADYgEQbAAAAAAAAYAMSbQAAAAAAAIANSLQBAAAAAAAANiDRBgAAAAAAANiARBsAAAAAAABgAxJtAAAAAAAAgA1ItAEAAAAAAAA2INEGAAAAAAAA2IBEGwAAAAAAAGADEm0AAAAAAACADUi0AQAAAAAAADYg0QYAAAAAAADYgEQbAAAAAAAAYAMSbQAAAAAAAIANSLQBAAAAAAAANiDRBgAAAAAAANiARBsAAAAAAABgAxJtAAAAAAAAgA0eeqJt2rRpKly4sPz8/BQYGKjNmzcnWH7jxo0KDAyUn5+fihQpohkzZrhsnzNnjhwOR6zl1q1bKfkxAAAAAAAA8Ih7qIm2xYsXq0ePHho0aJD27NmjqlWrqkGDBgoLC4uz/LFjx9SwYUNVrVpVe/bs0cCBA9WtWzctW7bMpVymTJkUHh7usvj5+f0bHwkAAAAAAACPKK+H+eYTJkxQx44d1alTJ0lSUFCQVq9erenTp2v06NGxys+YMUMFChRQUFCQJKlkyZLauXOnxo8fr2bNmjnLORwO5c6d+1/5DAAAAAAAAID0EFu03blzR7t27VLdunVd1tetW1dbt26Nc59t27bFKl+vXj3t3LlTd+/eda67fv26ChYsqHz58umFF17Qnj17Eozl9u3bunr1qssCAAAAAAAAuOOhJdouXLigqKgoBQQEuKwPCAhQREREnPtERETEWT4yMlIXLlyQJJUoUUJz5szRN998o4ULF8rPz09VqlTRn3/+GW8so0ePVubMmZ1L/vz5k/npAAAAAAAA8Kh56JMhOBwOl9fGmFjrEit///qKFSuqdevWevrpp1W1alV9+eWXKl68uKZMmRJvnQMGDNCVK1ecy8mTJ5P6cQAAAAAAAPCIemhjtOXIkUOenp6xWq+dO3cuVqu1GLlz546zvJeXl7Jnzx7nPh4eHqpQoUKCLdp8fX3l6+vr5icAAAAAAAAA/s9Da9Hm4+OjwMBArVmzxmX9mjVrVLly5Tj3qVSpUqzyP/zwg8qXLy9vb+849zHGaO/evcqTJ489gQMAAAAAAABxeKhdR3v16qXPPvtMISEhOnTokHr27KmwsDB16dJF0r0unW3btnWW79Kli06cOKFevXrp0KFDCgkJUXBwsHr37u0sM2zYMK1evVpHjx7V3r171bFjR+3du9dZJwAAAAAAAJASHlrXUUlq2bKlLl68qOHDhys8PFylS5dWaGioChYsKEkKDw9XWFiYs3zhwoUVGhqqnj176pNPPlHevHk1efJkNWvWzFnm8uXLevPNNxUREaHMmTOrXLly2rRpk5599tl//fMBAAAAAADg0eEwMbMJwOnq1avKnDmzrly5ohoffpXkenaNa5t4IQAAAAAAAKRq9+eKMmXKFG+5hz7rKAAAAAAAAPC/gEQbAAAAAAAAYAMSbQAAAAAAAIANSLQBAAAAAAAANiDRBgAAAAAAANiARBsAAAAAAABgAxJtAAAAAAAAgA28HnYAj5LAPvOSvO+ucW1tjAQAAAAAAAB2I9GWRpG0AwAAAAAASF3oOgoAAAAAAADYgEQbAAAAAAAAYAMSbQAAAAAAAIANSLQBAAAAAAAANiDRBgAAAAAAANiARBsAAAAAAABgAxJtAAAAAAAAgA1ItAEAAAAAAAA2INEGAAAAAAAA2IBEGwAAAAAAAGADEm0AAAAAAACADUi0AQAAAAAAADYg0QYAAAAAAADYgEQbAAAAAAAAYAOvpOy0du1arV27VufOnVN0dLTLtpCQEFsCAwAAAAAAANIStxNtw4YN0/Dhw1W+fHnlyZNHDocjJeICAAAAAAAA0hS3E20zZszQnDlz1KZNm5SIBwAAAAAAAEiT3B6j7c6dO6pcuXJKxAIAAAAAAACkWW4n2jp16qQFCxakRCwAAAAAAABAmuV219Fbt25p5syZ+vHHH1WmTBl5e3u7bJ8wYYJtwQEAAAAAAABphduJtn379qls2bKSpP3797tsY2IEAAAAAAAAPKrcTrStX78+JeIAAAAAAAAA0jS3x2i736lTp3T69Gm7YgEAAAAAAADSLLcTbdHR0Ro+fLgyZ86sggULqkCBAsqSJYs+/PBDRUdHp0SMAAAAAAAAQKrndtfRQYMGKTg4WB999JGqVKkiY4x++uknDR06VLdu3dLIkSNTIk4AAAAAAAAgVXM70TZ37lx99tlneumll5zrnn76aT322GPq2rUriTYAAAAAAAA8ktzuOnrp0iWVKFEi1voSJUro0qVLtgQFAAAAAAAApDVuJ9qefvppTZ06Ndb6qVOn6umnn7YlKAAAAAAAACCtcbvr6NixY9WoUSP9+OOPqlSpkhwOh7Zu3aqTJ08qNDQ0JWIEAAAAAAAAUj23W7RVr15df/zxh5o0aaLLly/r0qVLatq0qQ4fPqyqVaumRIwAAAAAAABAqud2izZJyps3L5MeAAAAAAAAAPexlGjbt2+fSpcuLQ8PD+3bty/BsmXKlLElMAAAAAAAACAtsZRoK1u2rCIiIpQrVy6VLVtWDodDxphY5RwOh6KiomwPEgAAAAAAAEjtLCXajh07ppw5czr/DQAAAAAAAMCVpURbwYIFnf8+ceKEKleuLC8v110jIyO1detWl7JI/QL7zEvyvrvGtbUxEgAAAAAAgLTN7VlHa9SooUuXLsVaf+XKFdWoUcOWoAAAAAAAAIC0xu1EmzFGDocj1vqLFy8qQ4YMtgQFAAAAAAAApDWWuo5KUtOmTSXdm/CgXbt28vX1dW6LiorSvn37VLlyZfsjRJpBN1QAAAAAAPAos5xoy5w5s6R7Ldr8/f2VLl065zYfHx9VrFhRnTt3tj9CAAAAAAAAIA2wnGibPXu2JKlQoULq3bs33UQBAAAAAACA+1hOtMUYMmRISsQBAAAAAAAApGluJ9okaenSpfryyy8VFhamO3fuuGzbvXu3LYEBAAAAAAAAaYnbs45OnjxZ7du3V65cubRnzx49++yzyp49u44ePaoGDRqkRIwAAAAAAABAqud2om3atGmaOXOmpk6dKh8fH/Xt21dr1qxRt27ddOXKlZSIEQAAAAAAAEj13E60hYWFqXLlypKkdOnS6dq1a5KkNm3aaOHChfZGBwAAAAAAAKQRbifacufOrYsXL0qSChYsqO3bt0uSjh07JmOMvdEBAAAAAAAAaYTbibaaNWtq5cqVkqSOHTuqZ8+eqlOnjlq2bKkmTZq4HcC0adNUuHBh+fn5KTAwUJs3b06w/MaNGxUYGCg/Pz8VKVJEM2bMiLfsokWL5HA49PLLL7sdFwAAAAAAAOAOt2cdnTlzpqKjoyVJXbp0UbZs2bRlyxa9+OKL6tKli1t1LV68WD169NC0adNUpUoVffrpp2rQoIEOHjyoAgUKxCp/7NgxNWzYUJ07d9b8+fP1008/qWvXrsqZM6eaNWvmUvbEiRPq3bu3qlat6u5HBAAAAAAAANzmdqLNw8NDHh7/1xCuRYsWatGiRZLefMKECerYsaM6deokSQoKCtLq1as1ffp0jR49Olb5GTNmqECBAgoKCpIklSxZUjt37tT48eNdEm1RUVF6/fXXNWzYMG3evFmXL19OUnwAAAAAAACAVW4n2iTp1q1b2rdvn86dO+ds3RbjpZdeslTHnTt3tGvXLvXv399lfd26dbV169Y499m2bZvq1q3rsq5evXoKDg7W3bt35e3tLUkaPny4cubMqY4dOybaFVWSbt++rdu3bztfX7161dJnAAAAAAAAAGK4nWj7/vvv1bZtW124cCHWNofDoaioKEv1XLhwQVFRUQoICHBZHxAQoIiIiDj3iYiIiLN8ZGSkLly4oDx58uinn35ScHCw9u7da+0DSRo9erSGDRtmuTwAAAAAAADwILcnQ/jvf/+rV155ReHh4YqOjnZZrCbZ7udwOFxeG2NirUusfMz6a9euqXXr1po1a5Zy5MhhOYYBAwboypUrzuXkyZNufAIAAAAAAAAgCS3azp07p169esVqWeauHDlyyNPTM1brtXPnzsVbd+7cueMs7+XlpezZs+vAgQM6fvy4XnzxRef2mK6tXl5eOnz4sIoWLRqrXl9fX/n6+ibr8wAAAAAAAODR5naLtubNm2vDhg3JfmMfHx8FBgZqzZo1LuvXrFmjypUrx7lPpUqVYpX/4YcfVL58eXl7e6tEiRL67bfftHfvXufy0ksvqUaNGtq7d6/y58+f7LgBAAAAAACAuLjdom3q1Kl65ZVXtHnzZj311FPOCQhidOvWzXJdvXr1Ups2bVS+fHlVqlRJM2fOVFhYmLp06SLpXpfO06dPa968eZKkLl26aOrUqerVq5c6d+6sbdu2KTg4WAsXLpQk+fn5qXTp0i7vkSVLFkmKtR4AAAAAAACwk9uJtgULFmj16tVKly6dNmzY4DJmmsPhcCvR1rJlS128eFHDhw9XeHi4SpcurdDQUBUsWFCSFB4errCwMGf5woULKzQ0VD179tQnn3yivHnzavLkyWrWrJm7HwMAAAAAAACwlduJtvfff1/Dhw9X//795eHhds/TWLp27aquXbvGuW3OnDmx1lWvXl27d++2XH9cdQAAAAAAAAB2cztTdufOHbVs2dKWJBsAAAAAAADwv8LtbNkbb7yhxYsXp0QsAAAAAAAAQJrldtfRqKgojR07VqtXr1aZMmViTYYwYcIE24IDAAAAAAAA0gq3E22//fabypUrJ0nav3+/y7b7J0YAAAAAAAAAHiVuJdqioqI0dOhQPfXUU8qWLVtKxQQAAAAAAACkOW6N0ebp6al69erpypUrKRUPAAAAAAAAkCa5PRnCU089paNHj6ZELAAAAAAAAECa5XaibeTIkerdu7e+/fZbhYeH6+rVqy4LAAAAAAAA8ChyezKE+vXrS5Jeeukll8kPjDFyOByKioqyLzoAAAAAAAAgjXA70bZ+/fqUiAMAAAAAAABI09xOtFWvXj0l4gAAAAAAAADSNLcTbZJ0+fJlBQcH69ChQ3I4HHryySfVoUMHZc6c2e74AAAAAAAAgDTB7ckQdu7cqaJFi2rixIm6dOmSLly4oAkTJqho0aLavXt3SsQIAAAAAAAApHput2jr2bOnXnrpJc2aNUteXvd2j4yMVKdOndSjRw9t2rTJ9iDx6AnsMy/J++4a19bGSAAAAAAAAKxxO9G2c+dOlySbJHl5ealv374qX768rcEBAAAAAAAAaYXbXUczZcqksLCwWOtPnjwpf39/W4ICAAAAAAAA0hq3E20tW7ZUx44dtXjxYp08eVKnTp3SokWL1KlTJ7Vq1SolYgQAAAAAAABSPbe7jo4fP14Oh0Nt27ZVZGSkJMnb21tvv/22PvroI9sDBAAAAAAAANICS4m2ffv2qXTp0vLw8JCPj48mTZqk0aNH68iRIzLGqFixYkqfPn1KxwoAAAAAAACkWpa6jpYrV04XLlyQJBUpUkQXL15U+vTp9dRTT6lMmTIk2QAAAAAAAPDIs5Roy5Ili44dOyZJOn78uKKjo1M0KAAAAAAAACCtsdR1tFmzZqpevbry5Mkjh8Oh8uXLy9PTM86yR48etTVAAAAAAAAAIC2wlGibOXOmmjZtqr/++kvdunVT586d5e/vn9KxAQAAAAAAAGmG5VlH69evL0natWuXunfvTqINAAAAAAAAuI/lRFuM2bNnp0QcQIoJ7DMvyfvuGtfWxkgAAAAAAMD/MrcTbTdu3NBHH32ktWvX6ty5c7EmRmCMNgAAAAAAADyK3E60derUSRs3blSbNm2ckyMAAAAAAAAAjzq3E23fffedVq1apSpVqqREPAAAAAAAAECa5OHuDlmzZlW2bNlSIhYAAAAAAAAgzXK7RduHH36oDz74QHPnzlX69OlTIiYgVWJSBQAAAAAAkBC3E20ff/yxjhw5ooCAABUqVEje3t4u23fv3m1bcAAAAAAAAEBa4Xai7eWXX06BMAAAAAAAAIC0ze1E25AhQ1IiDgAAAAAAACBNc3syBAAAAAAAAACxWW7RljVrVjkcjkTLXbp0KVkBAQAAAAAAAGmR5URbUFBQCoYBAAAAAAAApG2WE21vvPFGSsYBAAAAAAAApGmM0QYAAAAAAADYgEQbAAAAAAAAYAMSbQAAAAAAAIANSLQBAAAAAAAANiDRBgAAAAAAANjA0qyjvXr1slzhhAkTkhwMAAAAAAAAkFZZSrTt2bPHUmUOhyNZwQAAAAAAAABplaVE2/r161M6DgAAAAAAACBNY4w2AAAAAAAAwAaWWrQ9aMeOHVqyZInCwsJ0584dl23Lly+3JTAAAAAAAAAgLXG7RduiRYtUpUoVHTx4UCtWrNDdu3d18OBBrVu3TpkzZ06JGAEAAAAAAIBUz+1E26hRozRx4kR9++238vHx0aRJk3To0CG1aNFCBQoUSIkYAQAAAAAAgFTP7UTbkSNH1KhRI0mSr6+vbty4IYfDoZ49e2rmzJm2BwgAAAAAAACkBW4n2rJly6Zr165Jkh577DHt379fknT58mXdvHnT3ugAAAAAAACANMLtyRCqVq2qNWvW6KmnnlKLFi3UvXt3rVu3TmvWrFGtWrVSIkYAAAAAAAAg1XM70TZ16lTdunVLkjRgwAB5e3try5Ytatq0qQYPHmx7gAAAAAAAAEBa4HaiLVu2bM5/e3h4qG/fvurbt6+tQQEAAAAAAABpjdtjtIWFhSW4uGvatGkqXLiw/Pz8FBgYqM2bNydYfuPGjQoMDJSfn5+KFCmiGTNmuGxfvny5ypcvryxZsihDhgwqW7asPv/8c7fjAgAAAAAAANzhdou2QoUKyeFwxLs9KirKcl2LFy9Wjx49NG3aNFWpUkWffvqpGjRooIMHD6pAgQKxyh87dkwNGzZU586dNX/+fP3000/q2rWrcubMqWbNmkm61+Ju0KBBKlGihHx8fPTtt9+qffv2ypUrl+rVq+fuxwUAAAAAAAAscTvRtmfPHpfXd+/e1Z49ezRhwgSNHDnSrbomTJigjh07qlOnTpKkoKAgrV69WtOnT9fo0aNjlZ8xY4YKFCigoKAgSVLJkiW1c+dOjR8/3ploe/7551326d69u+bOnastW7aQaAMAAAAAAECKcTvR9vTTT8daV758eeXNm1fjxo1T06ZNLdVz584d7dq1S/3793dZX7duXW3dujXOfbZt26a6deu6rKtXr56Cg4N19+5deXt7u2wzxmjdunU6fPiwxowZE28st2/f1u3bt52vr169aukzAAAAAAAAADHcHqMtPsWLF9eOHTssl79w4YKioqIUEBDgsj4gIEARERFx7hMRERFn+cjISF24cMG57sqVK8qYMaN8fHzUqFEjTZkyRXXq1Ik3ltGjRytz5szOJX/+/JY/BwAAAAAAACAloUXbg629jDEKDw/X0KFD9fjjj7sdwIPjvRljEhwDLq7yD6739/fX3r17df36da1du1a9evVSkSJFYnUrjTFgwAD16tXL+frq1ask25CiAvvMS/K+u8a1tTESAAAAAABgF7cTbVmyZIkz2ZU/f34tWrTIcj05cuSQp6dnrNZr586di9VqLUbu3LnjLO/l5aXs2bM713l4eKhYsWKSpLJly+rQoUMaPXp0vIk2X19f+fr6Wo4dAAAAAAAAeJDbibb169e7vPbw8FDOnDlVrFgxeXlZr87Hx0eBgYFas2aNmjRp4ly/Zs0aNW7cOM59KlWqpJUrV7qs++GHH1S+fPlY47PdzxjjMgYbAAAAAAAAYDe3E20Oh0OVK1eOlVSLjIzUpk2bVK1aNct19erVS23atFH58uVVqVIlzZw5U2FhYerSpYuke106T58+rXnz7nWz69Kli6ZOnapevXqpc+fO2rZtm4KDg7Vw4UJnnaNHj1b58uVVtGhR3blzR6GhoZo3b56mT5/u7kcFAAAAAAAALHM70VajRg2Fh4crV65cLuuvXLmiGjVqKCoqynJdLVu21MWLFzV8+HCFh4erdOnSCg0NVcGCBSVJ4eHhCgsLc5YvXLiwQkND1bNnT33yySfKmzevJk+erGbNmjnL3LhxQ127dtWpU6eULl06lShRQvPnz1fLli3d/agAAAAAAACAZW4n2uKbrODixYvKkCGD2wF07dpVXbt2jXPbnDlzYq2rXr26du/eHW99I0aM0IgRI9yOAwAAAAAAAEgOy4m2pk2bSrrXdbRdu3YukwdERUVp3759qly5sv0RAgAAAAAAAGmA5URb5syZJd1r0ebv76906dI5t/n4+KhixYrq3Lmz/RECAAAAAAAAaYDlRNvs2bMlSYUKFVLv3r2T1E0UAAAAAAAA+F/l9hhtQ4YMSYk4AAAAAAAAgDTN7URb4cKF45wMIcbRo0eTFRAAAAAAAACQFrmdaOvRo4fL67t372rPnj36/vvv1adPH7viAgAAAAAAANIUtxNt3bt3j3P9J598op07dyY7IAAAAAAAACAtcjvRFp8GDRpowIABzkkTAPw7AvvMS/K+u8a1TbG6AAAAAAB41HjYVdHSpUuVLVs2u6oDAAAAAAAA0hS3W7SVK1fOZTIEY4wiIiJ0/vx5TZs2zdbgAAAAAAAAgLTC7UTbyy+/7PLaw8NDOXPm1PPPP68SJUrYFRcAAAAAAACQpridaBsyZEhKxAEAAAAAAACkabaN0QYAAAAAAAA8yiy3aPP09LRULioqKsnBAAAAAAAAAGmV5USbMUYFCxbUG2+8oXLlyqVkTADSuMA+85K8765xbW2MBAAAAACAf4/lRNvPP/+skJAQTZo0SYULF1aHDh30+uuvK2vWrCkZHwAAAAAAAJAmWB6jrUKFCpo+fbrCw8PVq1cvrVixQvny5dOrr76qNWvWpGSMAAAAAAAAQKrn9mQIfn5+at26tdauXav9+/fr3Llzql+/vi5dupQS8QEAAAAAAABpguWuo/c7deqU5syZozlz5uiff/5Rnz59lClTJrtjAwAAAAAAANIMy4m2O3fuaMWKFQoODtbmzZvVoEEDBQUFqWHDhvLwcLthHAAAAAAAAPA/xXKiLU+ePPL399cbb7yhadOmKVeuXJKk69evu5SjZRsAAAAAAAAeRZYTbX///bf+/vtvffjhhxoxYkSs7cYYORwORUVF2RogAAAAAAAAkBZYTrStX78+JeMAAAAAAAAA0jTLibbq1aunZBwAAAAAAABAmpakWUcB4N8S2GdekvfdNa6tjZEAAAAAAJAwpgsFAAAAAAAAbECiDQAAAAAAALABiTYAAAAAAADABiTaAAAAAAAAABtYmgyhadOmlitcvnx5koMBAAAAAAAA0ipLLdoyZ87sXDJlyqS1a9dq586dzu27du3S2rVrlTlz5hQLFAAAAAAAAEjNLLVomz17tvPf/fr1U4sWLTRjxgx5enpKkqKiotS1a1dlypQpZaIEAAAAAAAAUjm3x2gLCQlR7969nUk2SfL09FSvXr0UEhJia3AAAAAAAABAWuF2oi0yMlKHDh2Ktf7QoUOKjo62JSgAAAAAAAAgrbHUdfR+7du3V4cOHfTXX3+pYsWKkqTt27fro48+Uvv27W0PEAAAAAAAAEgL3E60jR8/Xrlz59bEiRMVHh4uScqTJ4/69u2r9957z/YAAQAAAAAAgLTA7USbh4eH+vbtq759++rq1auSxCQIAAAAAAAAeOS5PUabdG+cth9//FELFy6Uw+GQJJ05c0bXr1+3NTgAAAAAAAAgrXC7RduJEydUv359hYWF6fbt26pTp478/f01duxY3bp1SzNmzEiJOAEAAAAAAIBUze0Wbd27d1f58uX1999/K126dM71TZo00dq1a20NDgAAAAAAAEgr3G7RtmXLFv3000/y8fFxWV+wYEGdPn3atsAAAAAAAACAtMTtFm3R0dGKioqKtf7UqVPy9/e3JSgAAAAAAAAgrXE70VanTh0FBQU5XzscDl2/fl1DhgxRw4YN7YwNAAAAAAAASDPc7jo6ceJE1ahRQ08++aRu3bql1157TX/++ady5MihhQsXpkSMAAAAAAAAQKrndqItb9682rt3rxYuXKjdu3crOjpaHTt21Ouvv+4yOQIAAAAAAADwKHE70SZJ6dKlU4cOHdShQwe74wEAAAAAAADSJLcTbZ6enqpWrZqWLVumbNmyOdefPXtWefPmjXOiBABIDQL7zEvyvrvGtbUxEgAAAADA/yK3J0Mwxuj27dsqX7689u/fH2sbAAAAAAAA8ChyO9HmcDi0bNkyvfjii6pcubK+/vprl20AAAAAAADAoyhJLdo8PT01adIkjR8/Xi1bttSIESNozQYAAAAAAIBHWpImQ4jx5ptvqnjx4mrevLk2btxoV0wAAAAAAABAmuN2i7aCBQvK09PT+fr555/X9u3bderUKVsDAwAAAAAAANISt1u0HTt2LNa6YsWKac+ePTp79qwtQQEAAAAAAABpjdst2uLj5+enggUL2lUdAAAAAAAAkKZYatGWLVs2/fHHH8qRI4eyZs2a4Oyily5dsi04AAAAAAAAIK2wlGibOHGi/P39JUlBQUG2BjBt2jSNGzdO4eHhKlWqlIKCglS1atV4y2/cuFG9evXSgQMHlDdvXvXt21ddunRxbp81a5bmzZun/fv3S5ICAwM1atQoPfvss7bGDeDRFthnXpL33TWurY2RAAAAAABSC0uJtjfeeCPOfyfX4sWL1aNHD02bNk1VqlTRp59+qgYNGujgwYMqUKBArPLHjh1Tw4YN1blzZ82fP18//fSTunbtqpw5c6pZs2aSpA0bNqhVq1aqXLmy/Pz8NHbsWNWtW1cHDhzQY489ZlvsAAAAAAAAwP0sJdquXr1qucJMmTJZLjthwgR17NhRnTp1knSvtdzq1as1ffp0jR49Olb5GTNmqECBAs5WdSVLltTOnTs1fvx4Z6Ltiy++cNln1qxZWrp0qdauXau2bWlFAgAAAAAAgJRhKdGWJUuWBMdlkyRjjBwOh6Kioiy98Z07d7Rr1y7179/fZX3dunW1devWOPfZtm2b6tat67KuXr16Cg4O1t27d+Xt7R1rn5s3b+ru3bvKli1bvLHcvn1bt2/fdr52J7EIAMlBF1QAAAAA+N9hKdG2fv1629/4woULioqKUkBAgMv6gIAARURExLlPREREnOUjIyN14cIF5cmTJ9Y+/fv312OPPabatWvHG8vo0aM1bNiwJHwKAAAAAAAA4B5Libbq1aunWAAPtpSLaRnnTvm41kvS2LFjtXDhQm3YsEF+fn7x1jlgwAD16tXL+frq1avKnz+/pfgBAAAAAAAAyWKiLS43b95UWFiY7ty547K+TJkylvbPkSOHPD09Y7VeO3fuXKxWazFy584dZ3kvLy9lz57dZf348eM1atQo/fjjj4nG5OvrK19fX0txAwAAAAAAAHFxO9F2/vx5tW/fXt99912c262O0ebj46PAwECtWbNGTZo0ca5fs2aNGjduHOc+lSpV0sqVK13W/fDDDypfvrzL+Gzjxo3TiBEjtHr1apUvX95SPAAAAAAAAEByeLi7Q48ePfT3339r+/btSpcunb7//nvNnTtXjz/+uL755hu36urVq5c+++wzhYSE6NChQ+rZs6fCwsLUpUsXSfe6dN4/U2iXLl104sQJ9erVS4cOHVJISIiCg4PVu3dvZ5mxY8fq/fffV0hIiAoVKqSIiAhFRETo+vXr7n5UAAAAAAAAwDK3W7StW7dOX3/9tSpUqCAPDw8VLFhQderUUaZMmTR69Gg1atTIcl0tW7bUxYsXNXz4cIWHh6t06dIKDQ1VwYIFJUnh4eEKCwtzli9cuLBCQ0PVs2dPffLJJ8qbN68mT56sZs2aOctMmzZNd+7cUfPmzV3ea8iQIRo6dKi7HxcAAAAAAACwxO1E240bN5QrVy5JUrZs2XT+/HkVL15cTz31lHbv3u12AF27dlXXrl3j3DZnzpxY66pXr57g+xw/ftztGAAAAAAAAIDkcrvr6BNPPKHDhw9LksqWLatPP/1Up0+f1owZM5QnTx7bAwQAAAAAAADSArdbtPXo0UPh4eGS7nXHrFevnr744gv5+PjE2QINAAAAAAAAeBS4nWh7/fXXnf8uV66cjh8/rt9//10FChRQjhw5bA0OAAAAAAAASCvcTrQ9KH369HrmmWfsiAUAAAAAAABIs9xOtBljtHTpUq1fv17nzp1TdHS0y/bly5fbFhwAAAAAAACQVridaOvevbtmzpypGjVqKCAgQA6HIyXiAgAAAAAAANIUtxNt8+fP1/Lly9WwYcOUiAcAAAAAAABIkzzc3SFz5swqUqRISsQCAAAAAAAApFluJ9qGDh2qYcOG6Z9//kmJeAAAAAAAAIA0ye2uo6+88ooWLlyoXLlyqVChQvL29nbZvnv3btuCAwAAAAAAANIKtxNt7dq1065du9S6dWsmQwCAVCSwz7wk77trXFsbIwEAAACAR5PbibZVq1Zp9erV+s9//pMS8QAAAAAAAABpkttjtOXPn1+ZMmVKiVgAAAAAAACANMvtRNvHH3+svn376vjx4ykQDgAAAAAAAJA2ud11tHXr1rp586aKFi2q9OnTx5oM4dKlS7YFBwAAAAAAAKQVbifagoKCUiAMAAAAAAAAIG1zK9F29+5dbdiwQYMHD1aRIkVSKiYAwEPGDKYAAAAA4D63xmjz9vbWihUrUioWAAAAAAAAIM1yezKEJk2a6KuvvkqBUAAAAAAAAIC0y+0x2ooVK6YPP/xQW7duVWBgoDJkyOCyvVu3brYFBwAAAAAAAKQVbifaPvvsM2XJkkW7du3Srl27XLY5HA4SbQAAAAAAAHgkuZ1oO3bsWErEAQAAAAAAAKRpbo/Rdj9jjIwxdsUCAAAAAAAApFlJSrTNmzdPTz31lNKlS6d06dKpTJky+vzzz+2ODQAAAAAAAEgz3O46OmHCBA0ePFj//e9/VaVKFRlj9NNPP6lLly66cOGCevbsmRJxAgAAAAAAAKma24m2KVOmaPr06Wrbtq1zXePGjVWqVCkNHTqURBsAAAAAAAAeSW53HQ0PD1flypVjra9cubLCw8NtCQoAAAAAAABIa9xOtBUrVkxffvllrPWLFy/W448/bktQAAAAAAAAQFrjdtfRYcOGqWXLltq0aZOqVKkih8OhLVu2aO3atXEm4AAAj7bAPvOSvO+ucW0TLwQAAAAAqYTbLdqaNWumn3/+WTly5NBXX32l5cuXK0eOHPrll1/UpEmTlIgRAAAAAAAASPXcbtEmSYGBgZo/f77dsQAAAAAAAABpltst2gAAAAAAAADEZrlFm4eHhxwOR4JlHA6HIiMjkx0UAAAAAAAAkNZYTrStWLEi3m1bt27VlClTZIyxJSgAAAAAAAAgrbGcaGvcuHGsdb///rsGDBiglStX6vXXX9eHH35oa3AAAAAAAABAWpGkMdrOnDmjzp07q0yZMoqMjNTevXs1d+5cFShQwO74AAAAAAAAgDTBrUTblStX1K9fPxUrVkwHDhzQ2rVrtXLlSpUuXTql4gMAAAAAAADSBMtdR8eOHasxY8Yod+7cWrhwYZxdSQEASCmBfeYled9d49raGAkAAAAAxM1yoq1///5Kly6dihUrprlz52ru3Llxllu+fLltwQEAAAAAAABpheVEW9u2beVwOFIyFgAAAAAAACDNspxomzNnTgqGAQDAv4duqAAAAABSQpJmHQUAAAAAAADgikQbAAAAAAAAYAPLXUcBAEBsdEMFAAAAEIMWbQAAAAAAAIANSLQBAAAAAAAANiDRBgAAAAAAANiARBsAAAAAAABgAxJtAAAAAAAAgA2YdRQAgFSCGUwBAACAtI0WbQAAAAAAAIANaNEGAMD/GFrGAQAAAA8HiTYAABAvknYAAACAdXQdBQAAAAAAAGzw0BNt06ZNU+HCheXn56fAwEBt3rw5wfIbN25UYGCg/Pz8VKRIEc2YMcNl+4EDB9SsWTMVKlRIDodDQUFBKRg9AAAAAAAAcM9DTbQtXrxYPXr00KBBg7Rnzx5VrVpVDRo0UFhYWJzljx07poYNG6pq1aras2ePBg4cqG7dumnZsmXOMjdv3lSRIkX00UcfKXfu3P/WRwEAAAAAAMAj7qEm2iZMmKCOHTuqU6dOKlmypIKCgpQ/f35Nnz49zvIzZsxQgQIFFBQUpJIlS6pTp07q0KGDxo8f7yxToUIFjRs3Tq+++qp8fX3/rY8CAAAAAACAR9xDS7TduXNHu3btUt26dV3W161bV1u3bo1zn23btsUqX69ePe3cuVN3795Nciy3b9/W1atXXRYAAAAAAADAHQ8t0XbhwgVFRUUpICDAZX1AQIAiIiLi3CciIiLO8pGRkbpw4UKSYxk9erQyZ87sXPLnz5/kugAAAAAAAPBoeuiTITgcDpfXxphY6xIrH9d6dwwYMEBXrlxxLidPnkxyXQAAAAAAAHg0eT2sN86RI4c8PT1jtV47d+5crFZrMXLnzh1neS8vL2XPnj3Jsfj6+jKeGwAAKSywz7wk77trXNsUqcvOmAAAAICHlmjz8fFRYGCg1qxZoyZNmjjXr1mzRo0bN45zn0qVKmnlypUu63744QeVL19e3t7eKRovAABAQkjaAQAA4KEl2iSpV69eatOmjcqXL69KlSpp5syZCgsLU5cuXSTd69J5+vRpzZt378K1S5cumjp1qnr16qXOnTtr27ZtCg4O1sKFC5113rlzRwcPHnT++/Tp09q7d68yZsyoYsWK/fsfEgAAwE0k7QAAANKmh5poa9mypS5evKjhw4crPDxcpUuXVmhoqAoWLChJCg8PV1hYmLN84cKFFRoaqp49e+qTTz5R3rx5NXnyZDVr1sxZ5syZMypXrpzz9fjx4zV+/HhVr15dGzZs+Nc+GwAAAAAAAB4tDzXRJkldu3ZV165d49w2Z86cWOuqV6+u3bt3x1tfoUKFnBMkAAAAAAAAAP+Whz7rKAAAAAAAAPC/gEQbAAAAAAAAYAMSbQAAAAAAAIANSLQBAAAAAAAANiDRBgAAAAAAANiARBsAAAAAAABgAxJtAAAAAAAAgA1ItAEAAAAAAAA2INEGAAAAAAAA2IBEGwAAAAAAAGADEm0AAAAAAACADUi0AQAAAAAAADYg0QYAAAAAAADYgEQbAAAAAAAAYAMSbQAAAAAAAIANSLQBAAAAAAAANvB62AEAAAAg5QT2mZfkfXeNa2tjJAAAAP/7SLQBAAAgUSTsAAAAEkfXUQAAAAAAAMAGJNoAAAAAAAAAG5BoAwAAAAAAAGxAog0AAAAAAACwAYk2AAAAAAAAwAbMOgoAAIB/lZ0zmDIbKgAASE1o0QYAAAAAAADYgEQbAAAAAAAAYAO6jgIAAACiGyoAAEg+WrQBAAAAAAAANqBFGwAAAGAjWsYBAPDoItEGAAAApFLM0AoAQNpC11EAAAAAAADABiTaAAAAAAAAABuQaAMAAAAAAABswBhtAAAAANzCeG8AAMSNFm0AAAAAAACADWjRBgAAAOChoGUcAOB/DS3aAAAAAAAAABvQog0AAABAmkfrOABAakCiDQAAAADuQ9IOAJBUdB0FAAAAAAAAbECLNgAAAABIIbSOA4BHCy3aAAAAAAAAABuQaAMAAAAAAABsQNdRAAAAAEjl7OyCSndWAEg5tGgDAAAAAAAAbECLNgAAAABAktA6DgBckWgDAAAAADx0diXt6GYL4GGi6ygAAAAAAABgA1q0AQAAAACQwmgdBzwaSLQBAAAAAJCG/K93syUpibSMrqMAAAAAAACADWjRBgAAAAAA/ifROg7/NhJtAAAAAAAACaBrLKwi0QYAAAAAAJAGkbRLfRijDQAAAAAAALDBQ2/RNm3aNI0bN07h4eEqVaqUgoKCVLVq1XjLb9y4Ub169dKBAweUN29e9e3bV126dHEps2zZMg0ePFhHjhxR0aJFNXLkSDVp0iSlPwoAAAAAAECaROs4ezzUFm2LFy9Wjx49NGjQIO3Zs0dVq1ZVgwYNFBYWFmf5Y8eOqWHDhqpatar27NmjgQMHqlu3blq2bJmzzLZt29SyZUu1adNGv/76q9q0aaMWLVro559//rc+FgAAAAAAAB5BDzXRNmHCBHXs2FGdOnVSyZIlFRQUpPz582v69Olxlp8xY4YKFCigoKAglSxZUp06dVKHDh00fvx4Z5mgoCDVqVNHAwYMUIkSJTRgwADVqlVLQUFB/9KnAgAAAAAAwKPooXUdvXPnjnbt2qX+/fu7rK9bt662bt0a5z7btm1T3bp1XdbVq1dPwcHBunv3rry9vbVt2zb17NkzVpmEEm23b9/W7du3na+vXLkiSbp69aqibv/jzsdycfXqVZfXqbGu1BhTaq0rNcaUWutKjTGl1rpSY0ypta7UGFNqrSs1xpRa60qNMaXWulJjTKm1rtQYU2qtKzXGlFrrSo0xpda6UmNMqbWu1BhTaq0rNcaUWuuq9v7CJNezaUQrl9d21pVcMZ/RGJNwQfOQnD592kgyP/30k8v6kSNHmuLFi8e5z+OPP25Gjhzpsu6nn34yksyZM2eMMcZ4e3ubL774wqXMF198YXx8fOKNZciQIUYSCwsLCwsLCwsLCwsLCwsLCwtLvMvJkycTzHc99MkQHA6Hy2tjTKx1iZV/cL27dQ4YMEC9evVyvo6OjtalS5eUPXv2BPe7evWq8ufPr5MnTypTpkzxlkuMXfU8CnWlxphSa12pMabUWldqjCm11pUaY0qtdaXGmFJrXakxptRaV2qMKbXWlRpjSq11pcaYUmtdqTGm1FpXaowptdaVGmNKrXWlxphSa12pMabUWpfVeowxunbtmvLmzZtgfQ8t0ZYjRw55enoqIiLCZf25c+cUEBAQ5z65c+eOs7yXl5eyZ8+eYJn46pQkX19f+fr6uqzLkiWL1Y+iTJkyJfsLYmc9j0JdqTGm1FpXaowptdaVGmNKrXWlxphSa12pMabUWldqjCm11pUaY0qtdaXGmFJrXakxptRaV2qMKbXWlRpjSq11pcaYUmtdqTGm1FpXaowptdZlpZ7MmTMnWs9DmwzBx8dHgYGBWrNmjcv6NWvWqHLlynHuU6lSpVjlf/jhB5UvX17e3t4JlomvTgAAAAAAAMAOD7XraK9evdSmTRuVL19elSpV0syZMxUWFqYuXbpIutel8/Tp05o3b54kqUuXLpo6dap69eqlzp07a9u2bQoODtbChf83OF737t1VrVo1jRkzRo0bN9bXX3+tH3/8UVu2bHkonxEAAAAAAACPhoeaaGvZsqUuXryo4cOHKzw8XKVLl1ZoaKgKFiwoSQoPD1dYWJizfOHChRUaGqqePXvqk08+Ud68eTV58mQ1a9bMWaZy5cpatGiR3n//fQ0ePFhFixbV4sWL9dxzz9kev6+vr4YMGRKr2+nDqudRqCs1xpRa60qNMaXWulJjTKm1rtQYU2qtKzXGlFrrSo0xpda6UmNMqbWu1BhTaq0rNcaUWutKjTGl1rpSY0ypta7UGFNqrSs1xpRa60qNMaXWuuyMSZIcxiQ2LykAAAAAAACAxDy0MdoAAAAAAACA/yUk2gAAAAAAAAAbkGgDAAAAAAAAbECiDQAA4BEXGRn5sEMAACBFbNq0ifMc/lUk2gAAAP6HLVq0KMHtd+/edZnBPSnOnz+vu3fvJqsOAABSQo0aNXTp0qUUqfvAgQPat2+fczlw4ECKvA/SFhJtj4CrV68+7BAeqqZNm1paUoO9e/dqyZIl2rJli1LjhMAnT55Uhw4dLJffvXt3CkaTdGntqdaNGze0adOmhx0GbHbq1ClFR0cnWCYqKkpnz57VuXPnFBUV5fZ73H/8Dw0N1TfffONcVq1a5XZ98XH32BCfs2fPavjw4TZEZP/vZvfu3XrhhReSXU94eLj++9//2hCRde3atdPq1avj3BYVFaVXXnlFO3futFTXzJkzdfv2bUmSMUajRo1S1qxZlTt3bmXJkkW9evVK9Htt1fLly1WmTBlb6nLHuXPnEi2zefPmfyGSlPHZZ5/pjTfe0OzZsyVJixcvVsmSJVWkSBENGTLEcj0//PCDy7l0wYIFKlu2rDJkyKBixYpp8uTJtsf+v8quY6i7/vnnH3377bfO1wMGDFCvXr2cS58+fXTr1q1kv4+7x+Np06apdu3aatGihdatW+ey7cKFCypSpIjluuw8D9oZ16PM6vfhzp07Lq+PHDmiHj16qFGjRurUqZN27dpl+T3tvK/avHmzKlSo4HxdsWJFlStXTmXLllXZsmVVpkwZ/fjjj7a8F/cAaZiBJbNmzTJt27Y1ISEhxhhjFi1aZEqUKGEKFy5sPvjgg4cW17hx4xLcfuXKFfPcc88l+30uXbpk5s6da7n89u3bzcCBA02fPn3M6tWrk/3+ydGuXTuXxcfHxzRr1izWend8++23pmPHjqZPnz7m0KFDLtsuXbpkatSokWgdrVq1MlevXjXGGHPt2jVTt25d43A4jI+Pj3E4HKZ8+fLm77//diuulLZ3717j4eFhuby3t7cZPny4iYqKSsGo3Ofh4WHOnj37sMOwzN2/e3R0tDl69Ki5e/euMcaY27dvm0WLFpm5c+ea8+fPp1SYlp06dcpMmjTJvPPOO+a///2vmTx5sjl16pTl/cuWLWvKlSuX6JLa+fv7myNHjsS5bfny5aZy5crGx8fHeHh4GA8PD+Pj42MqV65sVqxYYan+lStXmrJlyzpfZ8yY0TgcDufi4eFhlixZYsdHcfs7mtL1JLWuH374wfTu3dsMGDDA+X9z6NAh07hxY+Ph4WHq1atnqZ4DBw6YqVOnmk8//dR5HD9//rzp0aOH8fPzMyVLlnQrruQKCgoyGTJkMFu3bnVZHxkZaV5++WUTEBAQ61wWn/uPnzNmzDAZMmQwH3/8sfnpp5/MlClTTObMmc2UKVMsxzZz5kzTvHlz06pVK7N9+3ZjjDFr1641ZcuWNenSpTNvvvmm5bo++eQTU6tWLfPKK6+YtWvXumw7f/68KVy4sKV6cubMGe9v4+bNm+bdd9813t7eluNKTSZOnGgyZMhgmjZtavLkyWNGjBhhsmfPbkaMGGGGDx9uMmfObD799FNLdd3/XVi6dKnx9PQ07777rvniiy/Me++9Z3x9fc2CBQtsi/3gwYOW/w/t9G+cU+089hljzF9//WXpWnTGjBnmhRdecL7OmDGjee6558zzzz9vnn/+eZM7d24zYcKEZMfjzuebNGmSSZ8+vXnnnXdM69atja+vrxk1apRze0REhOW67DwP2hnXvykqKsp88803pnHjxg87FCer34f7jzF79uwx6dOnN2XLljWdO3c2FSpUMD4+Pubnn3+29J4Oh8OcO3cuWXHHePXVV82kSZOcrzNmzGg2btxojh8/bo4dO2Z69uxpmjZtast7uXtssDNnMWzYMHPjxg239nFHdHR0itWdEt5++223jvkk2iyw86IkIe3atTOnT592ax8/Pz/nD+lB165dMxUrVrTlgt6dH/ny5cuNp6enyZAhg8mcObPx8PAwEydOTPJ7Hzx40ISEhDhvAg4dOmS6dOli2rdvH+tC2oqMGTPGe3NrxRdffGE8PT1No0aNzH/+8x/j5+dn5s+f79xu9UR7/8mjd+/epnDhwmbXrl3GGGN+++03U7JkSdOzZ0+3Yrtw4YJZt26duXjxojHm3o3FRx99ZIYNG2YOHjyY6P5ff/11gsvEiRPdOtivWrXK5MuXzzz77LPm8OHDbn2WB504ccLSYoXD4bAt0bZ9+3YTGhrqsm7u3LmmUKFCJmfOnKZz587m1q1byXoPd35/v//+uylYsKDx8PAwxYoVM0ePHjWBgYEmQ4YMJn369CZHjhzmjz/+cDuGP/74w4wbN86ZHPv444+T9Dv65JNPjK+vr3E4HCZLliwmc+bMxuFwGF9fX/PJJ59YqmPo0KHOZciQIcbHx8d069bNZf3QoUMt1dWgQQNz+fJl5+sRI0a4JLgvXLiQYkmR+I5FM2bMMD4+PqZLly5mxYoVZuvWreann34yK1asMF26dDG+vr5m5syZidb/4osvms8++yze9xszZoxp0KCBpVjtOjb8+uuvCS6LFy9+aIm2OXPmGIfDYbJnz24cDofJmTOn+fzzz42/v79p166d+e233yzVs3LlSucDE4fDYYoWLWrWrVtncuTIYZ5//nmzcuVKtz5HlixZTNasWRNdEvPBBx+YrFmzOj9HZGSkadq0qcmVK5c5cOCA5XjuP35WqFAh1k34rFmzTJkyZSzVNW7cOOPt7W0CAwNN+vTpTfr06c3IkSNN9uzZzdChQ926oLXzJnjcuHEmXbp05tVXX3WeT40xZtOmTaZo0aKmePHiZsuWLZbqOnnypMvn2LRpk3nttdfMf/7zH/P666/HSn4mJrnJxBIlSpgvvvjCGGPM7t27jZeXl8txIiQkxAQGBlqK5f7vQpUqVWLdxI0bN85UqFDBUl1W2JmMioiIMMOGDUu0nF3nVLuvrxJj9W9VtWpVs3z5cufrB88Tn3/+ualYseK/Fo8xxjz55JPO76gxxmzdutXkypXLDB482Bjj3m/ZzvOgnXHdL76H0VFRUZava+Pyxx9/mP79+5s8efIYPz8/S4m2Jk2aWFqSy+r34f5jzAsvvGCaN2/ukpxp3769qV+/vqX3dDgc5q233jI9e/ZMcLGiaNGiZtu2bc7XD36vdu/ebfLkyWOprsS489uxO2eR0g0TvL29Ld2fPuhhNShI6AF5XEi0WWDnRYkx8d9oeHt7mxUrVjhfW7FkyRLj5+cXq4XDtWvXTKVKlUzx4sVNREREovVcuXIlwWXz5s2Wf+Tly5c3HTt2dH75P/zwQ5M9e3ZL+z7ou+++Mz4+PiZbtmzGz8/PfPfddyZnzpymdu3aplatWsbLy8vtZFtyE23lypUzkydPdr5esmSJyZgxo/M7YfVEe//Jo1SpUmbx4sUu21etWmUef/xxy3H9/PPPzqRF1qxZzc6dO03hwoXN448/booVK2bSpUvnTOQlFJOHh4fL074HF3cvIi5fvmzeeOMNkyFDBpe/m7ti3vvB5f71np6eluuy66lW/fr1zUcffeR8vW/fPuPl5WU6depkPv74Y5M7d24zZMiQBOtI7AY6U6ZMlv/ujRs3Ni+99JLZt2+f6dGjh3nyySdN48aNzZ07d8zt27dN48aNTevWrd36jKNGjTJeXl7Gw8PD5M6d2wQEBBgPDw/j7e2daKva+3377bfG09PTvPfee+bMmTPO9WfOnDE9e/Y0Xl5eZtWqVW7FZkzyftMPXkQ8eBJNySfU8cVdtGhRl3PMg4KDg02RIkUSrb9gwYJmx44d8b7fvn37TM6cOS3FatexIaF67v89W2Hn78YYY55++mkzevRoY4wxixcvNg6HwzzzzDPmr7/+slyHMcZUrFjRdOvWzVy7ds18/PHHxuFwmOLFi5uNGze6VU+MOXPmOJfZs2cbPz8/M3bsWJf1c+bMsVTXf//7X5MnTx5z+PBh07x5c5MjRw6zb98+t+K5//iZI0eOWNcrR44cMRkzZrRUV4kSJUxwcLAxxpj169cbh8NhatWqlaTW3HbfBB88eNCUL1/e5MmTxyxZssR069bNeHl5mR49epibN29arqdSpUrOhzFfffWV8fDwMC+99JLp16+fadKkifH29racfLUjmZguXTqXm3dfX1+zf/9+5+s///zTZMmSxVI891/H5MqVK9Y1xuHDh03mzJkt1WXFw2jxatc51e7rq0mTJiW49O3b11J9AQEBLv//OXLkMMeOHXO+Pnz4sMmUKVOi9dh5PE6XLp1LDMYYs3//fhMQEGD69+/v1m/ZzvOgnXEZc+/e65VXXjF+fn4mV65c5oMPPjCRkZHO7Um5/rh586aZM2eOqVq1qvH29jYeHh5m0qRJ5tq1a5b2t6sXkF3fh/uPMfny5Yv1gGPv3r0mICDA0mdzOBymcuXKztaacS1WWoEac6+hy/Hjx52vly1b5tLy6/jx48bHx8dSXXb+duzOWdjVMCG+pKaHh4dp27atW0nOlGpQYIW79xsk2iyw86LEGHtvNIy59/Q4Xbp0Zt26dcaYe0m2KlWqmMcff9zlZtZKTPEt7sTk7+/v0nrp1q1bxtPTM0kZ5kqVKplBgwYZY4xZuHChyZo1qxk4cKBz+8CBA02dOnXcqjO5ibYMGTKYo0ePuqxbv3698ff3N9OnT3cr0Xb/zcqDLQqOHz9u/Pz8LMdVu3Zt06lTJ3P16lUzbtw4ky9fPtOpUyfn9o4dO5qXX345wTry5s2bYLe0PXv2JPkid8mSJcbT09NkypTJ7ZYYxtw7mca17Nmzx/Tr18+kS5fOraSBXU+1cufO7XIRN3DgQFOlShXn6y+//DLRFlHp06c37733Xqwb55hl2LBhlv/uOXPmNHv27DHGGHP9+nXjcDjM5s2bndu3bt1qChQoYKkuY4xZt26d8fDwMEOGDDGXLl1yrr948aIZPHiw8fT0tJxAqFatmvP3HJdBgwaZatWqWY4tRnJ+0w9eRDxY18NItPn5+Znff/893v0OHTpk6djg6+vrcqzasWOHuXPnjvP10aNHLV8I2nVsyJEjhwkODjbHjx+Pc1m1apXlv7edvxtj7v1/xPy9oqKijJeXl9mwYYPl/WNkzpzZeQ68e/eu8fT0jNXqNTmSew5r3bq18fPzizNJZoXD4TDz5s0zX3/9tcmfP7+zu2eM/fv3W7oxNyb29ZWPj0+s+qyy+ybYmHut/lq2bGk8PDxMxowZzaZNm9yOy9/f3xnXc8895/JgxhhjpkyZYrmrux3JxOzZs7u0IMiXL5/LDeOff/5pOVHqcDjM+vXrza+//horoWHMvWOV1bqscCfRZlfrWbvOqXZfXzkcDpM3b15TqFChOJe8efNaqs/K+cbX1zfReuw8HufPnz/O39qBAwdMQECAadOmjeW67DwP2hmXMcZ069bNFC9e3CxZssTMmjXLFCxY0DRq1Mjcvn3bGHPv9+xwOCzV9fPPP5vOnTubTJkymfLly5ugoCATERFhvLy83Gqx/KCknm/s+j54eHg475UKFiwY68HQ0aNHLd8r2dmbJWfOnGb9+vXxbl+/fr3JkSOHpbrs/O2kRM7CjoYJDofDlC1bNlZi0+FwmAoVKriV5EyJBgVWkWhLAXZelBhz76l5o0aNzKFDh5w3F8eOHTNeXl5mzZo1znXuGDNmjMmUKZNZv369+c9//mOKFi3q1phHmTJlMmPGjDEbNmyIc5k1a5blH3lcB7KkHqgzZcpk/vzzT2PM/9343P/E9LfffrP8JCO5scTIkyePS3PhGBs2bDAZM2Y0gwYNspxoi0n25MqVK1bLvJ07d1o+SBtz74lIzPf0zp07xsPDw2Xcgt27d5vHHnsswTpefPFF5wV7XPbu3Wv5pH+/X375xZQoUcKULFnSfPbZZ0lqiRGXNWvWmMDAQOPv72+GDBli+YmdnU+1fH19TVhYmPN1lSpVzIcffuh8fezYsUSPD5UrVzZBQUHxbnfn5uLBk2zGjBldWuSEhYVZumiO0aJFiwTHSOrcubN59dVXLdXl7++f4AX977//nqSbsv+1RFtgYKDp1atXvPv16tXL0hPJPHnymDVr1sS7ffXq1SZ37tyWYrXr2FCvXj2X30dS6zHG3t+NMYl/F5JTj7ut4hKSlLjuf4Dw3//+1/j6+pratWsn6eHCgw8IR44c6bJ91qxZlhNHdv3NjbH/JvjOnTtmwIABxtvb27Rq1cpkzZrV1KxZ0+2uXJkzZ3YmNHPlyhUrufnXX3+Z9OnTW6rLjmRilSpVzKJFi+LdvnLlSlO6dGlL8Tz44PjB3+OCBQvMk08+aakuK9z5Tdv1UNuuc6rd11eFChWK1RPiflYTd8WKFTNLly6Nd/vixYtN0aJFE63HzuNxq1atTPfu3ePctn//fpMzZ07Lddl5HrQzLmOMKVCggEuy5sKFC+a5554zdevWNbdu3XLr+sPT09P06NEj1jXWw0q02fV9iBlmJGvWrMbb29vlQYMx9/7/ChUqZCkmO7tBvvDCC6Z9+/bxbn/jjTdMo0aNLNVl52/H7pyFw+EwTz31VLLHRB41apQpXLhwrHvdpHw/7W5Q4A53fw9eD3syhrSgRIkS2rdvn0qWLCnp3sxA9/v9999VqFAhy/X98ssv6tu3r5o1a6b58+erXLlyzm158+ZVwYIF3Y6xb9+++vvvv1WrVi0VKlRIGzdu1GOPPWZ5/2eeeUaSVL169Ti3Z8mSxa3ZWlavXq3MmTM7X0dHR2vt2rXav3+/c91LL71kuT5J8vDwkJ+fn7JkyeJc5+/vrytXriS43zfffOPyOq5Y3Inn2Wef1XfffaeKFSu6rK9evbpWrlxpeVa6atWq6fDhw5KkJ598UseOHXPZHhoaqlKlSlmqS7o3M0+6dOkkSd7e3kqfPr1y5Mjh3J49e3ZdvHgxwTr69OmjGzduxLu9WLFiWr9+veWYIiMjNWTIEI0fP17vvPOORo0aJT8/P8v7x2fXrl3q37+/Nm/erE6dOik0NFS5cuVyq44VK1a4vU9cAgICdOzYMeXPn1937tzR7t27NWzYMOf2a9euydvbO8E6GjVqpMuXL8e7PVu2bGrbtq2lePLmzauwsDAVKFBAkjR27FiXz3n+/HllzZrVUl3SvePV559/Hu/2Nm3aWI4tOjo6wb+Ft7f3vz7brsPhkMPhiLXu33rvuHz88cdq1KiRvv/+e9WtW1cBAQFyOByKiIjQmjVrdOLECYWGhiZaf7Vq1TR58mTVrl07zu2TJ09WtWrVLMVq17HhrbfeSrCeAgUKOGdCTIydv5sY95+7knOuOHjwoCIiIiTdm+ns8OHDsT73vzmb5p49e1xeV6pUSZGRkS7rrX7vE5tRNHfu3Bo9erTl2D777DNlzJhR0r1zxpw5c1zOXZLUrVu3ROv5z3/+o2XLlqlq1aou65988kmtXbtWNWrUsBzT3r171aZNG924cUOrV69WjRo1dObMGXXq1ElPPfWUPv74Y3Xq1MlSXdWrV9fChQtVpkwZlStXThs2bHD5v1+/fr3l67UcOXLo5MmTLtecpUqV0rp161SzZk2dPn060TrGjBmjDBkyxLs9LCxMb731lqV4Hrxuifl/jHH37l3169fPUl2SlDVr1gS/h+7MFp49e3aNGTNGtWrVinP7gQMH9OKLLyZaj13nVLuvrwIDA7Vr1y61aNEizu0Oh8PS+bRhw4b64IMP1KhRo1jXZ//884+GDRumRo0aJVqPncfj/v37xzubZKlSpbR+/XotXbrUUl12ngftjEu6N0vp/fd82bNn15o1a1SvXj01bNhQn332meW6atasqeDgYJ07d05t2rRRvXr1/rVrmbjY9X148HqgaNGiLq+3b9+uJk2aWIopsd/DxYsX9fnnn6tHjx6J1tWrVy/Vrl1b2bNnV58+fZzHhHPnzmnMmDGaP3++fvjhB0tx2fnbsTtnIUn16tWLdWx314ABA1S7dm21bt1aL774okaPHp3ovVF8rl+/rmzZskmSMmTIoAwZMihPnjzO7fny5dPZs2eTFa9tUiTd9z9my5YtzsxpXD755BO3ZtiKERoaavLly2dGjRrlbK3lblb3wcEpfX19zbPPPuv2oJUzZ850mT3lQREREZYHF09o/Al3x6EoU6aM+e6775yvf/vtN+fYb8YYs3nz5kQH/rUzHmPutVy7f0yUB61fv97tWUzjcuTIEXPy5EnL5UuUKOHypODbb791GUdm+/btJl++fMmOyx1PPfWUKVy4cJK6X8Xlzz//NC1atDCenp6mVatWto3LlRxvvvmmqVSpktm0aZPp1auXyZ49u7PZvzHGzJ8/35QvX96W97LirbfeMrNmzYp3++jRo03Dhg0t15cuXboEv4cnT5603Gz/2WefTXD2so8//tg8++yzidbz4Fg0fn5+ZvDgwbHWW+FwOEzDhg2dx0ovLy9Tt25d5+uGDRv+6y3ajLnXErJv376mWrVqpnjx4qZ48eKmWrVqpl+/frFatMRn9+7dxtfX1zRv3tz88ssv5vLly+by5cvm559/Nk2bNjW+vr6JjtvojvuPzWmRXecKu4eHeFByW2WnJgULFoy361vMYnWWyV9//TXeyaGMudfixOp1jI+Pj+ncuXOcraRnzZplMmfObHkA9YMHD5rs2bObtm3bmg8//NBkzJjRtG7d2owcOdK0bdvW+Pr6mtmzZ1uqy+4WNalNfF2nktIK3q7Ws3afU+1y4MCBWF1173fnzh1LvWMiIiJM7ty5TYECBczYsWPNV199Zb7++mszZswYkz9/fpMnTx5LYz2nVv/2edAdTzzxRJzj0saMs/3000+79XsOCwszw4YNM4UKFTIBAQHOcSWTMth8jP+l882cOXNiTU4WHR1tvv/+e/PKK68YHx8ft3oSffLJJ86Z4WNa3Xl43JshPil5ATvYnbOws7utMfe+223btjVlypQx+/btM97e3m7nPooWLerSgm3atGnm6tWrzte7du2y3ErVXe7+HhzG/MvNB/5HnT592q0WZDHOnj2r9u3b69q1a9q+fbt+/fVXPfnkk5b3b9++vaVyVlsIpDYzZsxQ/vz5432aNmjQIJ09e9atpz7/q4YNG6YnnnhCr776apzbBw0apN9//13Lli1L1vssXbpUzZs3t1S2U6dOCgoKSvaTEEnq2rWrgoODVaNGDX300UcqW7Zskuvy8PBQRESELS3azp8/r6ZNm+qnn35SxowZNXfuXJena7Vq1VLFihU1cuTIZL+XHY4dOyY/Pz+Xpz8JSexvdfbsWeXNm1dRUVGJ1jV37ly9/fbbGj9+vN588015ed1rVB0ZGalPP/1Uffr00bRp09SuXbsE6ylcuHCi7+VwOHT06NFEyz3MY+jJkyeVN29eeXp62l53jK+//lqdOnXSpUuXXNZnzZpVn332mV5++WVL9SxatCjeY4t0r+VK8+bN9fXXXydYT5EiRbRjxw5lz57d0vsmx9mzZ/Xpp5/qgw8+SPH3ut+JEycslbPaer1Xr14urz/55BO1bt3apdW4JE2YMMFagDa4evWqMmXKJOle6+v7Wxp5enpaagFjt7179ybrvHC/7777Tg0aNIh3e1hYmDp27Kg1a9ZYqu/IkSN6//33tWrVKl2/fl2S5OXlpQoVKqhPnz6Wf4f79u3Trl274j1uHThwQEuXLtWQIUMs1ReX8PBwjRw5UlOnTk207NWrVy3VGfNdsUNkZKTz3JGQFStW6MaNG2rdunWc2//++2998803euONN5IVj7vn1NTo2LFjevvtt7VmzRpnqx+Hw6E6depo2rRpKlKkyEOO0NXy5cs1dOhQ7du3z1J5u86DdsfVrVs3hYeHa8mSJbG2Xbt2TXXq1NGOHTssXV89aM2aNQoJCdFXX32l/Pnzq3nz5mrevLmzB1N8HuwF1KpVKwUFBSkgIMBlfWItuzt06KBJkybJ39/f7djvV7ZsWXXq1Emvv/66W70xEnP8+HGFhIRozpw5On36tF5//XW1bdtWNWrUcOua7OTJk1q6dKn+/PNPSdLjjz+u5s2bK3/+/LbF+jB5enoqPDzclvul+y1atEg9evTQ+fPn9dtvv7mV++jSpYvKly8fb6vyjz76SJs3b9aqVavsCtfp7bff1ocffhir1X18SLRZ0L17d02aNCne7adPn1aNGjX0xx9/JPk9Jk+erPXr12vKlCnKly9fkutBbHYd7O3WsGFDLVy40HmzNHLkSL3zzjvOrrEXL15U1apVdfDgQVve7+bNm/L09JSvr2+C5SIjI3X48GF5e3urePHizvVff/21PvjgA/3++++6ffu2LTG5I6brcIkSJRIst3v37kTrmjt3rl599dVE/xbuuHLlijJmzBjrBH3p0iVlzJhRPj4+btW3d+9e/fnnn8qTJ4+qVKny0LoAeHh4aMSIEfEmS69du6YPPvjA8oVg7969NWHCBPn7+zu7ABw5ckTXr19Xt27dNHHiRNtifxjCw8O1du1aZcuWTbVr13b5f79x44Y+/vhjtxI/J06cUEREhBwOhwICApI0tMDNmze1evVqlwvBunXrJth97EF+fn76+uuvVa9evVjboqKi1KxZM+3YsSPRrmt2JrkT8+uvv+qZZ55J0k1KavL8888n+vt3OBxat25dvNsfTNbFx0qy7ttvv9XgwYOd3U79/f1dusQ5HA4tXrzY0gOZKVOm6N1337UUW2I8PDxUrlw5derUSa+99lqsRGRqYIzRuXPnFB0drRw5crjddWb37t2J3ihbcfDgQa1fv17e3t5q0aKFsmTJogsXLmjkyJGaMWOGChcubOnaw8PDI8HvpjFGDofDlt/gwYMHFRwcrPnz56eebkGy3t2sQ4cOluoLCQmx/N4///yzvvnmG929e1e1a9dW3bp1Le8bl0uXLumvv/6SdK8ra0z3LCseTNDEx+owLbNmzdIPP/wgb29vde/eXc8995zWrVun9957T4cPH1abNm306aefWo7PjvOg3XH9/fffOnPmTLzDxFy/fl27du2Kd1gfq+8xf/58hYSEaN++fYn+Fj08PBKt08pv2q4EzVtvvaXFixfr9u3bevnll9WpU6d4u4Mn5vbt21q+fLk+++wzbd26VQ0aNNBrr72mVq1aud3QxU5du3bV2LFjndfZn3/+uZo0aeJ8ffnyZb322muWhgx50LVr11y6zHp4eLjV+CElr9lOnjyp3bt3q3bt2m7/DhNi58OPGzduaNeuXZa7lz+IRJsFWbNmVc+ePeO8OTpz5oyef/555c6dW5s2bXoI0SXs0KFDatSokaWWHdHR0ZozZ46WL1+u48ePy+FwqHDhwmrevLnatGlj+Ubf7pNtYs6dO5fgAcDubLzVJx3unoQyZcqkvXv3Op8cutNSyC4HDx7UCy+84GyR0bhxY02fPl0tWrTQr7/+qk6dOql79+6Wn9TUrFkz0TIOh0Nr165NtNz9454lxMqT/JQ8qcUlse/oa6+9pk8//VT+/v66fv26mjVrpjVr1sjb21t3795VYGCg1qxZ4zI+YXzmzZtnKSar4z0UKlTI0m//wbF6ErJ9+3YtXLjQecFbvHhxvfrqq7HGPYxPzZo1tXz5ckt/D3cYY3Tx4kU5HI4ktbjasWOH6tatq+joaN29e1f58uXTihUrnBfR7vymJ06cqAkTJujMmTMuLQzy5s2r9957z9IYIu566qmnFBoaGufve9KkSRo0aJDWrFmjSpUqOddHRUWpefPm2rZtmzZs2JBoIjw1J9p27dql3r176+uvv47VAufKlSt6+eWXFRQUpKeffjrBev7880998MEH+vTTT+Os5+2339aIESP+1VYiD45PtmXLFgUGBjrH9ZQST9bFeOmll9S4cWN17NhR0r1E26+//ur8PGPHjtWGDRssHT+zZcumwMBAzZ49O9kPGbdt26aQkBB9+eWXunv3rpo2baqOHTu6NTZbjLFjx+rdd991/n02bdqk5557zvlw5tq1a+rXr5+mTZuWaF12tuL08fHR4MGDNWjQIEs3xHH59ttv1axZM929e9cZ36xZs9SiRQuVLl1a7733nuWxZjdu3GipXFITBdevX9eiRYsUHBysHTt2qGLFimrWrJl69uyZpPrsYozRDz/8oODgYOfx4vz58wnu4+HhoYIFC6pcuXIJjhW1YsUKSzGsWLFCr7zyivz8/OTl5aVr167p448/TpFzgxUPfh/jGiPOatJ1/PjxGjhwoMqUKaNDhw5JutcrY8KECXr33Xf1zjvvWG5RkpDo6GitWrVKwcHB+uqrr1JNXCnFrkS9FXae62/duqUlS5Zo9uzZ2rhxo/Lnz68OHTqoXbt2zrETrciRI4eefPJJtW7dWq+88oqzhZy3t7fbiTar9/xWEjR23g/u3btXgwYNcrbm8vf3182bN53bHQ6Htm3bpgoVKliK/8SJEypQoECs+4DIyEjdunXLlh5L7vrrr79UrFixf+W9kvvAlkSbBZs3b1b9+vU1duxYvfPOO8714eHhev7555UjRw798MMPlrOx/2YiyuoXxBijF198UaGhoXr66adVokQJGWN06NAh/fbbb3rppZcsnYQk+56GSFL69Ol14sQJ5cyZU5JUv359zZ4925mltnLwsfvGLuZi6Y033nCZyOJBjRs3TrSe++N68GbF3USbHS0WXnrpJd24cUM9e/bUF198ocWLF6tYsWJq3bq1evbs6XarwIQuhq9evaqFCxfq9u3b/3qLEztPanZ8R++Pp0+fPlq2bJmWLl2qZ555Rvv371eLFi1Uv359S61NEmpa73A4dOPGDUVGRqbpVj52/6YjIiLUt29fffPNN7p27Zqke9+JJk2aaPTo0bG6TMSnTp06KlCggGbNmqUbN26of//+Wrx4sdasWaNy5cpZ/l59+OGHzgv6evXqKSAgwNkSZvXq1Ro9erR69+6t999/P9mf/X4PHoMeNGTIEE2ZMkWbNm1S6dKlFRUVpRYtWmjLli1av369pYtUDw8PrVu3LtGWEnZMFODuBdJrr72mkiVLavDgwXFuHzVqlA4ePKj58+cnWM+bb76pLFmyaOzYsXFu79evn65evarp06dbiislutsm9n+dkEKFCmnp0qUqX758nHX99ttvqlWrls6dO5doXWfOnNGbb76pn376SZMnT1abNm3cjudB//zzj7788kvNnj1bmzdvVqFChdShQwe98cYblpN5dp4j7DxehYaG6q233lLevHn1+eefu7Q6t6pSpUp69tlnNXLkSM2cOVO9e/fW448/rlmzZiX5qb3dtmzZos8++0zLli1ztq7buHGjqlSpYrmO4cOHWyrnTgvj5HQ369q1qxYtWqQCBQqoQ4cOat26tVstxh5UoUIFPf3005oxY4a8vLw0YsQIBQUF6cKFC27VkxIt7aTkHWNKliypPn36qEOHDtqwYYNq1qypmjVraunSpbY8YPvzzz8VEhKiuXPn6u+//1a9evUs3ePYHZedPVvsesBjVy8gDw8PnT171nltbJdjx44pJCRE8+bN0+nTp1WrVi117Ngx3klB7pc1a1aVKVNGrVu3VsuWLZ1/p6Qk2hK6141JSjkcDksTuNh5P9ixY0cVK1ZMAwYMcNb16aef6rHHHpMxRiEhITLGJDjJ2f1CQ0N18eJFl3PzyJEj9eGHHyoyMlI1a9bU4sWLE+3Wa+dxxsPDQ4899phq1KjhXNyd4MGqZPeMsGVkuEfAt99+a3x9fc2CBQuMMcaEh4ebJ554wlSsWDHOwXITEt/gyEkdnD8hVqcEDgkJMf7+/mbdunWxtq1du9b4+/ubuXPn2hKTOx4chPHBQQgjIiISHczW4XCYc+fO2RbTL7/8Yrp06WKyZMliypUrZ6ZMmWIuXbrkdj1WPps734Pnn3/eZfHy8jLPPfecy7oaNWokWEdAQIBzUNi///7bOBwOM3PmTDc/WcLu3r1rgoKCTM6cOU2xYsXMwoUL3a7j119/NUuWLDFLly41v/76q9v72/m3t+s7GlNHqVKlzOLFi122r1q1yjz++OOW4onPmTNnzFtvvWW8vb1NvXr1klVXUv3666+WlsTYOTjrlStXTOHChU3OnDlNjx49zIwZM8z06dPNu+++a3LkyGEef/xxy8f4rFmzmsOHD7usGzNmjMmaNav55ZdfLH+v8uXLZ1asWBHv9uXLl5u8efNaiskdVgZ5/e9//2vy5MljDh8+bJo3b25y5Mhh9u3bZ/k9UnqigPtZPf/FKFKkSILfv3379lkamP+JJ54wv/zyS7zbd+7caYoXL245LrsHIzYmeQNc+/r6mqNHjzpf79ixw9y5c8f5+ujRo8bHx8etOmfPnm2yZs1qmjRpYnbt2uX2MSE+f/31lxk0aJDJnz+/8fLysjyBQUqeI5Lr8uXL5o033jAZMmQwkydPdnv/zJkzO49Td+/eNZ6eniY0NNS2+JJjzJgx5oknnjCPPfaY6d27t9m7d68xxiRpsrCyZcvGu5QrV86kT5/e0v/hrVu3zIIFC0zNmjWNn5+fadKkiVmyZEmSYoqpq3bt2iZ9+vTmlVdeMd9//72Jjo52qx5jjPH393c539y6dct4enqa8+fPu1WPw+EwhQoVMk2aNDEvv/xyvIu7knOMSZcunTlx4oTztY+Pj9m+fXuS6opx8+ZNM2fOHFO1alXj7e1tPDw8zKRJk9y6h7M7rgcn5vL390/ycaZz586mT58+8W7v27ev6dKli9sxJZXD4XBOEJDQklTR0dFmyZIlJlu2bJb/Rv/884+ZP3++qVGjhkmXLp1p2rSpWb58eZIG5I+ZVOPB5cyZM6Zfv34mXbp0plSpUpbqsvN888QTT5hNmzbFW9f27dtNgQIFLNVljDE1atQwU6dOdb7+6aefjIeHhxkxYoRZtmyZKVGihOnZs2ei9dh5nNm0aZP58MMPTa1atZzH8UKFCpkOHTqYzz//3Jw6dcry50vs+5kpU6ZkXZMmPpooJN2bejckJEQdOnTQ7du3NWbMGGXKlEmrV692u9lkdHS0y+vkPPWxy8KFCzVw4MA4u1jUrFlT/fv31xdffGG5u9m/yUq3tuLFiyda7sFBUuNToUIFVahQQRMnTtTSpUs1e/Zs9evXTy+++KI6duyoOnXqWI77wZiSMw7Xg9PC+/v7a8GCBW59r86dO+ec1CNLlixKnz59ssaGeNAXX3yhDz74QP/884+GDh3qMiC+Fb/88os6duyogwcPunSnK1WqlIKDgy03hf63Wfl/jSlz9uxZlS5d2mVbqVKlYk3RbdW1a9c0ZswYTZo0SaVKldLq1avd6kplZ1fUsmXLunQnifnM5r6G1VZbu167dk1+fn4JlrEyAPekSZPk6empAwcOxHry+v7776tKlSqaPHmyBg4cmGhd0r0uDvfr27evPDw8VLduXcstAi5evKgnnngi3u3FixfX33//bakuu02ZMkWXL1/W008/rYwZM2rt2rV66qmn3Krj559/tuUpd2KteBPryvWg06dPJ/gUP2PGjAoPD0+0nhMnTiTYeilHjhxJ/j2nBtmyZdORI0eck5LEtGyL8eeff7rdUqddu3bKly+f6tevr6+//to5tpdJ5hhfRYsWVf/+/ZU/f34NHDhQq1evTlI9yXXw4EFFREQkWMZqK87MmTNrzpw5euGFF/Tqq6/q/fffj3Ns0PhcvXrV2WLGy8tL6dKlS1LLOMm+oTRiDBw4UP369dPw4cOTPUlMzBiCD9q7d6/69++v/fv3q3PnzonW89hjjzm7my1dutTZcqNVq1Zux+Tr66tWrVqpVatWOnHihObMmaOuXbvq7t27OnjwoFv3E9evX3dpReXr66t06dLp6tWrbnVf7NKlixYtWqSjR4/a0tLODrdu3XI5v/v4+CT5nPHLL7/os88+0+LFi1W8eHG1bt1aS5YsUb58+VS7dm23/uZ2xiUpVtfaB1+7Y9OmTQm2UmrRooVee+01t2NKjmHDhqXIWJnr16/X7NmztXz5cnl5eVn6HUv3xpp9/fXX9frrr+vIkSOaPXu2unXrpsjISI0cOVLt2rVTzZo1LR17Hvxc0dHRCgkJ0bBhw+Th4aFPPvkk2ROtJMXJkyddutMOHz7c5XiQJ08et8a43L9/vz7++GPn66VLl6pOnToaNGiQpHt/0+7duyfa48bO40zVqlVVtWpVvf/++7p7965z6JINGzY4e0oVK1ZMhw8fTrSu27dv6+233473OvbEiROWhy6KC4k2N7z22mu6fPmyOnbsqGeeeUZr1qyxdSalh2nfvn3xdnORpAYNGmjy5MmW6orvxjxz5sx64oknEh3DJyWkxMHez89PrVu3VuvWrXXs2DF17NhR9evX1/nz5y0dPIwxateunXPMl1u3bqlLly7OLsgPY8IBh8Ph0hzaw8PD7cGa4/L999+rf//+OnbsmHr37q1evXq5PfDlwYMHVatWLZUsWVLz589XyZIlnd2bJ06cqFq1amn79u0PbTDT5Bo8eLDSp0/vbEJ+/+e4cOGC2wn9O3fuaOrUqRo1apRy5Mih2bNnW54t9n7du3ePd9v9XVGtJNruH8fNGKPSpUsrNDQ0SQP8J3Rj6M7N+apVqzRw4MA4L5Zz5cqlAQMGaNasWZYSbaVLl9bWrVtj3TD37t1bxhjLN2Yx3brmzJkTKxEdGRmpUaNG6dlnn7VUl13uT2plyZJFxhiVLVtWc+bMcSlnpXtzgQIFbOlGF9+N9P3c6QqXM2dOHT58ON5ZbX///XdLN7CZM2fWkSNH4v1e//XXX25fO9iZqEmuatWqafLkyapdu3ac2ydPnux2F8QJEyZo8ODBat26tQYPHuzWA5j4bNy4USEhIVq2bJk8PT3VokUL57hy/7ZatWrFeQOb1GTijh07NHjwYBUvXlzvvfee23+v+79PxhgdPnzYZUILydr3yRhjaSgNq4YPH645c+bo888/V6tWrdSmTZtYD56S6tixYxo8eLAWL16spk2b6sCBA3r88ccT3S8qKsr5YNTOGaJj6jTGxHoAb9Xq1atdrm2jo6O1du1a7d+/37kusWFopk2bpokTJ2r58uUKCQnRgAED1KhRI3Xs2FF169Z9aBMxffbZZ87rnsjISM2ZMyfW8bdbt26J1lO5cmW9++67+uWXXxJ8gPVvx2U3Ox/w2PV//uqrr9o2xEdYWJjmzJmjOXPm6Pjx46pataqmTZumV155xWWsUauKFi2qESNGaPjw4Vq9erWCg4P1wgsvKGPGjLp48aJbdS1fvlwDBw7U+fPnNWDAAL377rtuT7b2wQcfKH369JLuXb+PHDnS+du+f4y1xPj6+urUqVPO648Hh/A5efKk832suHbtmsuwFVu2bHG5lyhVqpTOnDmTaD0pdZzx9vZWtWrVVKFCBVWqVEmrV6/WrFmznJO6JKZs2bLKnz9/vEnRX3/9lURbSitXrpzLF8Db21uXL1+O1SrEymyHdsuaNWuCX04rfcOle08/ExqHKCAgwHILivhuzK9fv67o6Gg1bNhQCxYssNT//8FWX3G1ArPCzoP9/U6dOuU88P/zzz/q06eP5RuoB3/UcU1B/2+3IDTGuLT+u379usqVKxdrLAKrrf9++eUX9evXT9u3b1eXLl30448/Jnmg2CFDhqhOnTpatmyZy3egXLlyatWqlZo2baqhQ4fqyy+/tFSfXSc1O76j1apVcz55efLJJ2NNLBAaGhrvrFQPMsZo3rx5+uCDD5xJmY4dOyb5BiG+3314eLiGDRumkJAQy604H0w8OBwO5cuXL0mJtqVLl9ry1P2PP/5Q5cqV491euXJl9e7d21Jdbdu21caNG9WlS5dY2/r06SNjjKVxuaZMmaK6desqV65cql69ugICAuRwOBQREaFNmzbJ19dXa9assRSTXR5MalWqVEmRkZEu6//tG7IHW/EmV+3atTVy5EjVr18/1jZjjEaNGhVvcul+1apV05QpU+KdDGby5MmqWrWqW7ElN1Gzb98+l9fGGP3++++6fv26y3oryZV+/fqpUqVKeuWVV9S3b19n0vvw4cMaM2aMfvzxR23dutXKx9LRo0fVtm1bHTlyRAsWLEh0bNPEnDx50nlOPnbsmCpXrqwpU6aoRYsWbj/cSeiGOmYsR6vsasUZGRmpIUOGaPz48XrnnXc0atSoRFv2xuXB71PM5AfuJv5+/vlnhYSEaNKkSSpcuLA6dOig119/PdHxeuIzcOBADRw40JkkrVixoooWLSpjTJJb8V64cEHDhg3TzJkz9Z///Edbt251q/V7eHi4li1bpuDgYHXv3l0NGjRQ69atk3S8i5n1MCQkRFu2bNELL7ygqVOnqn79+kma3CKum8O33nrL+W+r/492trS7/72Tek6IGes0Ru7cuWO11nI4HJYSWjVr1lRwcLDOnTunNm3aqF69eqkirpiydvVssfMBjx29gOy6HliwYIFmz56t9evXKyAgQG3btnWOQ2YHDw8PNWjQQA0aNNCFCxcs9+KQ7j3M6devn3777Td1795d/fr1S1KjjvvvAaR7150PTmJo9eFVuXLl9NVXX8U7nuXy5cvdeiiSN29eHTp0SAUKFND169f166+/auLEic7tFy9etJy4s/M4c+vWLW3dulXr16/Xhg0btGPHDhUuXFjVq1fX9OnTLffGatSokS5fvhzv9mzZsiXrXpzJECywc7bDuCSn6+jcuXMtlUus+aqnp6ciIiLivRC0YwbM6Oho7dq1S506dVKdOnU0fvz4RPfx8PBQ5syZnQfsy5cvK1OmTM4LEmOMrl69anmg+bgcOXJEnTt3tjTbmnQvKbNixQoFBwdr8+bNatCggTp06KCGDRsmeRawlJCU75Vd36cYHh4eSpcund56660EB6q0clGSM2dOfffdd7G6KcXYsWOHGjZsaKm72PPPP2/pIsDKjbwd39HEHD16VD4+PpYG8i5TpoyOHDmid999Vz169Ij3BJjU1rgPdkUdPXp0kmb1k5J+7LNzcHEvLy+dPn063gcNERERypcvn+WHFna5du2a5s+fr+3btztbnuTOnVuVKlXSa6+9liKtqf+NYQxq1KihFStW2DZj7NWrV5UxY8ZYx97o6Ghdv37drb/TkSNHFBgYqCeeeELvvfeennjiCTkcDh06dEgff/yx/vjjD+3cuTPRC/w9e/aoUqVKeuGFF9S3b19nK4rff/9dY8eO1apVq7R161bLs795eHjol19+STRRk1DC2sPDI85ZAKWktar6+uuv1alTp1g3W1mzZtVnn32ml19+2VI9GTNmVP369TVjxoxkz9ZXp04drV+/Xjlz5lTbtm3VoUOHJLdgsXO2ZTuPV2XKlNH169c1e/bsJA/rEDOreGLceQBy69Yt51Aa27dvd3sojfhcu3ZNX3zxhWbPnq1du3bp2WefVfPmzS1N/nTjxg2NHz9eEyZMULFixTR69GjVrVs3WfHEdDebO3euTp8+rVatWlnubnb/ZAjt27dX69atbZ3gxC73txy6c+eOfv/9d0s3wA8++H/wWiiG1Qe1djp58qRmz56t2bNn659//lHLli01bdo07du3TyVLlvzX44kRk+SJaf20cuVK1axZ06Vny/fff2/puNyiRQvdvXs33llrGzduLB8fHy1ZsiTRmIKCghJNGCV2H5DQce/vv//W/PnzFRwcrL179yZYj4+Pj7PlU3Lvs2Le94033ohzwoh58+bFuS0uDRs21Nq1a9W+fXsNHTpUuXPnTnJcdlq2bJleffVVBQUF6e2333b+vaKiojRt2jS99957WrBggeUeLv369dM333yjgQMHKjQ0VFu3btXRo0edx7uZM2dq3rx52rJli1txJvU4I92bxXrHjh0qWrSoqlWrpurVqzsfSqc2JNpSgUyZMunXX3+Nt7tKckVGRibareDBg/2D3DnYJ+bHH3/UO++8Y6nvtB2Jn8Quct2dUSR79uzy9/fXG2+8oTZt2sRbr5UD9YkTJ/TDDz/o7t27ev7555PV7fHBFguVK1fWl19+GSs5k9zuRVa+TzGs3Kw4HI5YT27i4ufnpz///FP58+ePc/vJkyf1+OOPxxojK6XZnZxMrvsvQuL62yd1zKMHu6KOGDEiSV1R75dSibZDhw6pUaNGlr5XKfWQYd++ffrjjz/kcDj0+OOP/2vd+pIjpkWRuy1/YuzYsSPRliJ2XuiuWLFC/fr10969e2MllG/evKly5cpp/PjxevHFFy1/hp07d6pdu3Y6ePCgyxiCTz75pGbPnm25Jcy3336rDh06xOqCkj17dn322WduzSpuR6ImJZIrN2/e1OrVq/Xnn39Kkh5//HHVrVvXre/P/Pnz42zNnRQvvfSSOnbsqBdeeMHWLn7JZWeirVOnTgoKCkpSC6N/S8xQGhs3brQ8lEaMhGbY/e233xQcHKwFCxZYmtE2d+7cunbtmt599121atUq3muRpBybo6Ojnd3NVq5cKX9//0Rn+/Tw8FCBAgVi9ZJ50PLly92OJ7niamnXvn17t1rapbZrofisWbNGISEh+uqrr5Q/f341b95czZs3t/zgw07t27e3VG727NmJlrHrAY/ds7rf78cff1RwcLC++uor5ciRQ02bNtWkSZMS3OfcuXO2xfLhhx9q37598SYbW7RoobJly1oaKsTDw0NeXl7KkCFDgr9nOxLLMce+oKAgS+X79euncePGyd/fX0WKFJHD4dCRI0d0/fp19erVS+PGjbP83jdv3tRbb72lb7/9Vrlz59bMmTNdWuTXqFFD9evXV79+/RKty47jjHSvZ2GePHn08ssv6/nnn1e1atWS/aAuPlFRUVq5cqXlh4cPItFmg1u3bmnq1KmWuxf9W099Dh48qODgYM2fPz/RgQ/tPNgn5vjx4ypVqlSs8UDismnTJlWuXNmW8Vri426iza5ExqZNm9SwYUNnN0UvLy/NnTs3SQPsxsRlZ4uFB7nzfUoJJUqU0MiRI9WsWbM4ty9dulSDBg2ylMBNbU6dOiU/Pz/niWLz5s2aMWOGwsLCVLBgQb3zzjuqVKmSpbo2btxoqZzV1hAPdkUdMmRIsrqi3s/f31/79u1z+yFD4cKFtXPnznhbA7jzm36wReKD3G2R+G9M2HH37l2Fh4e7DHgbl4YNG2rhwoXOJ9MjR47UO++842xNdvHiRVWtWlUHDx60/N7Xr1+Xp6eny5goe/fu1eDBgxUaGpro32nEiBH69ddfE7zQffrpp50D7Sakbt26atGihTp16hTn9pCQEC1evDhJA+Dv3btXf/75p7M7fdmyZd2u459//tH333+vv/76y1lP3bp13RofRbL/YVFacP78eWXJksWWMUJjGGN0/vx5SzdtP//8sy5duqQGDRo4182bN09DhgzRjRs39PLLL2vKlCmWxuGxuxVnco0dO1bvvvuu8ze8adMmPffcc87Pcu3aNfXr10/Tpk1zq94Hh9Jo06aNRowY4db1m5Wb/Lt371r6Xjx4rfbgpDvJvSaKcf78eX3++eeJtrJr166dpVaSVq+z7RoP2a6WdnZfr9+4cUMLFizQ1q1bFRERIYfDoYCAAFWpUkWtWrVK8gOhGDEPfUJCQrRv3z7L34OUjis57HjAY3cvoLCwMGdrwuvXr+vvv//Wl19+Ge+1/IO++eYbS+WsfLayZcvq448/Vq1ateLcvnbtWvXu3dvS+K8pnVi+evWqFi5cqODgYO3c+f/aO++oprLv7T8JoCBFsIyKlVEQ7DoWbCOgIGBvKHbBUbH3ggUbOuqoWMdCtYuDDXuh2bCDBRuKigU7iChK2e8fvtwfgZST5FBmvvmslbVIzs3OSbj33HP22Xs/19CgQQOF0X+5iY6Oxu7duyU2wlxdXWFtba1Sf9SFZ0RvWloazp07h4iICISHhyMmJgYWFhZo164dbGxs0K5dO7VLNdy/fx/+/v4ICgrCp0+f8OPHD5XsaBxtjLx//x6XL1+Gjo4O2rdvDy0tLWRkZGDjxo1YunQpMjMzFe5m5VCQF+eXL1+wZ88e+Pn54erVq7C2tkavXr3yFUMsSs6ePYvRo0czOUQUDfg8UHaRwsuR0a5dOxgZGWHz5s3Q09PDrFmzcPToUZXV6AoiYqE4nU9eXl4IDAzE0aNH8xVHvn37Nrp06YIhQ4YwpXovXLiQ6TPnzZunUl9z8/r1a3h7e2P9+vUyj2nVqhXmzp0LJycnHDp0CD179kTnzp1hZWWFhw8f4siRI9i/f79QR0dd3r17x3wT4pmKmncn/9atW7C0tESJEiUkjlO33qUy1zTP8TguLg4tWrSAlZUVJk2alE+w48GDB1wEO1i/X97x08jICDExMUIEoTLRei9evEDfvn0RHR0NLS0tjB07FosXL8aoUaOwe/dudOvWDVOmTFHoEOY50TU1NUVUVJTMVM74+Hj8/vvvTIV6izOKHDWxsbFo3Lix3ILqjx49wrx587B582apkYQeHh5YvHgxU3QpTyXiLVu2YMiQIShZsiSICEuXLsWKFSvw+fNn6OrqYuTIkfjrr7+YdrtLlSqFZ8+eCWObo6MjAgICUKlSJQDKne+Ojo6wtbUVdulv376NJk2aYOjQobCyssKKFSswcuRIzJ8/X6GtHL59+4bTp09LRLra29srVchbVt2/3IhEIpw9e1ZmO89xgXcpDZ7RNAUxJypOyKqDp2w9ZF6Rdjzn63FxcbC3t8fXr1+FlDAiwtu3bxEZGQl9fX2cOnWKm/jVjRs3mCLaCrtfqqDuBg+vjZ3g4GD4+vriwoULcHZ2xsCBA+Hk5AR9fX3ExsYy/0Z5xxFpAQWsDnNDQ0PcvXtX5ibl8+fPUa9ePXz+/JmpbwVBZGQk/Pz8EBISgvT0dEybNg3Dhw9nrkn3+fNnhfPxsLAwpntJXtTJ1CjIiN7U1FScP39eqNcWGxsLc3NzCVEYFtLS0rB37174+fkhOjoatra26NevH7p3765yxJzG0cbAxYsX0alTJ6SkpEAkEqFp06YICAhA9+7dkZ2djYkTJ8LNzU3pXWqenD9/Hr6+vggJCYGZmRni4uIQGRkpsxiiNHimMUqDiHDz5k24ubmhQ4cOzDXaCiqEOQfe0QBpaWm4fv26wsKVZcqUQVRUlOA0SktLg5GREd6/f69SIeGFCxdi6tSpXM5DHudTDtnZ2QgMDMT+/fvx9OlTiEQimJmZoXfv3hg0aBBzwdT09HS0b98ely9fhr29vVBXIy4uDmfOnEHz5s0RFhbGVBhaXiFQkUiEBw8eID09nfmciIuLQ3h4OHR0dODi4gJjY2O8f/8e3t7e2LRpk/AbysLIyAi3bt1CjRo1YG1tjR49ekiEYa9fvx7+/v5qOaCICMePH4evry+OHj3KrGrLMxW1oOtd5lBUET59+vRBVlZWPsEO4Ofv1LNnT+jo6DALdsiC9fvlHT/zpuoqs6AeOHAgbt++jT/++AMhISGIiopCo0aN0LBhQ8ydO5c5KpHnRFdPTw83b96UGblx7949NGnSBN++fWPqGy8HPE9HFAss58OIESNgbGwsU118xowZ+Pz5M5NYh7z7U24lYpbzKvfifPPmzZgyZQoWLlwIa2tr3LhxA3PmzMHixYsxduxYhbZYzvdKlSoxKTxWqlQJoaGhQk3Q2bNnIzIyUqhDs2/fPnh5eTFHgx4+fBjDhw/PtyFbrlw5+Pn5Mac3y9vkyomA+P79u9zfnue4wLOURk7fwsLCFKab8kjF//TpE0JDQxVeh6ylDVhKFRQGytZD5hVpx3O+bmtri4oVKyIoKCjfRtyPHz8wdOhQvH79mqmOLs9NBp79AgA3Nzem4/z9/ZmOKwxY5x/a2tqYPn06Zs2aJeHs1dHRUcrRlhd1askaGxvjxIkTMqO6oqOj4ejoKLc4Pissm+y5jw0ICIC/vz/S0tLg6uqK/v37o2XLlkr/Vr///jtOnTolcy0UHh6OLl265BNCkgePTA3eEb25yc7OxtWrVxEeHo7w8HCcP39eqTXcpUuX4Ovri+DgYJibm2PAgAGYMWMGbt26pbYvRKM6ysDcuXPRsWNHzJkzB/7+/vDx8UHnzp0xf/58pRwFskhNTZXwzovFYub6G8uXL4e/vz++fPkCV1dXnD9/Hg0bNoSOjo5SzhqeaYyylFC/fPmCrKwsODo6KrULrO7vq8h7rozCJAvx8fGwtbVVeIEnJydLTEj09fVRqlQpJCcnq+RoW7BgAUaNGqWWo43X+ZQDEaFr1644duwYGjZsiPr16wsRPkOHDsX+/ftx8OBBJlu6uroIDw/H6tWrsXv3biGy0MLCAosXL8akSZOY5bRlRcvExMRg5syZuHPnDv744w8mW0eOHEGvXr2QkZEB4OdvuHXrVri4uKBevXrYt2+fwkg0sVgsOBYSEhIk0pUAwMnJian+gTSePHkihD9/+fIFnTp1wp49e5jfz1PZUV0HWnEnIiICx48flzreiEQieHp6wtnZWaEdRTvrrI4jnoSHhyM4OBitW7dG7969YWpqij59+mDmzJlK2dHS0sKrV69kOtpevXrFHAlTo0YNXLt2Taaj7dq1a0pFq8gqIg1IOuAVOdpkKW/n2MlxRBWmqnRUVFQ+dbzcuLi4oH///ky2eCoR5577+Pn5YdGiRYIzqVWrVtDV1cW6deuYHG0ssM4nPn36JFFYOTIyUkKNtlmzZszR5xcvXkTv3r3RtWtXTJkyRWKTaOXKlejduzciIiKYygPkVnvLITMzExs2bIC3tzcqV66MRYsWMfWLB58+fcKnT5+waNEiLF68OF+7KumZ6irssvL8+XMMGzZM4XX49OlTVK9eHf3791fLidSzZ0+m49St0SYWi9GsWTOsXLkSY8aMUehoCwwMVOvzcsNLafLy5cu4du1aPmcW8LM4vqenJ5o3b85ka8WKFahatapUZ2/p0qVRtWpVrFixgmmTgWe/gJ+/ffXq1dG4cWOp57wyyNooykkldnBwKFTBNjc3N2zcuBGRkZEYNGgQ+vbtq7IaMS9yFDllOdoOHDiglCInyyY7C2ZmZujTpw82bNgAe3t7tf5PHz9+RJ8+fXDw4MF8JV4iIyPRpUsXmeU2pBEXF4f27dvDysoKO3bsyJep0b59e6ZMDZ7jTHZ2Nq5duyakjl64cAFpaWmoXLkybG1tsWHDBmaRtjp16uDr16/o378/Ll++LHwPZee2stA42hiIjY1FZGQk6tati8WLF2PNmjVYtmwZ+vTpo5K9mJgYzJ49G0ePHgXwM/0lt7NHJBLh0qVLTB5iT09PzJgxAwsXLlSrZtLcuXNha2srkcY4ffp0lRxtsoo1GhkZwdLSUmmFn7lz5yp0Hq1atUpmm6oFDAuDuLg4QVEQgDB4paamCq+x7tzyCE7ldT7lEBgYiKioKJw9ezbfoBcWFobu3btj27ZtzAvOEiVKYMaMGVKdTomJifDy8lJp5y8hIQFz587F3r170bNnT9y9exfm5uZM7/X29saoUaPg7e2NLVu2YOrUqRg1ahRCQkKY5bjbtWuH3bt3o0GDBmjcuDEiIiIk/u/h4eGoXLky8/fJUYDz9fVFdHQ07O3t8fr1a8TExORLu2XpGy/S09Nx6tQp2Nra5ktn+fz5MyIiItCxY0eFDlNZzvwclFEIVWQrB5aamampqXJVj3KKcysiLi4O/fr1kzlJe/36NR4+fKjQjkgkyvfdVF0IJSUloWbNmgB+fg89PT1069ZNaTs8J7o9e/bE7NmzYW9vn+93T0pKwpw5c5QqtM/LAc/TEQVAYXQfyzn17NkzuU6CcuXKqVy2IK8S8cmTJ5VSIs45JxMSEvKlFNvZ2RVJqYIKFSogISEBVatWxY8fP3Djxg2JiNzU1FTm+nGLFy/GsGHDsHnzZonXW7VqhVatWmHkyJFYtGgRjh07pnQ/d+7ciXnz5uHbt2+YP38+RowYUaA1bfPCcyMmh8uXL6tdX4cne/bsQUBAAFatWqVWamxeFcddu3ahS5cuClM7VaVWrVp48eKFUu9JTk5GfHw8RCIRatasqXRdQXXn6zmYmJjg0aNHMhfv8fHxzE4bnpsMPPsFAKNGjcKePXvw5MkTuLm5YeDAgUqJh+RG1kZRcnIyXr58KYzNBZkhlJstW7ZgzZo1CA4Ohr+/PyZOnIiOHTuCiJiiiguCsWPHol+/fqhSpQo8PDyEdU6OIufq1auxa9cuJls8NtlzqF69Os6fP49q1aqhevXqzPUVpXHq1Cm0bdsWQ4YMwY4dO4TXo6KiBPEBVlEF4OcGub29fb5MjcaNG8PV1RU9e/bE/PnzFWZq8EwtNzY2RlpaGipVqgQbGxusWrUKtra2whxVGeLj49GvXz/Y2toWjAIxaVCISCSiN2/eCM8NDAzo0aNHKttzc3OjJUuWSNjbuXMnRUREUHh4OA0aNIgGDhzIZMvb25vMzc2patWqNH36dLp9+zYREWlra9Pdu3eZ+2RiYiK8l4joy5cvJBaL6ePHj8w2CgKRSEStWrUiGxsbmQ9bW9si7WNeYmJiSCwWKzxOJBKRWCwmkUiU75HzOoud3Pbevn2rTte5nU852Nvb09KlS+V+noODg8r9zQ3r756bd+/e0dixY6lEiRJkZ2dHV65cUfpzS5cuTQ8ePCAiooyMDNLS0qJjx44pZSMuLo7Kli1LgwcPpkWLFpGBgQENHDiQvL29afDgwVSyZEkKCAhgsuXh4UEmJiZkbW1N69evp/fv3xOR6v/DlJQUpgcLPj4+ZGdnJ7O9ffv2tG7dOoV2AgMDmR4s8LRVu3Zt+ueff2S279u3jywsLBTa+e2332jjxo0y22/evMk8xjg7O1OPHj2oR48epK2tTQ4ODsJzZ2dn5mtGLBZLjC8GBgb05MkTpvfm5p9//iFtbW1at24dZWZmCq9nZmbS2rVrSUdHh/bt28dk6/Pnz1S3bl0yNDQkDw8P8vHxoTVr1tCoUaPI0NCQ6tSpQ58/f1a6jzk8efKEBgwYQNra2uTi4kIPHz5Uyc7nz59p9uzZZGBgQC1atKCwsDCl3p9zL5D1YLlXVKhQgc6ePSuz/cyZM1ShQgWl+vX9+3dauXIllS1blmrXrs38f8uNSCSibdu20aFDh6hq1aoUHR0t0X7nzh0yMjJispX3HDU0NJQ4R5OSkpjP9xEjRlDLli0pKiqKJk+eTGXLlqXv378L7Tt27KCmTZsy2TI2NqZbt27JbI+NjSVjY2MmWzkcP36cGjZsSEZGRrRw4UL68uUL83tFIhF5e3vTmjVraM2aNaSrq0tz584Vni9evFjpeykv8s63CxJl5wwvXrygxYsXU61atahSpUo0Y8YMlccEop9j6OPHj1V+vyLOnDnDdL8hIkpISCBnZ2fS0tISxhUtLS3q1KkTJSQkMNngOV/38vKi0qVL04oVKygmJoZev35NSUlJFBMTQytWrCATExNasGABky1dXV16+vSpzPanT5+Snp5eofcrh/T0dNq1axd16NCBSpUqRX369KETJ05Qdna2Unbk8erVK7KxsSF3d3eFxzZq1IgaN24s81G7dm2VxoeHDx/SzJkzydTUlIyMjMjV1ZVCQkKUtqPudePp6UkikYiMjIyE72pkZERisZhmzJjBbMfa2prGjx9PqamptHLlShKJRGRhYUGRkZEq9ev8+fM0bNgwMjAwoCZNmtCqVatIW1ub4uLilLYVHx9PlSpVonHjxhER0blz58jAwIA8PDyUtlWuXDm6evWqzPYrV65QuXLlFNrhObZv2rRJWHepS864XrNmTTI1NaUpU6bQjRs3SEdHR6U1U140jjYGxGIxxcfHU0pKCiUnJ5OhoSHFxsaqtNAk+rkYi4qKEp7nHTSio6OpWrVqSvUxIiKCBg8eTPr6+tSgQQPS0tKi8+fPM79f2gWg6kIqhxcvXtCaNWtozJgxNGnSJNq0aZPSjrvCnHTxgnXy9vTpU6YHKyKRiOrXry/3Btm4cWMmW+qeTzlUqFCBbt68KbP9xo0bSi/uZKHMpPnLly80f/58MjIyoiZNmtDJkydV/lxpjvj4+Hil7cTHx1O/fv3I0NBQcLjq6OhQq1at6MCBA8x2tLS0yNPTM5+DQVVHG49Ffg7NmjWjw4cPy2wPDQ2lZs2aKd3H4sK8efOoWrVqEpsWOdy6dYuqV69O8+bNU2hnwoQJNGHCBJnt8fHxZGNjo9DO0KFDmR4s5B1ftLS0qG7duiqNL7wmukREycnJ5OHhQWXKlBGumzJlypCHhwd9+vRJKVs58HDAE/FxRBH9HI9ZHvLo06cPde/eXWZ7165dqXfv3kz9yc7OpsDAQKpWrRqZmprS5s2bJZymypB3k8nb21uifevWrcznlUgkImNjYzIxMSETExMSiURUunRp4bmxsTHzWPX27Vtq06YNiUQiMjQ0pP3790u029nZkaenJ5Mtngv9y5cvk42NDenq6tLEiRPp3bt3TO/LTfXq1alGjRoKH6qSnZ1NZ8+epSNHjhTrOZ8qm3M5REREkI2NjVob0gXlaMvOzqbr169Tw4YNacqUKQqPf/78OVWoUIGqVKlCS5YsoQMHDtD+/fvJ29ubqlSpQhUrVqTExESFdnj/7/7880+qVKmSxBxEJBJRpUqVaNmyZcx2eG8y8OqXNJ4+fUrz58+nX3/9lapWrUqpqalq2cvN+fPnyczMTOFx8+fPZ3qoSlZWFh0+fJi6detGJUqUUPr9eTdQVOHy5cs0fvx4cnZ2JicnJ5owYQJdvnxZKRs8NtmlkZqaSlu2bCFra2sSiURkY2NDW7ZsUTqQIjY2lkxMTGjIkCFkZGREI0aMUKk/JUuWpOfPn8tsf/78OZUsWVKhnYIa2z99+kRXr16la9euqTzny+Hs2bM0YMAA0tPTI5FIRNOmTVPboacRQ2BALBZLhEvS/68Rkfc5a80IfX19xMXFCbVjVq9eDXd3d6F+wPPnz2FhYYH09HSl+5qamoqdO3ciICAA169fR/PmzdG7d2+FsuPSCtC2atUKwcHBqFKlivAaaxrjxo0bMXnyZPz48QOlS5cGEeHz58/Q09ODr68vXF1dQUSIiYmRmybEI9RUUY22HFgLzSuSmk5ISMDkyZMVng+sKkesiMViTJkyRWF9P2XqZKl6PuVQokQJPHv2TFB9y8urV69gZmbGXJhfHsoUwM9J4Rs3bhxcXV1lnh8s53vea0fadcNqC4CgYJWdnY1y5coxpyflsGvXLgQEBODSpUvo1KkTBg0aBEdHR+jp6alUgJaXyi7wM+0iNjZWbiH8hg0byky9Kwg+ffqEHTt2YMiQIVILJW/btk1qmzR4CnYUN3gLWVy5cgU7d+6UUEbr37+/UvVtckNEeP/+PYgI5cuXVylFNi0tDX/99RdWrVqFWrVqYenSpXBwcFCpL9u2bcO8efOQmZkJLy8vuLu7c0nHV5WbN2+iZcuW6Ny5M6ZPn47atWsD+Clhv3z5chw9ehQXL15kuifxVCJWxJEjR6Cjo4OOHTsqPLYgFN1TUlJgYGCQ73/38eNHGBgYSK3VlJeGDRti4sSJGDZsmNT2nNq/t27dUmhLLBZDT08PI0eORI0aNWQeN378eIW2eJCcnIwJEybgxo0bsLa2xsqVK+Hs7IyLFy8CAMqXL4/Tp08z3/8UKewqw9q1a+W2v3z5En/99ZdS9d5yyjL4+/sjOjoaXbt2RVBQEHN92NyoU9QdYKuHvHfvXoVzQjc3Nzx+/BgnT57Md2/69u0bHB0dUatWLfj5+cm1wzM1LDcJCQlCiZWKFSsy173KwcXFBRkZGTJTK7t164YSJUpg3759hdovaTx//hyBgYEIDAzEjx8/cP/+feaa3Yp4+vQp6tWrp1QR/ILm7du3Cs+XvOd5cnIyjIyM8qVus5T34Ik0UZmYmBiV0hdlidndu3cPfn5+2L59Oz5+/Cikqcojd6mJCxcuoEePHujevTs2b94s8Tuy3p8tLS3h7e2NXr16SW3/559/MHv2bDx48ECuHbFYjKCgoHwp9Hnp2rUrU7+ePn2KMWPG4OTJkxICDY6Ojli/fr3c+6MikpOTsWvXLkGErl69ekz3Z2loHG0M8FxoAj/VJkNDQ2UqOF64cAFdunRRe9C4ffs2/Pz8sGvXLrx9+1busTnORGmng7IFaI8ePYpu3bph4sSJmDJliuBkef36NVasWIH169cjLCwMGzduhKWlpdzC0jxUjHIvEIkIS5cuxahRo/LVQWBdILLU5WD5rUqUKIG5c+di9uzZXAqUFrRCqzLnUw5aWlpISkqSWW9FGXUzRSjjaMurpJn7vFf2fOd57fDk6dOnCAgIQGBgIL5+/YqPHz9i79696N27N9fPYVXZBX5ORCIiIvDbb79Jbb9+/TpsbGwU1pziWVdt0aJFuHXrlsxJtouLCxo2bIjZs2crtAX8VB7LEezIqaNmYWGBfv36KSXY8b/Khw8fsH37dkycOFFlG5GRkUhLS0PLli2VqpfDywHP2xHFosDKYu/IkSNwc3PDhw8fJF4vW7YsfH19mSe4PJWI/xdYvXo1Fi9ejO3bt+cTQzl69CiGDBmC2bNnM9Wiq1GjhsKxTyQSyVXAvHPnjsJanX/++SdTMejhw4cjKioKgwcPxpEjRyAWi0FE8PHxgVgsxvTp02FgYIDQ0FCFtnjD6vhISEhQeMzly5fh5+eHvXv3ombNmnBzc8OAAQPUKu6urqNNlmNZ2XrIpqamCA4ORps2baS2R0VFoV+/fnj16pVcOwU9B1UVnpsMBcH379+xf/9++Pv74/z580IdLUdHR67iBYcOHcLs2bNx584dbjblcf36dUydOhWHDh2SuonZvXt3rFmzRuH9tCA2UHLz6dMnxMfHo1KlSvk2yOXBc5NdkZM6MzMThw8fZhJUkRYcBPzfvVrZ+7OXlxcCAwNx9OjRfPeN27dvo0uXLhgyZIjCzVhea2fgZ03uZs2aQUdHB6NHj5YQaPj777+RmZmJq1evKvX/lEVMTAz8/f0VbtzIQuNoY2Dbtm3o27cvtwVS+/bt0aRJE6xYsUJq+5QpUxATE4OzZ88qtFW5cmXY2dnB1tYWtra2UicWGRkZCqNinj17xtR3FgW3du3aoW3btlIVqABgzpw5WLlyJSpWrIiIiAi5NoOCgtCvXz+ui1N1Jze8OHbsGEaOHAlTU1Ns374dFhYWatkrqN3EvLx79465ULFYLIaTk5PM/9/3799x4sQJpoFV0Q0mOTkZkZGRTLZ4nu88bMmSeM9Rixo4cKDKu5pEhJMnT8Lf3x+HDx9GuXLl0LNnT5VvGnlRxsFpbW2NHj16yFRQ/fPPP3Hw4EFER0fLtZN74kVE8PDwwMKFC/Od+ywTr0aNGmHlypX5irDncPbsWUydOlVmofzcxMTEoFGjRgqPU8Ty5csxbtw46OnpAfi50GnRooVwHaWmpmLGjBnYuHGjXDuvX7/G+vXr4e3tDQBo06aNhPCOlpYWDh48yCS0MWfOHNjZ2QlKkDwhIpw6dQp+fn7CpPzdu3cK37dixQp8+fJFmOAREZycnHDq1CkAwC+//IKzZ8+ibt26TP3g5YDn7YjKO3FWx963b99w4sQJiUhCBwcHpdSqeW8+fv78WViIHTt2TELMREtLC506dWLu2759+3Dw4EFkZGSgQ4cOGDFiBPN7C4rs7Gz07dsXISEhqF27tkSk66NHj9C9e3fs27ev0BQBK1eujAsXLsjc8V+2bBnmzZvHFGleuXJl7Nq1C+3atcPLly9RtWpVhIWFwcbGBsDPyNWuXbtKiD7Jg+cmCi/q1q2Lt2/fon///nB3d2eOzstL3mwIV1dX+Pj45BNyYXV483KYlixZEo8fP5a5KH3x4gVq1qyp8HzgPV9/8eIF/v77b1y8eBFJSUkQiUSoUKECWrVqhVGjRqFq1arMtnhtMuTl06dPCAoKwqNHj1CpUiUMGTJEqX6NHj0ae/bsQbVq1TBs2DAMHDgQZcuWVakvsjZkUlJScPXqVUyZMgXDhw9XuGnIKwuof//+sLKywty5c6W2e3t74969exIF+wsaT09PzJkzB6VKlUJGRgbGjBkDPz8/4R7arVs37Nq1i2mOw3OTnaeTmvf9mVemBs/vyCsKNzcpKSk4ffo0nj59CpFIhF9//RXt27dXOzJf42hjgLcDIyQkBP369YOPjw88PDyEyVWO6smUKVOwa9cupsiTRYsWITIyEpcuXUJ6ejqqVq0q4Xjj4c1VFiMjI1y9elXYNcrLgwcPYGVlhadPn8pMIcth9OjRWL58ueBk2L59O3r06CE8T05ORv/+/ZVS61LX0ebm5oY1a9ZwUYpKSUnBhAkT8M8//2Dp0qUYN26cyrZYBrGrV6/KVbO1sbFBYGCgzAn4gQMHMHr0aLx+/ZqpT7JSZfISEBBQqLaKGz169JD6enJyMu7evQsdHR2cO3dObefwx48fsW3bNgQGBiImJkYtWzko42jbsmULJk+ejD179uRTYwoNDYWrqytWrVql9OJYnWva0NAQd+/elZvOWq9ePaaoIrFYjMaNG2P48OEYMGCAyjfovPccIyMjxMTECN+PNRJ07ty5+PjxIzZs2ADg53d1c3MTdmCPHz+ONm3a4K+//lLYp5o1ayIhIQElSpRAixYtYGtrCzs7O1hbWzOlz0nj6dOn8Pf3R2BgIF6+fIn+/ftjyJAhsLW1ZUqzbNKkCWbMmIG+ffsC+OlkGTJkCE6fPg0rKysMHjwYpUqVUqiGlQMvBzzviW5ue0QEZ2dn+Pr65nOQqqMQrI5qszocOXIEc+fOFRzZhoaGSEtLE9pFIhFzJO6WLVswatQomJubQ1dXF3fu3MH06dOxdOnSAuu/Muzdu1dqpGu/fv24fQZLRGi/fv1w/fp1XLhwId98YcWKFZg9ezZ27tyJPn36KPw8bW1tJCYmCpkLpUqVwu3bt4X0qaSkJFSuXJnZqRwYGMi0yGeNXiEixMfHIyMjAxYWFiopsorFYujr60NbW1tu3xQ5/3hGdAD8HKZmZmbYtGmTzPTsEydOYNSoUXj69KlcO/Hx8UhJSZGIWD979iwWL16MtLQ0dO/eHZ6ennJt5HD+/Hk4OTmhatWqcHBwQIUKFYSyGqdPn0ZiYiKOHz8uMzNIGjw2GUxNTXH79m2ULVsWCQkJaNWqFQCgfv36uHfvHlJTUxEdHc2sGikWi1GtWjWFzq39+/cz2ZJlQyQSYeTIkfDx8VEYeMErC6hmzZo4cOCATMf07du30a1bN7nRtyy8fv0a3t7eWL9+vcJjc8+tlixZAh8fH2zatAnW1ta4ceMGRo0ahZEjR8p0DuaG54a9WCzGmzdvuKgt8w4OAvhkavB0tPGKws1hx44dGDt2bL55funSpbFp0yZhjqkSalV4+x+hIAr4TZ8+XWYx6KlTpypt78ePHxQZGUkLFiwgOzs70tPTI7FYTLVq1VKqAOKVK1do0qRJ1KlTJ+rcuTNNmjRJrtqINPT19eUWeH38+DGVKlWKyZZYLJb47Q0NDSVsK6MiloO6BWjz9okH+/btIy0tLTIyMhIKN+c8WHn69CllZ2dTamoqff36VaLt5s2b1LlzZ4W/VefOncnQ0JA2bdok8fqHDx+oX79+VLJkSQnF3P8qISEhVL9+faXe8/DhQ1qxYgWNGTOGxo4dSytXruRS6Pjr16/Uu3dv6tOnj9q2iIiuXbtGnTp14mKLSPmC0gMGDCCRSERWVlbUvXt36tGjB1laWpJYLKZ+/fqp1Ad1runSpUvTpUuXZLZfunSJSpcuzWTr4sWLNHz4cDIyMiI9PT0aMGCA0gqTRNIFNlQZ9xo2bEinTp2SaefEiRNUp04d5n69ePGCtm3bRm5ubvTrr7+SSCSiUqVKUfv27Wnx4sV04cIFhTZyFNbs7OxIV1eXevToQfv27VNJrMPY2FhCkWvo0KESit2XLl2iKlWqKGXz30BBFFFX5jrmqUTcpUsX8vX1FZ7n/W7Lli0jJycnJlv16tWjOXPmCM8DAgLIwMCA6b3/ZrKzs+nEiRPUp08fKlGihEIFuIyMDHJ0dKSGDRtScnKy8Ppff/1F2tratHv3bubP5jVWFQQJCQnUoEEDoWB99erV6dq1a0rb4alMzZO+fftSrVq1pM5Hly9fTjo6OhQcHKzQzoQJE6h+/fpSi62/efOGGjRoIFecJ4fu3btLXH9PnjwhPT09cnBwoPHjx5OBgQGtXr1aoR0ioqZNm9LEiRNltk+cOJFZ9Zcnuc/3fv36kY2NDaWlpRHRz3tb586dmUVliIiGDBnCTbBIlkjOjRs31BJVUPV+U7JkSbmiBU+ePCFdXV0mW3fv3qX169fT5s2bhYL37969o4kTJ5Kuri5ZWVkx2cn9/2vUqBH5+flJtO/du5fZ1oIFC4T/vbrwFLMriDUqD3j6UkqUKCFXoCUxMZFZaOP69eukra1NQ4YMoZiYGEpPT6dv377R9evXadCgQaSjoyNX1E8RGkcbAyKRSGm1DxYuXbpE48ePJycnJ3JycqLx48fLXfApw8ePH2n27NmC846FadOmCQpbDRs2pAYNGpCBgQGJxWKaPn0682c3b96cVq1aJbN95cqVzMqCBTGJU3eRwtvxeuXKFbK0tCQrKyvy9fVVefKWmJhIrVq1IrFYTDo6OjRp0iRKS0ujQYMGkba2NvXq1YsuXryo0I6fnx+VLl2aHBwcKDExkfbv308VKlSgZs2a0Z07d9T5qgVGbhUjVrZs2UK9e/cmV1dXio6OJqKfijONGjUiPT09pRzUS5YsIW1tbRKLxVSxYkWqUKGC8H9YsWKFsl8nH1evXqWqVasyH3/q1CmaOnUqzZo1SzjX7927R926dSOxWEwdO3ZUu085qKLctnfvXurWrRvVqVOHrKysqFu3brR3716V+6DONW1jYyNX6XL69OlMCp+5+fr1KwUGBlK7du1ILBbTr7/+SosXL2ZSbiPiN+6VLl1a4n09evSgpKQk4XlCQgKz2qE0nj9/TkFBQTRs2DAyMjIiLS0the8pW7YstW3bljZv3iyh1qeKoy3vpk7t2rVp48aNwvNnz54xT+SJfjrL+/XrJ9VJlJycTK6urkzn2cuXL2nKlCky7UydOlXi/6AsRe1o46lEXL16dYnNvLzf7datW1S+fHkmW6VKlZJ4b2ZmJuno6NDr16+Z3l9Q8HRM5iYhIYHmzp1LVatWJbFYTIMGDaLTp08zKcB+/fqVWrduTW3atKFv377R6tWrSVtbm3bu3KlUH3KUYtesWUNr1qwhXV1dmjt3rvB88eLFXB1tr169ojFjxjAd6+LiQhYWFrRz504KCQkha2vrIlW1HjZsWD41cHXg5TD9+PEjmZubk6GhIXl4eAj/u5EjR5KhoSGZm5vThw8fFNqpUqWKxBxz0aJF1LBhQ+G5r6+vxHN56Orq0v3792W237t3j3lsX7BggdSHj48PHT9+nLKyspjsEEnem83MzPKpmUZHR//nNndUvd9UqVKFjh8/LrP92LFjTL9VaGgolShRQlCmrlmzJoWFhVG5cuXIxsaGQkNDmfuUez1ftmzZfArxCQkJKgeCqINIJKKpU6dyUXrlvUb9+PEjrV27VuZ8RlZbXnr06EGJiYlc7oM1atSgEydOyGw/fvw4Va9encnW0KFD5TrHe/XqRcOGDWOyJQ2No40BkUhEzs7O1KNHD7mPouTbt2905swZmjNnDrVp04ZKlixJtWvXpj/++IN27Nih8P2BgYGkq6tL69atox8/fgiv//jxQ5g8BQUFMfUlMDCQ9PT0aMOGDZSRkSG8npGRQevXryc9PT0KCAhgssVjwZkzaZA1Ecx5sMLL8ZqRkUGenp5UokQJmjRpEn379k0tewMGDKAGDRrQunXrBNn5Jk2a0LBhw5SWwn727Bm1a9eO9PT0SFdXl7y9vZkm73lRdM2oe+08fPiQZs6cSZUqVSJdXV1mR9uKFStIR0eHfvvtNypVqhSVKlWKvL29qWzZsjR//nx69+4dcx/CwsJILBaTl5eXhOPgw4cPNHfuXNLS0qLIyEhlv5oEjx8/JkNDQ6ZjAwMDSSQSUdmyZUkkElH58uVp+/btZGhoSEOHDs03sVDEoUOH5D58fHyKLGIhB3UcD//88w9pa2vTunXrJM7xzMxMWrt2Leno6NC+fftU7lt8fDzNnj2bqlatStra2kzRObwcbfr6+nTjxg2Z7Tdu3CB9fX2FdqQRHx9Pvr6+1L9/fzI1NSV9fX3q0KGDwvcZGxvT77//Tlu2bJGYVKniaGvYsKFwL3n27BmJRCIJGxcuXKDKlSsz2/vjjz9o2rRpMtunT59Oo0aNUmhnypQp9Mcff8hsHzlypFKbV3kpakebrMiJvA8W8kY9XL16VWIO8uTJE+adaWkLjIL4rZSFp2OSZ0RocnIyNWzYkOrUqUPa2tq0fft2pb9b9erVqUaNGgofysAreqVSpUoS52FiYiKJxeJ8Ef+FRUFEmvBymH78+JFGjRpFJiYmgkPDxMSERo4cSe/fv2eyoaurS8+fPxee29nZSUS4xcfHM0eHm5mZkb+/v8x2f39/MjMzY7LVqFEjqY8aNWqQjo4ONWrUiPn/knv+b2pqmm/zOSEhgUqWLMlkKy+fPn2iq1ev0rVr14TzXhV4Z1eoOoYOHTqU2rRpI7UtOzub2rRpwxStZ21tTePHj6fU1FRauXIliUQisrCwUGlenXtjwNTUlKKioiTaY2JimDOJeDq0eNviGRy0cOFCuY6oPn360OLFi5n6xes+yCsKl4jI3NycTp8+LbP99OnTZG5uzmRLGsoXK/gfxdDQUChMrS6sErEsBVe9vLwQHh6Oq1ev4tdff0W7du0wduxYtGvXDhUrVmTu04YNG7BkyRKMHTtW4nUdHR2MHz8emZmZWL9+PQYPHqzQ1pAhQ3D79m2MHTsWs2bNEmp1PH78GF++fMH48eMxdOhQ5r6py+rVqyWeV6xYEdu3b5d4TSQSYfz48cw2LSwsFNYRUVSvo0mTJvjy5QtOnTqlVm2dHMLDwxEcHIzWrVujd+/eMDU1RZ8+fZiUw/Jy//59PH78GOXLl8fr168lilMrgyIZZ1X49u0bgoOD4efnh+joaGRlZWH16tVwc3NjFgzw8/PDpk2b4ObmhoiICNjZ2SEsLAzx8fEwNjZWqj+bNm3C8OHDMX/+fInXy5Qpg4ULFyIpKQl///03kyqnLC5evMgsGb569WosWbIEM2fORHBwMPr164fVq1fj5s2bKsmOd+/eXeExLDV1cqNu0dHJkydLPP/x4we8vb3znW+rVq1SaKtXr16YPn06xo8fj9mzZ+PXX3+FSCQSxqtp06appdRas2ZNzJw5E1WrVoWnpydOnjzJ9D5fX1/hfM7MzERgYCDKlSsHAApVWXOoXbs2Ll68iMaNG0ttP3fuHLMIS0JCAsLDwxEeHo6IiAikpKSgdevWwj2nWbNmTPWPXr9+jZCQEPj5+WHChAlwcnLCwIEDlT6HAMDDwwNjx47FuXPnEB0djZYtW6JOnTpCe1hYmMzvLo2oqKh894bcuLi4oH///grtnDhxAps2bZLZPnjwYPzxxx9YtmwZc9/yosrvxQse96scypQpg8ePHwtCTk2bNpVof/ToUb7aQPLIfd0A+a8dAErd63kQHh4u/E1yauyxULlyZdSpUwcDBw7EP//8I6heurq6MtvIXZTfw8MDEyZMQI8ePWBkZCTRxlIgXlHdLmU5cuQIevXqhYyMDAA/hWG2bt0KFxcX1KtXD/v27ctX31MWSUlJErWyqlSpAj09Pbx580ZmXTNp8BJooAIoia2np4ejR4+iXbt2+O233/Dw4UMEBAQwjVO5MTExwd9//42NGzcKQjTly5dXapwpU6YMXr9+japVqyI7OxvXrl2TUNL98eMH828wdepUjBo1CtevX4e9vT0qVKgAkUiEpKQknD59Gr6+vvDx8WGyJU/I6PXr1+jfvz88PT3h6+vLZK99+/bQ1tbG58+f8fDhQwmxnefPn0uMNSw8ffoUY8aMwcmTJyUUIh0dHbF+/XqlztWlS5di3rx5yM7Oxi+//AIiwrt37zBz5kwsWbIEU6dOVapv6jBnzhz89ttvaNGiBaZMmYLatWtDJBLh3r17WLlypXCuKuLevXsICgqCgYEBxo8fj+nTp8PHx0elOXW1atWwdetWAECJEiVw48YNtG3bVmgPDw+XWV9cGrzuw7zv50OHDlVYN42l7h/ws678ypUrZbaPHDkSU6dOVSiywfM+6OXlhWPHjqFmzZoYOHCgMM7HxcVh165dqFixIubNm8dk69WrV3LnwRYWFnj58qXSfcxBI4bAAG/JanlKJTkoo1RSrVo1zJw5E3369FFZrUZfXx+3b9+WWUz8yZMnqF+/vkSRYkVER0dj9+7dePToEQDA3Nwcrq6usLa2ZrYhFosxYsQIoVjphg0bMHDgQGFB/fXrV2zdupW5eCwPxGIxfHx8FDqRFBXrHT58OHx8fFRWk8yLlpYWXr58KThY9fX1ce3aNWaJdwBIS0vDpEmTEBQUBE9PT8yePRunTp3CiBEjUKFCBWzbto1ZwY+VzMxMpgX6lStX4Ovri71798LCwgIDBw5Ev379UKVKFcTGxkossBVRqlQp3L9/XyiAX7JkSUHZUVnMzMywfft2mUU5z507h8GDByMhIUGmDVnO9xy1qCVLlmDx4sUYNWqUwv4YGhri1q1bMDMzQ3Z2NkqWLIkzZ85wXRyrA4+io7a2tgqPEYlECAsLY+7XlStXsHPnTolCyf3790fz5s2ZbeQlMjIS/v7+CAkJgZaWFlxcXODu7q5wDKxRowbTxEveOQX8LGz+559/Ijw8PN/GTWxsLOzs7DBz5kxMmzZN4Wfl3GtGjx4NW1tbNGnShEmwQB6PHz9GQEAAgoKC8PLlS7i6umLo0KGws7Njtu3n54cjR46gYsWK8PLykthgGj16NOzt7WWKjeRFT08P9+/fl1m8+NmzZ7CyspJQbpWGvr4+7t27J1dgw8rKivl+mldxOTQ0FHZ2dtDX15d4Xd7EmadqszSICOHh4fj27RtatWolOIAU0a9fP3z9+jWfImMOnTt3hr6+Pvbu3avQFst1IxKJ1C6+rS7qiLeYmJigQYMGGDhwIPr27StsTujo6DDfB3kX5edJy5Yt0bx5c3h7e2PLli2YOnUqzM3NsXXrVqUX1lpaWkhKSpIoMG5kZITY2FjBscsCL5VrngXPAUmH6evXrzFhwgR07doVAwcOlDhOkcP027dvOH36NGxtbfOJfH3+/BkRERHo2LGjwoV7//79kZqaio0bN2Lfvn3w8vJCUlKSME6FhIRg4cKFiI2NZfp+e/fuxerVq3H9+nXhXNTS0sJvv/2GyZMnw8XFhcmOIi5cuIBBgwYxjQu5hQKAn0rquUUkpk2bhhcvXmD37t1Mn52YmIhmzZpBR0cHo0ePhpWVFYgI9+7dw99//43MzExcvXqVSdguPDwcHTp0wNy5czFhwgRhDP748SN8fHywZMkShIWFKbyO8irSz5gxA9OmTcvnQGTZsLh27RqGDh2KuLg4YWwmItSpUwcBAQFyxdlyyLsGNzQ0RExMjEobx4qIjo5GyZIlmTboxGIx6tWrp3D9okidNccWLz+DWCyGi4uLwuAgVuE4noJhee2qI0z46dMneHp6Yu/evUhOTgYAGBsbw8XFBd7e3sz+EEW/Pav4mCw0jjYGeKuO8lQqOXHiBCIiIhAREYGbN2/CwsICNjY2aNeuHdq1a8d8QzcyMsKVK1dkKuU8ePAAzZo1U/pCUhcbGxumBWduT3lBw9vxyou8k8rcDhdWzMzMYGhoiMDAQDRp0kR4PTk5GWPHjkVISAjmz5+PGTNmMNnbs2ePXEW1jIwM9O7dG4cOHVJoS1tbG+PGjcOoUaMkdpyUWWDkIO3GreqAX6pUKTx8+FDmROjFixcwNzfHt2/f5PZHlvO9fPnymDp1KpMzJMcWr++Wmw8fPgg3rsTERGzduhXp6eno0qWLxI6gPG7cuIEWLVpgwIABmDRpEiwtLUFEiIuLg4+PD/bs2YOrV6+iYcOGavW1KElMTERgYCACAwMFVTJ3d3e4uLjkc4wUNBkZGejQoQMuXrwIe3t7YUf5/v37OH36NFq2bImzZ88qVCEDgL59+yIqKgrp6elo27Yt2rVrB1tbW4VqaXmRpoiVnZ2NkydPws/PD6GhoTA0NMT79+9VsqUOFStWxK5du2BnZye1/ezZsxgwYACSkpLk2ilXrhz2798vc0ETFRWFnj17Mn1HgI/iMk/V5uTkZEyYMAE3btyAtbU1Vq5cCWdnZ1y8eBHAzzHr9OnTTFH5N2/eRMuWLdGlSxdMnz5d2Fl+8OABli1bhqNHj+LixYsS96N/O+qMyenp6UJEaHR0tBAR2rdvX8TExCh1H+RBtWrVcPPmTeHekJP5oKrisrGxMa5cuQILCwtkZmZCV1cXoaGhcHJyUtqWWCxG6dKlJcan5ORkGBkZSTgbFUWi5UXV/5+0/kiDtT+8HKZr1qzB4cOHcfbsWantHTp0QI8ePTBmzBi5dhISEmBvb4+EhASIxWKsXbsWHh4eQnv37t1hZmaWL8tEERkZGcJYWa5cOab7lTI8ffoU9erVw5cvX7jaZcHNzQ2PHz/GyZMnoaurK9H27ds3ODo6olatWvDz81Noq2/fvjA2NsbmzZulto8YMQKpqakKnYAsawZlNyxiYmLw6NEjYROzUaNGzO8Vi8UICwsTIptbtWqF4ODgfHNulvsNT8RiMaZMmaIwWEKROivw0y9QrVq1fGNDZmYm0tPTlQrI4L1GNTY2xokTJ2RuEEdHR8PR0VFwdrHCa22SE7UJKB+FC/z8vYKCgmQGzyQnJ2PYsGEaR1tBUlwdK3lJTU3FuXPnEBkZifDwcMTGxqJWrVqwtbVVKHtsa2uLNm3aYNGiRVLb58yZg/PnzyMiIkJhP5YvX45x48YJ3vScSKGcxVBqaipmzJiBjRs3KvcFVYT3RJCX45V1gcqyGwLk3125desWLC0tUaJECWZ7M2bMwKJFi/K9J4cDBw7Aw8ND4UIzB11dXRw6dEiqbHxmZiZ69+6Nq1evMoXlOjg4IDo6Gl26dMGgQYPQsWNHiEQilR1tixcvFm5e6uzY8dgNkeV8L126tNKprHlvGq6urvDx8UGFChUkjmNJCwJ+SrB36dIFiYmJMDc3x549e+Do6Ii0tDSIxWKkpaXhn3/+YUoxHTZsGL58+YJ9+/ZJbe/duzeMjIzg7+/P1Dce8Byv7O3tER4ejvLly2Pw4MFwc3NTKg0hhzt37qBevXpyj/nzzz+Z0sJ//PiBVatWYc+ePYIse050Masse27u378vpI9GRkYiPT0dbdq0Qbt27WBjY6Nwh1rR+Pnu3Tts3749X3qwKraUxcXFBRkZGThw4IDU9m7duqFEiRIyz98cOnXqBFNTUyFFJS/Dhw/Hq1evcOzYMbX7XBQMHz4cUVFRGDx4MI4cOQKxWAwigo+PD8RiMaZPnw4DAwOEhoYy2Tt06BCGDx+ez8FgYmICX19fprHl3wSvBQaPiFB1yXv/MzIyQkxMjMrfjWf0Su5INHkoikTLizqONh7ZELxp3rw55s6diy5dukhtP3LkCBYuXIgrV64otJWRkYG4uDiUL18epqamEm2xsbGoWrWqUqnghcGhQ4cwe/Zs3Llzp9A/29TUFMHBwTIzIqKiotCvXz+8evVKoS0e2RWFxadPn7Bjxw74+fkhJiZG7rHyNqJzXmeNwJUVOZ0XlvkxT9/AsWPH8OHDBwwaNEh4zdvbG4sWLUJmZibs7Oywd+9epkhx3j4LW1tbtGjRAn/++afU9hkzZuDKlStKB7zwug+qS0FHeGscbQxERkaidevWTOltLPCs0SaNrKwsXLlyBYcPH8bGjRvx5csXhSfIkSNH0L17d0yePBlTpkwRFuVJSUlYuXIlfHx8cODAAabaGHkXP3knXuqGYSpLQU8EVSV3CDoRYenSpRg1alS+SQjLbkhee/JgtSeL3JFNilizZg1mz54tRM/kkJWVhd69e+PSpUuIiIiQGUmZl8TERAQEBCAgIADfvn1D3759sXHjRty6dUupFFmeKUZ5nXZ5SU1Nxbx58+Se7wcPHkSXLl24LI543zScnJygra2NGTNmYMeOHThy5AgcHByEeibjxo3D9evXER0drdCWhYUFNm7ciA4dOkhtP3PmDEaPHi04hGSxcOFCpr6z1GjgOV517doV7u7u6Ny5s1r/y8qVK+PChQsya7MsW7YM8+bNw/fv31X+jBxiYmKU2l3OS05NjHXr1iEtLU1hPUfeKRI8J5Q50VWdO3fG9OnTBSfp/fv3sXz5cuboqvDwcNjb22PixImYNm2acD998+YNli9fjjVr1uDUqVMyI+dYefbsGdLS0mBpacl03eeQnJyM+Ph4iEQi1KxZU2lnfuXKlbFr1y60a9cOL1++RNWqVREWFgYbGxsAP9Owu3btyrwhA/wsA3Hy5EmJUhMODg5KRYE6Oztj9+7dgiPD29sbY8aMEb7fhw8f0LZtW8TFxTHbLAhUiTaXR96IUAMDA3z48EHh+/bt24fdu3fj4cOHEIlEMDc3R//+/ZWqSck7grq4Rq/kRh1HW3HctDcxMUFsbKzc1LCGDRvi06dPan3O7du34efnx1xb7fTp0zh//jzatWsHOzs7REVFYenSpfj+/TsGDRrEHKUrKxMnpzTHlClTMHz4cIU1plh4/Pgx/vjjD+ayFSVLlsTjx4/lZkTUrFmT6V7PI7sCAOzs7LB//36l7wssnDlzBn5+fjh48CDKlSuHnj17Ys2aNXLfwzMLjOf8mOdGn52dHXr16iVEjV68eBFt27bFwoULYWVlhdmzZ8PJyYmp7rC0lHl1CAkJEWo9e3h4CHPbrKwsbNy4EVOmTMGuXbuUrmWszn3Q1taWaQ0nK0q3UFFZRuF/iOjoaDp27JjEa0FBQVSjRg0qX748/fHHH5Sens5sL0dZI0fZJ7fiRu7XWMnKyqLLly/Tn3/+SY6OjmRoaEhisZiqVatGQ4YMocDAQCY7a9eupRIlSpBYLCYTExMyMTEhsVhMOjo6tHr1aqW+Hw/FvBwePnxI//zzj6BMduTIEWrbti01bdqUFi9eTNnZ2Wr1R1kyMjIoNjZWqnJVWloaxcbGKiUXzqtfPMnOzqarV6/Svn376J9//qHr168r/J1lMW/ePDIxMRHULjMzM6lnz570yy+/KK2SlptTp05Rv379SFdXl8zNzWnWrFl0/fp1le2pCg/VNS0tLapQoQJNnz6d7t27V0g9Z6Ns2bIUGxtLRESpqakkEono6tWrQvu9e/eYlcT09fXp2bNnMtufPXvGJK0uS0WsUaNG1LhxYypVqhTzGMNzvGJRjcqrciWNvn37Uq1ataTaW758Oeno6FBwcDBTn6SRnJxMGzZsoMaNG6ukGJuUlER79uyhUaNGUe3atUkkEpGuri7Z2NgofC9PRSze6lpERKGhoVS+fPl8iljly5enQ4cOMdvZtGkTlSxZksRiMRkbGwv305IlS9LGjRuV6lNgYGC+e/Aff/wh9M3KykpC7U8WCQkJ5OzsTFpaWsJ7tbS0qFOnTpSQkMDcHy0tLXr16pXwXE9Pj+Lj44Xnr1+/LhIl4ryqjoaGhmrNPXiRV2VbW1ubHBwcCkS5/t27d7Rw4UK5x2RlZZGLiwuJRCKqXbs2devWjbp27UoWFhYkFoupb9++zPd73vOrvPPjvPNiZefHBYGq35G36ui1a9fIxsZGQr05h+TkZLKxsaGYmBiFdgwMDOjatWtyP8fAwEClPqakpNCmTZuoWbNmJBKJqGHDhkzv2759O2lra1OTJk3IwMCAAgICyNjYmIYPH07u7u5UokQJZjVweWqHWlpaNHr0aAmlY3VQRr2ZiKhGjRp04sQJme3Hjx+n6tWrM9lSpFzJOv7xVMAk+jmvmz9/PlWvXp3Kli1LYrGY/vnnH272iwqev1P58uUlFOInTZpEHTt2FJ4fPXqUatWqxdwvU1NTGjRoEPn7+yt1b5eFp6cniUQiMjIyEubZRkZGJBaLacaMGUw2eN4HJ06cKPPh5uZGenp6RX6fyEGjOsrA/PnzYWNjI9SIuH37Ntzd3TF06FBYWVlhxYoVMDU1zac6KIvcYbtEhHr16uHYsWNM3vi8ODs748KFC0hNTYWpqSlsbGywevVq2NraKr3bNm7cOPTo0QP79u0TdpUtLCzQq1cvVK1aVem+8eDAgQNwcXERQoe3bNmCESNGwNbWFkZGRpg/f74QbVNY7NixA+vXr8fly5fztZUsWRJubm6YOHFivoK0RYkyYdrh4eFwd3fHs2fPJBSQzMzM4O/vr3RB4gULFuDjx49wcHBAREQEZs+ejaioKISFhalVT8be3h729vbCd/P398eyZcsKvYAzD9W158+fCylAf/31F1q2bFlkdb3y8vHjR6HAvIGBAfT19SWiLk1MTJhVML9+/ZqvDkluSpYsifT0dIV2ZKmIxcTEYObMmbhz5w7++OMPpj7xpF69eti4caPUnb1v375hxowZ2LRpE378+CHXzo4dO9ClSxc4ODggMjJSiNBZuXIlPD09sX37dvTp00fp/oWFhcHPzw8HDhxA9erV0atXL6baL8DP6JeclNEHDx5AW1sbzZs3h4uLC2xtbdGqVSvmNFSeilg8bQE/i+8/e/YMJ06ckBDGcHBwEER5WBg5ciQ6d+6M4OBgCTu9e/dmKmydm02bNmHEiBHC8xMnTiAgIADbtm2DlZUVxo4diwULFshVzUtMTIS1tTV0dHSwaNGifEW3W7ZsyVx0Ozs7WyJiU0tLS2J3WZkaKTyj0ChPgkbe50VF3lTBgpobJCUlYcmSJdi6dSvmzp0r8zgfHx+cOXMGhw8fzpelcPjwYQwbNgxr1qzBxIkTmT5XnkJyDqxKrzzT2niphfJSueZ9Pq5cuRJ2dnZSy6CULl0a9vb2WLFiBXbs2CHXTt26dXHmzBn89ttvUttPnz6ttAhWZGQk/Pz8EBISgvT0dEybNg27du1CrVq1mN6/cuVKrFy5EuPHj8fZs2fRpUsXeHt7CyqmderUgY+PD1MUjayUNiMjI5ibmytV/yqvUEBelFUn7NatG6ZNm4YmTZrki0B6+/YtZsyYoVTqfF7V5dywztN4ERwcDF9fX1y4cAHOzs5Ys2YNnJycoK+vr1QGSg6PHj3CoUOHBLV6MzMzdO/evchSDxMSEqRGjalSVy01NVUiU+j8+fMS53bdunWZ0oeBn+nGObXbx44di/T0dFSrVg12dnawtbWFra2t0kqf3t7e6Natm4Rg2O+//66UYBjP+6C0Oo+ZmZnYsGEDvL29UblyZZmlsPKybds2qa+XLl0atWvXZs64kkmRufj+RVSsWFEigsPT05Nat24tPA8ODiYrKyuV7auzA9ivXz/avHkzPXz4UOXP5w3PCJHffvuNPD09KTs7m/z9/UlPT09iZ3/z5s1kaWmpsD/e3t60Zs0aWrNmDenq6tLcuXOF5zkPVlq3bk27d++W2b53715q27Yts70cCiKi7fTp00LUV5UqVWj8+PFyj3/06BGVKlWKbG1t6eDBg3T//n26d+8ehYSEULt27UhfX1/lPg4cOJB0dXWpXLlyQoSUMlStWpXev38vPF+3bl2+nVzWiDYnJydKTk4Wni9evJg+ffokPH///r1a17Q6RERE0ODBg8nAwIAMDQ3J3d2dLl68yPx+Dw8PSk1NFZ5v27ZN4vmnT5/IycmJ2V7eqCEDAwMhupRIuetZJBLRtm3b6NChQ1IfQUFBKu1CPXnyhAYMGEDa2trk4uKi1HjIc7xasWIF6enpUb9+/ejDhw/C61FRUVSzZk2ysLCg8+fPM9n6+vUrtW7dmtq0aUPfvn2j1atXk7a2Nu3cuZPxm/0kMTGRFi1aRGZmZvTLL7/Q2LFjSVtbW+loUh0dHWrZsiV5enrS6dOnpUb0siASiahv3740dOhQuY/CtlWcKVOmDN26dUt4PmrUKOrZs6fwPDw8XGHU7LBhw+j333+nb9++5Wv7+vUr/f777+Tm5sbUH0X31MWLFzNfMzyj0HhH0xdHPn36RP3796dy5cpRpUqVaM2aNZSVlUVz584lPT09atq0Ke3atUuujfr165Ofn5/Mdl9fX6pXrx5Tf1gius3MzJT6joq4efMm03GBgYHCIyAggHR1dWn58uUSr7NkfLRr145sbGzkPmxtbVX+Pk+fPqW7d+8qnQnx66+/yp1L3bp1i+m337x5M+nr61NoaGi+tsOHD5O+vj5t3rxZoZ1Xr16Rt7c31axZkypWrEiTJk2iq1evqnS/0dfXl5hn6OjoSHzX+/fvU9myZZWyyYOcaCFZ57qpqalSY8zHjx/J3NycDA0NycPDQxhDR44cSYaGhmRubi4xl5AHj+yKnO8YHx9PKSkpch+K0NLSolmzZtHnz58lXlflfFiyZAlpa2uTWCymihUrUoUKFYSMqxUrVjDbiYyMZHqwcPToUdq2bZvEa4sXL6aSJUuSlpYW2dvb08ePH5ls/frrr0JkY2pqKpUoUUJirnj9+nUqV64c47f8P378+EGRkZG0YMECsrW1FSK9LCwslLZVnNmxYwf9+uuvVKlSJdqwYQNlZGQwv9fY2FjqI+d869y5c75zWBk0jjYGSpYsKZGW0bp1a1q0aJHwPCEhQeWwaqLikTLIKwSdiO8k3MDAQEhJycrKIi0tLSEFkejnb6+npyfXBu+JYPny5eWG4j558kSlAZHXeaBOmPaYMWPIzs5Oalt2djbZ2dnR2LFjmfsyadIk4TF27FgqWbIkdejQQeL1SZMmMdnKu4jKuyBTBp6LOxYHoCqkpqbS1q1bqVWrViQWi6lOnTpM7+OdPiUSicjZ2VlmuLezs7NSC2FFD2X69u7dOxo7diyVKFGC7Ozs6MqVK8zvzd0nXuMVEVFcXBw1bdqUKlWqRPv27aPx48eTtrY2TZw4UWnnVHJyMjVs2JDq1KlD2tratH37dqXe7+TkRIaGhuTq6kpHjhyhzMxMIlJtort+/XqpThpl4ZluwTvFhehnaYDly5dT48aNSV9fnwwMDKhx48a0YsUK5vSiR48e5UvFOnPmDNnY2FCzZs3I29tbqT7p6enR06dPhecNGjQgHx8f4fmzZ89IV1dXro1KlSrRuXPnZLZHRkZSpUqVmPrDa0FHxNc5JhaLuW0KFAaqOFg8PDyoSpUqNGXKFKpbty6JxWJycnIiW1tbioiIYLKhq6srN4X/6dOnCs+nwkbddHeiop9r80oBz6FkyZIS53denjx5wvx/HDBgAIlEIrKysqLu3btTjx49yNLSksRiMfXr14+5PwMHDqQTJ05InNOq3G+MjY3p/v37wvO8/7snT54wlZnIzcOHD2nFihU0ZswYGjt2LK1cuVLp86FGjRq0d+9eme03b95U+vz8+PEjjRo1ikxMTIR5kImJCY0cOVJibllYyEu1VSZ9+48//qDSpUtTq1at6O+//xacTsqeD2FhYSQWi8nLy0vCcfXhwweaO3cuaWlpMTvHZJVrUmUOamtrS+vXrxeeX7hwgcRiMS1evJhCQkLI0tKSeX0zffp0srS0pG3btlG/fv2oWrVqwnyN6KczPHeAj7J8/fqVTp06RVOmTBFSPpUl7/WzatWqIvddHD9+nBo2bEhGRka0cOFC+vLlCzfbWVlZdOXKFWrQoAFNmTJFZTsaRxsD1apVEy7i79+/k56eHp05c0Zov3XrFpmYmKhsX52bP69Fvqurq9y6Ht7e3jRgwAAmW8V1Es6LUqVKyd1FjI2NZZoA5I2oUzfSbu/evWRvb0+lSpWi3r1708GDB+n79+9K3dTq1q1Lhw8fltl++PBhqlu3LnOfFO0CK7MTzLMWDM/ziqcDMC/x8fE0e/ZsKlOmDGlra6vUH3WvGUXRQkURNfTlyxeaP38+GRkZUZMmTejkyZMq2+I5XuWQmZlJffv2JbFYTAYGBkx12XKTO8ovp9ZXnz598kUAKkJLS4smTZqUL8JPlYUPr/pCPOsU8a55lBNFKBaLycHBgSZMmEDjx48nBwcHEovF1LZtWyZnY/fu3WnOnDnC8ydPnpCenh45ODjQ+PHjycDAQKm6p5aWlhQSEkJEP53LWlpaEo68y5cvU4UKFeTaKFGiBCUmJspsT0xMpBIlSjD3iRe8x2JemwI84elgqVatGp0+fZqIiB4/fkwikYgmTJigVH9MTEwURkKpM6fNy4sXL1R+79mzZ2nAgAGkp6dHlpaWNHv2bIlaRsqg6pzBzMyMi8PD2tqa/P39hefHjx8nbW1t2rFjB12/fp1atmxJ7u7uzPaqVKlCx48fl9l+7NgxqlKlCrO9vXv3Urdu3ahOnTpkZWVF3bp1k+tUyouFhQXVqFGDPD09JWrNqnK/adq0KR08eFB4npKSIlE38PTp00pF5fCKiOrVqxdNnz5dZntMTAyJRCJme7nJzs6mN2/e0Js3b1SqicwrU0MkEtH+/fspIiJC7oOFr1+/UmBgIP3+++9UsmRJ6tq1a76ACUW4uLjQiBEjZLb/8ccfzM7gMmXKUPXq1cnLy4vi4+MpOTlZ6oMFnnXV0tLSaODAgWRsbEyWlpb55ow2Njb0559/MtkiIvr27RudPXuW5syZQ23atKGSJUuSpaUljRw5knbu3Kn0mMzr+uHF5cuXycbGhnR1dWnixIn07t27AvssZceavGgcbQyMGDGCWrZsSVFRUTR58mQqW7Ysff/+XWjfsWMHNW3aVGX7eXddlYHXIp9XCDpv8u5QGxoaFsgOtTKDTsOGDenvv/+W2b5hwwamgq8si3tlfnMeYdqGhoYKo/XUid5Uh3+Lo03dXfO0tDRhYiIWi6lWrVq0ePFi5nO0ODqneVOhQgUqVaoUzZgxg2JiYig2Nlbqoyj48eMHzZo1i3R0dMjV1ZVMTEzIzs5ObgRJXnhF/l28eJGGDx9ORkZG1Lx5c1q3bh29fftWpYUPr+ix4hzRNnfuXKpWrZrUcycmJoaqVatGXl5eCu1UqVJFIt170aJFEvcEX19f5qLgRD8nuRUrVqSFCxeSjY1Nvs2O1atXU/v27eXa4Fl0myc8o9CK46YAEV8Hi7a2Nr18+VJ4rqenp9SilYjI2dmZRo0aJbN95MiR5OzsrJRNabx+/ZrGjh2rdHQcr3T3vKh6b+Y1zvBIAc/N0KFDqU2bNlLbsrOzqU2bNoV+vp8/f56GDRtGBgYG1KRJE1q1ahVpa2tTXFycUnb2798vN0pp6dKlEpsZ8uAZEXX37l2JUkJ5+fHjh0T0sSK+fv1Khw4dkpqalpKSQocOHWIW21O0HiwqMYQcHj58SDNnziRTU1MyMjIiV1dXYQNJHjVq1JAbjR0VFcV83Xz//p327NlDDg4OpKenR7169aJjx46p5NjMGxncrFkzWrZsmfD86dOnSkdd8uD3338nPT09qlevHo0ePZr27t1LSUlJKtvjef3wQiQSUalSpWjSpEn5glNUCVSRR0JCglr/R42jjYG3b99SmzZtSCQSkaGhYb6Bwc7Ojjw9PZnt5Sh25Dy0tLSobt26Eq81btyYyRavRT7PEHSeiEQiQbEtJ6y6dOnSwnNjY2O1nAaqTASXLVsmocSYm5iYGCpbtqzEYFtY8AjT5qValJuUlBSpqTFZWVlKRV/yrLXHc3HH6xo8f/48ubm5kaGhIenp6dGAAQMoLCxMaTv/BkdbcHAw9ejRg+rWrUv16tWjHj16MCuIEVE+h5O6ynRZWVnk5+dHnTp1EvrUtWtXCgoKUmoCdvPmTapXrx6ZmZkJ/7uXL1+Sk5MTGRkZ0datW5lt8SQtLY38/PyodevWpKOjQ2KxmHx8fJSqO8FL4TMiIkKp+hmFZYuIyNzcXG6afXBwMJmbmyu0o6urKxGhZGdnJ7EojI+PZ1bqJfp5fs6ZM4caNWpEjo6O+RatvXv3Jl9fX7k2JkyYQPXr15f6P3zz5g01aNCAOTKKZ7p8cY1C4wlPB4uiexcLFy5cIB0dHerTpw9dvnyZUlJSKDk5mS5dukS9e/cmHR0d5lqSPGrG5YZnunteitrRxiMFPDc540jz5s1p7969wqbTnj17qFmzZlS6dGl69OiR2v1WhdTUVNqyZQtZW1uTSCQiGxsb2rJlC3eVaBZ4RkTxxsfHR2a5FiKi9u3bS6QnyoPX3K+gHG05ZGVl0eHDh6lbt25MUdR6enoKo7FVWZ8+f/6cFixYQL/++itVrlyZPD09lZpPFFRdtbx8/PiR1q5dy7w5p62tTVWrVqVx48ZRSEiI2tFexfH6KczaoGfOnFErok1EVExkmf4FpKSkwMDAQEJtC/ipWGRgYIASJUow2VmwYAHTcV5eXgqPEYvFSEpKwi+//AIAMDQ0RGxsrNIqLFWrVsXWrVvh6Ogotf348eMYMWIEEhMTFdriqSIWFBTE1P8hQ4bIbEtOTsaYMWNw6tQp6OjoYObMmRg7dizmz5+Pv/76C3Xr1sXkyZPh6urK9FkZGRlwcHDA+fPn0aFDB1haWkIkEuHevXs4c+YMWrdujdOnT0NHR4fJHk++ffuG4OBg+Pv74/Lly+jYsSOOHj2KmJgY1KtXT+H7xWIxwsLCJFQlc/P+/XvY29szK3seOHAAM2bMQExMTD7Fvq9fv6Jx48b466+/0KVLF4W2atSooVBBTCQS4cmTJwpticViODk5CWqFoaGhsLOzExQ+v3//jhMnTjB9T7FYjMWLFwsKQzNmzMC0adOUUl2zsLDA48eP0bhxY7i7u6N///75FHpYEYvFGDFihPB7b9iwAQMHDhTsff36FVu3bi10dVbgp1qhq6sr9u3bBwsLC1haWoKIcP/+fcTHx6NPnz7YvXu3wv/zs2fPmD6PRcmZiNC5c2ccP34cDRs2FPp079493L59G127dsXBgweZPq9kyZIYMmQIVq1alU9xytfXF1OnTkWrVq1w7NgxJnsFwYMHD+Dn54ft27cjOTkZ9vb2OHz4MADgxYsXMDU1hVgszve+vNeMLBQpfMpSeMrL4MGDFR7D0xYA6Orq4tGjRzJVthMTE2Fubq5QGbdy5co4cOAAmjdvjuzsbJiYmGDnzp2CwuO9e/dgbW2NlJQUpn59+fJFKQUzaXz69AktWrRAUlISBg4cKChpxcXFYdeuXahYsSKio6Nljv25yTvvMDIyQkxMjErqb8OGDWM6LiAgQCm7RIQPHz5AJBJJqLkVBaVKlcK9e/eE8ahhw4Zwc3PDhAkTAPxUna5duza+ffum0Jaie1cOiq7DAwcOYMSIEfkUN01MTLB582b06tWL6buNHj0aoaGh6Nu3L06cOIF79+6hY8eOSE9Ph5eXF9q1a8dkJwdtbW2MHz8eHh4eMDc3F17X0dFBbGysUkrledVC894Lc1CkFqpoXpRDgwYN5LZbWVnB29sbPXv2xPv371GxYkVcvnxZUPu8cuUKunbtiqSkJLl2cnPt2jUMHToUcXFxwn2TiFCnTh0EBASgWbNmCm2IxWKmuVVmZqbcYxYuXIipU6fmm+vdu3dPuOd8/PgRGRkZCvskjxyleT8/P8TExCg83szMDNu3b0ebNm2ktp87dw6DBw9WW/FW2X4BQPPmzTF37lyZc+AjR45g4cKFuHLlikJbitaDb968gampqcK5n5mZGa5du6b2uPn161dMmzYNBw8eREZGBjp06IC1a9dKzI3fvn0r9Jf1e+WF9XvJIiEhAe7u7oiMjMS7d++Y7oHAz7n+4cOH4enpiWPHjuHixYt48uSJ4CPYsmULtm3bhvPnz6vUrzNnzsDPzw8HDx5EuXLl0LNnT6xZs0bh+9LS0nDu3DlEREQgPDwcMTExsLCwQLt27WBjY4N27dpJVUuVRWFdP8UNIsLNmzfh5uaGDh064K+//lLJjsbRpiTFafIG8FnkAz8nu/Hx8Th37ly+Nvr/Mr61atVimuxqaWnh9evXMifh6g6KecnMzIS2trbMdt4TQeCns2316tXYtWsXHj16BCKChYUF+vfvj4kTJzI5XatVq4abN28K59H69esxePBgqVLtqvDo0SP4+/tj27Zt+PLlCzp16oTevXujZ8+eMt+TM+GSNizkvC4SiZj/dw4ODnBxccHw4cOltvv7+2Pv3r04efIk25fixNChQxVOLAG2xR0PB+D48ePh7u6Ohg0bKvw8RdjY2DB9N1mS9wXJqlWr4O3tjaCgIMHpkMPhw4cxbNgwzJ07FxMnTiy0PgUEBGDChAk4dOgQbG1tJdrCwsLQvXt34dpUxPHjx+Hk5CSz/fnz53B3d8fp06eZ+rZv3z7s3r0bDx8+hEgkgrm5Ofr37y8h+64qWVlZCA0Nhb+/v+Bok+cwEYvFcHFxgZ6enly7iq4ZsVgMAwMDaGtrSx1ngJ/XS14HQEHbAoBffvkFx48fFxa+ebl69So6deqEt2/fyrXTv39/pKamYuPGjdi3bx+8vLyQlJQkOENCQkKwcOFCxMbGMvXLzMwMQUFB+P3335mOl8WnT5/g6emJvXv3Ijk5GQBgbGwMFxcXeHt7M89peG3wFQRJSUmYPn06Dh8+jNTUVAA/z+sePXpg6dKlqFChQqH3iaeDhadj8uvXrzh58iQePXoE4OeGj4ODQz5HiTyqV68OPz8/dOjQAU+ePEGtWrUwfvx4+Pj4MNvIzaVLl+Dv74/g4GBYWlpi0KBB6Nu3L0xNTZV2tLHcC0UiEcLCwuQew2tetHTpUqxduxajR49GWFgY3r17hzt37gjtPj4+OHLkCM6cOSPXjjRiYmIk5qKNGjVifu+hQ4dktl28eBHr1q0DESl0BOed++clMzMThw8fljsHlYeqjodSpUrh4cOHqFKlitT2Fy9ewNzcnMnRzbNfwE/HdmxsLKpVqya1/fnz52jYsCE+ffqk0JaWlhaSkpIEJ4qhoSFu3boFMzMzAPzXXoqYNm0aNm7ciAEDBkBPTw+7du2CjY0N9u3bp5SdvOvcvKSmpmLevHlKfa/v378jJCQE/v7+uHTpEjp16gQ3NzeZwSbS+Pr1K0aOHIkjR46gYsWK2LJlC9q2bSu029rawtHRETNmzGC2+fz5cwQEBCAgIABfvnzBp0+fEBwczLzxIY3U1FScP38e4eHhiIiIQGxsLMzNzSXGHnkU9PVT1JiYmEi9T3z58gVZWVlwdHTE3r17Vd/sVDkW7n+M169f06BBg6h06dJCAVtjY2MaNmyYSrnP0dHR5OnpSdOmTSvwQt4s4ZM8Q9ALK3Xt7t27NHnyZPrll1/kHsejeLCysMjP8y6iL+szlQnTfvr0KdODlUqVKsk9bx49esSsdPdfhmedjuJM/fr1yc/PT2a7r68v1atXT6GdZcuWSSh4RkZGSvw+nz9/Jg8PD6Y+2dvb09KlS2W2e3t7k4ODA5MtFlgKeWdlZZGLiwuJRCKqXbs2devWjbp27UoWFhYkFoupb9++KtUUUYS81Cpe6SR16tShsmXL0oQJE9Suo8fTFtHPFIncKX156dmzJ/Xp00ehnSdPnlDNmjVJJBKRtrY2bdy4UaK9W7duNHHiROZ+TZs2jXR0dGjy5MlcxgF1i27zrkvJi5SUFDIzM6Py5cvTxIkTadOmTfT333/TuHHjqFy5cmRubk6pqamF3i8eNfaKKzxqxkmDR7o7L0QiEV29elXteRFLCnhRlRfIy71796h79+6kpaVFgwcPZqoxWhAph8+ePaP58+dT9erVqWzZsiQWi+Wm96vSL1XWJDz6RfRz7MyrUJ2ba9euMddF5pWGb2trq/AhL901h19//ZV2794tPL98+TJpa2tLqGmywFOw6vLlyzRq1CgyNjamxo0b05o1a+jDhw9K9acg4CFmJ4usrCyKjo6mpUuXkoODA5UqVUqp870grh914SX8QfRTrEjaY//+/UrXlpSGJqKNgc+fP6NRo0b48uULBgwYIKQWxcXFYffu3TAxMcGNGzeYvZ0HDhxAnz59oKurC21tbaSmpmLlypWFGsUhDR4h6AC/8GVpfPnyBXv27IGfnx+uXr0Ka2tr9OrVC5MmTZL5Hh0dHTx79gympqYAfnrnr1y5wpRKqQwpKSnYuXMnfH19ERsbq/D78Y4KEIvFaNy4MYYPHy4z9ZAlTJsnenp6uHnzppCmlJd79+6hSZMmTDshPCMAFe28FjZr1qzB4cOHcfbsWantHTp0QI8ePTBmzBiFtn799VdcvXq1WETc5kVPTw8PHjyQuXv77NkzWFpaKr1zrk7UbMWKFXHixAmZEQA3b96Ek5OTUuk8eVF2bCiqyD95YxDPa+by5ctCNGutWrXg7u6OAQMGqHQt87QVFxeHFi1aCOUEcqdXrl69GnFxcYiOjkbdunUV2srIyEBcXBzKly8v3HtyiI2NRZUqVZS6RqOjo+Hm5gaRSITt27ejSZMmyn05jvCKpAeA169fY/369fD29gYAtGnTBl+/fhXatbS0cPDgQVSuXFmhrUWLFmHbtm24ePFivtSYt2/fonXr1hg2bBg8PT0V2uJJdnY2vLy8hMiHVatWwcrKSmjv06cPHB0d4e7uzuXzFN3neaZcK4qi4YG8dHd58LoXKkpd44miDI3c1KlTB+fPnxdS3UaMGAFvb2/hf/H27VvUqFFD4npSxKtXr+Dl5YWgoCB07NgRS5cuZZ4ri8VivHnzRqm0NFkEBwfD19cXFy5cgLOzMwYOHAgnJyfo6+srHdnIMyKKZ78AwNraGj169JAZ9fTnn3/i4MGDiI6OVmiLV7SrvPXU58+fsXv3bnz//l3h71WiRAkkJCRIjN16enp4+PChzPIMBY1YLEa1atUwZMgQmZHrANC1a1e1PkfZNGJtbW1Mnz4ds2bNgqGhofC6Kinz2dnZuHbtmpA6euHCBaSlpaFy5cqwtbUVHiylVYCCiShUl8LOnFMLtV11/wMsXLiQatWqJbOIcK1atcjb25vZXtOmTcnd3V0ourho0SIqW7Yst/6qy82bNyk4OJj27t3LFJmVF56F5nM4d+4cDRkyhAwMDKh+/fqkpaXFXKyXR/FgeZw9e5b69++vtPw876iA3AqD6hTTl0dISAjVr1+f+XhLS0vavn27zPZt27ZR7dq1mWzxjADkufPKozB4s2bN6PDhwzLbQ0NDqVmzZky2CrqQrTqYmJgoVDc2MTFRaIdn1KyOjg69evVKZvvLly+ZCvZK4+zZszRgwAClxwZekX/KUhgRbbn5+vUrBQUFkY2NDZUqVYr69++vcsQWL1uXLl2iOnXqCIIaOeIaVlZWdOHCBWY7skRgMjMzVRYOSE9Pp6lTp5Kuri516dJFiFTIecgjrwiTrAcLPAsRz5kzh0aPHi08NzAwoPHjx9P8+fNp/vz51KJFC5oyZQqTrRYtWkioe+bFz8+PrK2tmWwVV/T09CTmMx07dpQYv1jGvhxhrxxBKWkPlnE4x5a8KBqWc5OVzMxMOnjwIHXt2pW5b8VNKVkWrBkauWFRmRSJREy2kpOTafr06aSnp0ctW7akqKgo9s7n6k/9+vW5jDFaWlo0a9asfBGMqkT48IyI4tkvIqLNmzeTvr4+hYaG5ms7fPgw6evr0+bNm5W2y5uMjAzy8fGh8uXLU61atSQi1WSRd+1FxH/9pSy8FN1lcfr0aerXrx/p6upSlSpVaPz48Uzv4yFml4OhoSGJxWKqXLkyDRgwgLZu3Urx8fFKf5cceF4/vOC5BkhJSVH4SEtLU7mvbNsm/+McPXoUnp6eUndpfvnlF8yaNQtbt25l3iV98OABdu7cKexaTZs2DfPnz8f79+/z7QgrgmeUz+fPn2FgYIBGjRpJRHdkZ2fjy5cvzDaJCEOHDhWK9aanp2PUqFESheZZWb58Ofz9/fHlyxe4urri/PnzaNiwIXR0dGBiYsKlPzkoKh6cmxcvXiAwMBD+/v5IS0uDi4sLMjIyEBISotTOg6+vr7BLkJmZicDAQJWiAgCgZcuWaNmyJdauXYvg4GAEBASgQ4cOqFGjBtzc3DBkyBCZOfa52bp1qyAcMWHCBLRo0QJhYWGYMmUKHjx4gEGDBjF/v549e2L27Nmwt7fPVxsnKSkJc+bMwcCBA5nt5YaKSTDuixcvJHZNPD094ezsrNQ1+OjRI7n12Ro0aCDU0fk307JlS/z999/4+++/pbZv2LABLVu2LNQ+ZWVlyY0g0NLSUlgEOjc8xoZHjx6hQ4cOMts7dOiAsWPHMveJB+Hh4cxFglnR09PD4MGDUaNGDXh5eWHPnj1Yv369QsGFgrRlbW2Nu3fvIiYmBg8fPgQApWseyROB+f79O5o1a8YsApP3vW/fvoVIJELp0qWZI18AoHv37sLfRISlS5di1KhRKv1Pnz59qvR7ZBEaGooVK1ZIvDZhwgRhZ9ra2hqTJ09mKkT88OFDtGrVSmZ7q1atMHXqVPU6XMSkp6dL3PsuXLiQLwJY0b3RysoKb968wcCBA+Hm5qawiL888gpRKbqfyxNcAQA3NzeFn1nY0drt2rVjFjtTBmkZGjNnzlTZnrT/O0u91uXLl2PZsmWoWLEidu/ejW7duqnch44dO6ot3AL8PA82btyIyMhIoU4f65w/LzzHK579An5GIUZFRaFr166wtLRE7dq1BXG1hw8fwsXFBSNGjODWf1XYuXMn5s2bh2/fvmH+/PkYMWIE070n79oLkL7+UrT24rnOzc7OVvo9iuBRV23Lli1Ys2aNIGY3ceJEdOzYEUSkdJ9XrFgBW1tbWFhYKPtVpMLz+imOGBsbM42T+vr6sLe3x5o1a5jW0jloUkcZKFOmDC5duoTatWtLbb9//z5atWrFXHBZWhi6qmmDvNS/eKpD8izWq62tjRkzZmDhwoUSaq/KhNMq2x9FE0FnZ2ecP38enTt3xoABA+Do6AgtLS2lQ3x5qmjK4vHjxwgICMC2bdvw+vVr2Nvby1U8/Ouvv+Dp6YkGDRrg3r17AIDZs2dj1apVGDduHMaMGaOUMzg1NRUtW7bE8+fPMXDgQIlJxM6dO1G1alVER0dLhErLgmeqrVgsRlBQkEJlT5bwcR79MjQ0REREhMxQ9uvXr8PGxkYo7q2oPzwU0gqCixcvwsbGBt27d8fUqVMlFD5XrlyJQ4cOITw8HK1bt5Zrh2d6uiI1TWUUaHmNDWXKlEFERITM/9Ht27fRrl075nsOK/LuH7wVPl++fImgoCAEBAQgLS1NWPTLSjMvLFs5G055x39lNpwKQgTm1KlTcHd3h6mpKYKCglT6brkpaAGDly9fMqV7Ghsb48aNG0I/evbsib///lvYmHn69Cnq1KnDlP6mra2Nly9fyhQ8SEpKQpUqVZRynPNAlqqjkZERateujenTpzMXiOc19vFMuVYGRXNUsViM6tWro3HjxnIFTlg2RgvyXpieno69e/ciLS0N9vb2Egqpijh//jx8fX0REhICMzMzxMXFITIyUuF9Ly+8zgWxWAw9PT106NBBYp6dF0W/Oe8U22/fvgmOh8uXL6Njx444evQoYmJiuJd+Kep+BQcHSxVXc3Fx4dx7dk6cOIGZM2ciISEBU6dOxeTJk/MFKMiD19qLp8o1T3inEedGFTG7/yV4Cn9ERkYqPCY7Oxtv3rzBhg0bYGhoKHcdnQ+VY+H+h9DS0pIrePD69WvS0tJiticSiWjbtm106NAh4VGqVCnasmWLxGustnikH9rb28stwurn58e1IDgr3t7eZG5uTlWrVqXp06cLRXZ5FIiUhaKURC0tLZo0aRI9fPhQ4vWC7JM6pKam0qZNm6hMmTIKQ2ktLS2FlLXw8HASiUTUvn17iSKTypKcnEweHh5UpkwZITS7TJky5OHhoZRdkUhE3t7etGbNGlqzZg3p6urS3Llzhec5D1ZbvMLHeVyDLVq0oD///FNm+9KlS6lFixbM/clJd5P2ndQNjVeX/fv3U7ly5YS0vJxH2bJlmYsJKzoXFi9ezPwdhw4dyvRggdfY4OzsTKNGjZLZPnLkSHJ2dma2x4qi1FEe6WZ79+4lR0dH0tPTo+7du9OhQ4eULo5cELaIfp6b5ubmUtME0tLSyMLCQm6Kdw68RWBGjBhBJUuWpAULFqj1/XJTUAIGr1+/prFjx5Kuri7T8fr6+nLTqW/cuEH6+vpMtqSlKuWmKIo2ExEdPHhQ6iMwMJBGjx5Nenp6FBwczGSLt9gUz/RtFhSddx4eHmRiYkINGzZUu1A5r3vh1KlTJVLAvn//To0aNSIdHR0qXbo06evr08WLFxXaWbZsGdWuXZsqV65MU6dOpZiYGCJSfe7Iq0zLkCFDuNwDxWKx3BTb58+f07BhwxTakcbDhw9p5syZZGpqSkZGRuTq6kohISFM7+VZOJ1nv4orly9fJhsbG9LV1aWJEyfSu3fvCuVzZa29CkJ8J3e5l+fPn9PcuXNp6tSpFBkZyWyDdxqxNJQRsysoCvL6URVewh9EREFBQcz3vLt375KhoaFyfSXSRLQpIq/nNC/KFt2TFSmVGxap8BxbPKJ8TE1NERUVhVq1akltj4+Px++//45Xr14pZTc3z549Q1paGiwtLZl+g9xERkbC398fISEhqFmzJu7evavSDiALin5DnvLzimCNCpBG7t9MS0sLLi4ucHd3h7W1tcz3lCpVCvfv3xeK1ZcsWRJRUVFo0aKFSn3IDRHh/fv3ICKUL1+eKVQ3NzwjAHnuvPIoDL5lyxZMnjwZe/bsyVf8PjQ0FK6urli1ahVTCoFYLMaVK1cUFiRmLYRaEHz9+hUnT54U0mEtLCzg4OCQL5pWFiznAgAkJCSo1U9l4TU28Ir8U5bExESYmppKjWqoW7cul3SznGLEAwYMkBl5BLClzPO0BfCLROMpAgMA9erVw7Zt27gKIKgT0ZacnIwxY8YIJQZmzpyJsWPHYv78+fjrr78EMQlXV1eFtn777Te4ubnJFHpZu3YtAgMDcePGDYW2xGIxSpcuLXNsICJ8/vy5eBRIzsWGDRuwbds2XL58WeGxeeejRkZGiI2NVWknPzdRUVHw8vJCVFQU3r9/r1Y6nDxYzrvv379j//798Pf3x8WLF9GpUye4u7vDwcFBqXkDr3thvXr1sGTJEiHCPSAgAFOmTMHNmzdRrVo1uLm54e3btzh69KhcOzwyNHIjFotRr149IY3v1q1bsLS0FNJcMzMzcffu3UI73xXNq2JjY9GkSRO1+pOdnY2jR4/Cz88Px48fZypFUxiF01XpV3ElJ8Jx5MiRqFGjhszjWO+rrMgaG3hms9y+fRtdunRBYmIizM3NsWfPHjg6OiItLQ1isRhpaWn4559/JEotyGLEiBEIDg5G3bp1JdKIVbmev379imnTpuHgwYPIyMhAhw4dsHbtWol1RGGL2eVQHIUHeGbOKSP09ePHDxw/flyp9HqNo42B4jx546X+xXNhEBQUhE+fPkko4o0YMQJ+fn4AgNq1a+PkyZMqqc6kpqZi586dCAgIwPXr19G8eXP07t0bkydPVtqWLFgH8a9fv2LPnj3w9/fHlStXkJWVhVWrVsHNzY0pFVIeSUlJ8Pb2hq+vL/NiDPi5UA4MDERgYCASEhLQqlUruLu7w8XFhSnkm7cSanGFp4IiLwfgwIEDsWvXLpl1Onbv3s3Un8JUSNMgHR5jw4EDBzBixIh86aEmJibYvHmzUvU/eMEj3Yynw5x3+j2vDScrKyvMnj1bZr2q7du3w9vbG/fv32fq148fP7jXiFJnbB89ejRCQ0PRt29fnDhxAvfu3UPHjh2Rnp4OLy8vtGvXjtnWihUr8OeffyI8PDyf8zY2NhZ2dnaYOXMmpk2bptBWUFAQ02fmrStW1Dx69AjNmzfHp0+fFB6bdz6anJwMIyMjYfNSmfkoz5RrVpQ97549e4bAwEBs27ZNUPJlrQHG615oZGSEGzduCOOCq6srDA0NsWXLFgBATEwMnJ2dFY4LS5YsQWBgINLT0+Hq6opBgwahXr16KjvaFixYwHScl5eXUnZV5dmzZ6hWrZrMMVkZRxtPxwPPUhO8HSKy0spzIxKJCjXdvTDK2khDnqONl8q1k5OT4PDesWMHjhw5AgcHB/j6+gIAxo0bh+vXrzOpvAL80oinTZuGjRs3YsCAAdDT08OuXbtgY2ODffv2MdsoKHheP8WRgl4zaRxtDBTU5O3Dhw9CccfExERs3boV6enp6NKlC9q2bctkg9eAyHNh0LJlS4wYMULwOJ84cQJdunRBYGAgrKysMHbsWNSpU0cY2FTl9u3b8PPzw65du/D27Vu1bOVGlQWIKvLzPKMCAMDe3h7h4eEoX748Bg8eDDc3N5l1BWXB84YGAI0bN2bagWaJVuBJcXVG8ajTUVy/Ww6ZmZlYvXo1du/ejYcPH0IkEsHc3Bz9+/fHhAkToKOjo9BGeno6zpw5I0T/zZo1S2IHWVtbGwsXLoSurm6BfQ9WVBkbclA38q+g+PbtG/bt24eAgABcuXIF3bt3h7+/v0oiBsUJXhtOs2fPxo4dO3DlyhWpIjAtWrTAwIED4e3tzdQv1o2kVatWyWxbu3atxHN1xvbq1avDz88PHTp0wJMnT1CrVi2MHz8ePj4+TP3MTc5i9eLFi7C3txc2Ge7fv4/Tp0+jZcuWOHv2LNO48G/l1q1b6NixI16/fq3wWB7z0RyxpMjISHTs2BHDhg1Dp06d5Nbn4oWy86vnz58Lm4c/fvzA/fv3uTnaWB0/xsbGuHr1qlCHzczMDHPnzhWEG54+fQorKyvmTdHCzNBggbXukzKCYdJQxtHG0/HA01HA2yFy6NAhmW0XL17EunXrQERKbbj/W5E1NvB0/JUrVw5hYWFo0KCBUHP1ypUraNq0KYCfNdetra2RnJysdP/VqatWs2ZNeHt7o1+/fgCAK1euoHXr1khPTy+UcVke/yZHmyqZc2KxGG/evFEY+awqGkcbA5mZmUqpeymCZ+gqL3guDMqWLYuIiAjUr18fAODh4YG3b98iJCQEABAREYFhw4ZxS+vKyMjgOglXZ6c/KysLoaGh8Pf3V7iY5hkVAPws3O/u7o7OnTtLHZhzHJPyFkO8d7Jy77iSHKU7lh3XvAtFWbAsFIcNG4a1a9eqHXlYHLG1tcWBAwdgbGxc1F3Jx7dv32Bvb49Lly6hQ4cOsLKyAhHh/v37OHPmDFq3bo1Tp04pdJBt3rwZR44cQWhoKICf12zdunWhp6cH4Odkadq0aVwjXdUlKysLR44cgb+/v9zJ9b+Jgkg3S0xMhJeXF/z9/QvdFq8NJ54iMMDPazo358+fx2+//Sac78DPcTksLEymjZzUQnmwju06Ojp49uwZTE1NAfwsOXDlyhWVC4H/+PEDq1atwp49ewSlV3Nzc7i6umLSpElKO3CJCNevX8fTp08hEolgZmbGvOlTFIwbNw6PHz9WrsCyGvBOuVYGlkLmuVNHc8Rlhg0bBkdHR6XKjii6F8bGxqJx48YKVf2sra3h4uKCyZMn4+7du2jQoAHi4+OFayoyMhJDhgxRWp2vMDI0WOAtGCYLZRxtPB0PPAunF4ZD5P79+5g1axZCQ0MxYMAALFq0SCjn8l+mMDJoCsNppEoacYkSJZCQkCBRKkhPTw8PHz5UKfuLJzyvH17wzJxTJIiWg6obDRpHGwO//PILBg8eDHd3d1hZWaltj3foKg94LgxKlSqFe/fuCXUvGjZsCDc3N0yYMAHAzx3K2rVrM+3QsDhXRCIRxo0bp/A4VtRVtHnz5g02b96MefPmyT2OZ1SALD5//ozdu3fDz88P165dQ4MGDRATE8PNvrKocyPluVDkCQ8H4OfPn5lssKToXblyBb/99psw6SMiiQXm9+/fcejQoSJRs5o3bx6CgoIQGhoqNU2sa9euGDZsGObPny/Xzu+//45JkyahR48eAPKfVzt27MCGDRtw6dKlAvkessiJcFCEIscPb4VPnhR0uhmPOj6q2uK54ZSSkoJZs2Zh7969QkqgiYkJ+vbtiyVLlqjlCC/qlH5FE2/exMTEoFGjRkzHhoeHw93dHc+ePRNUK3Ocbf7+/vj9998LpI/ykOU4SUlJwbVr1/D48WOcO3cOjRs3Vmjr7NmzaN++vcz27OxsLFmyBHPmzJF5TFGlhgGKz93Ro0djz549qFatGoYNG4aBAwcKmR+8YR0fQkJC4OrqirZt2+Lu3bto1qyZsMkD/IwOTUhIQHBwsMp9USVDo6gyBmTNkRVF7iQnJyMyMpJpPObpeMi7kA4NDYWdnZ1QTkUZZfGCdIi8evUKXl5eCAoKQseOHbF06dIiUVd1dnbG7t27Ubp0aQCAt7c3xowZI9yzPnz4gLZt2yIuLo7r56qz9mKtZ503ekkdpxHPNGJpteAL+r7KCs/rhxc8M+fEYjFcXFwkNi6lwVLvTRoaRxsDS5cuRWBgIOLj49G8eXMMHz4cffv2ZQ5fzwvP0FWeUT68FgZWVlbw9vZGz5498f79e1SsWBGXL1/Gb7/9BuCnI6Br165ISkpSaKsonCvqLmJYJ2+8owJyExkZCT8/P4SEhCA9PR3Tpk3D8OHDZdYeKiyKeoGYA8+6GDzOUUX9yXGWsdzIimPh0hwsLCywdOlSmfXF9u3bh9mzZwtRLbKoWLEizp49i7p16wIAypcvj6tXrwqFex8+fIhmzZohJSWFa/8VIRaLUb16dTRu3Biybq0ikUjhzphYLIaBgQG0tbXl2slbv60gKax0s6J0tPGORAPUF4GRRlGPo4om3jmok2qWkpKCnTt3wtfXF7GxsUz/w/j4eDRs2BAtWrTAhAkTBBGRuLg4rF27FteuXcOtW7cK/XfLG5GYg5GRESwtLTF69GhmcZoSJUpgxIgRWL58eb4U8jt37mDIkCFISkrCy5cv1e53QSBPcAX4v2g7RU4kddMYAeXGhzNnzuDo0aOoWLEixo0bJ/HbL1iwAEZGRpg0aZLafVImQ4NnxoAyyBp/eEbG8XQ8FNd+5ZCSkoIlS5Zg3bp1aNSoEZYtW8ZcPqggKKo5pCr3NWXrWfN0GvFOb84bVSXtvspj3FOWoUOHMs1bVHVEqQLPzDlNjbZixLlz5+Dv749//vkHANC7d28MHz5c6boKPENXC8IRpe7CYOnSpVi7di1Gjx6NsLAwvHv3Dnfu3BHafXx8cOTIEZw5c0Ypu4WFoomgIlgnb7yjAl6/fo2AgAD4+/sjLS0Nrq6u6N+/P1q2bKlUod3s7GwEBgZi//79Eqk3vXv3xqBBg9RaKKq7QJTWt19//RW9evVSqm8HDx6UeWxR1MWIjIwU/iYiODs7w9fXN98OHUs6Mcv4UqlSJYXpMgWBrq4uHj16JHPnNyedPj09Xa4dPT09xMTEyKxBeP/+fTRq1EihHd7kjsZwc3PDwIED8y16WOCl8MmTwko3K0pHG1CwkWi8UGUcDQsLw9ixYxEdHZ0vMjYlJQWtWrXC33//zRTxVZCpZmFhYfDz88OBAwdQvXp19OrVC7169WKK9ho7dizu3buHs2fP5msjInTo0AF16tTBunXrmPpfWCiT4nz58mUMHToUGRkZCAoKQuvWrYUotkWLFqFPnz5Yt25dgamGFjQ8F3WKIsVv3bqFdu3aqTzWKOsMZrkGN23apLKTpbAc8Lw+R14EU1E6HgqzX8uXL8eyZctQsWJFLFmyRCk1w4KiqGpyyVp78axnzfPexTONuLDSt/8r8MycK2hHG7/CY/8DtG3bFm3btsX69euxZ88eBAYGom3btjA3N4e7uzumT5/ObCvvREJV5wWvOme5EYlEahUFnDFjBr5+/Yr9+/ejYsWK+bz7Fy5cYC7wD/BzrrBSWPnwRIShQ4cKN+z09HSMGjVK5agAMzMz9OnTBxs2bIC9vb1KAzARoWvXrjh27BgaNmyI+vXrg4hw7949DB06FPv378fBgweVtssDIkKXLl1w/Phxtfsmrf6htLoYrKh7juZ1oGlpacHa2rrAJsxFVavIyMgIb9++lXmNJSUlMaXHVqlSBXfu3JHpaLt16xaqVKmiVl9VYePGjVi9erVQX2jWrFno1KkT3N3d4eDgwPy73717V1D4/P3331VW+ORJjprcrl27ZB4jEokKpK5TYVK6dGls3LgRGzZsUHnDqTiKwPj4+OCPP/6Qev6ULl0aI0eOxOrVq5kcbcruXNepU0duStCLFy8QGBgobBK5uLggIyMDISEhSikxRkREYOnSpVLbRCIRJk6ciFmzZinV98Lg48ePCAoKYnK0tWjRAjdv3sTMmTNha2uLESNGIDo6Gi9fvkRwcDDTQv3y5cv4+PEjnJychNe2bdsGLy8vpKWloXv37li3bl2RiJsEBgZys2VsbMwUKa4sYWFh8Pf3x/79+wVncE5tIHmwXIOrVq0q0mimwkRenIc0MQ9ZtTMBvo6HwuzXzJkzoaenh1q10gIEMgAAFnZJREFUaiEoKEim2ElRRDIVNrLmhZ6enoiKisKQIUNw4sQJTJo0CSdOnEB6ejqOHz+uVD1rnveuxMREiWu1efPm0NbWxqtXr5ReR/K+p/KEpSSKSCRiGgN5Ub16dVy/fh3Vq1fH+/fvcffuXbRp00ZoT0pKElKfWSjIDB+No00F9PX14e7uDnd3dxw9ehSDBw/GrFmzlHK0yXOwsBROzA0PRxTPhYFYLMaiRYtkOiqUCavl6VwpbuS9Ycu7WQOKb9jVq1fH+fPnUa1aNVSvXl2lmkmBgYGIiorC2bNn86W7hIWFoXv37ti2bRtzbai8qc2ZmZkIDAxUSekuMDAQ586d49a3HPLWxVBWlvu/fI7yxtbWFkuWLBHCu/Py559/wsbGRqEdZ2dnzJs3D506dconnPDt2zcsWLAAnTp14tFlpSlZsiRcXV3h6uqKZ8+eITAwEKNHj0ZGRgbi4uKYSw60aNECLVq0gI+Pj6DwOXXq1CJT+FS20LcsWOr4FIWtvKiz4ZTbkS8vpYuVW7duSTzPERD58uWLxOvyIh9jY2OxbNkyme0ODg7466+/VOqfIuQtXJ2dnYWC9+vWrYOjoyO0tLSwadMmpT/n+fPnQiqJNOrVq4dnz54pbbe4oauri9WrV+Pt27fYuHEj9PX1cfXqVeZ7/vz582FjYyM42m7fvg13d3cMHToUVlZWWLFiBUxNTRXWyizuhIWFcdtU4uEMLspr8N9GcXU88O7X4MGDi51Ii0gk4hYMwoOjR48iICAAHTp0wOjRo1GrVi1YWFhwrWctC3n3rqysLJQoUULiNW1tbaaSM+pSmMmIgYGBCkuiFDaDBw/GmDFjcPfuXYSFhcHS0lIoTwX8zExSZh2X29HWt29frF27Vm7mhjJoHG0q8PXrV+zduxcBAQG4cOECatasiWnTpjG/n8XBwuos4LXI57kwkFVvysjICLVr18b06dOZ5cQLyrmiDooUod69e8dkh/cN+8GDB7hw4QL8/PzQrFkzWFhYCOcW601y9+7d8PT0lFpTxs7ODjNnzsTOnTuZf+/Vq1dLPK9YsSK2b98u8RprFAzvvuWti3H27FmVdpKL4zkaFxcn1EDMuyh///59ofUjL15eXmjRogWsra0xefJkYWEYFxeH1atXIy4ujkkExtPTE8HBwahduzbGjh0LCwsLiEQi3L9/H+vXr0dmZiY8PT0L+usoJGfCSkQqp+rq6elh8ODBqFGjBry8vLBnzx6sX7++SKJN5MGa/qZol7F06dIS14q8DQaetgB+G055ayKtXLkSEyZMUHkR2KhRI+E8yqFz584AILyuqIbjmzdv5NZ+0tbWZr538eTUqVMYP348PDw8YG5urpatL1++5KtblptSpUrh69evan1GceDx48cYOnQoHj16hE2bNiEwMBDt2rXDpk2bBIEYecTExEhshO7ZswctWrTA1q1bAfyMLPHy8vrXO9qaNGnCxQ4vZ3BxvQaVpbg5hoDCdTwog6J+8Yzg5IWibBtlg0HU5dWrV4Iz+9dff4Wuri6GDx9eqH2QRt7fCZCemfRvj0YcNWoU9uzZgydPnqhVEoUnvDPncjtMjx07JjMyXhU0jjYlOHfuHAICAvDPP/8gKysLvXv3xuLFi5VWseJZMJDXIp/nwuDAgQNSX09OTsaVK1cwcOBABAUFoU+fPgpt8Xau8ODmzZsKjykIZTOWiUTr1q3RunVrrF27Frt374a/vz+ysrIwevRo9O/fH927d5cbpXHr1i0sX75cZruTkxOzAAfAN7WZZ99y18XYvXu3WnUxCuocVWcya2dnJ/Fc2qK8KKhTpw5Onz4Nd3d39OvXT+gHEcHS0hInT54UBA7kUaFCBVy8eBEeHh6YOXOmhLqgvb09Nm7cyG03Slm+f/8upI7mLM7Wr18PR0dHpdNapCl8/v3338Wy/hJr+hvPDQbemxW8I9F4wWMcrVy5Mm7fvi1TEOfWrVuoVKmS2p+jLDm1b5s2bQpLS0sMGjQIffv2Vdle7k2GvBTlJgMv1q9fj5kzZ6Jjx47Yv38/ypcvj+HDh2PFihXo378/evXqpbBG26dPnyTGx8jISDg6OgrPmzVrhsTExAL9HoWBotTRHBSlDfFyBvO+BnlmDChDcXVq/RthCToQiUQyswAKgrxRduoEg/AgOztbwkGtpaWVr8ROUVCU6c2FCa+SKDzhmTlX0GjEEBhYsmSJoDrarFkzuLm5wdXVtchq5eTGwcFBWMxLY8mSJYiMjMTJkyeVsluQRVU3bNiAbdu24fLlywqPrVixIk6cOIFGjRpJbb958yacnJyYFEwLivfv36NEiRIFfj6o+j+Ji4uDn58fduzYgY8fPyIjI0PmsSVKlMCzZ89kTvZevXoFMzMz5h0tnsV/efZNLBZDT08PHTp0kFu0lGUnisc5mneypY6K382bN5kcA6wqdwVFTEyMoC5qYWGBRo0aIS0tDdevX1fKUf3x40fEx8cDAGrVqlWkTpHcYgjDhg3DwIEDUbZsWaXtFJbCJ094ihjkhue9SFlbvD5bXTvfvn3D1KlTcfDgQWRkZKBDhw5Yu3ZtvgW1PMaNG4eIiAhcvXpVarp18+bNYWtrq9RGCiss3//r16/Ys2cP/P39ceXKFWRlZWHVqlVwc3NjVnnNiaaXN61lVW/mCUuKc2RkJFO/ypYti7Vr12LAgAH52u7evYshQ4bg9evXclVHq1evju3bt+P333/Hjx8/YGxsjNDQULRv3x7Az1TSdu3aFaqqcUHAS2To0qVL8Pf3R3BwsIQz2NTUVCmhKd7XYEGIobGgrmBYDvJEB5SF532iMPulbBH8/0UKQ+VaFv/W86ogySmJsm3bNqVLovCCZ+Ycb2HCvGgi2hhYvXo1Bg0aBDc3N6VyfgsD3hFIhYGDgwPmzJnDdOzHjx/lRqZUqFBBUIYrTJKTkzF79mwJZbry5ctj2LBhmDt3rtz0lYJClsJWlSpV4OXlhVmzZiEqKkqujaysLGhryx4WtLS0lKo/wLP4L8++8ayLweMczZsCp069vt9++w2NGzfG8OHD0b9/f6UKghYmjRo1yuecjI+Ph62trVIL4TJlyqB58+ace6camzZtQrVq1WBmZobIyEiJhV5uFE0I+/Xrh2rVqmHSpEmoUKECnj59ig0bNuQ77t8uPKCBjXnz5iEwMBADBgyArq4udu/eDQ8PD6V2befMmYP9+/fDwsICY8eORe3atSESiXDv3j1s2LABWVlZmD17doH0n2WsLVWqFNzc3ODm5oYHDx7Az88Pf/75J2bOnAl7e3scPnwYgPyxjyXyryjmC8qmOMvjzp07Mjeb6tati8uXL2PJkiVybTg6OmLmzJlYtmwZDh48iFKlSkncg2/duoWaNWsy9ac4w0tkqGXLlmjZsiXWrFkjOIMnT56M7OxsnD59GlWrVmVyBvO+BgtCDI0FXoJhxTXOozD7VRwdaMWt+D3vetbKwDNai+d5VZTp2zxKoqgLz8w53sKEedFEtDFw6tQpTJw4scAkudWBdwRSDgXpLb916xY6duyI169fKzw2r6c5LwUlMy2Pjx8/omXLlnj58iUGDBgAKysroS7erl27YGlpifPnzyM2NhaXL1/mthhW9D+R5eHPi7zfSpp0eW6+f/+OEydOMP/e1atXx4kTJ2BlZSW1/f79+3BwcMDz588V2uLdN14UxTkqb2cs9+57RkYGevbsCXd3d6mprcWNgoqKKiyGDh3KdA0qmlzXqFFDoZ2CiFRQB01E2/+Rd3NrxowZmDZtmsopXTVr1oS3tzf69esHALhy5Qpat26N9PR0paJKnj17Bg8PD5w8eVIi3bpjx47YuHEjatSoIRzLc7Gi6u+YlZWF0NBQ+Pv7C442VaICUlJSsHPnTvj5+SEmJqbQx5cnT56gRo0aXH5LZ2dn7N69W3DeeXt7Y8yYMTA2NgYAfPjwAW3btkVcXJxMG+/evUPPnj1x4cIFGBgYICgoSKK2W/v27WFtbQ1vb28A/96Up7zwHEtynMHbt29HcnKyhDNYHjyvQZ4ZA0UBr8g4gG+0UHHtV2EhFouZit/LcnYUNcU1cqy42mJBWkmUYcOGqVQSpTBQJnOuoKNKNY42Brp16wZbW1tMnDhRavvatWsRHh5eJIMOr0U+74WBPMaNG4fHjx/j2LFjCo8tjs6ViRMn4uzZszhz5ky+SKakpCQ4ODigdu3aOHXqFNauXSs1j18VFN08eKRJKDvgKJoI6urq4s6dOzJrksTHx6N+/fr49u2bws/kORjyrItRFOcoy03227dvQgriuXPnUKNGDbi5uWHIkCGoUqUKt77w5N/uaPsvwzP9TRn+jY423ildJUqUQEJCgsRYrqenh4cPH6oUXfLp0yfEx8eDiGBubi61nldxXbgq8z8JCwuDv78/9u/fj+rVq6NXr17o1asXGjdurHY/lEFLSwuvX7/GL7/8AkA9VbO8tvL+n5TZ2ElJSYGBgUG+/8vHjx9hYGAgFIj+NzoLpFEQi1NpzmAWeFyDXbt2ha2tLSZNmiS1vSjXJoVNUabSyaO49kseuUtgFJfi98rA8zcvqntXYfZLEbxKohQmjx49QvPmzYskgj0vmtRRBm7evIk///xTZntRSnJLUz3JDWskG091SFmqnCkpKbh27RoeP36Mc+fOMfWLxUlVmEU5AeDgwYPYvHmz1ElyxYoVsXz5cjg7O8PLy4ubkw1QHHbMI02Cd3FxnsV/eYbY80ynLI7nKPBzIT5kyBAMGTIEjx8/RkBAADZv3oz58+fD3t6eydGtoWi4fPkyPn78CCcnJ+G1bdu2wcvLC2lpaejevTvWrVtXqMqjvBU+WeGZIqHIFq/i4rxTurKysiRUsYCfCoXKpPHnxsTEBM2aNZN7DM89WF6pZiy8ePECgYGB8Pf3R1paGlxcXJCRkYGQkBDmWlq8yftbqqNqlteWOv8nWdd03kX1f2k/nnfKlZaWFrp37y4hpMICj2swNjYWy5Ytk9lelGuTwiYuLg6mpqZF3Y18FNd+yaM4Fr8vKgrz3qUMhdkvXiVRCpNv377lq4NZVGgcbQy8ffu22Epy81rk81wYyFLlNDIygqOjI0aPHs1ciL041i94/fq1XGXEevXqQSwW51NyVZfieMNWNBF0dnbGvHnz4OTkJLX4r5eXl6CIWZjwPK+K4zmal5o1a2LmzJmoWrUqPD09lRZH4YWiHf+iqjlT3PDy8oKtra3gaLt9+zbc3d0xdOhQWFlZYcWKFTA1NcX8+fMLrU+8nfCs8FzkK7LFa8OJd0qXtA01aXVEitNEtyhwdnYW0lrWrVsHR0dHaGlpYdOmTUXdNQ1FQN4oXN61d4qaN2/eFNu1SWGjcYjwpWTJknB1dYWrq6tQ/H706NFFVvz+v8C/1UHJs6Z1YbF169ZCj1yXhcbRxgBvSW6e8Frk81wYhIeHc+lTcaVcuXJ4+vSpzPS7hIQEIa2DJ//GG3ZRFuDW8JPIyEj4+/sjJCQEWlpacHFxgbu7e5H0hWXX/992Qy8IYmNjsXjxYuH5nj170KJFC2zduhXAz7HAy8urUB1tysLLQcZzg0GRLV6OXp4iMID0DTVFBaH/Fzl16hTGjx8PDw8PmJubF3V3BHKKR+d9raht/ddRVmTo30ZxXpto+O9QHIrf/xf4t0YGBwYGFnUX8sEzc66g0TjaGCiuUTk84b0w+C/j6OiI2bNn4/Tp0/nSeb5//465c+fC0dGxiHonSVFPwCtUqICLFy/Cw8MDs2bNklr8V5U6Nf/rKPq/JiYmIjAwEIGBgUhISECrVq2wbt06uLi45NvNL0w0kzQ2Pn36JHFdREZGSowpzZo1Q2JiYlF0rdDhucGgyBavDSfeKV3/hqjZwkLe2Hfu3Dn4+/ujadOmsLS0xKBBg9C3b99C7J10eKqaKbKlrPDVf5n/+nXzv7A20VA0SCt+v379+mJb/D43Rb3ukUVxzEr6t8Izc66g0TjaGPhfiMrR1HpgZ8GCBWjatCnMzc0xZswYWFpaAvg5iG7cuBHfv3/Htm3bCr1fxTVNonr16jh27BhT8V8NbMjbGbO3t0d4eDjKly+PwYMHw83NDbVr1y7E3inmw4cPQjHVxMREbN26Fenp6ejSpYvGmY+fDuqEhARUrVoVP378wI0bN7BgwQKhPTU1VW7KkAbV4LXh9F9I6SquixV5Y1/Lli3RsmVLrFmzBnv27IG/vz8mT56M7OxsnD59GlWrVoWhoWEh9vYneSMS1YmsYrHFuyZocT0X/uso+t3/F9YmGgqfvMXv9+zZU+yL3+emuEaO/Ruzkoor/6bMOY3qKCPKSHL/G+GpDvm/QEJCAkaPHo1Tp05JnA/29vZYv369zN+xICloiWJp/FfUyP5tyFMc6tq1K9zd3dG5c+dCUSRShtu3b6NLly5ITEyEubk59uzZA0dHR6SlpUEsFiMtLQ3//POP0oWl/2uMHDkSt2/fxrJly3Dw4EEEBQXh1atXQgTtzp074ePjg6tXrxZxT2Xzb1Rbq169Ok6cOAErKyup7ffv34eDgwOeP38u107NmjXx119/oUePHlLb9+/fj6lTpzKrjhYFxfX/p6za2oMHD+Dn54ft27cjOTkZ9vb2SqlDaii+58J/HZbf/b++NtFQ+IjFYlSrVg2NGzeW6+wtrrUNC1ORU4MGRWgcbUryX43K+S8sDIqCT58+4dGjRwCAWrVq/asksHmgmYBrUAYnJydoa2tjxowZ2LFjB44cOQIHBwf4+voCAMaNG4fr168jOjq6iHtatLx79w49e/bEhQsXYGBggKCgIImxuX379rC2toa3t3cR9lI+/0YnPK8Np3HjxiEiIgJXr16VmtLVvHlz2Nra5lM5LU781xYrWVlZCA0Nhb+/v8bRpiT/tXPh34Iyv/t/dW2iofAZOnQoUxTrfz01W4MGHmgcbRoA/DcWBhoKH80EXIMylCtXDmFhYWjQoAG+fPkCIyMjXLlyBU2bNgXwM2LI2toaycnJRdvRYkJKSgoMDAzyXV8fP36EgYFBvhqRxYl/oxOe14bTmzdv0KRJE2hpaclM6bpx44amPqUGDRo0aNCgQcN/FI2jTQMAzcJAgwYNBY9YLEZSUpKgypvXGfPmzRuYmpoiKyurKLupgQP/Ric8zw0nTUqXBg0aNGjQoEHD/y4aR5sGAc3CQIMGDQWJWCzGmzdvUL58eQA/HW23bt2CmZkZAI2jTUPRUhAbTpqULg0aNGjQoEGDhv89NI42DfnQLAw0aNBQEIjFYjg5OaFkyZIAgNDQUNjZ2QnKuN+/f8eJEyc0jjYNRYZmw0mDBg0aNGjQoEGDumgcbRo0aNCgoVAoCmVcDRpUQbPhpEGDBg0aNGjQoEFVNI42DRo0aNCgQYMGDRo0aNCgQYMGDRo4IC7qDmjQoEGDBg0aNGjQoEGDBg0aNGjQ8F9A42jToEGDBg0aNGjQoEGDBg0aNGjQoIEDGkebBg0aNGjQoEGDBg0aNGjQoEGDBg0c0DjaNGjQoEGDBg0aNGjQoEGDBg0aNGjggMbRpkGDBg0aNGjQoEGDBg0aNGjQoEEDBzSONg0aNGjQoEGDBg0aNGjQoEGDBg0aOKBxtGnQoEGDBg0aNGjQoEGDBg0aNGjQwAGNo02DBg0aNGjQoEGDBg0aNGjQoEGDBg78P0j8SsDzufyrAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print the entropies with the corresponding feature\n",
    "print(list(zip(selected_features, NMI)))\n",
    "\n",
    "# plot the NMI in seaborn in a bar plot (sorted by entropy)\n",
    "# Sort the features by entropy\n",
    "sorted_indices = np.argsort(NMI)[::-1]\n",
    "sorted_features = [selected_features[i] for i in sorted_indices]\n",
    "sorted_entropies = [NMI[i] for i in sorted_indices]\n",
    "\n",
    "# Plot the entropies\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.barplot(x=sorted_features, y=sorted_entropies)\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Normalized Mutual Information')\n",
    "plt.title('NMI of features given Y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe from this that the most dependent features to the target are :\n",
    "- CVDSTRK3 : \"(Ever told) you had a stroke\"\n",
    "- BPHIGH4 : \"Ever told you had high blood pressure\"\n",
    "- DIFFWALK : \"Do you have serious difficulty walking or climbing stairs?\"\n",
    "- CHCCOPD1 : \"(Ever told) you have Chronic Obstructive Pulmonary Disease or COPD, emphysema or chronic bronchitis?\"\n",
    "- USEEQUIP : \"Do you now have any health problem that requires you to use special equipment, such as a cane, a wheelchair, a\n",
    "special bed, or a special telephone? \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 62 features over 62\n",
      "(328135, 62)\n"
     ]
    }
   ],
   "source": [
    "# Keep only the features with a NMI above a certain threshold\n",
    "# threshold = 0.02\n",
    "threshold = -1 # keep all features\n",
    "selected_features_2 = [selected_features[i] for i in range(len(selected_features)) if NMI[i] > threshold]\n",
    "x_train_filtered_2 = x_train_filtered[:, [selected_features.index(feature) for feature in selected_features_2]]\n",
    "print(f\"Selected {len(selected_features_2)} features over {len(selected_features)}\")\n",
    "print(x_train_filtered_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing the class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def fix_class_imbalance(X, y, target_value=1):\n",
    "    \"\"\"\n",
    "    Fix class imbalance by oversampling the minority class or undersampling the majority class.\n",
    "    \n",
    "    Parameters:\n",
    "    X (numpy.ndarray): Feature matrix of shape (n_samples, n_features)\n",
    "    y (numpy.ndarray): Target vector of shape (n_samples,), containing values -1 and 1\n",
    "    target_value (int): Class value to balance to (default is 1)\n",
    "    \n",
    "    Returns:\n",
    "    X_balanced (numpy.ndarray): Feature matrix with balanced classes\n",
    "    y_balanced (numpy.ndarray): Balanced target vector\n",
    "    \"\"\"\n",
    "    # Separate samples by class\n",
    "    class_1_indices = np.where(y == target_value)[0]\n",
    "    class_minus_1_indices = np.where(y != target_value)[0]\n",
    "    \n",
    "    # Find class counts\n",
    "    class_1_count = len(class_1_indices)\n",
    "    class_minus_1_count = len(class_minus_1_indices)\n",
    "    \n",
    "    if class_1_count == class_minus_1_count:\n",
    "        # If classes are already balanced, return the original data\n",
    "        return X, y\n",
    "    \n",
    "    elif class_1_count < class_minus_1_count:\n",
    "        # If class 1 is the minority, oversample class 1\n",
    "        oversample_size = class_minus_1_count - class_1_count\n",
    "        oversampled_indices = np.random.choice(class_1_indices, oversample_size, replace=True)\n",
    "        new_indices = np.concatenate([np.arange(len(y)), oversampled_indices])\n",
    "    else:\n",
    "        # If class -1 is the minority, oversample class -1\n",
    "        oversample_size = class_1_count - class_minus_1_count\n",
    "        oversampled_indices = np.random.choice(class_minus_1_indices, oversample_size, replace=True)\n",
    "        new_indices = np.concatenate([np.arange(len(y)), oversampled_indices])\n",
    "    \n",
    "    # Create the balanced dataset\n",
    "    X_balanced = X[new_indices]\n",
    "    y_balanced = y[new_indices]\n",
    "    \n",
    "    return X_balanced, y_balanced\n",
    "\n",
    "\n",
    "# Usage:\n",
    "# x_train_fixed, y_train_fixed = fix_class_imbalance(x_train_filtered_2, y_train[:, -1], target_value=1)\n",
    "# print(x_train_fixed.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328135, 35)\n",
      "[[1. 0. 1. ... 0. 1. 0.]\n",
      " [0. 1. 1. ... 1. 1. 0.]\n",
      " [1. 0. 1. ... 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 1. ... 0. 1. 0.]\n",
      " [1. 0. 1. ... 0. 1. 0.]\n",
      " [0. 1. 1. ... 1. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# create a numpy array x_train_filtered_2_OHE with one hot encoding\n",
    "\n",
    "def one_hot_encode(x_train_filtered, selected_features, selected_features_2):\n",
    "    x_train_filtered_2_OHE = np.zeros((x_train_filtered.shape[0], 0))\n",
    "    for feature in selected_features_2:\n",
    "        feature_values = x_train_filtered[:, selected_features.index(feature)]\n",
    "        unique_values = np.unique(feature_values)\n",
    "        for value in unique_values:\n",
    "            x_train_filtered_2_OHE = np.hstack((x_train_filtered_2_OHE, np.array([feature_values == value]).T))\n",
    "    return x_train_filtered_2_OHE\n",
    "\n",
    "x_train_filtered_2_OHE = one_hot_encode(x_train_filtered, selected_features, selected_features_2)\n",
    "print(x_train_filtered_2_OHE.shape)\n",
    "print(x_train_filtered_2_OHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(478578, 35)\n"
     ]
    }
   ],
   "source": [
    "# data splitting\n",
    "def split_data(x, y, ratio=0.8):\n",
    "    indices = np.random.permutation(x.shape[0])\n",
    "    train_indices = indices[:int(ratio * x.shape[0])]\n",
    "    test_indices = indices[int(ratio * x.shape[0]):]\n",
    "    return x[train_indices], y[train_indices], x[test_indices], y[test_indices]\n",
    "\n",
    "y_train_mapped = (1+y_train[:,1])/2\n",
    "\n",
    "x_train_filtered_2_OHE_train, y_train_train, x_train_filtered_2_OHE_test, y_train_test = split_data(x_train_filtered_2_OHE, y_train_mapped)\n",
    "\n",
    "# fix class imbalance\n",
    "x_train_filtered_2_OHE_train_fixed, y_train_train_fixed = fix_class_imbalance(x_train_filtered_2_OHE_train, y_train_train, target_value=1)\n",
    "print(x_train_filtered_2_OHE_train_fixed.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularized Logistic Regression(0/99): loss=0.693147180559945, w0=-0.009363363965748531, w1=0.009363363965748531\n",
      "Regularized Logistic Regression(1/99): loss=0.6808784651863432, w0=-0.018002651285901596, w1=0.01867328630241402\n",
      "Regularized Logistic Regression(2/99): loss=0.6699794386269928, w0=-0.02603202685647605, w1=0.027840152008904884\n",
      "Regularized Logistic Regression(3/99): loss=0.6601825653941895, w0=-0.03354014934701637, w1=0.03680281683694831\n",
      "Regularized Logistic Regression(4/99): loss=0.6513087716920523, w0=-0.040596160354768145, w1=0.045521158878581595\n",
      "Regularized Logistic Regression(5/99): loss=0.6432305831340079, w0=-0.047254425953683565, w1=0.053970422313851546\n",
      "Regularized Logistic Regression(6/99): loss=0.6358514716766284, w0=-0.05355817669444485, w1=0.06213697208999574\n",
      "Regularized Logistic Regression(7/99): loss=0.6290943057579194, w0=-0.0595422536489377, w1=0.07001513266381804\n",
      "Regularized Logistic Regression(8/99): loss=0.6228948415593887, w0=-0.06523515947275678, w1=0.0776048485800125\n",
      "Regularized Logistic Regression(9/99): loss=0.6171979844646321, w0=-0.07066058205520963, w1=0.08490996290804978\n",
      "Regularized Logistic Regression(10/99): loss=0.6119555689960976, w0=-0.07583852363463575, w1=0.09193695742100039\n",
      "Regularized Logistic Regression(11/99): loss=0.6071249738043412, w0=-0.08078613737645321, w1=0.09869403619062858\n",
      "Regularized Logistic Regression(12/99): loss=0.602668200943183, w0=-0.0855183482222918, w1=0.10519046352184515\n",
      "Regularized Logistic Regression(13/99): loss=0.5985512190400718, w0=-0.09004831516096656, w1=0.11143608952200965\n",
      "Regularized Logistic Regression(14/99): loss=0.5947434621275979, w0=-0.09438777711520609, w1=0.11744101357462726\n",
      "Regularized Logistic Regression(15/99): loss=0.5912174254462452, w0=-0.09854731343411968, w1=0.12321534878354601\n",
      "Regularized Logistic Regression(16/99): loss=0.5879483260280371, w0=-0.10253654167486992, w1=0.12876906004990216\n",
      "Regularized Logistic Regression(17/99): loss=0.5849138099877237, w0=-0.10636426924066636, w1=0.1341118556074962\n",
      "Regularized Logistic Regression(18/99): loss=0.5820936959533676, w0=-0.11003861096146852, w1=0.13925311716905542\n",
      "Regularized Logistic Regression(19/99): loss=0.5794697480650447, w0=-0.11356708143162222, w1=0.1442018577837761\n",
      "Regularized Logistic Regression(20/99): loss=0.5770254741187529, w0=-0.1169566685356536, w1=0.1489666994243405\n",
      "Regularized Logistic Regression(21/99): loss=0.5747459456136808, w0=-0.12021389285983745, w1=0.15355586447330094\n",
      "Regularized Logistic Regression(22/99): loss=0.5726176371397803, w0=-0.12334485642792777, w1=0.1579771768620655\n",
      "Regularized Logistic Regression(23/99): loss=0.5706282829615645, w0=-0.12635528328453463, w1=0.16223806977925015\n",
      "Regularized Logistic Regression(24/99): loss=0.5687667489386531, w0=-0.12925055378482023, w1=0.16634559771863547\n",
      "Regularized Logistic Regression(25/99): loss=0.5670229181375298, w0=-0.13203573396595877, w1=0.170306451262394\n",
      "Regularized Logistic Regression(26/99): loss=0.5653875886639199, w0=-0.13471560102386526, w1=0.17412697345266615\n",
      "Regularized Logistic Regression(27/99): loss=0.5638523823967276, w0=-0.13729466566198914, w1=0.17781317693883433\n",
      "Regularized Logistic Regression(28/99): loss=0.5624096634399071, w0=-0.1397771918913202, w1=0.18137076133139687\n",
      "Regularized Logistic Regression(29/99): loss=0.561052465231382, w0=-0.14216721472317884, w1=0.18480513037062762\n",
      "Regularized Logistic Regression(30/99): loss=0.5597744253598861, w0=-0.14446855609471485, w1=0.18812140864653726\n",
      "Regularized Logistic Regression(31/99): loss=0.558569727242221, w0=-0.14668483929244966, w1=0.1913244576995465\n",
      "Regularized Logistic Regression(32/99): loss=0.557433047905445, w0=-0.1488195020829602, w1=0.19441889139780708\n",
      "Regularized Logistic Regression(33/99): loss=0.556359511201732, w0=-0.15087580871841322, w1=0.19740909053455624\n",
      "Regularized Logistic Regression(34/99): loss=0.5553446458582455, w0=-0.15285686095273046, w1=0.20029921662237643\n",
      "Regularized Logistic Regression(35/99): loss=0.5543843478314112, w0=-0.15476560818043084, w1=0.20309322488458545\n",
      "Regularized Logistic Regression(36/99): loss=0.5534748464946829, w0=-0.15660485679143302, w1=0.20579487645967046\n",
      "Regularized Logistic Regression(37/99): loss=0.5526126742421522, w0=-0.15837727882092206, w1=0.2084077498453391\n",
      "Regularized Logistic Regression(38/99): loss=0.5517946391376402, w0=-0.16008541996179296, w1=0.2109352516152478\n",
      "Regularized Logistic Regression(39/99): loss=0.5510178002808779, w0=-0.16173170699832393, w1=0.21338062644531525\n",
      "Regularized Logistic Regression(40/99): loss=0.5502794455995733, w0=-0.16331845471222048, w1=0.21574696648836503\n",
      "Regularized Logistic Regression(41/99): loss=0.5495770718090259, w0=-0.1648478723062513, w1=0.2180372201363615\n",
      "Regularized Logistic Regression(42/99): loss=0.5489083663100962, w0=-0.16632206938560132, w1=0.22025420020889835\n",
      "Regularized Logistic Regression(43/99): loss=0.5482711908220422, w0=-0.16774306153292537, w1=0.22240059160556266\n",
      "Regularized Logistic Regression(44/99): loss=0.5476635665694626, w0=-0.16911277550935577, w1=0.22447895845824886\n",
      "Regularized Logistic Regression(45/99): loss=0.5470836608627102, w0=-0.17043305411070384, w1=0.22649175081760925\n",
      "Regularized Logistic Regression(46/99): loss=0.5465297749288899, w0=-0.17170566070532373, w1=0.2284413109060303\n",
      "Regularized Logistic Regression(47/99): loss=0.5460003328662488, w0=-0.17293228347782122, w1=0.23032987896743434\n",
      "Regularized Logistic Regression(48/99): loss=0.5454938716086691, w0=-0.17411453940050262, w1=0.23215959874234252\n",
      "Regularized Logistic Regression(49/99): loss=0.5450090317992756, w0=-0.1752539779526817, w1=0.2339325225946267\n",
      "Regularized Logistic Regression(50/99): loss=0.5445445494830212, w0=-0.17635208460633858, w1=0.2356506163145773\n",
      "Regularized Logistic Regression(51/99): loss=0.5440992485377767, w0=-0.17741028409498766, w1=0.23731576362112347\n",
      "Regularized Logistic Regression(52/99): loss=0.5436720337719841, w0=-0.1784299434812481, w1=0.23892977038446955\n",
      "Regularized Logistic Regression(53/99): loss=0.5432618846245079, w0=-0.1794123750374433, w1=0.2404943685887077\n",
      "Regularized Logistic Regression(54/99): loss=0.5428678494090622, w0=-0.18035883895236662, w1=0.24201122005261383\n",
      "Regularized Logistic Regression(55/99): loss=0.5424890400515464, w0=-0.18127054587631108, w1=0.24348191992550097\n",
      "Regularized Logistic Regression(56/99): loss=0.5421246272739549, w0=-0.18214865931557242, w1=0.24490799997360457\n",
      "Regularized Logistic Regression(57/99): loss=0.5417738361832384, w0=-0.18299429788667904, w1=0.24629093167147414\n",
      "Regularized Logistic Regression(58/99): loss=0.5414359422277245, w0=-0.18380853743997422, w1=0.24763212911167023\n",
      "Regularized Logistic Regression(59/99): loss=0.541110267487443, w0=-0.18459241306120877, w1=0.24893295174505375\n",
      "Regularized Logistic Regression(60/99): loss=0.5407961772680598, w0=-0.1853469209594513, w1=0.2501947069631394\n",
      "Regularized Logistic Regression(61/99): loss=0.5404930769711107, w0=-0.18607302024871145, w1=0.251418652533005\n",
      "Regularized Logistic Regression(62/99): loss=0.5402004092159018, w0=-0.1867716346303291, w1=0.25260599889456714\n",
      "Regularized Logistic Regression(63/99): loss=0.5399176511908328, w0=-0.18744365398259083, w1=0.2537579113292425\n",
      "Regularized Logistic Regression(64/99): loss=0.5396443122140401, w0=-0.1880899358635163, w1=0.2548755120084368\n",
      "Regularized Logistic Regression(65/99): loss=0.5393799314851798, w0=-0.1887113069323807, w1=0.25595988192956143\n",
      "Regularized Logistic Regression(66/99): loss=0.5391240760118899, w0=-0.1893085642951951, w1=0.2570120627468776\n",
      "Regularized Logistic Regression(67/99): loss=0.5388763386960104, w0=-0.18988247677886114, w1=0.2580330585037707\n",
      "Regularized Logistic Regression(68/99): loss=0.5386363365660383, w0=-0.19043378613844963, w1=0.2590238372727445\n",
      "Regularized Logistic Regression(69/99): loss=0.5384037091435405, w0=-0.190963208201794, w1=0.25998533270884244\n",
      "Regularized Logistic Regression(70/99): loss=0.5381781169323632, w0=-0.191471433955171, w1=0.26091844552191906\n",
      "Regularized Logistic Regression(71/99): loss=0.537959240020505, w0=-0.1919591305736994, w1=0.2618240448727022\n",
      "Regularized Logistic Regression(72/99): loss=0.5377467767854226, w0=-0.19242694239977154, w1=0.2627029696972951\n",
      "Regularized Logistic Regression(73/99): loss=0.5375404426943634, w0=-0.1928754918725959, w1=0.26355602996447924\n",
      "Regularized Logistic Regression(74/99): loss=0.5373399691920733, w0=-0.19330538041179524, w1=0.2643840078697573\n",
      "Regularized Logistic Regression(75/99): loss=0.537145102668882, w0=-0.19371718925769624, w1=0.2651876589699951\n",
      "Regularized Logistic Regression(76/99): loss=0.5369556035028062, w0=-0.19411148027086195, w1=0.26596771326204344\n",
      "Regularized Logistic Regression(77/99): loss=0.53677124516983, w0=-0.19448879669324376, w1=0.26672487620869023\n",
      "Regularized Logistic Regression(78/99): loss=0.5365918134170505, w0=-0.19484966387307412, w1=0.2674598297149442\n",
      "Regularized Logistic Regression(79/99): loss=0.5364171054938051, w0=-0.1951945899555909, w1=0.2681732330574999\n",
      "Regularized Logistic Regression(80/99): loss=0.5362469294363352, w0=-0.1955240665415418, w1=0.26886572377004764\n",
      "Regularized Logistic Regression(81/99): loss=0.5360811034018859, w0=-0.1958385693152275, w1=0.2695379184869074\n",
      "Regularized Logistic Regression(82/99): loss=0.5359194550485019, w0=-0.19613855864371071, w1=0.2701904137473172\n",
      "Regularized Logistic Regression(83/99): loss=0.5357618209570928, w0=-0.19642448014891586, w1=0.27082378676254576\n",
      "Regularized Logistic Regression(84/99): loss=0.5356080460925889, w0=-0.19669676525391538, w1=0.27143859614785376\n",
      "Regularized Logistic Regression(85/99): loss=0.535457983301315, w0=-0.19695583170488312, w1=0.2720353826212489\n",
      "Regularized Logistic Regression(86/99): loss=0.5353114928419018, w0=-0.1972020840700353, w1=0.27261466967078646\n",
      "Regularized Logistic Regression(87/99): loss=0.5351684419472852, w0=-0.19743591421668816, w1=0.27317696419210297\n",
      "Regularized Logistic Regression(88/99): loss=0.5350287044155443, w0=-0.19765770176762687, w1=0.2737227570977894\n",
      "Regularized Logistic Regression(89/99): loss=0.5348921602274953, w0=-0.19786781453786897, w1=0.27425252390003557\n",
      "Regularized Logistic Regression(90/99): loss=0.5347586951891257, w0=-0.19806660895279504, w1=0.2747667252679715\n",
      "Regularized Logistic Regression(91/99): loss=0.5346282005971046, w0=-0.1982544304486247, w1=0.27526580756098246\n",
      "Regularized Logistic Regression(92/99): loss=0.5345005729257413, w0=-0.19843161385612776, w1=0.27575020333925393\n",
      "Regularized Logistic Regression(93/99): loss=0.5343757135338829, w0=-0.198598483768347, w1=0.276220331852692\n",
      "Regularized Logistic Regression(94/99): loss=0.5342535283903669, w0=-0.19875535489323462, w1=0.2766765995092871\n",
      "Regularized Logistic Regression(95/99): loss=0.5341339278167408, w0=-0.19890253239188715, w1=0.2771194003239881\n",
      "Regularized Logistic Regression(96/99): loss=0.5340168262460614, w0=-0.19904031220303905, w1=0.27754911634899193\n",
      "Regularized Logistic Regression(97/99): loss=0.5339021419966749, w0=-0.19916898135456143, w1=0.2779661180864273\n",
      "Regularized Logistic Regression(98/99): loss=0.5337897970599627, w0=-0.19928881826253073, w1=0.2783707648842249\n",
      "Regularized Logistic Regression(99/99): loss=0.5336797169011035, w0=-0.19940009301850772, w1=0.27876340531605037\n"
     ]
    }
   ],
   "source": [
    "from implementations import *\n",
    "\n",
    "# Initialize the weights\n",
    "initial_w = np.zeros(x_train_filtered_2_OHE_train_fixed.shape[1])\n",
    "max_iters = 100\n",
    "gamma = 0.1\n",
    "lambda_ = 0.01\n",
    "\n",
    "# Y values are 1 and -1, we need to convert them to 1 and 0\n",
    "# Run the logistic regression\n",
    "w, loss = reg_logistic_regression(y_train_train_fixed, x_train_filtered_2_OHE_train_fixed, lambda_, initial_w, max_iters, gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 ... 0 0 1]\n",
      "Train accuracy: 0.7420992884026392, precision: 0.21712919390532393, recall: 0.7352599164477368, F1: 0.33525455348814376\n",
      "Test accuracy: 0.7432154448626328, precision: 0.21537040837266572, recall: 0.7293259207783183, F1: 0.33254119138149557\n"
     ]
    }
   ],
   "source": [
    "def predict_logistic_regression(y, x, w):\n",
    "    return np.array([1 if p > 0.5 else 0 for p in sigmoid(x @ w)])\n",
    "\n",
    "# calculate the accuracy, precision, recall and F1 score\n",
    "def accuracy_precision_recall_f1(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    accuracy = (tp + tn) / len(y_true)\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Predict the values\n",
    "y_pred_train = predict_logistic_regression(y_train_train, x_train_filtered_2_OHE_train, w)\n",
    "y_pred_test = predict_logistic_regression(y_train_test, x_train_filtered_2_OHE_test, w)\n",
    "\n",
    "print(y_pred_train)\n",
    "# Calculate the metrics\n",
    "accuracy_train, precision_train, recall_train, f1_train = accuracy_precision_recall_f1(y_train_train, y_pred_train)\n",
    "accuracy_test, precision_test, recall_test, f1_test = accuracy_precision_recall_f1(y_train_test, y_pred_test)\n",
    "\n",
    "print(f\"Train accuracy: {accuracy_train}, precision: {precision_train}, recall: {recall_train}, F1: {f1_train}\")\n",
    "print(f\"Test accuracy: {accuracy_test}, precision: {precision_test}, recall: {recall_test}, F1: {f1_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTS WITH SKLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       BPHIGH4 CVDSTRK3 CHCCOPD1 HAVARTH3 CHCKIDNY DIABETE3 INTERNET QLACTLM2  \\\n",
      "0          0.0      0.0      0.0      0.0      0.0      0.0      1.0      1.0   \n",
      "1          1.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "2          0.0      0.0      0.0      1.0      0.0      0.0      1.0      0.0   \n",
      "3          0.0      0.0      0.0      1.0      0.0      0.0      0.0      0.0   \n",
      "4          0.0      0.0      0.0      0.0      0.0      0.0      1.0      0.0   \n",
      "...        ...      ...      ...      ...      ...      ...      ...      ...   \n",
      "328130     1.0      0.0      0.0      0.0      0.0      0.0      1.0      0.0   \n",
      "328131     0.0      0.0      0.0      0.0      0.0      0.0      1.0      0.0   \n",
      "328132     0.0      0.0      0.0      0.0      0.0      0.0      0.0      1.0   \n",
      "328133     0.0      0.0      0.0      0.0      0.0      0.0      1.0      0.0   \n",
      "328134     1.0      0.0      0.0      1.0      0.0      0.0      1.0      0.0   \n",
      "\n",
      "       USEEQUIP DIFFWALK DIFFDRES DIFFALON _RFHLTH _HCVU651 _RFHYPE5 _DRDXAR1  \n",
      "0           0.0      0.0      0.0      0.0     1.0      1.0      1.0      0.0  \n",
      "1           0.0      0.0      0.0      0.0     2.0      1.0      2.0      0.0  \n",
      "2           0.0      0.0      0.0      0.0     1.0     -1.0      1.0      1.0  \n",
      "3           0.0      0.0      0.0      0.0     1.0     -1.0      1.0      1.0  \n",
      "4           0.0      0.0      0.0      0.0     1.0     -1.0      1.0      0.0  \n",
      "...         ...      ...      ...      ...     ...      ...      ...      ...  \n",
      "328130      0.0      0.0      0.0      0.0     1.0      1.0      2.0      0.0  \n",
      "328131      0.0      0.0      0.0      0.0     1.0      1.0      1.0      0.0  \n",
      "328132      1.0      1.0      0.0      0.0     1.0     -1.0      1.0      0.0  \n",
      "328133      0.0      0.0      0.0      0.0     1.0      1.0      1.0      0.0  \n",
      "328134      0.0      0.0      0.0      0.0     1.0      1.0      2.0      1.0  \n",
      "\n",
      "[328135 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "# apply MCA (Multiple classification analysis)\n",
    "import pandas as pd\n",
    "import prince\n",
    "\n",
    "X = pd.DataFrame(x_train_filtered_2, columns=selected_features_2)\n",
    "# set all dtypes to string in X\n",
    "X = X.astype(str)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mca = prince.MCA(n_components=5)\n",
    "mca = mca.fit(X)\n",
    "mca = mca.transform(X) # same as calling ca.fs_r_sup(df_new) for *another* test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot histogram of the first two dimensions using seaborn\n",
    "plt.figure(figsize=(10, 10))\n",
    "# first dimension\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.histplot(mca.iloc[:, 0], bins=100)\n",
    "plt.title('First dimension')\n",
    "# second dimension\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.histplot(mca.iloc[:, 1], bins=100)\n",
    "plt.title('Second dimension')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularized Logistic Regression(0/999): loss=0.693147180559945, w0=0.015608228520368361, w1=-0.0018180553387894148\n",
      "Regularized Logistic Regression(1/999): loss=0.6906328247309305, w0=0.031033390844158744, w1=-0.0036437237755170262\n",
      "Regularized Logistic Regression(2/999): loss=0.6881756024835682, w0=0.046277689705290365, w1=-0.005476803589434191\n",
      "Regularized Logistic Regression(3/999): loss=0.6857741654871097, w0=0.06134333524198785, w1=-0.007317089238002945\n",
      "Regularized Logistic Regression(4/999): loss=0.6834271876074957, w0=0.07623254236110814, w1=-0.009164371916296654\n",
      "Regularized Logistic Regression(5/999): loss=0.6811333654827922, w0=0.09094752823214827, w1=-0.01101844009073404\n",
      "Regularized Logistic Regression(6/999): loss=0.6788914190109959, w0=0.10549050991110996, w1=-0.012879080006846623\n",
      "Regularized Logistic Regression(7/999): loss=0.676700091754743, w0=0.11986370209371232, w1=-0.014746076170960486\n",
      "Regularized Logistic Regression(8/999): loss=0.6745581512675001, w0=0.13406931499680888, w1=-0.016619211805829915\n",
      "Regularized Logistic Regression(9/999): loss=0.6724643893458776, w0=0.14810955236628967, w1=-0.018498269280401373\n",
      "Regularized Logistic Regression(10/999): loss=0.6704176222126775, w0=0.16198660960932174, w1=-0.020383030514016725\n",
      "Regularized Logistic Regression(11/999): loss=0.6684166906352484, w0=0.1757026720482579, w1=-0.022273277355472472\n",
      "Regularized Logistic Regression(12/999): loss=0.6664604599836745, w0=0.18925991329331052, w1=-0.02416879193745552\n",
      "Regularized Logistic Regression(13/999): loss=0.6645478202331823, w0=0.20266049373059838, w1=-0.026069357006958095\n",
      "Regularized Logistic Regression(14/999): loss=0.6626776859150967, w0=0.21590655912208312, w1=-0.027974756232335977\n",
      "Regularized Logistic Regression(15/999): loss=0.6608489960204835, w0=0.22900023931359598, w1=-0.02988477448775119\n",
      "Regularized Logistic Regression(16/999): loss=0.6590607138605087, w0=0.24194364704699675, w1=-0.03179919811577129\n",
      "Regularized Logistic Regression(17/999): loss=0.6573118268873721, w0=0.2547388768724524, w1=-0.03371781516894759\n",
      "Regularized Logistic Regression(18/999): loss=0.6556013464794925, w0=0.2673880041566253, w1=-0.035640415631211016\n",
      "Regularized Logistic Regression(19/999): loss=0.6539283076944721, w0=0.2798930841826135, w1=-0.037566791619953264\n",
      "Regularized Logistic Regression(20/999): loss=0.6522917689931679, w0=0.2922561513373866, w1=-0.03949673756967076\n",
      "Regularized Logistic Regression(21/999): loss=0.6506908119380256, w0=0.3044792183825066, w1=-0.04143005039804718\n",
      "Regularized Logistic Regression(22/999): loss=0.6491245408686565, w0=0.31656427580393, w1=-0.04336652965535736\n",
      "Regularized Logistic Regression(23/999): loss=0.6475920825574452, w0=0.3285132912367648, w1=-0.0453059776580663\n",
      "Regularized Logistic Regression(24/999): loss=0.6460925858478074, w0=0.34032820896088184, w1=-0.04724819960748393\n",
      "Regularized Logistic Regression(25/999): loss=0.644625221277551, w0=0.3520109494634052, w1=-0.04919300369432026\n",
      "Regularized Logistic Regression(26/999): loss=0.6431891806896024, w0=0.363563409064167, w1=-0.05114020118997202\n",
      "Regularized Logistic Regression(27/999): loss=0.6417836768322225, w0=0.3749874596003691, w1=-0.053089606525346986\n",
      "Regularized Logistic Regression(28/999): loss=0.6404079429506583, w0=0.3862849481667556, w1=-0.05504103735800559\n",
      "Regularized Logistic Regression(29/999): loss=0.6390612323720374, w0=0.39745769690774185, w1=-0.056994314628380574\n",
      "Regularized Logistic Regression(30/999): loss=0.637742818085162, w0=0.4085075028581159, w1=-0.05894926260580439\n",
      "Regularized Logistic Regression(31/999): loss=0.6364519923167212, w0=0.4194361378289936, w1=-0.060905708925040845\n",
      "Regularized Logistic Regression(32/999): loss=0.6351880661053111, w0=0.4302453483358981, w1=-0.06286348461400282\n",
      "Regularized Logistic Regression(33/999): loss=0.6339503688745274, w0=0.44093685556593176, w1=-0.06482242411329164\n",
      "Regularized Logistic Regression(34/999): loss=0.632738248006277, w0=0.4515123553811837, w1=-0.06678236528817778\n",
      "Regularized Logistic Regression(35/999): loss=0.6315510684153491, w0=0.4619735183556328, w1=-0.06874314943360085\n",
      "Regularized Logistic Regression(36/999): loss=0.6303882121261754, w0=0.47232198984291784, w1=-0.07070462127275048\n",
      "Regularized Logistic Regression(37/999): loss=0.6292490778526187, w0=0.4825593900725347, w1=-0.07266662894975275\n",
      "Regularized Logistic Regression(38/999): loss=0.628133080581542, w0=0.49268731427209406, w1=-0.07462902401695579\n",
      "Regularized Logistic Regression(39/999): loss=0.6270396511608138, w0=0.502707332813444, w1=-0.07659166141728961\n",
      "Regularized Logistic Regression(40/999): loss=0.6259682358923394, w0=0.5126209913805593, w1=-0.07855439946213948\n",
      "Regularized Logistic Regression(41/999): loss=0.6249182961306335, w0=0.5224298111572151, w1=-0.08051709980515373\n",
      "Regularized Logistic Regression(42/999): loss=0.6238893078873783, w0=0.5321352890326245, w1=-0.08247962741237308\n",
      "Regularized Logistic Regression(43/999): loss=0.6228807614423499, w0=0.5417388978232591, w1=-0.0844418505290505\n",
      "Regularized Logistic Regression(44/999): loss=0.621892160961048, w0=0.5512420865092293, w1=-0.0864036406435075\n",
      "Regularized Logistic Regression(45/999): loss=0.6209230241193022, w0=0.5606462804837183, w1=-0.08836487244834483\n",
      "Regularized Logistic Regression(46/999): loss=0.6199728817350839, w0=0.5699528818139921, w1=-0.09032542379930798\n",
      "Regularized Logistic Regression(47/999): loss=0.6190412774077104, w0=0.5791632695126745, w1=-0.09228517567208966\n",
      "Regularized Logistic Regression(48/999): loss=0.6181277671645876, w0=0.5882787998180283, w1=-0.09424401211732572\n",
      "Regularized Logistic Regression(49/999): loss=0.6172319191156055, w0=0.5973008064820858, w1=-0.09620182021402769\n",
      "Regularized Logistic Regression(50/999): loss=0.6163533131152594, w0=0.6062306010655268, w1=-0.09815849002167529\n",
      "Regularized Logistic Regression(51/999): loss=0.615491540432546, w0=0.6150694732383123, w1=-0.10011391453117742\n",
      "Regularized Logistic Regression(52/999): loss=0.6146462034286617, w0=0.6238186910851377, w1=-0.1020679896148908\n",
      "Regularized Logistic Regression(53/999): loss=0.6138169152424983, w0=0.6324795014148343, w1=-0.10402061397587489\n",
      "Regularized Logistic Regression(54/999): loss=0.6130032994839115, w0=0.6410531300729189, w1=-0.10597168909654613\n",
      "Regularized Logistic Regression(55/999): loss=0.6122049899347249, w0=0.6495407822565563, w1=-0.10792111918688106\n",
      "Regularized Logistic Regression(56/999): loss=0.6114216302574089, w0=0.6579436428312416, w1=-0.10986881113230254\n",
      "Regularized Logistic Regression(57/999): loss=0.6106528737113609, w0=0.6662628766486053, w1=-0.11181467444138185\n",
      "Regularized Logistic Regression(58/999): loss=0.6098983828767014, w0=0.674499628864713, w1=-0.11375862119346469\n",
      "Regularized Logistic Regression(59/999): loss=0.6091578293854889, w0=0.6826550252583772, w1=-0.11570056598633006\n",
      "Regularized Logistic Regression(60/999): loss=0.6084308936602433, w0=0.6907301725489736, w1=-0.11764042588397662\n",
      "Regularized Logistic Regression(61/999): loss=0.6077172646596682, w0=0.6987261587133317, w1=-0.11957812036462169\n",
      "Regularized Logistic Regression(62/999): loss=0.6070166396314378, w0=0.7066440533012707, w1=-0.12151357126899574\n",
      "Regularized Logistic Regression(63/999): loss=0.6063287238719332, w0=0.7144849077494594, w1=-0.12344670274899515\n",
      "Regularized Logistic Regression(64/999): loss=0.605653230492783, w0=0.7222497556932198, w1=-0.12537744121676658\n",
      "Regularized Logistic Regression(65/999): loss=0.6049898801940703, w0=0.729939613276008, w1=-0.12730571529427243\n",
      "Regularized Logistic Regression(66/999): loss=0.6043384010440751, w0=0.7375554794562821, w1=-0.1292314557633897\n",
      "Regularized Logistic Regression(67/999): loss=0.6036985282653925, w0=0.745098336311534, w1=-0.1311545955165923\n",
      "Regularized Logistic Regression(68/999): loss=0.6030700040272932, w0=0.7525691493392354, w1=-0.13307506950824802\n",
      "Regularized Logistic Regression(69/999): loss=0.6024525772441731, w0=0.7599688677545475, w1=-0.1349928147065737\n",
      "Regularized Logistic Regression(70/999): loss=0.6018460033799464, w0=0.7672984247845823, w1=-0.13690777004627433\n",
      "Regularized Logistic Regression(71/999): loss=0.6012500442582341, w0=0.7745587379590992, w1=-0.13881987638189142\n",
      "Regularized Logistic Regression(72/999): loss=0.6006644678781995, w0=0.7817507093974769, w1=-0.14072907644188729\n",
      "Regularized Logistic Regression(73/999): loss=0.6000890482358849, w0=0.7888752260918662, w1=-0.1426353147834812\n",
      "Regularized Logistic Regression(74/999): loss=0.599523565150907, w0=0.7959331601864238, w1=-0.144538537748253\n",
      "Regularized Logistic Regression(75/999): loss=0.5989678040983583, w0=0.8029253692525311, w1=-0.1464386934185282\n",
      "Regularized Logistic Regression(76/999): loss=0.5984215560457828, w0=0.8098526965599454, w1=-0.14833573157455476\n",
      "Regularized Logistic Regression(77/999): loss=0.5978846172950774, w0=0.8167159713438246, w1=-0.1502296036524772\n",
      "Regularized Logistic Regression(78/999): loss=0.5973567893291821, w0=0.8235160090675648, w1=-0.15212026270311368\n",
      "Regularized Logistic Regression(79/999): loss=0.5968378786634276, w0=0.8302536116814503, w1=-0.15400766335154295\n",
      "Regularized Logistic Regression(80/999): loss=0.5963276967013968, w0=0.8369295678770546, w1=-0.1558917617574951\n",
      "Regularized Logistic Regression(81/999): loss=0.5958260595951821, w0=0.8435446533374072, w1=-0.15777251557655514\n",
      "Regularized Logistic Regression(82/999): loss=0.595332788109893, w0=0.8500996309829062, w1=-0.15964988392216642\n",
      "Regularized Logistic Regression(83/999): loss=0.5948477074923062, w0=0.8565952512129822, w1=-0.16152382732844037\n",
      "Regularized Logistic Regression(84/999): loss=0.5943706473435202, w0=0.8630322521435173, w1=-0.16339430771376326\n",
      "Regularized Logistic Regression(85/999): loss=0.5939014414955004, w0=0.8694113598400433, w1=-0.16526128834519394\n",
      "Regularized Logistic Regression(86/999): loss=0.5934399278913982, w0=0.8757332885467275, w1=-0.16712473380364612\n",
      "Regularized Logistic Regression(87/999): loss=0.592985948469526, w0=0.8819987409111855, w1=-0.16898460994985046\n",
      "Regularized Logistic Regression(88/999): loss=0.5925393490508784, w0=0.8882084082051274, w1=-0.1708408838910809\n",
      "Regularized Logistic Regression(89/999): loss=0.5920999792300877, w0=0.8943629705409055, w1=-0.17269352394864662\n",
      "Regularized Logistic Regression(90/999): loss=0.5916676922697137, w0=0.9004630970839679, w1=-0.1745424996261283\n",
      "Regularized Logistic Regression(91/999): loss=0.5912423449977572, w0=0.9065094462612798, w1=-0.17638778157835647\n",
      "Regularized Logistic Regression(92/999): loss=0.5908237977082995, w0=0.9125026659657524, w1=-0.17822934158111967\n",
      "Regularized Logistic Regression(93/999): loss=0.5904119140651729, w0=0.9184433937567177, w1=-0.18006715250158556\n",
      "Regularized Logistic Regression(94/999): loss=0.5900065610085656, w0=0.9243322570565105, w1=-0.1819011882694314\n",
      "Regularized Logistic Regression(95/999): loss=0.5896076086644646, w0=0.9301698733431988, w1=-0.18373142384866759\n",
      "Regularized Logistic Regression(96/999): loss=0.589214930256853, w0=0.9359568503395209, w1=-0.18555783521014194\n",
      "Regularized Logistic Regression(97/999): loss=0.5888284020225745, w0=0.9416937861980829, w1=-0.18738039930471354\n",
      "Regularized Logistic Regression(98/999): loss=0.5884479031287737, w0=0.9473812696828641, w1=-0.18919909403708468\n",
      "Regularized Logistic Regression(99/999): loss=0.5880733155928398, w0=0.9530198803471063, w1=-0.19101389824027495\n",
      "Regularized Logistic Regression(100/999): loss=0.5877045242047688, w0=0.9586101887076123, w1=-0.19282479165072935\n",
      "Regularized Logistic Regression(101/999): loss=0.5873414164518668, w0=0.9641527564155441, w1=-0.1946317548840449\n",
      "Regularized Logistic Regression(102/999): loss=0.5869838824457245, w0=0.9696481364237696, w1=-0.1964347694113013\n",
      "Regularized Logistic Regression(103/999): loss=0.5866318148513857, w0=0.9750968731507942, w1=-0.19823381753598907\n",
      "Regularized Logistic Regression(104/999): loss=0.5862851088186412, w0=0.9804995026413748, w1=-0.20002888237151614\n",
      "Regularized Logistic Regression(105/999): loss=0.5859436619153853, w0=0.9858565527238393, w1=-0.20181994781928322\n",
      "Regularized Logistic Regression(106/999): loss=0.5856073740629598, w0=0.9911685431642024, w1=-0.20360699854731815\n",
      "Regularized Logistic Regression(107/999): loss=0.5852761474734298, w0=0.9964359858171017, w1=-0.20539001996944986\n",
      "Regularized Logistic Regression(108/999): loss=0.5849498865887272, w0=1.0016593847736526, w1=-0.2071689982250165\n",
      "Regularized Logistic Regression(109/999): loss=0.5846284980216023, w0=1.0068392365062464, w1=-0.20894392015909158\n",
      "Regularized Logistic Regression(110/999): loss=0.5843118904983255, w0=1.011976030010369, w1=-0.21071477330322066\n",
      "Regularized Logistic Regression(111/999): loss=0.583999974803084, w0=1.0170702469434951, w1=-0.212481545856648\n",
      "Regularized Logistic Regression(112/999): loss=0.5836926637240176, w0=1.0221223617611062, w1=-0.2142442266680333\n",
      "Regularized Logistic Regression(113/999): loss=0.5833898720008484, w0=1.0271328418499028, w1=-0.21600280521763712\n",
      "Regularized Logistic Regression(114/999): loss=0.5830915162740403, w0=1.0321021476582526, w1=-0.21775727159996894\n",
      "Regularized Logistic Regression(115/999): loss=0.5827975150354565, w0=1.037030732823946, w1=-0.2195076165068847\n",
      "Regularized Logistic Regression(116/999): loss=0.5825077885804515, w0=1.0419190442992978, w1=-0.22125383121112377\n",
      "Regularized Logistic Regression(117/999): loss=0.5822222589613651, w0=1.0467675224736595, w1=-0.2229959075502741\n",
      "Regularized Logistic Regression(118/999): loss=0.5819408499423653, w0=1.0515766012933971, w1=-0.2247338379111568\n",
      "Regularized Logistic Regression(119/999): loss=0.5816634869556, w0=1.0563467083793736, w1=-0.22646761521461573\n",
      "Regularized Logistic Regression(120/999): loss=0.5813900970586205, w0=1.0610782651420088, w1=-0.22819723290070548\n",
      "Regularized Logistic Regression(121/999): loss=0.5811206088930283, w0=1.0657716868939433, w1=-0.22992268491426676\n",
      "Regularized Logistic Regression(122/999): loss=0.5808549526443146, w0=1.0704273829603828, w1=-0.2316439656908809\n",
      "Regularized Logistic Regression(123/999): loss=0.5805930600028502, w0=1.0750457567871434, w1=-0.2333610701431911\n",
      "Regularized Logistic Regression(124/999): loss=0.5803348641259903, w0=1.0796272060464804, w1=-0.23507399364758008\n",
      "Regularized Logistic Regression(125/999): loss=0.5800802996012651, w0=1.0841721227407162, w1=-0.2367827320312025\n",
      "Regularized Logistic Regression(126/999): loss=0.5798293024106084, w0=1.0886808933037297, w1=-0.23848728155935475\n",
      "Regularized Logistic Regression(127/999): loss=0.5795818098956105, w0=1.0931538987003597, w1=-0.24018763892317677\n",
      "Regularized Logistic Regression(128/999): loss=0.5793377607237462, w0=1.097591514523753, w1=-0.24188380122767864\n",
      "Regularized Logistic Regression(129/999): loss=0.5790970948555602, w0=1.1019941110907092, w1=-0.24357576598007918\n",
      "Regularized Logistic Regression(130/999): loss=0.5788597535127706, w0=1.1063620535350607, w1=-0.2452635310784577\n",
      "Regularized Logistic Regression(131/999): loss=0.5786256791472707, w0=1.1106957018991463, w1=-0.24694709480069624\n",
      "Regularized Logistic Regression(132/999): loss=0.5783948154109941, w0=1.1149954112233929, w1=-0.2486264557937207\n",
      "Regularized Logistic Regression(133/999): loss=0.5781671071266208, w0=1.1192615316340742, w1=-0.2503016130630195\n",
      "Regularized Logistic Regression(134/999): loss=0.577942500259098, w0=1.1234944084292684, w1=-0.2519725659624434\n",
      "Regularized Logistic Regression(135/999): loss=0.5777209418879466, w0=1.1276943821630618, w1=-0.25363931418427094\n",
      "Regularized Logistic Regression(136/999): loss=0.5775023801803374, w0=1.131861788728037, w1=-0.25530185774953473\n",
      "Regularized Logistic Regression(137/999): loss=0.5772867643649032, w0=1.1359969594360755, w1=-0.2569601969986071\n",
      "Regularized Logistic Regression(138/999): loss=0.5770740447062729, w0=1.140100221097524, w1=-0.2586143325820308\n",
      "Regularized Logistic Regression(139/999): loss=0.5768641724803023, w0=1.1441718960987521, w1=-0.2602642654515912\n",
      "Regularized Logistic Regression(140/999): loss=0.5766570999499796, w0=1.148212302478139, w1=-0.26190999685162547\n",
      "Regularized Logistic Regression(141/999): loss=0.5764527803419867, w0=1.152221754000526, w1=-0.26355152831056017\n",
      "Regularized Logistic Regression(142/999): loss=0.576251167823895, w0=1.1562005602301624, w1=-0.2651888616326705\n",
      "Regularized Logistic Regression(143/999): loss=0.5760522174819785, w0=1.1601490266021908, w1=-0.26682199889005975\n",
      "Regularized Logistic Regression(144/999): loss=0.5758558852996251, w0=1.1640674544926781, w1=-0.26845094241484846\n",
      "Regularized Logistic Regression(145/999): loss=0.5756621281363258, w0=1.1679561412872648, w1=-0.27007569479156757\n",
      "Regularized Logistic Regression(146/999): loss=0.5754709037072288, w0=1.1718153804484257, w1=-0.2716962588497555\n",
      "Regularized Logistic Regression(147/999): loss=0.5752821705632373, w0=1.1756454615813883, w1=-0.2733126376567475\n",
      "Regularized Logistic Regression(148/999): loss=0.5750958880716374, w0=1.1794466704987359, w1=-0.27492483451065614\n",
      "Regularized Logistic Regression(149/999): loss=0.574912016397242, w0=1.1832192892837385, w1=-0.27653285293353663\n",
      "Regularized Logistic Regression(150/999): loss=0.574730516484029, w0=1.1869635963524112, w1=-0.2781366966647312\n",
      "Regularized Logistic Regression(151/999): loss=0.5745513500372658, w0=1.1906798665143485, w1=-0.2797363696543892\n",
      "Regularized Logistic Regression(152/999): loss=0.5743744795061048, w0=1.194368371032365, w1=-0.28133187605715854\n",
      "Regularized Logistic Regression(153/999): loss=0.5741998680666327, w0=1.1980293776809483, w1=-0.28292322022604094\n",
      "Regularized Logistic Regression(154/999): loss=0.5740274796053626, w0=1.2016631508035702, w1=-0.2845104067064106\n",
      "Regularized Logistic Regression(155/999): loss=0.5738572787031552, w0=1.2052699513688716, w1=-0.2860934402301904\n",
      "Regularized Logistic Regression(156/999): loss=0.573689230619559, w0=1.2088500370257511, w1=-0.28767232571017803\n",
      "Regularized Logistic Regression(157/999): loss=0.573523301277554, w0=1.2124036621573688, w1=-0.28924706823452623\n",
      "Regularized Logistic Regression(158/999): loss=0.5733594572486884, w0=1.2159310779341121, w1=-0.2908176730613623\n",
      "Regularized Logistic Regression(159/999): loss=0.5731976657385978, w0=1.2194325323655204, w1=-0.29238414561355486\n",
      "Regularized Logistic Regression(160/999): loss=0.5730378945728953, w0=1.2229082703512075, w1=-0.29394649147361296\n",
      "Regularized Logistic Regression(161/999): loss=0.5728801121834222, w0=1.226358533730806, w1=-0.2955047163787211\n",
      "Regularized Logistic Regression(162/999): loss=0.5727242875948468, w0=1.2297835613329358, w1=-0.2970588262159053\n",
      "Regularized Logistic Regression(163/999): loss=0.5725703904116065, w0=1.233183589023245, w1=-0.2986088270173259\n",
      "Regularized Logistic Regression(164/999): loss=0.5724183908051751, w0=1.23655884975152, w1=-0.3001547249556906\n",
      "Regularized Logistic Regression(165/999): loss=0.5722682595016576, w0=1.2399095735978982, w1=-0.30169652633979294\n",
      "Regularized Logistic Regression(166/999): loss=0.5721199677696907, w0=1.2432359878181936, w1=-0.30323423761016327\n",
      "Regularized Logistic Regression(167/999): loss=0.5719734874086497, w0=1.2465383168883677, w1=-0.3047678653348368\n",
      "Regularized Logistic Regression(168/999): loss=0.571828790737148, w0=1.2498167825481485, w1=-0.30629741620522977\n",
      "Regularized Logistic Regression(169/999): loss=0.5716858505818265, w0=1.2530716038438343, w1=-0.30782289703212634\n",
      "Regularized Logistic Regression(170/999): loss=0.5715446402664147, w0=1.256302997170276, w1=-0.30934431474176877\n",
      "Regularized Logistic Regression(171/999): loss=0.5714051336010691, w0=1.259511176312073, w1=-0.3108616763720498\n",
      "Regularized Logistic Regression(172/999): loss=0.5712673048719704, w0=1.262696352484006, w1=-0.31237498906880706\n",
      "Regularized Logistic Regression(173/999): loss=0.5711311288311769, w0=1.2658587343707006, w1=-0.3138842600822122\n",
      "Regularized Logistic Regression(174/999): loss=0.5709965806867271, w0=1.2689985281655511, w1=-0.315389496763254\n",
      "Regularized Logistic Regression(175/999): loss=0.5708636360929852, w0=1.2721159376089224, w1=-0.3168907065603172\n",
      "Regularized Logistic Regression(176/999): loss=0.5707322711412178, w0=1.2752111640256432, w1=-0.3183878970158459\n",
      "Regularized Logistic Regression(177/999): loss=0.5706024623504045, w0=1.2782844063618066, w1=-0.31988107576309804\n",
      "Regularized Logistic Regression(178/999): loss=0.5704741866582655, w0=1.2813358612208785, w1=-0.3213702505229835\n",
      "Regularized Logistic Regression(179/999): loss=0.5703474214125094, w0=1.284365722899158, w1=-0.3228554291009844\n",
      "Regularized Logistic Regression(180/999): loss=0.5702221443622906, w0=1.2873741834205754, w1=-0.3243366193841568\n",
      "Regularized Logistic Regression(181/999): loss=0.570098333649869, w0=1.2903614325708492, w1=-0.32581382933821024\n",
      "Regularized Logistic Regression(182/999): loss=0.569975967802473, w0=1.2933276579310347, w1=-0.32728706700466415\n",
      "Regularized Logistic Regression(183/999): loss=0.569855025724352, w0=1.2962730449104467, w1=-0.32875634049807934\n",
      "Regularized Logistic Regression(184/999): loss=0.5697354866890213, w0=1.2991977767789882, w1=-0.33022165800335984\n",
      "Regularized Logistic Regression(185/999): loss=0.5696173303316876, w0=1.3021020346988967, w1=-0.33168302777312647\n",
      "Regularized Logistic Regression(186/999): loss=0.5695005366418556, w0=1.3049859977559146, w1=-0.33314045812516124\n",
      "Regularized Logistic Regression(187/999): loss=0.5693850859561059, w0=1.3078498429898953, w1=-0.3345939574399142\n",
      "Regularized Logistic Regression(188/999): loss=0.5692709589510453, w0=1.3106937454248704, w1=-0.3360435341580798\n",
      "Regularized Logistic Regression(189/999): loss=0.5691581366364188, w0=1.3135178780985628, w1=-0.3374891967782347\n",
      "Regularized Logistic Regression(190/999): loss=0.5690466003483837, w0=1.316322412091379, w1=-0.33893095385453675\n",
      "Regularized Logistic Regression(191/999): loss=0.5689363317429407, w0=1.319107516554897, w1=-0.3403688139944861\n",
      "Regularized Logistic Regression(192/999): loss=0.5688273127895148, w0=1.321873358739827, w1=-0.3418027858567445\n",
      "Regularized Logistic Regression(193/999): loss=0.5687195257646869, w0=1.3246201040234937, w1=-0.3432328781490105\n",
      "Regularized Logistic Regression(194/999): loss=0.5686129532460675, w0=1.3273479159368238, w1=-0.34465909962595237\n",
      "Regularized Logistic Regression(195/999): loss=0.5685075781063125, w0=1.3300569561908635, w1=-0.34608145908719323\n",
      "Regularized Logistic Regression(196/999): loss=0.5684033835072752, w0=1.3327473847028308, w1=-0.3474999653753511\n",
      "Regularized Logistic Regression(197/999): loss=0.5683003528942919, w0=1.335419359621708, w1=-0.34891462737412854\n",
      "Regularized Logistic Regression(198/999): loss=0.5681984699905966, w0=1.338073037353391, w1=-0.35032545400645265\n",
      "Regularized Logistic Regression(199/999): loss=0.5680977187918647, w0=1.340708572585393, w1=-0.3517324542326672\n",
      "Regularized Logistic Regression(200/999): loss=0.5679980835608751, w0=1.343326118311136, w1=-0.35313563704876744\n",
      "Regularized Logistic Regression(201/999): loss=0.5678995488222998, w0=1.3459258258538043, w1=-0.35453501148468525\n",
      "Regularized Logistic Regression(202/999): loss=0.567802099357603, w0=1.348507844889793, w1=-0.35593058660261745\n",
      "Regularized Logistic Regression(203/999): loss=0.5677057202000598, w0=1.351072323471762, w1=-0.3573223714953987\n",
      "Regularized Logistic Regression(204/999): loss=0.5676103966298839, w0=1.3536194080512782, w1=-0.35871037528491756\n",
      "Regularized Logistic Regression(205/999): loss=0.5675161141694627, w0=1.356149243501083, w1=-0.36009460712057373\n",
      "Regularized Logistic Regression(206/999): loss=0.5674228585787019, w0=1.3586619731369693, w1=-0.3614750761777776\n",
      "Regularized Logistic Regression(207/999): loss=0.5673306158504688, w0=1.361157738739302, w1=-0.36285179165648745\n",
      "Regularized Logistic Regression(208/999): loss=0.5672393722061374, w0=1.3636366805741587, w1=-0.36422476277978716\n",
      "Regularized Logistic Regression(209/999): loss=0.5671491140912351, w0=1.3660989374141188, w1=-0.3655939987925011\n",
      "Regularized Logistic Regression(210/999): loss=0.5670598281711787, w0=1.3685446465587026, w1=-0.3669595089598451\n",
      "Regularized Logistic Regression(211/999): loss=0.5669715013271087, w0=1.3709739438544701, w1=-0.3683213025661138\n",
      "Regularized Logistic Regression(212/999): loss=0.5668841206518113, w0=1.3733869637147662, w1=-0.3696793889134025\n",
      "Regularized Logistic Regression(213/999): loss=0.5667976734457306, w0=1.3757838391391637, w1=-0.37103377732036397\n",
      "Regularized Logistic Regression(214/999): loss=0.5667121472130677, w0=1.3781647017325576, w1=-0.37238447712099937\n",
      "Regularized Logistic Regression(215/999): loss=0.5666275296579618, w0=1.3805296817239536, w1=-0.3737314976634773\n",
      "Regularized Logistic Regression(216/999): loss=0.5665438086807559, w0=1.382878907984945, w1=-0.37507484830898924\n",
      "Regularized Logistic Regression(217/999): loss=0.5664609723743413, w0=1.3852125080478823, w1=-0.3764145384306327\n",
      "Regularized Logistic Regression(218/999): loss=0.5663790090205821, w0=1.3875306081237402, w1=-0.37775057741232543\n",
      "Regularized Logistic Regression(219/999): loss=0.566297907086813, w0=1.3898333331197015, w1=-0.3790829746477492\n",
      "Regularized Logistic Regression(220/999): loss=0.5662176552224155, w0=1.3921208066564401, w1=-0.38041173953932067\n",
      "Regularized Logistic Regression(221/999): loss=0.5661382422554633, w0=1.3943931510851297, w1=-0.38173688149719043\n",
      "Regularized Logistic Regression(222/999): loss=0.5660596571894411, w0=1.3966504875041714, w1=-0.3830584099382724\n",
      "Regularized Logistic Regression(223/999): loss=0.5659818892000316, w0=1.3988929357756577, w1=-0.3843763342852912\n",
      "Regularized Logistic Regression(224/999): loss=0.5659049276319704, w0=1.4011206145415596, w1=-0.385690663965867\n",
      "Regularized Logistic Regression(225/999): loss=0.5658287619959682, w0=1.4033336412396569, w1=-0.38700140841161423\n",
      "Regularized Logistic Regression(226/999): loss=0.5657533819656937, w0=1.405532132119214, w1=-0.38830857705727223\n",
      "Regularized Logistic Regression(227/999): loss=0.5656787773748235, w0=1.4077162022564005, w1=-0.3896121793398558\n",
      "Regularized Logistic Regression(228/999): loss=0.5656049382141525, w0=1.409885965569469, w1=-0.3909122246978306\n",
      "Regularized Logistic Regression(229/999): loss=0.5655318546287615, w0=1.4120415348336866, w1=-0.3922087225703115\n",
      "Regularized Logistic Regression(230/999): loss=0.5654595169152472, w0=1.414183021696029, w1=-0.39350168239628086\n",
      "Regularized Logistic Regression(231/999): loss=0.5653879155190072, w0=1.4163105366896451, w1=-0.3947911136138323\n",
      "Regularized Logistic Regression(232/999): loss=0.565317041031581, w0=1.4184241892480902, w1=-0.3960770256594287\n",
      "Regularized Logistic Regression(233/999): loss=0.565246884188047, w0=1.4205240877193313, w1=-0.39735942796719037\n",
      "Regularized Logistic Regression(234/999): loss=0.56517743586447, w0=1.4226103393795433, w1=-0.3986383299681925\n",
      "Regularized Logistic Regression(235/999): loss=0.5651086870754052, w0=1.4246830504466774, w1=-0.39991374108979066\n",
      "Regularized Logistic Regression(236/999): loss=0.5650406289714464, w0=1.4267423260938175, w1=-0.4011856707549605\n",
      "Regularized Logistic Regression(237/999): loss=0.5649732528368312, w0=1.4287882704623398, w1=-0.40245412838165745\n",
      "Regularized Logistic Regression(238/999): loss=0.56490655008709, w0=1.4308209866748478, w1=-0.4037191233821943\n",
      "Regularized Logistic Regression(239/999): loss=0.5648405122667445, w0=1.4328405768479295, w1=-0.40498066516263653\n",
      "Regularized Logistic Regression(240/999): loss=0.5647751310470498, w0=1.4348471421047067, w1=-0.4062387631222145\n",
      "Regularized Logistic Regression(241/999): loss=0.5647103982237868, w0=1.4368407825871778, w1=-0.4074934266527528\n",
      "Regularized Logistic Regression(242/999): loss=0.5646463057150933, w0=1.4388215974683984, w1=-0.40874466513811325\n",
      "Regularized Logistic Regression(243/999): loss=0.5645828455593417, w0=1.4407896849644535, w1=-0.4099924879536606\n",
      "Regularized Logistic Regression(244/999): loss=0.5645200099130596, w0=1.4427451423462532, w1=-0.411236904465735\n",
      "Regularized Logistic Regression(245/999): loss=0.5644577910488878, w0=1.4446880659511478, w1=-0.4124779240311455\n",
      "Regularized Logistic Regression(246/999): loss=0.5643961813535829, w0=1.446618551194367, w1=-0.4137155559966772\n",
      "Regularized Logistic Regression(247/999): loss=0.5643351733260575, w0=1.4485366925802914, w1=-0.4149498096986111\n",
      "Regularized Logistic Regression(248/999): loss=0.5642747595754584, w0=1.450442583713543, w1=-0.41618069446226047\n",
      "Regularized Logistic Regression(249/999): loss=0.5642149328192833, w0=1.4523363173099122, w1=-0.4174082196015174\n",
      "Regularized Logistic Regression(250/999): loss=0.5641556858815362, w0=1.4542179852071293, w1=-0.41863239441841565\n",
      "Regularized Logistic Regression(251/999): loss=0.5640970116909146, w0=1.4560876783754564, w1=-0.4198532282027075\n",
      "Regularized Logistic Regression(252/999): loss=0.5640389032790357, w0=1.4579454869281396, w1=-0.4210707302314484\n",
      "Regularized Logistic Regression(253/999): loss=0.563981353778696, w0=1.4597915001316948, w1=-0.4222849097685991\n",
      "Regularized Logistic Regression(254/999): loss=0.5639243564221643, w0=1.4616258064160428, w1=-0.42349577606463695\n",
      "Regularized Logistic Regression(255/999): loss=0.5638679045395072, w0=1.463448493384495, w1=-0.42470333835617985\n",
      "Regularized Logistic Regression(256/999): loss=0.5638119915569484, w0=1.465259647823591, w1=-0.42590760586562093\n",
      "Regularized Logistic Regression(257/999): loss=0.5637566109952594, w0=1.4670593557127884, w1=-0.42710858780077715\n",
      "Regularized Logistic Regression(258/999): loss=0.5637017564681782, w0=1.4688477022340205, w1=-0.4283062933545434\n",
      "Regularized Logistic Regression(259/999): loss=0.5636474216808625, w0=1.4706247717810967, w1=-0.4295007317045646\n",
      "Regularized Logistic Regression(260/999): loss=0.5635936004283693, w0=1.4723906479689803, w1=-0.4306919120129107\n",
      "Regularized Logistic Regression(261/999): loss=0.5635402865941658, w0=1.4741454136429284, w1=-0.43187984342576663\n",
      "Regularized Logistic Regression(262/999): loss=0.5634874741486654, w0=1.4758891508874863, w1=-0.4330645350731309\n",
      "Regularized Logistic Regression(263/999): loss=0.5634351571477945, w0=1.4776219410353713, w1=-0.434245996068524\n",
      "Regularized Logistic Regression(264/999): loss=0.5633833297315853, w0=1.4793438646762143, w1=-0.43542423550870496\n",
      "Regularized Logistic Regression(265/999): loss=0.5633319861227952, w0=1.4810550016651822, w1=-0.43659926247339814\n",
      "Regularized Logistic Regression(266/999): loss=0.56328112062555, w0=1.4827554311314646, w1=-0.4377710860250299\n",
      "Regularized Logistic Regression(267/999): loss=0.5632307276240175, w0=1.4844452314866587, w1=-0.4389397152084717\n",
      "Regularized Logistic Regression(268/999): loss=0.5631808015811008, w0=1.4861244804330165, w1=-0.44010515905079306\n",
      "Regularized Logistic Regression(269/999): loss=0.563131337037158, w0=1.48779325497158, w1=-0.44126742656102336\n",
      "Regularized Logistic Regression(270/999): loss=0.5630823286087477, w0=1.4894516314102069, w1=-0.4424265267299196\n",
      "Regularized Logistic Regression(271/999): loss=0.5630337709873929, w0=1.4910996853714733, w1=-0.44358246852974553\n",
      "Regularized Logistic Regression(272/999): loss=0.562985658938374, w0=1.492737491800469, w1=-0.4447352609140536\n",
      "Regularized Logistic Regression(273/999): loss=0.5629379872995379, w0=1.4943651249724843, w1=-0.44588491281748\n",
      "Regularized Logistic Regression(274/999): loss=0.5628907509801342, w0=1.495982658500582, w1=-0.4470314331555416\n",
      "Regularized Logistic Regression(275/999): loss=0.5628439449596693, w0=1.4975901653430752, w1=-0.44817483082444504\n",
      "Regularized Logistic Regression(276/999): loss=0.5627975642867823, w0=1.499187717810885, w1=-0.44931511470089863\n",
      "Regularized Logistic Regression(277/999): loss=0.5627516040781437, w0=1.5007753875748078, w1=-0.4504522936419337\n",
      "Regularized Logistic Regression(278/999): loss=0.5627060595173713, w0=1.5023532456726802, w1=-0.4515863764847299\n",
      "Regularized Logistic Regression(279/999): loss=0.5626609258539657, w0=1.5039213625164367, w1=-0.4527173720464518\n",
      "Regularized Logistic Regression(280/999): loss=0.5626161984022691, w0=1.5054798078990763, w1=-0.4538452891240857\n",
      "Regularized Logistic Regression(281/999): loss=0.5625718725404378, w0=1.5070286510015312, w1=-0.4549701364942859\n",
      "Regularized Logistic Regression(282/999): loss=0.5625279437094365, w0=1.508567960399439, w1=-0.45609192291322836\n",
      "Regularized Logistic Regression(283/999): loss=0.5624844074120499, w0=1.510097804069829, w1=-0.45721065711646697\n",
      "Regularized Logistic Regression(284/999): loss=0.5624412592119122, w0=1.5116182493977028, w1=-0.45832634781879555\n",
      "Regularized Logistic Regression(285/999): loss=0.5623984947325528, w0=1.5131293631825442, w1=-0.4594390037141206\n",
      "Regularized Logistic Regression(286/999): loss=0.5623561096564621, w0=1.5146312116447234, w1=-0.4605486334753318\n",
      "Regularized Logistic Regression(287/999): loss=0.5623140997241692, w0=1.5161238604318217, w1=-0.46165524575418315\n",
      "Regularized Logistic Regression(288/999): loss=0.562272460733342, w0=1.5176073746248702, w1=-0.4627588491811777\n",
      "Regularized Logistic Regression(289/999): loss=0.5622311885378972, w0=1.5190818187445077, w1=-0.4638594523654576\n",
      "Regularized Logistic Regression(290/999): loss=0.5621902790471301, w0=1.520547256757045, w1=-0.46495706389469804\n",
      "Regularized Logistic Regression(291/999): loss=0.5621497282248593, w0=1.5220037520804592, w1=-0.46605169233500704\n",
      "Regularized Logistic Regression(292/999): loss=0.5621095320885849, w0=1.5234513675902999, w1=-0.46714334623082854\n",
      "Regularized Logistic Regression(293/999): loss=0.5620696867086624, w0=1.5248901656255185, w1=-0.4682320341048525\n",
      "Regularized Logistic Regression(294/999): loss=0.5620301882074913, w0=1.526320207994225, w1=-0.4693177644579256\n",
      "Regularized Logistic Regression(295/999): loss=0.5619910327587191, w0=1.5277415559793608, w1=-0.4704005457689714\n",
      "Regularized Logistic Regression(296/999): loss=0.5619522165864541, w0=1.5291542703442955, w1=-0.4714803864949092\n",
      "Regularized Logistic Regression(297/999): loss=0.5619137359645006, w0=1.5305584113383632, w1=-0.4725572950705807\n",
      "Regularized Logistic Regression(298/999): loss=0.5618755872155985, w0=1.531954038702307, w1=-0.4736312799086799\n",
      "Regularized Logistic Regression(299/999): loss=0.5618377667106818, w0=1.5333412116736584, w1=-0.4747023493996848\n",
      "Regularized Logistic Regression(300/999): loss=0.561800270868148, w0=1.5347199889920615, w1=-0.47577051191179703\n",
      "Regularized Logistic Regression(301/999): loss=0.5617630961531407, w0=1.5360904289045003, w1=-0.4768357757908799\n",
      "Regularized Logistic Regression(302/999): loss=0.561726239076842, w0=1.5374525891704813, w1=-0.47789814936040587\n",
      "Regularized Logistic Regression(303/999): loss=0.5616896961957831, w0=1.5388065270671307, w1=-0.47895764092140164\n",
      "Regularized Logistic Regression(304/999): loss=0.5616534641111591, w0=1.540152299394241, w1=-0.4800142587524009\n",
      "Regularized Logistic Regression(305/999): loss=0.5616175394681618, w0=1.5414899624792433, w1=-0.4810680111093991\n",
      "Regularized Logistic Regression(306/999): loss=0.5615819189553195, w0=1.5428195721821125, w1=-0.48211890622581133\n",
      "Regularized Logistic Regression(307/999): loss=0.5615465993038518, w0=1.5441411839002168, w1=-0.48316695231243184\n",
      "Regularized Logistic Regression(308/999): loss=0.5615115772870322, w0=1.5454548525731049, w1=-0.4842121575574018\n",
      "Regularized Logistic Regression(309/999): loss=0.5614768497195627, w0=1.5467606326872239, w1=-0.4852545301261719\n",
      "Regularized Logistic Regression(310/999): loss=0.5614424134569606, w0=1.5480585782805778, w1=-0.486294078161476\n",
      "Regularized Logistic Regression(311/999): loss=0.561408265394952, w0=1.5493487429473445, w1=-0.4873308097833018\n",
      "Regularized Logistic Regression(312/999): loss=0.56137440246888, w0=1.5506311798424006, w1=-0.48836473308886846\n",
      "Regularized Logistic Regression(313/999): loss=0.5613408216531186, w0=1.5519059416858245, w1=-0.4893958561526033\n",
      "Regularized Logistic Regression(314/999): loss=0.5613075199604991, w0=1.5531730807673174, w1=-0.49042418702612206\n",
      "Regularized Logistic Regression(315/999): loss=0.5612744944417457, w0=1.5544326489505789, w1=-0.49144973373821554\n",
      "Regularized Logistic Regression(316/999): loss=0.5612417421849194, w0=1.5556846976776246, w1=-0.4924725042948334\n",
      "Regularized Logistic Regression(317/999): loss=0.5612092603148734, w0=1.556929277973048, w1=-0.4934925066790726\n",
      "Regularized Logistic Regression(318/999): loss=0.5611770459927133, w0=1.558166440448239, w1=-0.4945097488511683\n",
      "Regularized Logistic Regression(319/999): loss=0.5611450964152729, w0=1.5593962353055402, w1=-0.4955242387484886\n",
      "Regularized Logistic Regression(320/999): loss=0.5611134088145915, w0=1.5606187123423416, w1=-0.49653598428552714\n",
      "Regularized Logistic Regression(321/999): loss=0.5610819804574051, w0=1.5618339209551455, w1=-0.49754499335390345\n",
      "Regularized Logistic Regression(322/999): loss=0.5610508086446434, w0=1.5630419101435775, w1=-0.49855127382235975\n",
      "Regularized Logistic Regression(323/999): loss=0.5610198907109354, w0=1.5642427285143252, w1=-0.4995548335367663\n",
      "Regularized Logistic Regression(324/999): loss=0.5609892240241248, w0=1.5654364242850616, w1=-0.5005556803201203\n",
      "Regularized Logistic Regression(325/999): loss=0.5609588059847905, w0=1.56662304528829, w1=-0.5015538219725563\n",
      "Regularized Logistic Regression(326/999): loss=0.5609286340257773, w0=1.5678026389751578, w1=-0.5025492662713503\n",
      "Regularized Logistic Regression(327/999): loss=0.5608987056117328, w0=1.568975252419232, w1=-0.5035420209709328\n",
      "Regularized Logistic Regression(328/999): loss=0.5608690182386536, w0=1.5701409323201976, w1=-0.5045320938028939\n",
      "Regularized Logistic Regression(329/999): loss=0.560839569433436, w0=1.5712997250075467, w1=-0.5055194924760031\n",
      "Regularized Logistic Regression(330/999): loss=0.5608103567534378, w0=1.5724516764441991, w1=-0.5065042246762207\n",
      "Regularized Logistic Regression(331/999): loss=0.5607813777860424, w0=1.5735968322300893, w1=-0.5074862980667144\n",
      "Regularized Logistic Regression(332/999): loss=0.5607526301482358, w0=1.5747352376057024, w1=-0.5084657202878763\n",
      "Regularized Logistic Regression(333/999): loss=0.5607241114861846, w0=1.575866937455577, w1=-0.5094424989573444\n",
      "Regularized Logistic Regression(334/999): loss=0.560695819474825, w0=1.5769919763117595, w1=-0.5104166416700214\n",
      "Regularized Logistic Regression(335/999): loss=0.5606677518174549, w0=1.5781103983572164, w1=-0.5113881559980991\n",
      "Regularized Logistic Regression(336/999): loss=0.5606399062453379, w0=1.5792222474292104, w1=-0.5123570494910814\n",
      "Regularized Logistic Regression(337/999): loss=0.5606122805173062, w0=1.5803275670226309, w1=-0.5133233296758106\n",
      "Regularized Logistic Regression(338/999): loss=0.5605848724193767, w0=1.5814264002932863, w1=-0.5142870040564917\n",
      "Regularized Logistic Regression(339/999): loss=0.5605576797643694, w0=1.5825187900611608, w1=-0.515248080114723\n",
      "Regularized Logistic Regression(340/999): loss=0.5605307003915321, w0=1.5836047788136274, w1=-0.5162065653095234\n",
      "Regularized Logistic Regression(341/999): loss=0.5605039321661723, w0=1.5846844087086256, w1=-0.5171624670773644\n",
      "Regularized Logistic Regression(342/999): loss=0.5604773729792949, w0=1.5857577215778043, w1=-0.5181157928322011\n",
      "Regularized Logistic Regression(343/999): loss=0.560451020747244, w0=1.586824758929621, w1=-0.5190665499655062\n",
      "Regularized Logistic Regression(344/999): loss=0.5604248734113525, w0=1.5878855619524033, w1=-0.5200147458463009\n",
      "Regularized Logistic Regression(345/999): loss=0.5603989289375952, w0=1.5889401715173896, w1=-0.5209603878211934\n",
      "Regularized Logistic Regression(346/999): loss=0.5603731853162486, w0=1.5899886281817144, w1=-0.5219034832144138\n",
      "Regularized Logistic Regression(347/999): loss=0.5603476405615561, w0=1.5910309721913756, w1=-0.5228440393278508\n",
      "Regularized Logistic Regression(348/999): loss=0.5603222927113972, w0=1.5920672434841523, w1=-0.5237820634410914\n",
      "Regularized Logistic Regression(349/999): loss=0.5602971398269634, w0=1.593097481692504, w1=-0.5247175628114583\n",
      "Regularized Logistic Regression(350/999): loss=0.560272179992438, w0=1.5941217261464198, w1=-0.525650544674051\n",
      "Regularized Logistic Regression(351/999): loss=0.5602474113146819, w0=1.5951400158762532, w1=-0.5265810162417868\n",
      "Regularized Logistic Regression(352/999): loss=0.560222831922923, w0=1.5961523896155003, w1=-0.5275089847054435\n",
      "Regularized Logistic Regression(353/999): loss=0.5601984399684513, w0=1.5971588858035757, w1=-0.5284344572337006\n",
      "Regularized Logistic Regression(354/999): loss=0.5601742336243185, w0=1.598159542588528, w1=-0.5293574409731832\n",
      "Regularized Logistic Regression(355/999): loss=0.5601502110850424, w0=1.5991543978297436, w1=-0.5302779430485078\n",
      "Regularized Logistic Regression(356/999): loss=0.5601263705663156, w0=1.6001434891006134, w1=-0.5311959705623274\n",
      "Regularized Logistic Regression(357/999): loss=0.5601027103047189, w0=1.601126853691164, w1=-0.5321115305953761\n",
      "Regularized Logistic Regression(358/999): loss=0.5600792285574381, w0=1.6021045286106639, w1=-0.5330246302065167\n",
      "Regularized Logistic Regression(359/999): loss=0.5600559236019874, w0=1.603076550590206, w1=-0.5339352764327879\n",
      "Regularized Logistic Regression(360/999): loss=0.5600327937359342, w0=1.6040429560852443, w1=-0.5348434762894512\n",
      "Regularized Logistic Regression(361/999): loss=0.5600098372766306, w0=1.6050037812781242, w1=-0.535749236770044\n",
      "Regularized Logistic Regression(362/999): loss=0.5599870525609472, w0=1.6059590620805633, w1=-0.5366525648464237\n",
      "Regularized Logistic Regression(363/999): loss=0.5599644379450117, w0=1.6069088341361197, w1=-0.537553467468819\n",
      "Regularized Logistic Regression(364/999): loss=0.5599419918039517, w0=1.607853132822619, w1=-0.5384519515658842\n",
      "Regularized Logistic Regression(365/999): loss=0.5599197125316406, w0=1.608791993254569, w1=-0.5393480240447458\n",
      "Regularized Logistic Regression(366/999): loss=0.5598975985404485, w0=1.6097254502855312, w1=-0.5402416917910566\n",
      "Regularized Logistic Regression(367/999): loss=0.5598756482609958, w0=1.6106535385104876, w1=-0.5411329616690475\n",
      "Regularized Logistic Regression(368/999): loss=0.55985386014191, w0=1.6115762922681536, w1=-0.5420218405215799\n",
      "Regularized Logistic Regression(369/999): loss=0.5598322326495891, w0=1.6124937456432853, w1=-0.5429083351701997\n",
      "Regularized Logistic Regression(370/999): loss=0.5598107642679637, w0=1.6134059324689636, w1=-0.5437924524151901\n",
      "Regularized Logistic Regression(371/999): loss=0.5597894534982687, w0=1.6143128863288325, w1=-0.5446741990356274\n",
      "Regularized Logistic Regression(372/999): loss=0.5597682988588112, w0=1.615214640559333, w1=-0.5455535817894346\n",
      "Regularized Logistic Regression(373/999): loss=0.5597472988847499, w0=1.6161112282519052, w1=-0.5464306074134336\n",
      "Regularized Logistic Regression(374/999): loss=0.5597264521278711, w0=1.6170026822551635, w1=-0.5473052826234085\n",
      "Regularized Logistic Regression(375/999): loss=0.5597057571563706, w0=1.6178890351770503, w1=-0.5481776141141521\n",
      "Regularized Logistic Regression(376/999): loss=0.5596852125546402, w0=1.6187703193869656, w1=-0.5490476085595295\n",
      "Regularized Logistic Regression(377/999): loss=0.5596648169230549, w0=1.619646567017883, w1=-0.54991527261253\n",
      "Regularized Logistic Regression(378/999): loss=0.5596445688777651, w0=1.6205178099684168, w1=-0.5507806129053263\n",
      "Regularized Logistic Regression(379/999): loss=0.5596244670504898, w0=1.6213840799048989, w1=-0.5516436360493316\n",
      "Regularized Logistic Regression(380/999): loss=0.5596045100883165, w0=1.6222454082634141, w1=-0.5525043486352579\n",
      "Regularized Logistic Regression(381/999): loss=0.5595846966534982, w0=1.6231018262518146, w1=-0.5533627572331711\n",
      "Regularized Logistic Regression(382/999): loss=0.5595650254232611, w0=1.623953364851718, w1=-0.5542188683925513\n",
      "Regularized Logistic Regression(383/999): loss=0.5595454950896066, w0=1.624800054820485, w1=-0.5550726886423518\n",
      "Regularized Logistic Regression(384/999): loss=0.5595261043591235, w0=1.6256419266931612, w1=-0.5559242244910567\n",
      "Regularized Logistic Regression(385/999): loss=0.5595068519527989, w0=1.6264790107844198, w1=-0.5567734824267402\n",
      "Regularized Logistic Regression(386/999): loss=0.5594877366058332, w0=1.6273113371904753, w1=-0.5576204689171247\n",
      "Regularized Logistic Regression(387/999): loss=0.5594687570674559, w0=1.628138935790964, w1=-0.5584651904096423\n",
      "Regularized Logistic Regression(388/999): loss=0.559449912100748, w0=1.628961836250822, w1=-0.5593076533314922\n",
      "Regularized Logistic Regression(389/999): loss=0.5594312004824628, w0=1.6297800680221328, w1=-0.5601478640897036\n",
      "Regularized Logistic Regression(390/999): loss=0.559412621002852, w0=1.6305936603459636, w1=-0.5609858290711912\n",
      "Regularized Logistic Regression(391/999): loss=0.5593941724654926, w0=1.6314026422541787, w1=-0.5618215546428196\n",
      "Regularized Logistic Regression(392/999): loss=0.5593758536871185, w0=1.6322070425712267, w1=-0.5626550471514615\n",
      "Regularized Logistic Regression(393/999): loss=0.5593576634974509, w0=1.633006889915917, w1=-0.5634863129240586\n",
      "Regularized Logistic Regression(394/999): loss=0.5593396007390354, w0=1.6338022127031775, w1=-0.5643153582676826\n",
      "Regularized Logistic Regression(395/999): loss=0.5593216642670786, w0=1.6345930391457928, w1=-0.5651421894695953\n",
      "Regularized Logistic Regression(396/999): loss=0.5593038529492874, w0=1.6353793972561204, w1=-0.5659668127973106\n",
      "Regularized Logistic Regression(397/999): loss=0.5592861656657118, w0=1.6361613148477898, w1=-0.5667892344986544\n",
      "Regularized Logistic Regression(398/999): loss=0.5592686013085884, w0=1.6369388195373993, w1=-0.567609460801826\n",
      "Regularized Logistic Regression(399/999): loss=0.559251158782188, w0=1.6377119387461634, w1=-0.5684274979154601\n",
      "Regularized Logistic Regression(400/999): loss=0.5592338370026624, w0=1.6384806997015793, w1=-0.569243352028687\n",
      "Regularized Logistic Regression(401/999): loss=0.5592166348978969, w0=1.639245129439049, w1=-0.5700570293111946\n",
      "Regularized Logistic Regression(402/999): loss=0.5591995514073623, w0=1.64000525480349, w1=-0.5708685359132902\n",
      "Regularized Logistic Regression(403/999): loss=0.5591825854819706, w0=1.6407611024509519, w1=-0.5716778779659609\n",
      "Regularized Logistic Regression(404/999): loss=0.5591657360839318, w0=1.6415126988501785, w1=-0.5724850615809368\n",
      "Regularized Logistic Regression(405/999): loss=0.559149002186612, w0=1.6422600702841867, w1=-0.5732900928507514\n",
      "Regularized Logistic Regression(406/999): loss=0.5591323827743956, w0=1.6430032428518144, w1=-0.5740929778488035\n",
      "Regularized Logistic Regression(407/999): loss=0.5591158768425487, w0=1.6437422424692476, w1=-0.5748937226294185\n",
      "Regularized Logistic Regression(408/999): loss=0.5590994833970816, w0=1.6444770948715455, w1=-0.5756923332279122\n",
      "Regularized Logistic Regression(409/999): loss=0.559083201454619, w0=1.6452078256141465, w1=-0.5764888156606496\n",
      "Regularized Logistic Regression(410/999): loss=0.5590670300422658, w0=1.645934460074347, w1=-0.5772831759251086\n",
      "Regularized Logistic Regression(411/999): loss=0.5590509681974792, w0=1.6466570234527826, w1=-0.578075419999941\n",
      "Regularized Logistic Regression(412/999): loss=0.55903501496794, w0=1.6473755407748798, w1=-0.5788655538450349\n",
      "Regularized Logistic Regression(413/999): loss=0.5590191694114278, w0=1.648090036892307, w1=-0.5796535834015767\n",
      "Regularized Logistic Regression(414/999): loss=0.5590034305956957, w0=1.6488005364843956, w1=-0.5804395145921112\n",
      "Regularized Logistic Regression(415/999): loss=0.5589877975983488, w0=1.6495070640595588, w1=-0.5812233533206045\n",
      "Regularized Logistic Regression(416/999): loss=0.5589722695067231, w0=1.6502096439566913, w1=-0.5820051054725071\n",
      "Regularized Logistic Regression(417/999): loss=0.5589568454177654, w0=1.6509083003465543, w1=-0.5827847769148113\n",
      "Regularized Logistic Regression(418/999): loss=0.5589415244379179, w0=1.6516030572331508, w1=-0.5835623734961184\n",
      "Regularized Logistic Regression(419/999): loss=0.5589263056830006, w0=1.652293938455082, w1=-0.5843379010466951\n",
      "Regularized Logistic Regression(420/999): loss=0.5589111882780976, w0=1.6529809676868956, w1=-0.5851113653785397\n",
      "Regularized Logistic Regression(421/999): loss=0.558896171357445, w0=1.6536641684404154, w1=-0.5858827722854405\n",
      "Regularized Logistic Regression(422/999): loss=0.5588812540643189, w0=1.6543435640660615, w1=-0.5866521275430374\n",
      "Regularized Logistic Regression(423/999): loss=0.5588664355509267, w0=1.655019177754157, w1=-0.5874194369088852\n",
      "Regularized Logistic Regression(424/999): loss=0.5588517149782976, w0=1.655691032536216, w1=-0.5881847061225124\n",
      "Regularized Logistic Regression(425/999): loss=0.558837091516178, w0=1.6563591512862337, w1=-0.5889479409054856\n",
      "Regularized Logistic Regression(426/999): loss=0.5588225643429243, w0=1.6570235567219478, w1=-0.5897091469614668\n",
      "Regularized Logistic Regression(427/999): loss=0.5588081326454006, w0=1.6576842714060933, w1=-0.5904683299762782\n",
      "Regularized Logistic Regression(428/999): loss=0.5587937956188754, w0=1.6583413177476516, w1=-0.5912254956179602\n",
      "Regularized Logistic Regression(429/999): loss=0.5587795524669216, w0=1.658994718003076, w1=-0.5919806495368347\n",
      "Regularized Logistic Regression(430/999): loss=0.5587654024013152, w0=1.6596444942775102, w1=-0.5927337973655661\n",
      "Regularized Logistic Regression(431/999): loss=0.5587513446419399, w0=1.6602906685260004, w1=-0.5934849447192173\n",
      "Regularized Logistic Regression(432/999): loss=0.5587373784166875, w0=1.6609332625546922, w1=-0.594234097195317\n",
      "Regularized Logistic Regression(433/999): loss=0.5587235029613626, w0=1.661572298022006, w1=-0.5949812603739159\n",
      "Regularized Logistic Regression(434/999): loss=0.5587097175195908, w0=1.6622077964398205, w1=-0.5957264398176478\n",
      "Regularized Logistic Regression(435/999): loss=0.5586960213427226, w0=1.6628397791746214, w1=-0.5964696410717922\n",
      "Regularized Logistic Regression(436/999): loss=0.5586824136897429, w0=1.6634682674486705, w1=-0.5972108696643329\n",
      "Regularized Logistic Regression(437/999): loss=0.5586688938271803, w0=1.664093282341123, w1=-0.5979501311060156\n",
      "Regularized Logistic Regression(438/999): loss=0.5586554610290185, w0=1.6647148447891733, w1=-0.5986874308904124\n",
      "Regularized Logistic Regression(439/999): loss=0.5586421145766064, w0=1.6653329755891633, w1=-0.599422774493978\n",
      "Regularized Logistic Regression(440/999): loss=0.5586288537585727, w0=1.6659476953976942, w1=-0.6001561673761132\n",
      "Regularized Logistic Regression(441/999): loss=0.5586156778707395, w0=1.6665590247327198, w1=-0.6008876149792197\n",
      "Regularized Logistic Regression(442/999): loss=0.5586025862160368, w0=1.6671669839746361, w1=-0.6016171227287643\n",
      "Regularized Logistic Regression(443/999): loss=0.5585895781044208, w0=1.6677715933673507, w1=-0.6023446960333362\n",
      "Regularized Logistic Regression(444/999): loss=0.5585766528527893, w0=1.668372873019354, w1=-0.6030703402847036\n",
      "Regularized Logistic Regression(445/999): loss=0.5585638097849024, w0=1.6689708429047734, w1=-0.6037940608578775\n",
      "Regularized Logistic Regression(446/999): loss=0.5585510482312999, w0=1.6695655228644213, w1=-0.6045158631111678\n",
      "Regularized Logistic Regression(447/999): loss=0.5585383675292238, w0=1.6701569326068237, w1=-0.605235752386243\n",
      "Regularized Logistic Regression(448/999): loss=0.55852576702254, w0=1.6707450917092554, w1=-0.605953734008189\n",
      "Regularized Logistic Regression(449/999): loss=0.5585132460616601, w0=1.6713300196187515, w1=-0.6066698132855659\n",
      "Regularized Logistic Regression(450/999): loss=0.5585008040034664, w0=1.6719117356531152, w1=-0.6073839955104692\n",
      "Regularized Logistic Regression(451/999): loss=0.558488440211235, w0=1.672490259001913, w1=-0.6080962859585852\n",
      "Regularized Logistic Regression(452/999): loss=0.5584761540545643, w0=1.6730656087274682, w1=-0.6088066898892504\n",
      "Regularized Logistic Regression(453/999): loss=0.5584639449092996, w0=1.6736378037658337, w1=-0.6095152125455099\n",
      "Regularized Logistic Regression(454/999): loss=0.5584518121574613, w0=1.6742068629277715, w1=-0.6102218591541745\n",
      "Regularized Logistic Regression(455/999): loss=0.5584397551871747, w0=1.6747728048997006, w1=-0.6109266349258785\n",
      "Regularized Logistic Regression(456/999): loss=0.5584277733925977, w0=1.675335648244665, w1=-0.6116295450551357\n",
      "Regularized Logistic Regression(457/999): loss=0.5584158661738524, w0=1.675895411403266, w1=-0.6123305947203991\n",
      "Regularized Logistic Regression(458/999): loss=0.5584040329369573, w0=1.6764521126945957, w1=-0.6130297890841168\n",
      "Regularized Logistic Regression(459/999): loss=0.5583922730937587, w0=1.6770057703171724, w1=-0.6137271332927889\n",
      "Regularized Logistic Regression(460/999): loss=0.5583805860618636, w0=1.677556402349855, w1=-0.6144226324770237\n",
      "Regularized Logistic Regression(461/999): loss=0.5583689712645742, w0=1.678104026752745, w1=-0.6151162917515957\n",
      "Regularized Logistic Regression(462/999): loss=0.5583574281308231, w0=1.678648661368097, w1=-0.615808116215501\n",
      "Regularized Logistic Regression(463/999): loss=0.5583459560951087, w0=1.6791903239212118, w1=-0.6164981109520137\n",
      "Regularized Logistic Regression(464/999): loss=0.5583345545974321, w0=1.6797290320213116, w1=-0.6171862810287427\n",
      "Regularized Logistic Regression(465/999): loss=0.5583232230832327, w0=1.6802648031624237, w1=-0.6178726314976862\n",
      "Regularized Logistic Regression(466/999): loss=0.5583119610033302, w0=1.6807976547242487, w1=-0.6185571673952882\n",
      "Regularized Logistic Regression(467/999): loss=0.5583007678138601, w0=1.6813276039730218, w1=-0.6192398937424954\n",
      "Regularized Logistic Regression(468/999): loss=0.5582896429762136, w0=1.6818546680623634, w1=-0.6199208155448113\n",
      "Regularized Logistic Regression(469/999): loss=0.558278585956982, w0=1.6823788640341257, w1=-0.62059993779235\n",
      "Regularized Logistic Regression(470/999): loss=0.5582675962278923, w0=1.6829002088192353, w1=-0.6212772654598946\n",
      "Regularized Logistic Regression(471/999): loss=0.5582566732657539, w0=1.6834187192385182, w1=-0.6219528035069497\n",
      "Regularized Logistic Regression(472/999): loss=0.5582458165523989, w0=1.6839344120035207, w1=-0.6226265568777958\n",
      "Regularized Logistic Regression(473/999): loss=0.5582350255746268, w0=1.6844473037173306, w1=-0.6232985305015447\n",
      "Regularized Logistic Regression(474/999): loss=0.5582242998241477, w0=1.6849574108753824, w1=-0.6239687292921957\n",
      "Regularized Logistic Regression(475/999): loss=0.5582136387975275, w0=1.6854647498662592, w1=-0.6246371581486856\n",
      "Regularized Logistic Regression(476/999): loss=0.5582030419961349, w0=1.6859693369724886, w1=-0.6253038219549458\n",
      "Regularized Logistic Regression(477/999): loss=0.5581925089260864, w0=1.686471188371322, w1=-0.6259687255799568\n",
      "Regularized Logistic Regression(478/999): loss=0.5581820390981923, w0=1.6869703201355262, w1=-0.6266318738777971\n",
      "Regularized Logistic Regression(479/999): loss=0.5581716320279069, w0=1.6874667482341499, w1=-0.6272932716877017\n",
      "Regularized Logistic Regression(480/999): loss=0.5581612872352749, w0=1.6879604885332862, w1=-0.6279529238341144\n",
      "Regularized Logistic Regression(481/999): loss=0.5581510042448803, w0=1.6884515567968417, w1=-0.6286108351267369\n",
      "Regularized Logistic Regression(482/999): loss=0.5581407825857984, w0=1.6889399686872808, w1=-0.6292670103605867\n",
      "Regularized Logistic Regression(483/999): loss=0.5581306217915425, w0=1.6894257397663757, w1=-0.6299214543160478\n",
      "Regularized Logistic Regression(484/999): loss=0.5581205214000171, w0=1.689908885495947, w1=-0.6305741717589222\n",
      "Regularized Logistic Regression(485/999): loss=0.5581104809534696, w0=1.6903894212385915, w1=-0.6312251674404838\n",
      "Regularized Logistic Regression(486/999): loss=0.5581004999984404, w0=1.6908673622584183, w1=-0.6318744460975292\n",
      "Regularized Logistic Regression(487/999): loss=0.5580905780857177, w0=1.6913427237217589, w1=-0.6325220124524308\n",
      "Regularized Logistic Regression(488/999): loss=0.5580807147702884, w0=1.691815520697891, w1=-0.6331678712131869\n",
      "Regularized Logistic Regression(489/999): loss=0.5580709096112954, w0=1.6922857681597405, w1=-0.633812027073477\n",
      "Regularized Logistic Regression(490/999): loss=0.5580611621719886, w0=1.6927534809845821, w1=-0.6344544847127078\n",
      "Regularized Logistic Regression(491/999): loss=0.5580514720196819, w0=1.6932186739547412, w1=-0.6350952487960685\n",
      "Regularized Logistic Regression(492/999): loss=0.5580418387257075, w0=1.6936813617582824, w1=-0.635734323974581\n",
      "Regularized Logistic Regression(493/999): loss=0.5580322618653727, w0=1.694141558989685, w1=-0.636371714885149\n",
      "Regularized Logistic Regression(494/999): loss=0.5580227410179167, w0=1.6945992801505314, w1=-0.6370074261506097\n",
      "Regularized Logistic Regression(495/999): loss=0.5580132757664676, w0=1.695054539650171, w1=-0.6376414623797855\n",
      "Regularized Logistic Regression(496/999): loss=0.5580038656979999, w0=1.695507351806397, w1=-0.6382738281675326\n",
      "Regularized Logistic Regression(497/999): loss=0.5579945104032925, w0=1.6959577308460951, w1=-0.6389045280947918\n",
      "Regularized Logistic Regression(498/999): loss=0.5579852094768877, w0=1.6964056909059109, w1=-0.6395335667286369\n",
      "Regularized Logistic Regression(499/999): loss=0.5579759625170504, w0=1.6968512460328822, w1=-0.6401609486223274\n",
      "Regularized Logistic Regression(500/999): loss=0.5579667691257291, w0=1.6972944101851035, w1=-0.6407866783153547\n",
      "Regularized Logistic Regression(501/999): loss=0.5579576289085131, w0=1.6977351972323471, w1=-0.6414107603334943\n",
      "Regularized Logistic Regression(502/999): loss=0.5579485414745954, w0=1.6981736209567073, w1=-0.6420331991888522\n",
      "Regularized Logistic Regression(503/999): loss=0.5579395064367347, w0=1.698609695053224, w1=-0.6426539993799163\n",
      "Regularized Logistic Regression(504/999): loss=0.5579305234112145, w0=1.6990434331305038, w1=-0.6432731653916041\n",
      "Regularized Logistic Regression(505/999): loss=0.5579215920178072, w0=1.6994748487113363, w1=-0.6438907016953086\n",
      "Regularized Logistic Regression(506/999): loss=0.5579127118797361, w0=1.6999039552333124, w1=-0.6445066127489526\n",
      "Regularized Logistic Regression(507/999): loss=0.5579038826236379, w0=1.7003307660494222, w1=-0.6451209029970322\n",
      "Regularized Logistic Regression(508/999): loss=0.5578951038795271, w0=1.7007552944286624, w1=-0.6457335768706652\n",
      "Regularized Logistic Regression(509/999): loss=0.5578863752807597, w0=1.7011775535566356, w1=-0.6463446387876407\n",
      "Regularized Logistic Regression(510/999): loss=0.5578776964639967, w0=1.7015975565361292, w1=-0.6469540931524675\n",
      "Regularized Logistic Regression(511/999): loss=0.55786906706917, w0=1.7020153163877185, w1=-0.6475619443564166\n",
      "Regularized Logistic Regression(512/999): loss=0.557860486739447, w0=1.70243084605034, w1=-0.6481681967775724\n",
      "Regularized Logistic Regression(513/999): loss=0.5578519551211969, w0=1.702844158381864, w1=-0.6487728547808806\n",
      "Regularized Logistic Regression(514/999): loss=0.5578434718639556, w0=1.703255266159674, w1=-0.6493759227181932\n",
      "Regularized Logistic Regression(515/999): loss=0.5578350366203931, w0=1.7036641820812328, w1=-0.6499774049283145\n",
      "Regularized Logistic Regression(516/999): loss=0.5578266490462807, w0=1.7040709187646466, w1=-0.6505773057370494\n",
      "Regularized Logistic Regression(517/999): loss=0.557818308800457, w0=1.704475488749215, w1=-0.6511756294572489\n",
      "Regularized Logistic Regression(518/999): loss=0.5578100155447954, w0=1.7048779044959947, w1=-0.6517723803888562\n",
      "Regularized Logistic Regression(519/999): loss=0.5578017689441745, w0=1.7052781783883386, w1=-0.6523675628189524\n",
      "Regularized Logistic Regression(520/999): loss=0.5577935686664439, w0=1.7056763227324483, w1=-0.6529611810218013\n",
      "Regularized Logistic Regression(521/999): loss=0.5577854143823946, w0=1.7060723497579107, w1=-0.6535532392588994\n",
      "Regularized Logistic Regression(522/999): loss=0.5577773057657276, w0=1.706466271618235, w1=-0.6541437417790149\n",
      "Regularized Logistic Regression(523/999): loss=0.5577692424930222, w0=1.706858100391377, w1=-0.6547326928182375\n",
      "Regularized Logistic Regression(524/999): loss=0.5577612242437092, w0=1.7072478480802786, w1=-0.6553200966000223\n",
      "Regularized Logistic Regression(525/999): loss=0.5577532507000373, w0=1.707635526613373, w1=-0.6559059573352347\n",
      "Regularized Logistic Regression(526/999): loss=0.5577453215470465, w0=1.7080211478451168, w1=-0.6564902792221938\n",
      "Regularized Logistic Regression(527/999): loss=0.557737436472537, w0=1.7084047235565034, w1=-0.6570730664467174\n",
      "Regularized Logistic Regression(528/999): loss=0.5577295951670428, w0=1.7087862654555641, w1=-0.6576543231821683\n",
      "Regularized Logistic Regression(529/999): loss=0.5577217973238008, w0=1.7091657851778783, w1=-0.6582340535894948\n",
      "Regularized Logistic Regression(530/999): loss=0.557714042638725, w0=1.709543294287081, w1=-0.6588122618172789\n",
      "Regularized Logistic Regression(531/999): loss=0.5577063308103767, w0=1.7099188042753464, w1=-0.6593889520017769\n",
      "Regularized Logistic Regression(532/999): loss=0.5576986615399394, w0=1.710292326563889, w1=-0.6599641282669628\n",
      "Regularized Logistic Regression(533/999): loss=0.5576910345311895, w0=1.7106638725034597, w1=-0.6605377947245744\n",
      "Regularized Logistic Regression(534/999): loss=0.5576834494904711, w0=1.7110334533748257, w1=-0.6611099554741536\n",
      "Regularized Logistic Regression(535/999): loss=0.5576759061266693, w0=1.7114010803892428, w1=-0.661680614603093\n",
      "Regularized Logistic Regression(536/999): loss=0.5576684041511831, w0=1.711766764688949, w1=-0.6622497761866735\n",
      "Regularized Logistic Regression(537/999): loss=0.557660943277901, w0=1.7121305173476256, w1=-0.6628174442881121\n",
      "Regularized Logistic Regression(538/999): loss=0.5576535232231736, w0=1.7124923493708761, w1=-0.6633836229586011\n",
      "Regularized Logistic Regression(539/999): loss=0.5576461437057919, w0=1.71285227169669, w1=-0.6639483162373516\n",
      "Regularized Logistic Regression(540/999): loss=0.5576388044469579, w0=1.7132102951959025, w1=-0.6645115281516367\n",
      "Regularized Logistic Regression(541/999): loss=0.5576315051702632, w0=1.7135664306726603, w1=-0.6650732627168323\n",
      "Regularized Logistic Regression(542/999): loss=0.5576242456016628, w0=1.7139206888648628, w1=-0.6656335239364575\n",
      "Regularized Logistic Regression(543/999): loss=0.5576170254694527, w0=1.7142730804446336, w1=-0.6661923158022203\n",
      "Regularized Logistic Regression(544/999): loss=0.5576098445042444, w0=1.7146236160187502, w1=-0.6667496422940554\n",
      "Regularized Logistic Regression(545/999): loss=0.5576027024389424, w0=1.7149723061291038, w1=-0.6673055073801667\n",
      "Regularized Logistic Regression(546/999): loss=0.5575955990087215, w0=1.7153191612531289, w1=-0.6678599150170682\n",
      "Regularized Logistic Regression(547/999): loss=0.5575885339510018, w0=1.7156641918042461, w1=-0.6684128691496249\n",
      "Regularized Logistic Regression(548/999): loss=0.5575815070054283, w0=1.716007408132295, w1=-0.6689643737110943\n",
      "Regularized Logistic Regression(549/999): loss=0.5575745179138469, w0=1.7163488205239663, w1=-0.669514432623167\n",
      "Regularized Logistic Regression(550/999): loss=0.5575675664202825, w0=1.7166884392032278, w1=-0.6700630497960062\n",
      "Regularized Logistic Regression(551/999): loss=0.557560652270918, w0=1.7170262743317484, w1=-0.6706102291282866\n",
      "Regularized Logistic Regression(552/999): loss=0.5575537752140715, w0=1.7173623360093215, w1=-0.671155974507239\n",
      "Regularized Logistic Regression(553/999): loss=0.5575469350001748, w0=1.7176966342742768, w1=-0.671700289808686\n",
      "Regularized Logistic Regression(554/999): loss=0.5575401313817536, w0=1.7180291791038989, w1=-0.6722431788970848\n",
      "Regularized Logistic Regression(555/999): loss=0.5575333641134048, w0=1.7183599804148377, w1=-0.6727846456255635\n",
      "Regularized Logistic Regression(556/999): loss=0.5575266329517768, w0=1.7186890480635142, w1=-0.6733246938359624\n",
      "Regularized Logistic Regression(557/999): loss=0.557519937655549, w0=1.7190163918465269, w1=-0.6738633273588738\n",
      "Regularized Logistic Regression(558/999): loss=0.5575132779854113, w0=1.7193420215010529, w1=-0.6744005500136807\n",
      "Regularized Logistic Regression(559/999): loss=0.557506653704044, w0=1.7196659467052438, w1=-0.674936365608595\n",
      "Regularized Logistic Regression(560/999): loss=0.5575000645760984, w0=1.7199881770786252, w1=-0.6754707779406974\n",
      "Regularized Logistic Regression(561/999): loss=0.5574935103681763, w0=1.7203087221824909, w1=-0.6760037907959753\n",
      "Regularized Logistic Regression(562/999): loss=0.5574869908488125, w0=1.720627591520278, w1=-0.6765354079493587\n",
      "Regularized Logistic Regression(563/999): loss=0.5574805057884537, w0=1.7209447945379708, w1=-0.6770656331647653\n",
      "Regularized Logistic Regression(564/999): loss=0.557474054959441, w0=1.721260340624471, w1=-0.6775944701951302\n",
      "Regularized Logistic Regression(565/999): loss=0.5574676381359901, w0=1.7215742391119877, w1=-0.6781219227824514\n",
      "Regularized Logistic Regression(566/999): loss=0.5574612550941734, w0=1.7218864992764038, w1=-0.6786479946578222\n",
      "Regularized Logistic Regression(567/999): loss=0.5574549056119018, w0=1.7221971303376622, w1=-0.6791726895414721\n",
      "Regularized Logistic Regression(568/999): loss=0.5574485894689069, w0=1.7225061414601293, w1=-0.6796960111428023\n",
      "Regularized Logistic Regression(569/999): loss=0.5574423064467218, w0=1.722813541752964, w1=-0.6802179631604236\n",
      "Regularized Logistic Regression(570/999): loss=0.5574360563286654, w0=1.7231193402704896, w1=-0.6807385492821941\n",
      "Regularized Logistic Regression(571/999): loss=0.5574298388998232, w0=1.723423546012551, w1=-0.681257773185256\n",
      "Regularized Logistic Regression(572/999): loss=0.5574236539470311, w0=1.7237261679248796, w1=-0.681775638536071\n",
      "Regularized Logistic Regression(573/999): loss=0.5574175012588571, w0=1.7240272148994467, w1=-0.6822921489904599\n",
      "Regularized Logistic Regression(574/999): loss=0.5574113806255868, w0=1.7243266957748165, w1=-0.6828073081936368\n",
      "Regularized Logistic Regression(575/999): loss=0.557405291839204, w0=1.7246246193365071, w1=-0.6833211197802453\n",
      "Regularized Logistic Regression(576/999): loss=0.5573992346933752, w0=1.7249209943173323, w1=-0.6838335873743973\n",
      "Regularized Logistic Regression(577/999): loss=0.5573932089834339, w0=1.7252158293977575, w1=-0.684344714589705\n",
      "Regularized Logistic Regression(578/999): loss=0.5573872145063645, w0=1.7255091332062282, w1=-0.6848545050293221\n",
      "Regularized Logistic Regression(579/999): loss=0.5573812510607843, w0=1.7258009143195276, w1=-0.6853629622859743\n",
      "Regularized Logistic Regression(580/999): loss=0.5573753184469298, w0=1.726091181263105, w1=-0.6858700899419973\n",
      "Regularized Logistic Regression(581/999): loss=0.5573694164666414, w0=1.7263799425114197, w1=-0.6863758915693741\n",
      "Regularized Logistic Regression(582/999): loss=0.5573635449233454, w0=1.726667206488273, w1=-0.686880370729766\n",
      "Regularized Logistic Regression(583/999): loss=0.5573577036220418, w0=1.7269529815671365, w1=-0.6873835309745528\n",
      "Regularized Logistic Regression(584/999): loss=0.5573518923692867, w0=1.7272372760714865, w1=-0.6878853758448631\n",
      "Regularized Logistic Regression(585/999): loss=0.5573461109731787, w0=1.7275200982751224, w1=-0.6883859088716116\n",
      "Regularized Logistic Regression(586/999): loss=0.5573403592433434, w0=1.7278014564025044, w1=-0.6888851335755347\n",
      "Regularized Logistic Regression(587/999): loss=0.5573346369909205, w0=1.7280813586290598, w1=-0.6893830534672226\n",
      "Regularized Logistic Regression(588/999): loss=0.5573289440285456, w0=1.728359813081519, w1=-0.6898796720471547\n",
      "Regularized Logistic Regression(589/999): loss=0.5573232801703408, w0=1.7286368278382123, w1=-0.690374992805736\n",
      "Regularized Logistic Regression(590/999): loss=0.5573176452318968, w0=1.7289124109294018, w1=-0.6908690192233282\n",
      "Regularized Logistic Regression(591/999): loss=0.5573120390302605, w0=1.7291865703375886, w1=-0.6913617547702842\n",
      "Regularized Logistic Regression(592/999): loss=0.5573064613839205, w0=1.7294593139978223, w1=-0.6918532029069832\n",
      "Regularized Logistic Regression(593/999): loss=0.5573009121127941, w0=1.7297306497980025, w1=-0.6923433670838656\n",
      "Regularized Logistic Regression(594/999): loss=0.5572953910382131, w0=1.7300005855791964, w1=-0.6928322507414623\n",
      "Regularized Logistic Regression(595/999): loss=0.5572898979829113, w0=1.7302691291359307, w1=-0.6933198573104309\n",
      "Regularized Logistic Regression(596/999): loss=0.5572844327710103, w0=1.730536288216502, w1=-0.6938061902115907\n",
      "Regularized Logistic Regression(597/999): loss=0.5572789952280065, w0=1.73080207052327, w1=-0.6942912528559522\n",
      "Regularized Logistic Regression(598/999): loss=0.5572735851807591, w0=1.731066483712953, w1=-0.6947750486447515\n",
      "Regularized Logistic Regression(599/999): loss=0.5572682024574759, w0=1.7313295353969254, w1=-0.6952575809694839\n",
      "Regularized Logistic Regression(600/999): loss=0.5572628468877022, w0=1.7315912331415144, w1=-0.695738853211936\n",
      "Regularized Logistic Regression(601/999): loss=0.5572575183023066, w0=1.73185158446828, w1=-0.6962188687442188\n",
      "Regularized Logistic Regression(602/999): loss=0.5572522165334697, w0=1.7321105968543087, w1=-0.6966976309288012\n",
      "Regularized Logistic Regression(603/999): loss=0.5572469414146718, w0=1.732368277732499, w1=-0.6971751431185363\n",
      "Regularized Logistic Regression(604/999): loss=0.5572416927806799, w0=1.732624634491842, w1=-0.6976514086567033\n",
      "Regularized Logistic Regression(605/999): loss=0.5572364704675367, w0=1.7328796744777117, w1=-0.6981264308770311\n",
      "Regularized Logistic Regression(606/999): loss=0.5572312743125484, w0=1.7331334049921399, w1=-0.6986002131037347\n",
      "Regularized Logistic Regression(607/999): loss=0.5572261041542722, w0=1.733385833294087, w1=-0.6990727586515446\n",
      "Regularized Logistic Regression(608/999): loss=0.5572209598325052, w0=1.733636966599726, w1=-0.6995440708257418\n",
      "Regularized Logistic Regression(609/999): loss=0.5572158411882746, w0=1.7338868120827169, w1=-0.7000141529221843\n",
      "Regularized Logistic Regression(610/999): loss=0.5572107480638228, w0=1.7341353768744723, w1=-0.7004830082273423\n",
      "Regularized Logistic Regression(611/999): loss=0.5572056803025993, w0=1.7343826680644354, w1=-0.7009506400183304\n",
      "Regularized Logistic Regression(612/999): loss=0.5572006377492476, w0=1.7346286927003334, w1=-0.7014170515629333\n",
      "Regularized Logistic Regression(613/999): loss=0.5571956202495955, w0=1.7348734577884617, w1=-0.7018822461196418\n",
      "Regularized Logistic Regression(614/999): loss=0.5571906276506431, w0=1.7351169702939353, w1=-0.7023462269376819\n",
      "Regularized Logistic Regression(615/999): loss=0.5571856598005522, w0=1.735359237140959, w1=-0.7028089972570463\n",
      "Regularized Logistic Regression(616/999): loss=0.5571807165486373, w0=1.7356002652130762, w1=-0.7032705603085228\n",
      "Regularized Logistic Regression(617/999): loss=0.5571757977453521, w0=1.7358400613534362, w1=-0.703730919313728\n",
      "Regularized Logistic Regression(618/999): loss=0.5571709032422817, w0=1.7360786323650514, w1=-0.7041900774851337\n",
      "Regularized Logistic Regression(619/999): loss=0.5571660328921302, w0=1.7363159850110481, w1=-0.7046480380261003\n",
      "Regularized Logistic Regression(620/999): loss=0.5571611865487124, w0=1.7365521260149193, w1=-0.7051048041309064\n",
      "Regularized Logistic Regression(621/999): loss=0.5571563640669429, w0=1.7367870620607768, w1=-0.7055603789847773\n",
      "Regularized Logistic Regression(622/999): loss=0.5571515653028244, w0=1.7370207997936038, w1=-0.7060147657639166\n",
      "Regularized Logistic Regression(623/999): loss=0.5571467901134414, w0=1.737253345819494, w1=-0.7064679676355349\n",
      "Regularized Logistic Regression(624/999): loss=0.5571420383569464, w0=1.737484706705906, w1=-0.7069199877578786\n",
      "Regularized Logistic Regression(625/999): loss=0.5571373098925528, w0=1.7377148889819052, w1=-0.7073708292802613\n",
      "Regularized Logistic Regression(626/999): loss=0.5571326045805255, w0=1.7379438991383982, w1=-0.7078204953430908\n",
      "Regularized Logistic Regression(627/999): loss=0.5571279222821692, w0=1.7381717436283883, w1=-0.7082689890778998\n",
      "Regularized Logistic Regression(628/999): loss=0.5571232628598208, w0=1.7383984288671985, w1=-0.7087163136073771\n",
      "Regularized Logistic Regression(629/999): loss=0.5571186261768398, w0=1.7386239612327195, w1=-0.7091624720453903\n",
      "Regularized Logistic Regression(630/999): loss=0.5571140120975986, w0=1.73884834706564, w1=-0.7096074674970232\n",
      "Regularized Logistic Regression(631/999): loss=0.5571094204874739, w0=1.7390715926696871, w1=-0.7100513030585949\n",
      "Regularized Logistic Regression(632/999): loss=0.5571048512128378, w0=1.739293704311842, w1=-0.7104939818176973\n",
      "Regularized Logistic Regression(633/999): loss=0.5571003041410472, w0=1.7395146882225934, w1=-0.7109355068532175\n",
      "Regularized Logistic Regression(634/999): loss=0.5570957791404377, w0=1.7397345505961515, w1=-0.7113758812353697\n",
      "Regularized Logistic Regression(635/999): loss=0.5570912760803133, w0=1.7399532975906766, w1=-0.7118151080257221\n",
      "Regularized Logistic Regression(636/999): loss=0.5570867948309368, w0=1.7401709353285073, w1=-0.7122531902772236\n",
      "Regularized Logistic Regression(637/999): loss=0.5570823352635242, w0=1.7403874698963846, w1=-0.7126901310342355\n",
      "Regularized Logistic Regression(638/999): loss=0.5570778972502332, w0=1.7406029073456775, w1=-0.7131259333325546\n",
      "Regularized Logistic Regression(639/999): loss=0.5570734806641555, w0=1.740817253692596, w1=-0.7135606001994453\n",
      "Regularized Logistic Regression(640/999): loss=0.5570690853793112, w0=1.7410305149184186, w1=-0.7139941346536661\n",
      "Regularized Logistic Regression(641/999): loss=0.5570647112706356, w0=1.7412426969697075, w1=-0.7144265397054944\n",
      "Regularized Logistic Regression(642/999): loss=0.5570603582139765, w0=1.741453805758522, w1=-0.7148578183567577\n",
      "Regularized Logistic Regression(643/999): loss=0.5570560260860817, w0=1.741663847162637, w1=-0.7152879736008587\n",
      "Regularized Logistic Regression(644/999): loss=0.5570517147645938, w0=1.7418728270257597, w1=-0.7157170084228044\n",
      "Regularized Logistic Regression(645/999): loss=0.5570474241280411, w0=1.7420807511577274, w1=-0.7161449257992295\n",
      "Regularized Logistic Regression(646/999): loss=0.5570431540558304, w0=1.742287625334737, w1=-0.7165717286984292\n",
      "Regularized Logistic Regression(647/999): loss=0.5570389044282388, w0=1.7424934552995404, w1=-0.71699742008038\n",
      "Regularized Logistic Regression(648/999): loss=0.5570346751264065, w0=1.742698246761659, w1=-0.7174220028967717\n",
      "Regularized Logistic Regression(649/999): loss=0.5570304660323284, w0=1.7429020053975863, w1=-0.7178454800910293\n",
      "Regularized Logistic Regression(650/999): loss=0.5570262770288482, w0=1.7431047368509918, w1=-0.7182678545983423\n",
      "Regularized Logistic Regression(651/999): loss=0.557022107999649, w0=1.7433064467329318, w1=-0.7186891293456913\n",
      "Regularized Logistic Regression(652/999): loss=0.5570179588292478, w0=1.7435071406220404, w1=-0.7191093072518754\n",
      "Regularized Logistic Regression(653/999): loss=0.5570138294029869, w0=1.7437068240647402, w1=-0.7195283912275336\n",
      "Regularized Logistic Regression(654/999): loss=0.5570097196070273, w0=1.7439055025754329, w1=-0.7199463841751759\n",
      "Regularized Logistic Regression(655/999): loss=0.5570056293283421, w0=1.7441031816367012, w1=-0.7203632889892069\n",
      "Regularized Logistic Regression(656/999): loss=0.5570015584547082, w0=1.7442998666995093, w1=-0.7207791085559535\n",
      "Regularized Logistic Regression(657/999): loss=0.5569975068747006, w0=1.7444955631833912, w1=-0.7211938457536884\n",
      "Regularized Logistic Regression(658/999): loss=0.5569934744776842, w0=1.7446902764766423, w1=-0.7216075034526575\n",
      "Regularized Logistic Regression(659/999): loss=0.5569894611538081, w0=1.744884011936526, w1=-0.7220200845151054\n",
      "Regularized Logistic Regression(660/999): loss=0.5569854667939993, w0=1.7450767748894507, w1=-0.7224315917952993\n",
      "Regularized Logistic Regression(661/999): loss=0.5569814912899532, w0=1.7452685706311604, w1=-0.722842028139557\n",
      "Regularized Logistic Regression(662/999): loss=0.5569775345341311, w0=1.7454594044269365, w1=-0.7232513963862691\n",
      "Regularized Logistic Regression(663/999): loss=0.55697359641975, w0=1.745649281511768, w1=-0.7236596993659268\n",
      "Regularized Logistic Regression(664/999): loss=0.5569696768407775, w0=1.7458382070905505, w1=-0.724066939901146\n",
      "Regularized Logistic Regression(665/999): loss=0.5569657756919266, w0=1.7460261863382607, w1=-0.7244731208066918\n",
      "Regularized Logistic Regression(666/999): loss=0.5569618928686475, w0=1.7462132244001476, w1=-0.7248782448895045\n",
      "Regularized Logistic Regression(667/999): loss=0.5569580282671217, w0=1.746399326391915, w1=-0.7252823149487242\n",
      "Regularized Logistic Regression(668/999): loss=0.556954181784257, w0=1.7465844973998952, w1=-0.7256853337757133\n",
      "Regularized Logistic Regression(669/999): loss=0.5569503533176791, w0=1.7467687424812317, w1=-0.7260873041540835\n",
      "Regularized Logistic Regression(670/999): loss=0.5569465427657277, w0=1.7469520666640648, w1=-0.7264882288597208\n",
      "Regularized Logistic Regression(671/999): loss=0.5569427500274491, w0=1.7471344749476965, w1=-0.7268881106608073\n",
      "Regularized Logistic Regression(672/999): loss=0.556938975002591, w0=1.74731597230278, w1=-0.7272869523178473\n",
      "Regularized Logistic Regression(673/999): loss=0.5569352175915954, w0=1.7474965636714888, w1=-0.7276847565836921\n",
      "Regularized Logistic Regression(674/999): loss=0.5569314776955949, w0=1.747676253967686, w1=-0.7280815262035617\n",
      "Regularized Logistic Regression(675/999): loss=0.5569277552164039, w0=1.7478550480770965, w1=-0.7284772639150708\n",
      "Regularized Logistic Regression(676/999): loss=0.5569240500565157, w0=1.7480329508574939, w1=-0.728871972448252\n",
      "Regularized Logistic Regression(677/999): loss=0.5569203621190943, w0=1.7482099671388545, w1=-0.7292656545255788\n",
      "Regularized Logistic Regression(678/999): loss=0.5569166913079715, w0=1.7483861017235252, w1=-0.7296583128619911\n",
      "Regularized Logistic Regression(679/999): loss=0.5569130375276385, w0=1.7485613593864067, w1=-0.7300499501649186\n",
      "Regularized Logistic Regression(680/999): loss=0.5569094006832422, w0=1.7487357448751122, w1=-0.7304405691343017\n",
      "Regularized Logistic Regression(681/999): loss=0.5569057806805797, w0=1.748909262910128, w1=-0.7308301724626187\n",
      "Regularized Logistic Regression(682/999): loss=0.5569021774260912, w0=1.7490819181849875, w1=-0.7312187628349052\n",
      "Regularized Logistic Regression(683/999): loss=0.5568985908268563, w0=1.7492537153664263, w1=-0.7316063429287825\n",
      "Regularized Logistic Regression(684/999): loss=0.5568950207905887, w0=1.749424659094549, w1=-0.7319929154144745\n",
      "Regularized Logistic Regression(685/999): loss=0.5568914672256295, w0=1.749594753982993, w1=-0.7323784829548354\n",
      "Regularized Logistic Regression(686/999): loss=0.5568879300409436, w0=1.749764004619084, w1=-0.7327630482053716\n",
      "Regularized Logistic Regression(687/999): loss=0.5568844091461136, w0=1.7499324155640008, w1=-0.7331466138142645\n",
      "Regularized Logistic Regression(688/999): loss=0.5568809044513348, w0=1.7500999913529303, w1=-0.7335291824223915\n",
      "Regularized Logistic Regression(689/999): loss=0.5568774158674095, w0=1.7502667364952216, w1=-0.7339107566633509\n",
      "Regularized Logistic Regression(690/999): loss=0.5568739433057442, w0=1.7504326554745466, w1=-0.7342913391634829\n",
      "Regularized Logistic Regression(691/999): loss=0.5568704866783422, w0=1.750597752749055, w1=-0.7346709325418956\n",
      "Regularized Logistic Regression(692/999): loss=0.5568670458977996, w0=1.750762032751525, w1=-0.7350495394104818\n",
      "Regularized Logistic Regression(693/999): loss=0.5568636208773006, w0=1.7509254998895238, w1=-0.7354271623739473\n",
      "Regularized Logistic Regression(694/999): loss=0.556860211530613, w0=1.7510881585455453, w1=-0.7358038040298277\n",
      "Regularized Logistic Regression(695/999): loss=0.5568568177720817, w0=1.7512500130771815, w1=-0.7361794669685139\n",
      "Regularized Logistic Regression(696/999): loss=0.5568534395166265, w0=1.7514110678172548, w1=-0.7365541537732736\n",
      "Regularized Logistic Regression(697/999): loss=0.5568500766797354, w0=1.7515713270739754, w1=-0.7369278670202732\n",
      "Regularized Logistic Regression(698/999): loss=0.5568467291774609, w0=1.7517307951310905, w1=-0.7373006092785995\n",
      "Regularized Logistic Regression(699/999): loss=0.5568433969264146, w0=1.7518894762480222, w1=-0.737672383110282\n",
      "Regularized Logistic Regression(700/999): loss=0.5568400798437646, w0=1.752047374660028, w1=-0.7380431910703135\n",
      "Regularized Logistic Regression(701/999): loss=0.5568367778472278, w0=1.752204494578332, w1=-0.7384130357066733\n",
      "Regularized Logistic Regression(702/999): loss=0.5568334908550691, w0=1.7523608401902797, w1=-0.7387819195603467\n",
      "Regularized Logistic Regression(703/999): loss=0.5568302187860932, w0=1.7525164156594757, w1=-0.7391498451653494\n",
      "Regularized Logistic Regression(704/999): loss=0.5568269615596438, w0=1.752671225125927, w1=-0.7395168150487466\n",
      "Regularized Logistic Regression(705/999): loss=0.5568237190955966, w0=1.7528252727061817, w1=-0.739882831730674\n",
      "Regularized Logistic Regression(706/999): loss=0.5568204913143564, w0=1.7529785624934797, w1=-0.7402478977243621\n",
      "Regularized Logistic Regression(707/999): loss=0.5568172781368521, w0=1.7531310985578836, w1=-0.7406120155361531\n",
      "Regularized Logistic Regression(708/999): loss=0.5568140794845331, w0=1.7532828849464173, w1=-0.7409751876655268\n",
      "Regularized Logistic Regression(709/999): loss=0.5568108952793649, w0=1.7534339256832057, w1=-0.7413374166051167\n",
      "Regularized Logistic Regression(710/999): loss=0.5568077254438244, w0=1.753584224769612, w1=-0.7416987048407334\n",
      "Regularized Logistic Regression(711/999): loss=0.5568045699008969, w0=1.7537337861843705, w1=-0.7420590548513851\n",
      "Regularized Logistic Regression(712/999): loss=0.5568014285740711, w0=1.7538826138837345, w1=-0.7424184691093002\n",
      "Regularized Logistic Regression(713/999): loss=0.5567983013873358, w0=1.7540307118015879, w1=-0.742776950079946\n",
      "Regularized Logistic Regression(714/999): loss=0.5567951882651745, w0=1.7541780838495977, w1=-0.7431345002220469\n",
      "Regularized Logistic Regression(715/999): loss=0.5567920891325636, w0=1.7543247339173413, w1=-0.7434911219876105\n",
      "Regularized Logistic Regression(716/999): loss=0.5567890039149671, w0=1.7544706658724387, w1=-0.7438468178219443\n",
      "Regularized Logistic Regression(717/999): loss=0.556785932538333, w0=1.7546158835606769, w1=-0.744201590163676\n",
      "Regularized Logistic Regression(718/999): loss=0.5567828749290886, w0=1.7547603908061442, w1=-0.7445554414447746\n",
      "Regularized Logistic Regression(719/999): loss=0.556779831014139, w0=1.7549041914113621, w1=-0.7449083740905713\n",
      "Regularized Logistic Regression(720/999): loss=0.5567768007208606, w0=1.755047289157415, w1=-0.7452603905197787\n",
      "Regularized Logistic Regression(721/999): loss=0.5567737839770998, w0=1.7551896878040658, w1=-0.745611493144512\n",
      "Regularized Logistic Regression(722/999): loss=0.5567707807111673, w0=1.755331391089898, w1=-0.7459616843703065\n",
      "Regularized Logistic Regression(723/999): loss=0.5567677908518355, w0=1.7554724027324307, w1=-0.7463109665961407\n",
      "Regularized Logistic Regression(724/999): loss=0.5567648143283347, w0=1.7556127264282502, w1=-0.7466593422144534\n",
      "Regularized Logistic Regression(725/999): loss=0.55676185107035, w0=1.7557523658531267, w1=-0.7470068136111646\n",
      "Regularized Logistic Regression(726/999): loss=0.5567589010080171, w0=1.7558913246621481, w1=-0.7473533831656961\n",
      "Regularized Logistic Regression(727/999): loss=0.556755964071918, w0=1.7560296064898318, w1=-0.7476990532509895\n",
      "Regularized Logistic Regression(728/999): loss=0.5567530401930791, w0=1.7561672149502505, w1=-0.7480438262335257\n",
      "Regularized Logistic Regression(729/999): loss=0.556750129302967, w0=1.7563041536371604, w1=-0.7483877044733462\n",
      "Regularized Logistic Regression(730/999): loss=0.5567472313334857, w0=1.7564404261241147, w1=-0.7487306903240696\n",
      "Regularized Logistic Regression(731/999): loss=0.5567443462169712, w0=1.7565760359645777, w1=-0.7490727861329145\n",
      "Regularized Logistic Regression(732/999): loss=0.5567414738861911, w0=1.7567109866920585, w1=-0.7494139942407166\n",
      "Regularized Logistic Regression(733/999): loss=0.556738614274338, w0=1.7568452818202103, w1=-0.749754316981947\n",
      "Regularized Logistic Regression(734/999): loss=0.5567357673150302, w0=1.7569789248429668, w1=-0.7500937566847327\n",
      "Regularized Logistic Regression(735/999): loss=0.556732932942304, w0=1.7571119192346467, w1=-0.7504323156708751\n",
      "Regularized Logistic Regression(736/999): loss=0.5567301110906138, w0=1.757244268450074, w1=-0.7507699962558706\n",
      "Regularized Logistic Regression(737/999): loss=0.5567273016948272, w0=1.7573759759246872, w1=-0.7511068007489266\n",
      "Regularized Logistic Regression(738/999): loss=0.5567245046902225, w0=1.7575070450746573, w1=-0.7514427314529811\n",
      "Regularized Logistic Regression(739/999): loss=0.5567217200124851, w0=1.7576374792970073, w1=-0.7517777906647248\n",
      "Regularized Logistic Regression(740/999): loss=0.5567189475977049, w0=1.7577672819697154, w1=-0.7521119806746152\n",
      "Regularized Logistic Regression(741/999): loss=0.5567161873823729, w0=1.757896456451837, w1=-0.7524453037668958\n",
      "Regularized Logistic Regression(742/999): loss=0.556713439303378, w0=1.7580250060836025, w1=-0.7527777622196173\n",
      "Regularized Logistic Regression(743/999): loss=0.5567107032980042, w0=1.758152934186541, w1=-0.7531093583046565\n",
      "Regularized Logistic Regression(744/999): loss=0.5567079793039272, w0=1.758280244063589, w1=-0.75344009428773\n",
      "Regularized Logistic Regression(745/999): loss=0.5567052672592121, w0=1.7584069389991874, w1=-0.7537699724284157\n",
      "Regularized Logistic Regression(746/999): loss=0.5567025671023098, w0=1.7585330222594076, w1=-0.7540989949801713\n",
      "Regularized Logistic Regression(747/999): loss=0.5566998787720545, w0=1.7586584970920436, w1=-0.7544271641903515\n",
      "Regularized Logistic Regression(748/999): loss=0.5566972022076605, w0=1.7587833667267305, w1=-0.754754482300228\n",
      "Regularized Logistic Regression(749/999): loss=0.5566945373487191, w0=1.7589076343750496, w1=-0.7550809515450039\n",
      "Regularized Logistic Regression(750/999): loss=0.5566918841351965, w0=1.7590313032306237, w1=-0.7554065741538348\n",
      "Regularized Logistic Regression(751/999): loss=0.5566892425074308, w0=1.7591543764692366, w1=-0.7557313523498463\n",
      "Regularized Logistic Regression(752/999): loss=0.5566866124061278, w0=1.7592768572489312, w1=-0.7560552883501518\n",
      "Regularized Logistic Regression(753/999): loss=0.5566839937723607, w0=1.7593987487101155, w1=-0.7563783843658667\n",
      "Regularized Logistic Regression(754/999): loss=0.5566813865475646, w0=1.75952005397566, w1=-0.7567006426021332\n",
      "Regularized Logistic Regression(755/999): loss=0.5566787906735365, w0=1.7596407761510036, w1=-0.7570220652581319\n",
      "Regularized Logistic Regression(756/999): loss=0.5566762060924307, w0=1.7597609183242615, w1=-0.757342654527102\n",
      "Regularized Logistic Regression(757/999): loss=0.5566736327467564, w0=1.759880483566323, w1=-0.7576624125963588\n",
      "Regularized Logistic Regression(758/999): loss=0.5566710705793756, w0=1.759999474930942, w1=-0.757981341647308\n",
      "Regularized Logistic Regression(759/999): loss=0.5566685195335002, w0=1.760117895454851, w1=-0.7582994438554703\n",
      "Regularized Logistic Regression(760/999): loss=0.5566659795526897, w0=1.7602357481578585, w1=-0.7586167213904914\n",
      "Regularized Logistic Regression(761/999): loss=0.5566634505808477, w0=1.7603530360429407, w1=-0.7589331764161626\n",
      "Regularized Logistic Regression(762/999): loss=0.5566609325622198, w0=1.7604697620963483, w1=-0.7592488110904383\n",
      "Regularized Logistic Regression(763/999): loss=0.5566584254413915, w0=1.7605859292876942, w1=-0.7595636275654504\n",
      "Regularized Logistic Regression(764/999): loss=0.5566559291632869, w0=1.7607015405700566, w1=-0.7598776279875304\n",
      "Regularized Logistic Regression(765/999): loss=0.5566534436731617, w0=1.76081659888008, w1=-0.7601908144972216\n",
      "Regularized Logistic Regression(766/999): loss=0.5566509689166055, w0=1.7609311071380642, w1=-0.7605031892292978\n",
      "Regularized Logistic Regression(767/999): loss=0.5566485048395378, w0=1.7610450682480578, w1=-0.7608147543127817\n",
      "Regularized Logistic Regression(768/999): loss=0.5566460513882044, w0=1.7611584850979574, w1=-0.7611255118709593\n",
      "Regularized Logistic Regression(769/999): loss=0.5566436085091768, w0=1.7612713605596004, w1=-0.7614354640213982\n",
      "Regularized Logistic Regression(770/999): loss=0.5566411761493477, w0=1.7613836974888633, w1=-0.7617446128759625\n",
      "Regularized Logistic Regression(771/999): loss=0.556638754255931, w0=1.7614954987257423, w1=-0.7620529605408336\n",
      "Regularized Logistic Regression(772/999): loss=0.5566363427764577, w0=1.7616067670944586, w1=-0.7623605091165218\n",
      "Regularized Logistic Regression(773/999): loss=0.556633941658775, w0=1.7617175054035388, w1=-0.7626672606978864\n",
      "Regularized Logistic Regression(774/999): loss=0.5566315508510418, w0=1.7618277164459173, w1=-0.7629732173741509\n",
      "Regularized Logistic Regression(775/999): loss=0.5566291703017292, w0=1.7619374029990174, w1=-0.763278381228919\n",
      "Regularized Logistic Regression(776/999): loss=0.556626799959616, w0=1.7620465678248516, w1=-0.763582754340191\n",
      "Regularized Logistic Regression(777/999): loss=0.556624439773787, w0=1.7621552136700942, w1=-0.7638863387803815\n",
      "Regularized Logistic Regression(778/999): loss=0.5566220896936324, w0=1.7622633432661905, w1=-0.7641891366163337\n",
      "Regularized Logistic Regression(779/999): loss=0.5566197496688429, w0=1.7623709593294319, w1=-0.7644911499093381\n",
      "Regularized Logistic Regression(780/999): loss=0.556617419649409, w0=1.7624780645610458, w1=-0.7647923807151448\n",
      "Regularized Logistic Regression(781/999): loss=0.5566150995856199, w0=1.7625846616472909, w1=-0.7650928310839852\n",
      "Regularized Logistic Regression(782/999): loss=0.5566127894280588, w0=1.7626907532595277, w1=-0.7653925030605825\n",
      "Regularized Logistic Regression(783/999): loss=0.5566104891276028, w0=1.7627963420543171, w1=-0.7656913986841709\n",
      "Regularized Logistic Regression(784/999): loss=0.55660819863542, w0=1.7629014306735085, w1=-0.7659895199885092\n",
      "Regularized Logistic Regression(785/999): loss=0.5566059179029671, w0=1.7630060217443155, w1=-0.7662868690018989\n",
      "Regularized Logistic Regression(786/999): loss=0.5566036468819882, w0=1.7631101178794073, w1=-0.7665834477472001\n",
      "Regularized Logistic Regression(787/999): loss=0.5566013855245123, w0=1.7632137216769868, w1=-0.7668792582418452\n",
      "Regularized Logistic Regression(788/999): loss=0.5565991337828508, w0=1.7633168357208802, w1=-0.7671743024978562\n",
      "Regularized Logistic Regression(789/999): loss=0.5565968916095966, w0=1.76341946258062, w1=-0.7674685825218582\n",
      "Regularized Logistic Regression(790/999): loss=0.5565946589576208, w0=1.7635216048115196, w1=-0.7677621003150987\n",
      "Regularized Logistic Regression(791/999): loss=0.5565924357800711, w0=1.763623264954765, w1=-0.7680548578734606\n",
      "Regularized Logistic Regression(792/999): loss=0.556590222030371, w0=1.7637244455374927, w1=-0.7683468571874779\n",
      "Regularized Logistic Regression(793/999): loss=0.5565880176622159, w0=1.7638251490728631, w1=-0.7686381002423497\n",
      "Regularized Logistic Regression(794/999): loss=0.5565858226295728, w0=1.763925378060161, w1=-0.7689285890179604\n",
      "Regularized Logistic Regression(795/999): loss=0.5565836368866779, w0=1.7640251349848513, w1=-0.7692183254888891\n",
      "Regularized Logistic Regression(796/999): loss=0.5565814603880331, w0=1.7641244223186787, w1=-0.7695073116244283\n",
      "Regularized Logistic Regression(797/999): loss=0.5565792930884074, w0=1.7642232425197317, w1=-0.7697955493885991\n",
      "Regularized Logistic Regression(798/999): loss=0.5565771349428321, w0=1.7643215980325366, w1=-0.7700830407401664\n",
      "Regularized Logistic Regression(799/999): loss=0.5565749859065998, w0=1.7644194912881148, w1=-0.77036978763265\n",
      "Regularized Logistic Regression(800/999): loss=0.5565728459352637, w0=1.7645169247040837, w1=-0.7706557920143466\n",
      "Regularized Logistic Regression(801/999): loss=0.5565707149846338, w0=1.7646139006847164, w1=-0.7709410558283388\n",
      "Regularized Logistic Regression(802/999): loss=0.5565685930107774, w0=1.764710421621027, w1=-0.7712255810125116\n",
      "Regularized Logistic Regression(803/999): loss=0.5565664799700144, w0=1.764806489890843, w1=-0.7715093694995715\n",
      "Regularized Logistic Regression(804/999): loss=0.5565643758189183, w0=1.7649021078588847, w1=-0.771792423217055\n",
      "Regularized Logistic Regression(805/999): loss=0.556562280514313, w0=1.7649972778768395, w1=-0.7720747440873464\n",
      "Regularized Logistic Regression(806/999): loss=0.5565601940132713, w0=1.7650920022834333, w1=-0.7723563340276948\n",
      "Regularized Logistic Regression(807/999): loss=0.5565581162731127, w0=1.7651862834045096, w1=-0.7726371949502235\n",
      "Regularized Logistic Regression(808/999): loss=0.5565560472514034, w0=1.7652801235530995, w1=-0.7729173287619496\n",
      "Regularized Logistic Regression(809/999): loss=0.5565539869059523, w0=1.7653735250294973, w1=-0.7731967373647941\n",
      "Regularized Logistic Regression(810/999): loss=0.5565519351948103, w0=1.765466490121333, w1=-0.7734754226556017\n",
      "Regularized Logistic Regression(811/999): loss=0.5565498920762698, w0=1.7655590211036472, w1=-0.7737533865261507\n",
      "Regularized Logistic Regression(812/999): loss=0.5565478575088604, w0=1.765651120238959, w1=-0.7740306308631708\n",
      "Regularized Logistic Regression(813/999): loss=0.55654583145135, w0=1.7657427897773417, w1=-0.7743071575483547\n",
      "Regularized Logistic Regression(814/999): loss=0.5565438138627414, w0=1.7658340319564911, w1=-0.7745829684583729\n",
      "Regularized Logistic Regression(815/999): loss=0.5565418047022712, w0=1.7659248490017978, w1=-0.7748580654648909\n",
      "Regularized Logistic Regression(816/999): loss=0.5565398039294085, w0=1.7660152431264229, w1=-0.7751324504345793\n",
      "Regularized Logistic Regression(817/999): loss=0.5565378115038523, w0=1.7661052165313558, w1=-0.7754061252291315\n",
      "Regularized Logistic Regression(818/999): loss=0.556535827385532, w0=1.766194771405489, w1=-0.7756790917052757\n",
      "Regularized Logistic Regression(819/999): loss=0.5565338515346027, w0=1.766283909925699, w1=-0.7759513517147897\n",
      "Regularized Logistic Regression(820/999): loss=0.5565318839114469, w0=1.7663726342568966, w1=-0.776222907104515\n",
      "Regularized Logistic Regression(821/999): loss=0.5565299244766702, w0=1.7664609465521068, w1=-0.776493759716372\n",
      "Regularized Logistic Regression(822/999): loss=0.5565279731911024, w0=1.7665488489525298, w1=-0.7767639113873696\n",
      "Regularized Logistic Regression(823/999): loss=0.556526030015793, w0=1.7666363435876193, w1=-0.7770333639496252\n",
      "Regularized Logistic Regression(824/999): loss=0.5565240949120125, w0=1.7667234325751406, w1=-0.777302119230374\n",
      "Regularized Logistic Regression(825/999): loss=0.5565221678412495, w0=1.7668101180212386, w1=-0.7775701790519837\n",
      "Regularized Logistic Regression(826/999): loss=0.5565202487652089, w0=1.7668964020205005, w1=-0.7778375452319714\n",
      "Regularized Logistic Regression(827/999): loss=0.5565183376458113, w0=1.7669822866560343, w1=-0.7781042195830129\n",
      "Regularized Logistic Regression(828/999): loss=0.5565164344451908, w0=1.7670677739995195, w1=-0.7783702039129591\n",
      "Regularized Logistic Regression(829/999): loss=0.5565145391256952, w0=1.7671528661112847, w1=-0.7786355000248492\n",
      "Regularized Logistic Regression(830/999): loss=0.5565126516498807, w0=1.767237565040362, w1=-0.7789001097169238\n",
      "Regularized Logistic Regression(831/999): loss=0.5565107719805165, w0=1.7673218728245614, w1=-0.7791640347826387\n",
      "Regularized Logistic Regression(832/999): loss=0.5565089000805769, w0=1.7674057914905232, w1=-0.7794272770106784\n",
      "Regularized Logistic Regression(833/999): loss=0.5565070359132447, w0=1.7674893230537907, w1=-0.7796898381849683\n",
      "Regularized Logistic Regression(834/999): loss=0.5565051794419077, w0=1.7675724695188708, w1=-0.7799517200846913\n",
      "Regularized Logistic Regression(835/999): loss=0.5565033306301572, w0=1.7676552328792963, w1=-0.780212924484299\n",
      "Regularized Logistic Regression(836/999): loss=0.556501489441788, w0=1.7677376151176851, w1=-0.7804734531535242\n",
      "Regularized Logistic Regression(837/999): loss=0.5564996558407952, w0=1.7678196182058143, w1=-0.7807333078573951\n",
      "Regularized Logistic Regression(838/999): loss=0.5564978297913747, w0=1.7679012441046622, w1=-0.7809924903562487\n",
      "Regularized Logistic Regression(839/999): loss=0.5564960112579203, w0=1.767982494764494, w1=-0.7812510024057446\n",
      "Regularized Logistic Regression(840/999): loss=0.5564942002050235, w0=1.7680633721248944, w1=-0.7815088457568782\n",
      "Regularized Logistic Regression(841/999): loss=0.5564923965974717, w0=1.7681438781148549, w1=-0.7817660221559914\n",
      "Regularized Logistic Regression(842/999): loss=0.5564906004002462, w0=1.768224014652823, w1=-0.7820225333447877\n",
      "Regularized Logistic Regression(843/999): loss=0.5564888115785235, w0=1.7683037836467517, w1=-0.7822783810603449\n",
      "Regularized Logistic Regression(844/999): loss=0.5564870300976701, w0=1.7683831869941782, w1=-0.7825335670351302\n",
      "Regularized Logistic Regression(845/999): loss=0.5564852559232442, w0=1.7684622265822705, w1=-0.7827880929970079\n",
      "Regularized Logistic Regression(846/999): loss=0.5564834890209941, w0=1.7685409042878901, w1=-0.7830419606692567\n",
      "Regularized Logistic Regression(847/999): loss=0.5564817293568555, w0=1.7686192219776475, w1=-0.7832951717705812\n",
      "Regularized Logistic Regression(848/999): loss=0.5564799768969513, w0=1.7686971815079675, w1=-0.7835477280151236\n",
      "Regularized Logistic Regression(849/999): loss=0.5564782316075902, w0=1.7687747847251356, w1=-0.7837996311124793\n",
      "Regularized Logistic Regression(850/999): loss=0.5564764934552667, w0=1.7688520334653712, w1=-0.7840508827677078\n",
      "Regularized Logistic Regression(851/999): loss=0.5564747624066565, w0=1.7689289295548705, w1=-0.784301484681345\n",
      "Regularized Logistic Regression(852/999): loss=0.5564730384286195, w0=1.7690054748098674, w1=-0.784551438549416\n",
      "Regularized Logistic Regression(853/999): loss=0.5564713214881954, w0=1.7690816710366912, w1=-0.7848007460634484\n",
      "Regularized Logistic Regression(854/999): loss=0.5564696115526038, w0=1.7691575200318246, w1=-0.7850494089104831\n",
      "Regularized Logistic Regression(855/999): loss=0.5564679085892436, w0=1.7692330235819582, w1=-0.7852974287730913\n",
      "Regularized Logistic Regression(856/999): loss=0.5564662125656906, w0=1.7693081834640383, w1=-0.7855448073293806\n",
      "Regularized Logistic Regression(857/999): loss=0.5564645234496967, w0=1.7693830014453353, w1=-0.7857915462530133\n",
      "Regularized Logistic Regression(858/999): loss=0.5564628412091902, w0=1.7694574792834936, w1=-0.786037647213214\n",
      "Regularized Logistic Regression(859/999): loss=0.5564611658122716, w0=1.7695316187265806, w1=-0.7862831118747858\n",
      "Regularized Logistic Regression(860/999): loss=0.5564594972272158, w0=1.769605421513143, w1=-0.7865279418981218\n",
      "Regularized Logistic Regression(861/999): loss=0.5564578354224684, w0=1.769678889372263, w1=-0.7867721389392135\n",
      "Regularized Logistic Regression(862/999): loss=0.5564561803666466, w0=1.7697520240236149, w1=-0.7870157046496675\n",
      "Regularized Logistic Regression(863/999): loss=0.5564545320285361, w0=1.76982482717751, w1=-0.787258640676717\n",
      "Regularized Logistic Regression(864/999): loss=0.5564528903770926, w0=1.7698973005349548, w1=-0.7875009486632336\n",
      "Regularized Logistic Regression(865/999): loss=0.5564512553814376, w0=1.7699694457877058, w1=-0.7877426302477375\n",
      "Regularized Logistic Regression(866/999): loss=0.5564496270108598, w0=1.7700412646183157, w1=-0.7879836870644128\n",
      "Regularized Logistic Regression(867/999): loss=0.5564480052348124, w0=1.7701127587001877, w1=-0.7882241207431163\n",
      "Regularized Logistic Regression(868/999): loss=0.5564463900229137, w0=1.7701839296976265, w1=-0.7884639329093918\n",
      "Regularized Logistic Regression(869/999): loss=0.5564447813449448, w0=1.770254779265895, w1=-0.7887031251844818\n",
      "Regularized Logistic Regression(870/999): loss=0.5564431791708483, w0=1.770325309051257, w1=-0.7889416991853386\n",
      "Regularized Logistic Regression(871/999): loss=0.5564415834707285, w0=1.770395520691032, w1=-0.789179656524637\n",
      "Regularized Logistic Regression(872/999): loss=0.5564399942148496, w0=1.7704654158136517, w1=-0.7894169988107842\n",
      "Regularized Logistic Regression(873/999): loss=0.5564384113736344, w0=1.770534996038698, w1=-0.7896537276479356\n",
      "Regularized Logistic Regression(874/999): loss=0.556436834917664, w0=1.7706042629769574, w1=-0.789889844636002\n",
      "Regularized Logistic Regression(875/999): loss=0.5564352648176762, w0=1.770673218230477, w1=-0.7901253513706645\n",
      "Regularized Logistic Regression(876/999): loss=0.5564337010445655, w0=1.7707418633926073, w1=-0.7903602494433849\n",
      "Regularized Logistic Regression(877/999): loss=0.5564321435693802, w0=1.7708102000480515, w1=-0.7905945404414173\n",
      "Regularized Logistic Regression(878/999): loss=0.5564305923633236, w0=1.770878229772918, w1=-0.7908282259478202\n",
      "Regularized Logistic Regression(879/999): loss=0.5564290473977511, w0=1.770945954134758, w1=-0.7910613075414689\n",
      "Regularized Logistic Regression(880/999): loss=0.5564275086441721, w0=1.7710133746926313, w1=-0.7912937867970647\n",
      "Regularized Logistic Regression(881/999): loss=0.5564259760742442, w0=1.7710804929971424, w1=-0.7915256652851493\n",
      "Regularized Logistic Regression(882/999): loss=0.5564244496597772, w0=1.7711473105904842, w1=-0.791756944572113\n",
      "Regularized Logistic Regression(883/999): loss=0.5564229293727294, w0=1.771213829006497, w1=-0.7919876262202097\n",
      "Regularized Logistic Regression(884/999): loss=0.5564214151852078, w0=1.771280049770708, w1=-0.7922177117875648\n",
      "Regularized Logistic Regression(885/999): loss=0.5564199070694659, w0=1.7713459744003763, w1=-0.7924472028281895\n",
      "Regularized Logistic Regression(886/999): loss=0.5564184049979045, w0=1.77141160440455, w1=-0.7926761008919906\n",
      "Regularized Logistic Regression(887/999): loss=0.5564169089430696, w0=1.771476941284101, w1=-0.7929044075247815\n",
      "Regularized Logistic Regression(888/999): loss=0.556415418877651, w0=1.7715419865317736, w1=-0.7931321242682956\n",
      "Regularized Logistic Regression(889/999): loss=0.5564139347744833, w0=1.7716067416322314, w1=-0.7933592526601949\n",
      "Regularized Logistic Regression(890/999): loss=0.5564124566065435, w0=1.7716712080621035, w1=-0.7935857942340828\n",
      "Regularized Logistic Regression(891/999): loss=0.5564109843469501, w0=1.7717353872900337, w1=-0.7938117505195148\n",
      "Regularized Logistic Regression(892/999): loss=0.5564095179689631, w0=1.771799280776712, w1=-0.7940371230420092\n",
      "Regularized Logistic Regression(893/999): loss=0.5564080574459822, w0=1.7718628899749331, w1=-0.7942619133230596\n",
      "Regularized Logistic Regression(894/999): loss=0.5564066027515465, w0=1.771926216329637, w1=-0.7944861228801435\n",
      "Regularized Logistic Regression(895/999): loss=0.5564051538593332, w0=1.7719892612779422, w1=-0.7947097532267354\n",
      "Regularized Logistic Regression(896/999): loss=0.5564037107431581, w0=1.77205202624921, w1=-0.7949328058723188\n",
      "Regularized Logistic Regression(897/999): loss=0.5564022733769725, w0=1.7721145126650697, w1=-0.795155282322392\n",
      "Regularized Logistic Regression(898/999): loss=0.5564008417348634, w0=1.77217672193947, w1=-0.7953771840784863\n",
      "Regularized Logistic Regression(899/999): loss=0.5563994157910541, w0=1.7722386554787217, w1=-0.7955985126381705\n",
      "Regularized Logistic Regression(900/999): loss=0.5563979955199014, w0=1.7723003146815364, w1=-0.7958192694950652\n",
      "Regularized Logistic Regression(901/999): loss=0.5563965808958945, w0=1.7723617009390766, w1=-0.7960394561388543\n",
      "Regularized Logistic Regression(902/999): loss=0.5563951718936567, w0=1.772422815634996, w1=-0.7962590740552918\n",
      "Regularized Logistic Regression(903/999): loss=0.5563937684879419, w0=1.7724836601454714, w1=-0.7964781247262157\n",
      "Regularized Logistic Regression(904/999): loss=0.556392370653635, w0=1.7725442358392585, w1=-0.7966966096295599\n",
      "Regularized Logistic Regression(905/999): loss=0.5563909783657518, w0=1.7726045440777252, w1=-0.7969145302393604\n",
      "Regularized Logistic Regression(906/999): loss=0.5563895915994364, w0=1.7726645862148944, w1=-0.79713188802577\n",
      "Regularized Logistic Regression(907/999): loss=0.5563882103299614, w0=1.772724363597484, w1=-0.7973486844550683\n",
      "Regularized Logistic Regression(908/999): loss=0.5563868345327282, w0=1.7727838775649511, w1=-0.7975649209896706\n",
      "Regularized Logistic Regression(909/999): loss=0.5563854641832644, w0=1.7728431294495344, w1=-0.7977805990881376\n",
      "Regularized Logistic Regression(910/999): loss=0.5563840992572235, w0=1.772902120576285, w1=-0.7979957202051902\n",
      "Regularized Logistic Regression(911/999): loss=0.5563827397303847, w0=1.7729608522631182, w1=-0.7982102857917177\n",
      "Regularized Logistic Regression(912/999): loss=0.556381385578652, w0=1.7730193258208446, w1=-0.7984242972947858\n",
      "Regularized Logistic Regression(913/999): loss=0.5563800367780531, w0=1.773077542553212, w1=-0.7986377561576509\n",
      "Regularized Logistic Regression(914/999): loss=0.5563786933047387, w0=1.773135503756947, w1=-0.7988506638197684\n",
      "Regularized Logistic Regression(915/999): loss=0.5563773551349824, w0=1.7731932107217936, w1=-0.7990630217168033\n",
      "Regularized Logistic Regression(916/999): loss=0.5563760222451786, w0=1.7732506647305544, w1=-0.79927483128064\n",
      "Regularized Logistic Regression(917/999): loss=0.5563746946118432, w0=1.7733078670591216, w1=-0.7994860939393951\n",
      "Regularized Logistic Regression(918/999): loss=0.5563733722116123, w0=1.7733648189765192, w1=-0.7996968111174245\n",
      "Regularized Logistic Regression(919/999): loss=0.5563720550212409, w0=1.7734215217449485, w1=-0.7999069842353356\n",
      "Regularized Logistic Regression(920/999): loss=0.5563707430176035, w0=1.7734779766198157, w1=-0.8001166147099982\n",
      "Regularized Logistic Regression(921/999): loss=0.5563694361776923, w0=1.7735341848497752, w1=-0.8003257039545498\n",
      "Regularized Logistic Regression(922/999): loss=0.5563681344786167, w0=1.7735901476767675, w1=-0.8005342533784126\n",
      "Regularized Logistic Regression(923/999): loss=0.5563668378976026, w0=1.773645866336058, w1=-0.8007422643872991\n",
      "Regularized Logistic Regression(924/999): loss=0.5563655464119918, w0=1.773701342056265, w1=-0.8009497383832239\n",
      "Regularized Logistic Regression(925/999): loss=0.5563642599992417, w0=1.7737565760594087, w1=-0.8011566767645139\n",
      "Regularized Logistic Regression(926/999): loss=0.5563629786369242, w0=1.7738115695609404, w1=-0.801363080925815\n",
      "Regularized Logistic Regression(927/999): loss=0.5563617023027245, w0=1.7738663237697845, w1=-0.801568952258108\n",
      "Regularized Logistic Regression(928/999): loss=0.5563604309744413, w0=1.7739208398883684, w1=-0.8017742921487135\n",
      "Regularized Logistic Regression(929/999): loss=0.5563591646299861, w0=1.7739751191126618, w1=-0.8019791019813046\n",
      "Regularized Logistic Regression(930/999): loss=0.5563579032473817, w0=1.7740291626322153, w1=-0.802183383135915\n",
      "Regularized Logistic Regression(931/999): loss=0.5563566468047626, w0=1.7740829716301882, w1=-0.8023871369889495\n",
      "Regularized Logistic Regression(932/999): loss=0.5563553952803729, w0=1.7741365472833968, w1=-0.8025903649131957\n",
      "Regularized Logistic Regression(933/999): loss=0.5563541486525676, w0=1.774189890762343, w1=-0.8027930682778307\n",
      "Regularized Logistic Regression(934/999): loss=0.5563529068998095, w0=1.7742430032312382, w1=-0.8029952484484344\n",
      "Regularized Logistic Regression(935/999): loss=0.5563516700006721, w0=1.7742958858480617, w1=-0.8031969067869946\n",
      "Regularized Logistic Regression(936/999): loss=0.556350437933835, w0=1.7743485397645733, w1=-0.8033980446519211\n",
      "Regularized Logistic Regression(937/999): loss=0.556349210678085, w0=1.7744009661263642, w1=-0.8035986633980525\n",
      "Regularized Logistic Regression(938/999): loss=0.556347988212317, w0=1.7744531660728775, w1=-0.8037987643766672\n",
      "Regularized Logistic Regression(939/999): loss=0.5563467705155302, w0=1.7745051407374508, w1=-0.803998348935495\n",
      "Regularized Logistic Regression(940/999): loss=0.5563455575668311, w0=1.7745568912473506, w1=-0.804197418418721\n",
      "Regularized Logistic Regression(941/999): loss=0.5563443493454292, w0=1.7746084187238071, w1=-0.8043959741670024\n",
      "Regularized Logistic Regression(942/999): loss=0.5563431458306385, w0=1.774659724282038, w1=-0.8045940175174712\n",
      "Regularized Logistic Regression(943/999): loss=0.5563419470018777, w0=1.7747108090312882, w1=-0.8047915498037488\n",
      "Regularized Logistic Regression(944/999): loss=0.5563407528386667, w0=1.7747616740748715, w1=-0.8049885723559517\n",
      "Regularized Logistic Regression(945/999): loss=0.5563395633206295, w0=1.7748123205101831, w1=-0.8051850865007057\n",
      "Regularized Logistic Regression(946/999): loss=0.5563383784274901, w0=1.7748627494287548, w1=-0.80538109356115\n",
      "Regularized Logistic Regression(947/999): loss=0.556337198139075, w0=1.7749129619162747, w1=-0.805576594856951\n",
      "Regularized Logistic Regression(948/999): loss=0.5563360224353108, w0=1.7749629590526175, w1=-0.8057715917043078\n",
      "Regularized Logistic Regression(949/999): loss=0.5563348512962237, w0=1.7750127419118826, w1=-0.805966085415966\n",
      "Regularized Logistic Regression(950/999): loss=0.5563336847019398, w0=1.7750623115624329, w1=-0.8061600773012219\n",
      "Regularized Logistic Regression(951/999): loss=0.5563325226326838, w0=1.7751116690669073, w1=-0.8063535686659365\n",
      "Regularized Logistic Regression(952/999): loss=0.556331365068778, w0=1.775160815482272, w1=-0.8065465608125408\n",
      "Regularized Logistic Regression(953/999): loss=0.5563302119906438, w0=1.775209751859837, w1=-0.8067390550400492\n",
      "Regularized Logistic Regression(954/999): loss=0.5563290633787986, w0=1.7752584792453023, w1=-0.8069310526440647\n",
      "Regularized Logistic Regression(955/999): loss=0.5563279192138564, w0=1.775306998678775, w1=-0.8071225549167906\n",
      "Regularized Logistic Regression(956/999): loss=0.5563267794765275, w0=1.775355311194807, w1=-0.8073135631470385\n",
      "Regularized Logistic Regression(957/999): loss=0.5563256441476175, w0=1.775403417822424, w1=-0.8075040786202383\n",
      "Regularized Logistic Regression(958/999): loss=0.5563245132080271, w0=1.7754513195851584, w1=-0.8076941026184457\n",
      "Regularized Logistic Regression(959/999): loss=0.5563233866387508, w0=1.7754990175010807, w1=-0.8078836364203553\n",
      "Regularized Logistic Regression(960/999): loss=0.5563222644208771, w0=1.775546512582822, w1=-0.8080726813013028\n",
      "Regularized Logistic Regression(961/999): loss=0.5563211465355881, w0=1.7755938058376168, w1=-0.808261238533282\n",
      "Regularized Logistic Regression(962/999): loss=0.5563200329641581, w0=1.7756408982673144, w1=-0.8084493093849467\n",
      "Regularized Logistic Regression(963/999): loss=0.556318923687954, w0=1.7756877908684303, w1=-0.8086368951216253\n",
      "Regularized Logistic Regression(964/999): loss=0.5563178186884342, w0=1.7757344846321617, w1=-0.8088239970053248\n",
      "Regularized Logistic Regression(965/999): loss=0.5563167179471475, w0=1.7757809805444198, w1=-0.8090106162947432\n",
      "Regularized Logistic Regression(966/999): loss=0.5563156214457351, w0=1.7758272795858596, w1=-0.8091967542452783\n",
      "Regularized Logistic Regression(967/999): loss=0.5563145291659263, w0=1.775873382731909, w1=-0.8093824121090332\n",
      "Regularized Logistic Regression(968/999): loss=0.5563134410895412, w0=1.7759192909527948, w1=-0.8095675911348302\n",
      "Regularized Logistic Regression(969/999): loss=0.5563123571984886, w0=1.775965005213581, w1=-0.8097522925682157\n",
      "Regularized Logistic Regression(970/999): loss=0.5563112774747659, w0=1.7760105264741868, w1=-0.8099365176514706\n",
      "Regularized Logistic Regression(971/999): loss=0.5563102019004587, w0=1.7760558556894162, w1=-0.8101202676236177\n",
      "Regularized Logistic Regression(972/999): loss=0.5563091304577391, w0=1.776100993808998, w1=-0.8103035437204325\n",
      "Regularized Logistic Regression(973/999): loss=0.556308063128868, w0=1.776145941777591, w1=-0.8104863471744504\n",
      "Regularized Logistic Regression(974/999): loss=0.5563069998961916, w0=1.776190700534839, w1=-0.8106686792149768\n",
      "Regularized Logistic Regression(975/999): loss=0.5563059407421428, w0=1.7762352710153788, w1=-0.8108505410680938\n",
      "Regularized Logistic Regression(976/999): loss=0.5563048856492397, w0=1.7762796541488746, w1=-0.8110319339566705\n",
      "Regularized Logistic Regression(977/999): loss=0.5563038346000861, w0=1.7763238508600478, w1=-0.8112128591003704\n",
      "Regularized Logistic Regression(978/999): loss=0.5563027875773693, w0=1.776367862068698, w1=-0.8113933177156613\n",
      "Regularized Logistic Regression(979/999): loss=0.5563017445638613, w0=1.776411688689735, w1=-0.8115733110158209\n",
      "Regularized Logistic Regression(980/999): loss=0.5563007055424187, w0=1.776455331633205, w1=-0.8117528402109504\n",
      "Regularized Logistic Regression(981/999): loss=0.5562996704959797, w0=1.7764987918043222, w1=-0.8119319065079796\n",
      "Regularized Logistic Regression(982/999): loss=0.5562986394075666, w0=1.7765420701034782, w1=-0.8121105111106751\n",
      "Regularized Logistic Regression(983/999): loss=0.5562976122602831, w0=1.7765851674262925, w1=-0.8122886552196489\n",
      "Regularized Logistic Regression(984/999): loss=0.5562965890373154, w0=1.7766280846636209, w1=-0.8124663400323703\n",
      "Regularized Logistic Regression(985/999): loss=0.5562955697219294, w0=1.7766708227015882, w1=-0.8126435667431688\n",
      "Regularized Logistic Regression(986/999): loss=0.5562945542974747, w0=1.776713382421618, w1=-0.8128203365432469\n",
      "Regularized Logistic Regression(987/999): loss=0.556293542747379, w0=1.7767557647004468, w1=-0.8129966506206847\n",
      "Regularized Logistic Regression(988/999): loss=0.5562925350551507, w0=1.7767979704101666, w1=-0.8131725101604532\n",
      "Regularized Logistic Regression(989/999): loss=0.5562915312043784, w0=1.7768400004182354, w1=-0.8133479163444189\n",
      "Regularized Logistic Regression(990/999): loss=0.5562905311787282, w0=1.7768818555875074, w1=-0.8135228703513528\n",
      "Regularized Logistic Regression(991/999): loss=0.556289534961947, w0=1.776923536776266, w1=-0.813697373356937\n",
      "Regularized Logistic Regression(992/999): loss=0.5562885425378576, w0=1.7769650448382386, w1=-0.8138714265337774\n",
      "Regularized Logistic Regression(993/999): loss=0.5562875538903631, w0=1.7770063806226255, w1=-0.8140450310514096\n",
      "Regularized Logistic Regression(994/999): loss=0.556286569003442, w0=1.777047544974125, w1=-0.8142181880763041\n",
      "Regularized Logistic Regression(995/999): loss=0.5562855878611501, w0=1.7770885387329571, w1=-0.814390898771879\n",
      "Regularized Logistic Regression(996/999): loss=0.5562846104476206, w0=1.7771293627348932, w1=-0.8145631642985073\n",
      "Regularized Logistic Regression(997/999): loss=0.5562836367470616, w0=1.7771700178112764, w1=-0.8147349858135212\n",
      "Regularized Logistic Regression(998/999): loss=0.5562826667437577, w0=1.7772105047890463, w1=-0.8149063644712263\n",
      "Regularized Logistic Regression(999/999): loss=0.5562817004220684, w0=1.7772508244907597, w1=-0.8150773014229032\n"
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "\n",
    "x_train_mca, y_train_mca, x_test_mca, y_test_mca = split_data(mca.to_numpy(), y_train_mapped)\n",
    "\n",
    "# fix class imbalance\n",
    "x_train_mca_fixed, y_train_mca_fixed = fix_class_imbalance(x_train_mca, y_train_mca, target_value=1)\n",
    "\n",
    "# Initialize the weights\n",
    "initial_w = np.zeros(x_train_mca_fixed.shape[1])\n",
    "max_iters = 1000\n",
    "gamma = 0.1\n",
    "lambda_ = 0\n",
    "\n",
    "# Run the logistic regression\n",
    "w, loss = reg_logistic_regression(y_train_mca_fixed, x_train_mca_fixed, lambda_, initial_w, max_iters, gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.6324226309293431, precision: 0.1764274803768354, recall: 0.8614044919601672, F1: 0.29287096206835905\n",
      "Test accuracy: 0.6323464427750773, precision: 0.17540508030849783, recall: 0.8580823814468674, F1: 0.29127012101985666\n"
     ]
    }
   ],
   "source": [
    "# Predict the values\n",
    "y_pred_train = predict_logistic_regression(y_train_mca, x_train_mca, w)\n",
    "y_pred_test = predict_logistic_regression(y_test_mca, x_test_mca, w)\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy_train, precision_train, recall_train, f1_train = accuracy_precision_recall_f1(y_train_mca, y_pred_train)\n",
    "accuracy_test, precision_test, recall_test, f1_test = accuracy_precision_recall_f1(y_test_mca, y_pred_test)\n",
    "\n",
    "print(f\"Train accuracy: {accuracy_train}, precision: {precision_train}, recall: {recall_train}, F1: {f1_train}\")\n",
    "print(f\"Test accuracy: {accuracy_test}, precision: {precision_test}, recall: {recall_test}, F1: {f1_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 score: 0.7201138965931756\n",
      "Selected features: [52]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 score: 0.7478287889380875\n",
      "Selected features: [52, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 score: 0.7578821853320119\n",
      "Selected features: [52, 0, 37]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 score: 0.7601049630587505\n",
      "Selected features: [52, 0, 37, 43]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 score: 0.7624172639329495\n",
      "Selected features: [52, 0, 37, 43, 31]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 score: 0.7661013243378311\n",
      "Selected features: [52, 0, 37, 43, 31, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 score: 0.7705533994093362\n",
      "Selected features: [52, 0, 37, 43, 31, 8, 12]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 score: 0.7713379716558499\n",
      "Selected features: [52, 0, 37, 43, 31, 8, 12, 38]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 score: 0.7737819760991537\n",
      "Selected features: [52, 0, 37, 43, 31, 8, 12, 38, 17]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 score: 0.7777372146915971\n",
      "Selected features: [52, 0, 37, 43, 31, 8, 12, 38, 17, 28]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 score: 0.7781954887218046\n",
      "Selected features: [52, 0, 37, 43, 31, 8, 12, 38, 17, 28, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 score: 0.7779986008649199\n",
      "Selected features: [52, 0, 37, 43, 31, 8, 12, 38, 17, 28, 4, 50]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 score: 0.777783962151632\n",
      "Selected features: [52, 0, 37, 43, 31, 8, 12, 38, 17, 28, 4, 50, 51]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 score: 0.7773487568565947\n",
      "Selected features: [52, 0, 37, 43, 31, 8, 12, 38, 17, 28, 4, 50, 51, 32]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 80\u001b[0m\n\u001b[1;32m     77\u001b[0m x_train_encoded_train, y_train_train, x_train_encoded_test, y_train_test \u001b[38;5;241m=\u001b[39m split_data(x_train_encoded_fixed, y_train_fixed)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Perform greedy feature selection with progress bar\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m selected_features_greedy, best_f1 \u001b[38;5;241m=\u001b[39m greedy_feature_selection(x_train_encoded_train, y_train_train, x_train_encoded_test, y_train_test)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelected features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mselected_features_greedy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest F1 score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_f1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[60], line 50\u001b[0m, in \u001b[0;36mgreedy_feature_selection\u001b[0;34m(x_train, y_train, x_test, y_test)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m tqdm(remaining_features, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating Features\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     49\u001b[0m     current_features \u001b[38;5;241m=\u001b[39m selected_features \u001b[38;5;241m+\u001b[39m [feature]  \u001b[38;5;66;03m# Add feature to the selected set\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     f1 \u001b[38;5;241m=\u001b[39m calculate_f1(x_train, y_train, x_test, y_test, current_features)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# Check if the current F1 score is the best so far\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m f1 \u001b[38;5;241m>\u001b[39m best_f1_iteration:\n",
      "Cell \u001b[0;32mIn[60], line 19\u001b[0m, in \u001b[0;36mcalculate_f1\u001b[0;34m(x_train, y_train, x_test, y_test, selected_features)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Select only the current features for training\u001b[39;00m\n\u001b[1;32m     18\u001b[0m x_train_selected \u001b[38;5;241m=\u001b[39m x_train[:, selected_features]\n\u001b[0;32m---> 19\u001b[0m x_test_selected \u001b[38;5;241m=\u001b[39m x_test[:, selected_features]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(x_train_selected, y_train)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm  # Import tqdm for the progress bar\n",
    "\n",
    "le = LabelEncoder()\n",
    "x_train_encoded = np.apply_along_axis(le.fit_transform, 0, x_train_filtered_2)\n",
    "\n",
    "# fix class imbalance\n",
    "x_train_encoded_fixed, y_train_fixed = fix_class_imbalance(x_train_encoded, y_train_mapped, target_value=1)\n",
    "\n",
    "# Function to calculate F1 score for given features\n",
    "def calculate_f1(x_train, y_train, x_test, y_test, selected_features):\n",
    "    model = CategoricalNB()\n",
    "    \n",
    "    # Select only the current features for training\n",
    "    x_train_selected = x_train[:, selected_features]\n",
    "    x_test_selected = x_test[:, selected_features]\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(x_train_selected, y_train)\n",
    "    \n",
    "    # Predict the values\n",
    "    y_pred_train = model.predict(x_train_selected)\n",
    "    y_pred_test = model.predict(x_test_selected)\n",
    "    \n",
    "    # Calculate F1 score on the test set\n",
    "    f1 = f1_score(y_test, y_pred_test)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "# Greedy algorithm to select features with tqdm progress bar\n",
    "def greedy_feature_selection(x_train, y_train, x_test, y_test):\n",
    "    n_features = x_train.shape[1]  # Total number of features\n",
    "    selected_features = []  # Initialize with no features\n",
    "    remaining_features = list(range(n_features))  # All features are initially candidates\n",
    "    \n",
    "    best_f1 = 0\n",
    "    progress_bar = tqdm(total=n_features, desc=\"Selecting Features\")  # Initialize tqdm progress bar\n",
    "    \n",
    "    while remaining_features:\n",
    "        # Track the best feature and F1 score in the current iteration\n",
    "        best_feature = None\n",
    "        best_f1_iteration = 0\n",
    "        \n",
    "        # Try adding each remaining feature and evaluate F1 score\n",
    "        for feature in tqdm(remaining_features, desc=\"Evaluating Features\", leave=False):\n",
    "            current_features = selected_features + [feature]  # Add feature to the selected set\n",
    "            f1 = calculate_f1(x_train, y_train, x_test, y_test, current_features)\n",
    "            \n",
    "            # Check if the current F1 score is the best so far\n",
    "            if f1 > best_f1_iteration:\n",
    "                best_f1_iteration = f1\n",
    "                best_feature = feature\n",
    "        \n",
    "        # Stop if no improvement is made\n",
    "        if best_f1_iteration <= best_f1:\n",
    "            break\n",
    "        \n",
    "        # Update selected features and remaining features\n",
    "        selected_features.append(best_feature)\n",
    "        remaining_features.remove(best_feature)\n",
    "        best_f1 = best_f1_iteration\n",
    "        print(f\"Best F1 score: {best_f1}\")\n",
    "        print(f\"Selected features: {selected_features}\")\n",
    "        \n",
    "        progress_bar.update(1)  # Update the progress bar\n",
    "        \n",
    "    progress_bar.close()  # Close the progress bar when done\n",
    "    return selected_features, best_f1\n",
    "\n",
    "# Example usage\n",
    "# Assuming x_train_encoded and y_train_mapped have been preprocessed as in the original code\n",
    "\n",
    "# Split the data (already done in your previous code)\n",
    "x_train_encoded_train, y_train_train, x_train_encoded_test, y_train_test = split_data(x_train_encoded_fixed, y_train_fixed)\n",
    "\n",
    "# Perform greedy feature selection with progress bar\n",
    "selected_features_greedy, best_f1 = greedy_feature_selection(x_train_encoded_train, y_train_train, x_train_encoded_test, y_train_test)\n",
    "\n",
    "print(f\"Selected features: {selected_features_greedy}\")\n",
    "print(f\"Best F1 score: {best_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MAXVO2_', 'GENHLTH', '_RFHYPE5', '_AGE_G', 'SMOKE100', 'CVDSTRK3', 'CHCCOPD1', '_CHOLCHK', 'SEX', 'DIFFWALK', 'MEDCOST']\n",
      "Train accuracy: 0.7646013003075277, precision: 0.7383600688882371, recall: 0.8200273122491679, F1: 0.7770538058651618\n",
      "Test accuracy: 0.7662956277577216, precision: 0.7390963737439173, recall: 0.8216624516388363, F1: 0.7781954887218047\n"
     ]
    }
   ],
   "source": [
    "selected_features_greedy = [52, 0, 37, 43, 31, 8, 12, 38, 17, 28, 4]\n",
    "feature_names = [selected_features_2[i] for i in selected_features_greedy]\n",
    "print(feature_names)\n",
    "\n",
    "# Select only the current features for training\n",
    "x_train_selected = x_train_encoded_train[:, selected_features_greedy]\n",
    "x_test_selected = x_train_encoded_test[:, selected_features_greedy]\n",
    "\n",
    "# Train the model\n",
    "model = CategoricalNB()\n",
    "model.fit(x_train_selected, y_train_train)\n",
    "\n",
    "# Predict the values\n",
    "y_pred_train = model.predict(x_train_selected)\n",
    "y_pred_test = model.predict(x_test_selected)\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy_train, precision_train, recall_train, f1_train = accuracy_precision_recall_f1(y_train_train, y_pred_train)\n",
    "accuracy_test, precision_test, recall_test, f1_test = accuracy_precision_recall_f1(y_train_test, y_pred_test)\n",
    "\n",
    "print(f\"Train accuracy: {accuracy_train}, precision: {precision_train}, recall: {recall_train}, F1: {f1_train}\")\n",
    "print(f\"Test accuracy: {accuracy_test}, precision: {precision_test}, recall: {recall_test}, F1: {f1_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model on the whole train set and get the predictions for the real test set\n",
    "x_test = np.genfromtxt(\"data/x_test.csv\",delimiter=\",\", skip_header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Selecting Features:   3%|         | 2/62 [2:20:36<70:18:13, 4218.23s/it]\n",
      "Selecting Features:  23%|       | 14/62 [2:19:03<7:56:45, 595.95s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: [10.0, 20.0, 30.0]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m         x_test_filtered[:, selected_features\u001b[38;5;241m.\u001b[39mindex(feature)] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([mapping_dict[feature](value) \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m feature_values])\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# apply labelEncoder\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m x_test_encoded \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mapply_along_axis(le\u001b[38;5;241m.\u001b[39mtransform, \u001b[38;5;241m0\u001b[39m, x_test_filtered)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# keep only greedy slected features\u001b[39;00m\n\u001b[1;32m     11\u001b[0m x_test_encoded_selected \u001b[38;5;241m=\u001b[39m x_test_encoded[:, selected_features_greedy]\n",
      "File \u001b[0;32m~/Anaconda3/envs/ada/lib/python3.11/site-packages/numpy/lib/shape_base.py:402\u001b[0m, in \u001b[0;36mapply_along_axis\u001b[0;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m buff[ind0] \u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m inds:\n\u001b[0;32m--> 402\u001b[0m     buff[ind] \u001b[38;5;241m=\u001b[39m asanyarray(func1d(inarr_view[ind], \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, matrix):\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;66;03m# wrap the array, to preserve subclasses\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     buff \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39m__array_wrap__(buff)\n",
      "File \u001b[0;32m~/Anaconda3/envs/ada/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:137\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _encode(y, uniques\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_)\n",
      "File \u001b[0;32m~/Anaconda3/envs/ada/lib/python3.11/site-packages/sklearn/utils/_encode.py:232\u001b[0m, in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    230\u001b[0m     diff \u001b[38;5;241m=\u001b[39m _check_unknown(values, uniques)\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m diff:\n\u001b[0;32m--> 232\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(diff)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msearchsorted(uniques, values)\n",
      "\u001b[0;31mValueError\u001b[0m: y contains previously unseen labels: [10.0, 20.0, 30.0]"
     ]
    }
   ],
   "source": [
    "# apply the same mapping to the test set\n",
    "x_test_filtered = np.zeros((x_test.shape[0], len(selected_features)))\n",
    "for feature in selected_features:\n",
    "    feature_values = x_test[:, features == feature].flatten()\n",
    "    if feature_values.size > 0:\n",
    "        x_test_filtered[:, selected_features.index(feature)] = np.array([mapping_dict[feature](value) for value in feature_values])\n",
    "\n",
    "# apply labelEncoder\n",
    "x_test_encoded = np.apply_along_axis(le.transform, 0, x_test_filtered)\n",
    "# keep only greedy slected features\n",
    "x_test_encoded_selected = x_test_encoded[:, selected_features_greedy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.7649017248295227, precision: 0.7384847335825072, recall: 0.8202868030485359, F1: 0.7772393413338653\n"
     ]
    }
   ],
   "source": [
    "x_train_selected = x_train_encoded_fixed[:, selected_features_greedy]\n",
    "\n",
    "model = CategoricalNB()\n",
    "model.fit(x_train_selected, y_train_fixed)\n",
    "\n",
    "# Predict the values\n",
    "y_pred_test = model.predict(x_test_encoded_selected)\n",
    "y_pred_train = model.predict(x_train_selected)\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy_train, precision_train, recall_train, f1_train = accuracy_precision_recall_f1(y_train_fixed, y_pred_train)\n",
    "print(f\"Train accuracy: {accuracy_train}, precision: {precision_train}, recall: {recall_train}, F1: {f1_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(y_pred_train == 1)/len(y_pred_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3360425675860997\n"
     ]
    }
   ],
   "source": [
    "# save to csv with header Id and Prediction\n",
    "Ids = x_test[:,0]\n",
    "y_pred_test_final = 2*y_pred_test-1\n",
    "\n",
    "# get proportion of 1\n",
    "print(np.sum(y_pred_test_final == 1)/len(y_pred_test_final))\n",
    "\n",
    "np.savetxt(\"data/submission_CategoricalNB_1.csv\", np.array([Ids, y_pred_test_final]).T, delimiter=\",\", fmt=\"%d\", header=\"Id,Prediction\", comments=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.genfromtxt(\"data/x_train.csv\", delimiter=\",\", skip_header=1)\n",
    "features = np.genfromtxt(\"data/x_train.csv\", delimiter=\",\", dtype=str, max_rows=1)\n",
    "y_train = np.genfromtxt(\"data/y_train.csv\", delimiter=\",\", skip_header=1)\n",
    "y_features = np.genfromtxt(\"data/y_train.csv\", delimiter=\",\", dtype=str, max_rows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 79 features over 79\n",
      "{'GENHLTH': 0.0, 'PHYSHLTH': 0.0, 'MENTHLTH': 0.0, 'POORHLTH': 0.0, 'HLTHPLN1': 0.0, 'MEDCOST': 0.0, 'CHECKUP1': 0.0, 'BPHIGH4': 0.0, 'BPMEDS': 0.0, 'BLOODCHO': 0.0, 'CHOLCHK': 0.0, 'TOLDHI2': 0.0, 'CVDSTRK3': 0.0, 'ASTHMA3': 0.0, 'ASTHNOW': 0.0, 'CHCSCNCR': 0.0, 'CHCOCNCR': 0.0, 'CHCCOPD1': 0.0, 'HAVARTH3': 0.0, 'ADDEPEV2': 0.0, 'CHCKIDNY': 0.0, 'DIABETE3': 0.0, 'SEX': 0.0, 'MARITAL': 0.0, 'EDUCA': 0.0, 'VETERAN3': 0.0, 'INCOME2': 0.0, 'INTERNET': 0.0, 'WTKG3': 0.0, 'QLACTLM2': 0.0, 'USEEQUIP': 0.0, 'BLIND': 0.0, 'DECIDE': 0.0, 'DIFFWALK': 0.0, 'DIFFDRES': 0.0, 'DIFFALON': 0.0, 'SMOKE100': 0.0, 'SMOKDAY2': 0.0, 'LASTSMK2': 0.0, 'USENOW3': 0.0, 'AVEDRNK2': 0.0, 'DRNK3GE5': 0.0, 'EXERANY2': 0.0, 'LMTJOIN3': 0.0, 'FLUSHOT6': 0.0, 'PDIABTST': 0.0, 'PREDIAB1': 0.0, 'INSULIN': 0.0, 'CIMEMLOS': 0.0, '_RFHLTH': 0.0, '_HCVU651': 0.0, '_RFHYPE5': 0.0, '_CHOLCHK': 0.0, '_RFCHOL': 0.0, '_LTASTH1': 0.0, '_CASTHM1': 0.0, '_DRDXAR1': 0.0, '_AGEG5YR': 0.0, '_AGE_G': 0.0, 'HTM4': 0.0, '_RFBMI5': 0.0, '_EDUCAG': 0.0, '_SMOKER3': 0.0, '_RFBING5': 0.0, '_BMI5CAT': 0.0, '_RFDRHV5': 0.0, 'FTJUDA1_': 0.0, 'MAXVO2_': 0.0, 'ACTIN11_': 0.0, 'ACTIN21_': 0.0, '_PACAT1': 0.0, '_PA150R2': 0.0, '_PA300R2': 0.0, '_PASTRNG': 0.0, '_PASTAE1': 0.0, '_LMTACT1': 0.0, '_LMTWRK1': 0.0, '_LMTSCL1': 0.0, '_INCOMG': 0.0}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x_train_encoded, x_train_encoded_fixed, y_train_fixed, x_test_encoded\n\u001b[1;32m     22\u001b[0m y_train_mapped \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39my_train[:,\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m \n\u001b[0;32m---> 23\u001b[0m x_train_encoded, x_train_encoded_fixed, y_train_fixed, x_test_encoded \u001b[38;5;241m=\u001b[39m cleaning_x_pipeline(x_train, y_train_mapped, x_test, final_features)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# print shape\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(x_train_encoded_fixed\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[104], line 7\u001b[0m, in \u001b[0;36mcleaning_x_pipeline\u001b[0;34m(x_train, y_train, x_test, features)\u001b[0m\n\u001b[1;32m      5\u001b[0m selected_features \u001b[38;5;241m=\u001b[39m select_features_with_low_nan_ratio(x_train, features, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# cleaning\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m x_train_filtered_mapped \u001b[38;5;241m=\u001b[39m apply_mapping(x_train, selected_features, mapping_dict)\n\u001b[1;32m      8\u001b[0m x_test_filtered_mapped \u001b[38;5;241m=\u001b[39m apply_mapping(x_test, selected_features, mapping_dict)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Label encoding\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[78], line 8\u001b[0m, in \u001b[0;36mapply_mapping\u001b[0;34m(x_train, selected_features, mapping_dict)\u001b[0m\n\u001b[1;32m      6\u001b[0m x_train_filtered \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((x_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlen\u001b[39m(selected_features)))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m selected_features:\n\u001b[0;32m----> 8\u001b[0m     feature_values \u001b[38;5;241m=\u001b[39m x_train[:, features \u001b[38;5;241m==\u001b[39m feature]\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m feature_values\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     10\u001b[0m         x_train_filtered[:, selected_features\u001b[38;5;241m.\u001b[39mindex(feature)] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([mapping_dict[feature](value) \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m feature_values])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "final_features = ['MAXVO2_', 'GENHLTH', '_RFHYPE5', '_AGE_G', 'SMOKE100', 'CVDSTRK3', 'CHCCOPD1', '_CHOLCHK', 'SEX', 'DIFFWALK', 'MEDCOST']\n",
    "final_features = list(mapping_dict.keys())\n",
    "def cleaning_x_pipeline(x_train, y_train, x_test, features):\n",
    "    # keep only features with less than 10% nan values\n",
    "    selected_features = select_features_with_low_nan_ratio(x_train, features, threshold=0.1)\n",
    "    # cleaning\n",
    "    x_train_filtered_mapped = apply_mapping(x_train, selected_features, mapping_dict)\n",
    "    x_test_filtered_mapped = apply_mapping(x_test, selected_features, mapping_dict)\n",
    "\n",
    "    # Label encoding\n",
    "    le = LabelEncoder()\n",
    "    combined = np.vstack((x_train_filtered_mapped, x_test_filtered_mapped))\n",
    "    combined_encoded = np.apply_along_axis(le.fit_transform, 0, combined)\n",
    "    x_train_encoded = combined_encoded[:x_train_filtered_mapped.shape[0], :]\n",
    "    x_test_encoded = combined_encoded[x_train_filtered_mapped.shape[0]:, :]\n",
    "\n",
    "    # fix class imbalance in the training set\n",
    "    x_train_encoded_fixed, y_train_fixed = fix_class_imbalance(x_train_encoded, y_train, target_value=1)\n",
    "\n",
    "    return x_train_encoded, x_train_encoded_fixed, y_train_fixed, x_test_encoded\n",
    "\n",
    "y_train_mapped = (1+y_train[:,1])/2 \n",
    "x_train_encoded, x_train_encoded_fixed, y_train_fixed, x_test_encoded = cleaning_x_pipeline(x_train, y_train_mapped, x_test, final_features)\n",
    "# print shape\n",
    "print(x_train_encoded_fixed.shape)\n",
    "print(x_test_encoded.shape)\n",
    "\n",
    "model = CategoricalNB()\n",
    "model.fit(x_train_encoded_fixed, y_train_fixed)\n",
    "\n",
    "# Predict the values\n",
    "y_pred_train = model.predict(x_train_encoded)\n",
    "y_pred_test = model.predict(x_test_encoded)\n",
    "\n",
    "# Calculate the metrics for train\n",
    "accuracy_train, precision_train, recall_train, f1_train = accuracy_precision_recall_f1(y_train_mapped, y_pred_train)\n",
    "print(f\"Train accuracy: {accuracy_train}, precision: {precision_train}, recall: {recall_train}, F1: {f1_train}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ids = x_test[:,0]\n",
    "y_pred_test_final = 2*y_pred_test-1\n",
    "\n",
    "np.savetxt(\"data/submission_CategoricalNB_2.csv\", np.array([Ids, y_pred_test_final]).T, delimiter=\",\", fmt=\"%d\", header=\"Id,Prediction\", comments=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
